{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"jobtitle turnstileLink\" data-tn-element=\"jobTitle\" href=\"/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CCtOQFBkQNFLW86JI2n3g743iSrihE9F06fLrjoo2w2CXh5WL40wQUQ5jRY6zHcIRg7ItokaiU1Y7ZMeIz2c8ZY7nk5isgPQUxxrA_H5YuPDW36tf6-NmEIkDkAM8OFnPUqrUyUHaf1B3dKBidCac5dwEMty7PuvJkipZOEDEHkfS8ZoCXI7bZ_LeColeNhrfXjzX6CJ6prVEDZElywlOG6cb7396spOjkeJrIVBJdOgXXNZkqp6uThGNKMuqQdZHHM_cBshz-SV9ayfsVA0BCZ3idN9zrmdR2OP63re6lMwURetwpfuwErIrdmHCTUX-M5yTTQE7fgNmYdhDTrof5fl-jAGKxkEgJ6XtFJKeDCR4qSOOkjBGCm0HK5ttS8ijVHUCnMU7eiSJ_eelsvo3ImXGNAtvtQoBEaRjGyNH-H-iS-6d-iLh2pMcPHNhRyCbhF2pR_slHOAoF2Pbn3I35Qj2mn44ib0o=&amp;vjs=3&amp;p=1&amp;sk=&amp;fvj=0\" id=\"sja1\" onclick=\"setRefineByCookie([]); sjoc('sja1',0); convCtr('SJ')\" onmousedown=\"sjomd('sja1'); clk('sja1');\" rel=\"noopener nofollow\" target=\"_blank\" title=\"Data Scientist II\"><b>Data</b> <b>Scientist</b> II</a>\n",
      "/pagead/clk?mo=r&ad=-6NYlbfkN0CCtOQFBkQNFLW86JI2n3g743iSrihE9F06fLrjoo2w2CXh5WL40wQUQ5jRY6zHcIRg7ItokaiU1Y7ZMeIz2c8ZY7nk5isgPQUxxrA_H5YuPDW36tf6-NmEIkDkAM8OFnPUqrUyUHaf1B3dKBidCac5dwEMty7PuvJkipZOEDEHkfS8ZoCXI7bZ_LeColeNhrfXjzX6CJ6prVEDZElywlOG6cb7396spOjkeJrIVBJdOgXXNZkqp6uThGNKMuqQdZHHM_cBshz-SV9ayfsVA0BCZ3idN9zrmdR2OP63re6lMwURetwpfuwErIrdmHCTUX-M5yTTQE7fgNmYdhDTrof5fl-jAGKxkEgJ6XtFJKeDCR4qSOOkjBGCm0HK5ttS8ijVHUCnMU7eiSJ_eelsvo3ImXGNAtvtQoBEaRjGyNH-H-iS-6d-iLh2pMcPHNhRyCbhF2pR_slHOAoF2Pbn3I35Qj2mn44ib0o=&vjs=3&p=1&sk=&fvj=0\n"
     ]
    }
   ],
   "source": [
    "job = \"Data Scientist\"\n",
    "url = \"https://www.indeed.com/q-Data-Scientist-jobs.html\"\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url), 'html.parser')\n",
    "\n",
    "panel = soup.find(id=\"sja1\")\n",
    "#print(panel)\n",
    "\n",
    "print(panel.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using job link, extract job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_link = \"https://www.indeed.com/\" + panel.get('href')\n",
    "soup_job = BeautifulSoup(urllib.request.urlopen(job_link), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = \"jobsearch-JobComponent-description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_panel = soup_job.find(class_=selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist II – Machine Learning and Advanced Decision Processing\n",
      "\n",
      "Valassis Digital is a leader in the business intelligence space, processing over 12 trillion calculations, 35 billion mobile location signals, and analyzing more than 120 billion digital ad opportunities every day. As a Data Scientist II, your work is central to keeping us at the forefront of our industry by uncovering intelligence in our vast, fast moving data environment.\n",
      "\n",
      "We are seeking data experts who are passionate about using cutting edge technology to solve unique problems. The ideal candidate is innately curious about how data can be used to tell a story and inform decisions. You enjoy searching for datasets to explore. You have honed your skills through a combination of education, work experience, and hobbies. You can embrace the messiness of creating new things.\n",
      "\n",
      "If this describes you, we are interested. You can be an integral part of a cross-disciplinary team working on highly visible projects that improve performance and grow our product suite.\n",
      "\n",
      "On any given day you’ll be…\n",
      "\n",
      "Using your deep knowledge of numerical and statistical packages (Python, Pandas, Numpy, Sklearn, R) to…\n",
      "Implement a gradient-boosting classifier to predict whether a person is likely to visit a car dealership based on the advertising signals they’ve received.\n",
      "Use a Bayesian dynamic time series model to estimate the causal impact of an advertising campaign on sales at your neighborhood grocery.\n",
      "Model the complex interactions between system architecture components to refactor and rethink key components and models in an advertising system.\n",
      "Develop algorithms to optimize the setting of every lever in our advertising infrastructure.\n",
      "Analyze data to better understand how a neighborhood’s consumption of web pages correlates with visits to a local big box store.\n",
      "Build a time series model to forecast future sales of diapers for one of our clients.\n",
      "Model the effects of environmental changes on promotion effectiveness with multiple regression.\n",
      "Writing complex database queries using distributed computing frameworks: MapReduce, Hadoop and Spark to establish links between large datasets in order to…\n",
      "Find the handful of outliers in 25 billion transactions.\n",
      "Evaluate competing bidding models for real-time ad bidding auctions to inform our bid pricing strategy.\n",
      "Feed data into your latest ensemble model aimed at maximizing the return on a client's online advertising budget.\n",
      "Leveraging your experience with real world data to…\n",
      "Derive a set of new features that will help us better understand the interplay between geography and audience features to improve our model performance.\n",
      "Discover and explore third party data sources to determine their value for improving our model performance.\n",
      "Build new data-driven products and bring them to market.\n",
      "Provide technical leadership to …\n",
      "Participate in planning, roadmap, and architecture discussions to help evolve our data science into revenue-generating products.\n",
      "Engage in code and model reviews to continually raise the bar on our work.\n",
      "Draw data flows and architecture designs on the white board to encourage understanding and cohesive development towards your solution.\n",
      "Meet with customers and help map business needs into product requirements.\n",
      "Our ideal candidate will likely have…\n",
      "3+ years of experience with responsibilities similar to those listed above, preferably at a software development company\n",
      "Advanced degree(s) in a relevant, quantitative field (e.g. mathematics, statistics, econometrics, computer science, engineering, etc.)\n",
      "Ability to mentor and guide team members of varying experience levels through data science, product and software development life cycles\n",
      "WHO WE ARE\n",
      "Valassis Digital is a leading digital marketing intelligence company, providing a best-in-class data management platform and online and offline targeting capabilities that fuels superior display advertising and expanded media offerings, such as in-store campaign solutions, for advertising clients. Through proprietary technology, Valassis Digital creates meaningful marketing and advertising impact for businesses – from planning, delivery, and attribution. Valassis Digital taps into meaningful, actionable insights that drive better campaign performance.\n",
      "\n",
      "WHAT'S IN IT FOR YOU?\n",
      "Valassis Digital offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401k matching, long-term cash incentives program, and PTO allowance. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available!\n",
      "\n",
      "Valassis Digital considers applicants for all positions without regard to race, color, creed, religion, national origin or ancestry, sex, age, disability, genetic information, veteran status, or any other legally protected status. In addition, Valassis Digital will provide reasonable accommodations for qualified individuals with disabilities. If you need assistance with completing the online application process, please contact DL_Recruiting@MaxPoint.com.\n",
      "ENG123\n",
      "#LI-CB1\n",
      "MXGD\n"
     ]
    }
   ],
   "source": [
    "print(job_panel.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale to all links on one search result page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = \"Data Scientist\"\n",
    "url = \"https://www.indeed.com/q-Data-Scientist-jobs.html\"\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url), 'html.parser')\n",
    "\n",
    "job_descriptions = list()\n",
    "\n",
    "for i in range(1, 10):\n",
    "    panel = soup.find(id= \"sja\" + str(i))\n",
    "    if(panel is None):\n",
    "        continue\n",
    "    job_link = \"https://www.indeed.com/\" + panel.get('href')\n",
    "    soup_job = BeautifulSoup(urllib.request.urlopen(job_link), 'html.parser')\n",
    "    selector = \"jobsearch-JobComponent-description\"\n",
    "    job_panel = soup_job.find(class_=selector)\n",
    "    job_descriptions.append(job_panel.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVS is a Fortune 7 company embarking on a journey of evolving its existing ExtraCare program into a world-class personalization and loyalty program. This is a top initiative within the company and we have a team dedicated to recruiting the best talent in the world to help propel us to this goal. The company has already invested in\n",
      "state-of-the-art technology and scaling of our loyalty program, now we are focused on optimizing our customer contact strategy. We are looking for the best and brightest to in our existing Analytics team and help deliver on this initiative.\n",
      "\n",
      "\n",
      "As the Data Scientist you will be responsible for developing data-driven business solutions using advanced analytical tools and techniques for enhancing customer loyalty and growth. As a member of Data Science and Innovation team, you will create analytical engines to drive customer growth through targeting, segmentation, offer optimization, customer profitability (LTV) and multi-channel campaign lever optimization (e.g., propensity modeling) and continuously enhancing analytical tools and processes. You will get to play an integral role in a team reshaping customer centric thinking and execution across the enterprise. The Data Scientist will report to the Sr. Manager of Data Science, with consistent exposure and engagement with the Directors and Sr. Directors.\n",
      "\n",
      "Responsibilities Include:\n",
      "Identify and develop innovative solutions leveraging advanced analytics to support personalization programs and customer strategies, this may include targeting, retention, segmentation, offer optimization, multi-channel contact cadence, CRM decision engine, and messaging, etc.Develop and manage portfolio of advanced analytics assets (attributes, models & solutions), support business use cases to ensure most effective use of these assetsUtilize modeling software, data management and system requirements, scoring process, and predictive model output integration into business systemsEvaluates/develops, tools, methodologies or infrastructure building capability and fostering innovationAssess new data sources (internal and external) and new metric development to foster data innovationOwns execution and delivery of analytical projects and insights to internal business partners and presenting to the leadershipPartner with cross functional leads on customer growth and execution teams to launch these strategies and close loop performance management.Helps and directs others on projects, including project management, business problem solving, guidance on methodologies and quality assurance of resultsManaging across multiple projects to ensure successful delivery in a fast paced environmentEnsuring effective stakeholder relationships\n",
      "This position is based in our Woonsocket, RI office.\n",
      "\n",
      "Required Qualifications\n",
      "3+ years of experience in customer/marketing analytics and insight development, database marketing, marketing research or related fields3+ years experience with wide variety of Advanced/Statistic Analytics techniques, including but not limited to linear/logistic regression, factor analysis, generalized linear models, tree models, correspondence and cluster analysis, survival analysis, attrition/retention modeling. Practical application of these techniques in a retail/loyalty marketing setting preferred3+ SQL experience is a requirement3+ years of programming skills using SAS, SQL, R/Python\n",
      "\n",
      "Preferred Qualifications\n",
      "Strong knowledge & experience of big data systems, tools and technologiesAdvanced problem solving and structuring skills-including data structuring, quantitative reasoning, and implications developmentDemonstrated capability to deliver work and provide positive leadership in a fast-paced, multi-project team-oriented environment. Flexible with ambiguity while operating effectively.Advanced degree in statistics, quantitative economics, behavioral psychology research or a related fieldStrategy & analytics experience in retail, consumer packaged goods, OR related consulting environmentPrior Experience working with Teradata tools and database solutions, experience working with SAS\n",
      "\n",
      "Education\n",
      "BA/BS in a quantitative field (e.g., math, statistics, economics, operations research, computer science, engineering) coupled with strong business acumen required.MS/PHD/MBA preferred\n",
      "\n",
      "Business Overview\n",
      "CVS Health, through our unmatched breadth of service offerings, is transforming the delivery of health care services in the U.S. We are an innovative, fast-growing company guided by values that focus on teamwork, integrity and respect for our colleagues and customers. What are we looking for in our colleagues? We seek fresh ideas, new perspectives, a diversity of experiences, and a dedication to service that will help us better meet the needs of the many people and businesses that rely on us each day. As the nation’s largest pharmacy health care provider, we offer a wide range of exciting and fulfilling career opportunities across our three business units – MinuteClinic, pharmacy benefit management (PBM) and retail pharmacy. Our energetic and service-oriented colleagues work hard every day to make a positive difference in the lives of our customers.\n"
     ]
    }
   ],
   "source": [
    "print(job_descriptions[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale to grab links from multiple search pages (and grab corresponding locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = 5\n",
    "\n",
    "job = \"Data Scientist\"\n",
    "base_url = \"https://www.indeed.com/\"\n",
    "base_search_url = base_url + \"jobs?q=Data+Scientist&start=\"\n",
    "\n",
    "# HTML class id used to grab the job description html element\n",
    "class_id = \"jobsearch-JobComponent-description\"\n",
    "\n",
    "\n",
    "# This list will hold each job description as a single list element\n",
    "job_descriptions = list()\n",
    "job_locations = list()\n",
    "\n",
    "for page_num in range(0, num_pages):\n",
    "    # Get links from each page of the search results, up to a specified number of pages\n",
    "    \n",
    "    # Retrieve the search results one page at a time (starting at 0, 10, 20, ....)\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(base_search_url + str(page_num*10)), 'html.parser')\n",
    "    \n",
    "    # There are two types of links, TurnstileLinks and \"sja#\" links\n",
    "    # turnstileLinks are not numbered, so we just grab all of the links available\n",
    "    turnstileLinks = soup.find_all(class_=\"turnstileLink\")\n",
    "    \n",
    "    # Extract the location information for each posting\n",
    "    locations = soup.find_all(class_='location')\n",
    "    for location in locations:\n",
    "        #print(location.get_text())\n",
    "        job_locations.append(location.get_text())\n",
    "    \n",
    "    for item in turnstileLinks:\n",
    "        # Extract the job description for each turnstileLink on this page of search results \n",
    "        time.sleep(0.05)\n",
    "        if item is not None:\n",
    "            # Grab the html code for the individual job posting\n",
    "            job_link = base_url + item.get('href')\n",
    "            soup_job = BeautifulSoup(urllib.request.urlopen(job_link), 'html.parser')\n",
    "\n",
    "            # Extract the job description text from the individual job posting\n",
    "            job_panel = soup_job.find(class_=class_id)\n",
    "            if job_panel is not None:\n",
    "                job_descriptions.append(job_panel.get_text())\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# Checking for same number of descriptions and locations\n",
    "print(len(job_descriptions))\n",
    "print(len(job_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(list(set(job_descriptions))))\n",
    "print(len(list(set(job_locations))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 46\n",
      "2 57\n",
      "6 10\n",
      "10 6\n",
      "16 68\n",
      "43 56\n",
      "46 2\n",
      "46 57\n",
      "56 43\n",
      "57 2\n",
      "57 46\n",
      "68 16\n"
     ]
    }
   ],
   "source": [
    "# Which postings are duplicates??\n",
    "for i in range(len(job_descriptions)):\n",
    "    for j in range(len(job_descriptions)):\n",
    "        if i != j and job_descriptions[i] == job_descriptions[j]:\n",
    "            print(i, j)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicate postings and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 12 0\n",
      "48 51 1\n",
      "76 0 2\n",
      "76 2 3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-cf4398ef39d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mposting_1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mposting_2\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mposting_1\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mjob_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mjob_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mjob_locations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mjob_locations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposting_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "num_jobs = len(job_descriptions)\n",
    "num_del = 0\n",
    "\n",
    "for posting_1 in range(num_jobs):\n",
    "    for posting_2 in  range(num_jobs):\n",
    "        if posting_1 != (posting_2 - num_del) and job_descriptions[posting_1] == job_descriptions[posting_2 - num_del] and num_del + posting_2 >= num_jobs :\n",
    "            assert job_locations[posting_1] == job_locations[posting_2 - num_del]\n",
    "            print(posting_1, posting_2, num_del)\n",
    "            del job_descriptions[posting_2 - num_del]\n",
    "            del job_locations[posting_2 - num_del]\n",
    "            num_del += 1\n",
    "            \n",
    "            if num_del + posting_2 >= num_jobs:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 12 0\n",
      "48 51 1\n",
      "76 0 2\n",
      "76 2 3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-cf4398ef39d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mposting_1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mposting_2\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mposting_1\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mjob_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mjob_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mjob_locations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mjob_locations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposting_2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposting_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposting_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_del\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "    # Version 2.0\n",
    "num_jobs = len(job_descriptions)\n",
    "num_del = 0\n",
    "dupe_index = list()\n",
    "keep_index = list()\n",
    "\n",
    "for posting_1 in range(num_jobs):\n",
    "    for posting_2 in  range(num_jobs):\n",
    "        if posting_1 != posting_2 and job_descriptions[posting_1] == job_descriptions[posting_2]:\n",
    "            assert job_locations[posting_1] == job_locations[posting_2 - num_del]\n",
    "            print(posting_1, posting_2, num_del)\n",
    "            \n",
    "            if posting_1 not in dupe_index:\n",
    "                keep_index.append(posting_1)\n",
    "            if posting_2 not in keep_index:\n",
    "                dupe_index.append(posting_2)\n",
    "                \n",
    "            # Keep track of how many job postings we need to remove\n",
    "            num_del += 1\n",
    "            \n",
    "            if num_del + posting_2 >= num_jobs:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3.0 -- Make a dataframe and remove duplicated rows!\n",
    "job_data = pd.DataFrame({\"text\": job_descriptions,\"location\":job_locations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morrisville, NC 27560</td>\n",
       "      <td>Data Scientist II – Machine Learning and Advan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Des Moines, IA</td>\n",
       "      <td>At EMC, you'll put your skills to good use as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>As a Data Scientist within our small and growi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Req ID: 30197\\n\\nAt NTT DATA Services, we know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remote</td>\n",
       "      <td>ContractHere at Fooji, the food doesn't just g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                location                                               text\n",
       "0  Morrisville, NC 27560  Data Scientist II – Machine Learning and Advan...\n",
       "1         Des Moines, IA  At EMC, you'll put your skills to good use as ...\n",
       "2          Arlington, VA  As a Data Scientist within our small and growi...\n",
       "3             Dallas, TX  Req ID: 30197\\n\\nAt NTT DATA Services, we know...\n",
       "4                 Remote  ContractHere at Fooji, the food doesn't just g..."
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 3.0 -- Job Descriptions and locations from multiple search pages, duplications removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = 5\n",
    "\n",
    "job = \"Data Scientist\"\n",
    "base_url = \"https://www.indeed.com/\"\n",
    "base_search_url = base_url + \"jobs?q=Data+Scientist&start=\"\n",
    "\n",
    "# HTML class id used to grab the job description html element\n",
    "class_id = \"jobsearch-JobComponent-description\"\n",
    "\n",
    "\n",
    "# This list will hold each job description as a single list element\n",
    "job_descriptions = list()\n",
    "job_locations = list()\n",
    "\n",
    "for page_num in range(0, num_pages):\n",
    "    # Get links from each page of the search results, up to a specified number of pages\n",
    "    \n",
    "    # Retrieve the search results one page at a time (starting at 0, 10, 20, ....)\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(base_search_url + str(page_num*10)), 'html.parser')\n",
    "    \n",
    "    # There are two types of links, TurnstileLinks and \"sja#\" links\n",
    "    # turnstileLinks are not numbered, so we just grab all of the links available\n",
    "    turnstileLinks = soup.find_all(class_=\"turnstileLink\")\n",
    "    \n",
    "    # Extract the location information for each posting\n",
    "    locations = soup.find_all(class_='location')\n",
    "    for location in locations:\n",
    "        #print(location.get_text())\n",
    "        job_locations.append(location.get_text())\n",
    "    \n",
    "    for item in turnstileLinks:\n",
    "        # Extract the job description for each turnstileLink on this page of search results \n",
    "        time.sleep(0.05)\n",
    "        if item is not None:\n",
    "            # Grab the html code for the individual job posting\n",
    "            job_link = base_url + item.get('href')\n",
    "            soup_job = BeautifulSoup(urllib.request.urlopen(job_link), 'html.parser')\n",
    "\n",
    "            # Extract the job description text from the individual job posting\n",
    "            job_panel = soup_job.find(class_=class_id)\n",
    "            if job_panel is not None:\n",
    "                job_descriptions.append(job_panel.get_text())\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# Checking for same number of descriptions and locations\n",
    "print(len(job_descriptions))  # Number of job descriptions found\n",
    "print(len(list(set(job_descriptions)))  # Number of unique job descriptions\n",
    "\n",
    "print(len(job_locations)) # Number of locations found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove duplications by making dataframe\n",
    "job_data = pd.DataFrame({\"text\": job_descriptions,\"location\":job_locations})\n",
    "job_data = job_data[~job_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.to_csv(\"data/test_scraping_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible Keywords to look out for:\n",
    "    R\n",
    "    Watson\n",
    "    Python\n",
    "    SQL\n",
    "    Ruby\n",
    "    SAS\n",
    "    Enterprise Miner\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(random.randrange(5, 50)/100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
