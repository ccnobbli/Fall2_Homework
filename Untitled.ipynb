{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraper Version 3.0 -- Job Descriptions and locations from multiple search pages, duplications removed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "import random  # for generating random delay times, to confuse Indeed\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for 5 pages of 'Data Engineer' job postings\n",
      "Progress: |███████████████████████████████████████████-------| 86.0% Complete\r"
     ]
    }
   ],
   "source": [
    "# Number of search result pages to scrape (~18 posts per page)\n",
    "num_pages = 5\n",
    "\n",
    "# Job title to search for\n",
    "job = \"Data Engineer\"\n",
    "\n",
    "# URL's we need\n",
    "base_url = \"https://www.indeed.com/\"\n",
    "base_search_url = base_url + \"jobs?q=\" + job.replace(\" \",\"_\") + \"&start=\"\n",
    "\n",
    "# HTML class id used to grab the html element containing the job description\n",
    "class_id = \"jobsearch-JobComponent-description\"\n",
    "\n",
    "\n",
    "# These lists will store each job description and location as individual list elements\n",
    "job_descriptions = list()\n",
    "job_locations = list()\n",
    "job_titles = list()\n",
    "\n",
    "# Will hold dictionaries, each being one job posting\n",
    "jobs = list()\n",
    "\n",
    "\n",
    "print(\"Looking for {0} pages of '{1}' job postings\".format(num_pages, job))\n",
    "# Start Scraping here\n",
    "\n",
    "num_done = 0\n",
    "\n",
    "\n",
    "for page_num in range(0, num_pages):\n",
    "    # Get links from each page of the search results, up to a specified number of pages\n",
    "    time.sleep(float(random.randrange(5, 50)/100))\n",
    "    # Retrieve the search results one page at a time (starting at 0, 10, 20, ....)\n",
    "    while True:\n",
    "        try:\n",
    "            soup = BeautifulSoup(urllib.request.urlopen(base_search_url + str(page_num*10)), 'html.parser')\n",
    "            break\n",
    "        except (URLError, HTTPError) as e:\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    \n",
    "    search_results = soup.find_all(\"div\", attrs={\"data-tu\":\"\"})\n",
    "    \n",
    "\n",
    "    \n",
    "    for result in search_results:\n",
    "        title = result.find(\"a\", attrs={\"data-tn-element\":\"jobTitle\"})\n",
    "        loc = result.find(\"div\", attrs={\"class\":\"sjcl\"})\n",
    "        if title is not None and loc is not None:\n",
    "            # We got a match for a sponsored job - \n",
    "            \n",
    "            # Grab the Title of the job, the job description, and the location\n",
    "            job_location = loc.find(\"div\", attrs={'class':'location'}).get_text()\n",
    "            \n",
    "            job_title = title.get_text()\n",
    "            \n",
    "            job_link = base_url + str(title.get('href'))[1:]\n",
    "            \n",
    "            # Force Indeed to give us the job description\n",
    "            while True:\n",
    "                try:\n",
    "                    soup_job = BeautifulSoup(urllib.request.urlopen(job_link), 'html.parser')\n",
    "                    break\n",
    "                except (URLError, HTTPError) as e:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                break\n",
    "                \n",
    "            job_desc = soup_job.find(class_=class_id).get_text()\n",
    "\n",
    "            if job_location is not None and job_title is not None and job_desc is not None:\n",
    "                # This job has a location, title, and description. Add it to our data\n",
    "                jobs.append({\"job_location\": job_location,\n",
    "                            \"job_title\": job_title,\n",
    "                             \"job_description\": job_desc,  # Extract job desc text here\n",
    "                            \"sponsored\": True})\n",
    "                \n",
    "                # Update progress bar\n",
    "                printProgressBar(num_done, num_pages*20, prefix='Progress:', suffix='Complete', length=50)\n",
    "                num_done += 1\n",
    "        \n",
    "        else:\n",
    "            # Check if this element is an organic job (not sponsored)\n",
    "            title = result.find(\"h2\", attrs={\"class\":\"jobtitle\"})\n",
    "            loc = result.find(\"span\", attrs={\"class\":\"location\"})\n",
    "            \n",
    "            if title is not None and loc is not None:\n",
    "                # We have a match for an organic job, extract info\n",
    "                \n",
    "                # Grab the Title of the job\n",
    "                title_element = title.find(\"a\", attrs={\"data-tn-element\":\"jobTitle\"})\n",
    "                job_title = title_element.get_text()\n",
    "\n",
    "                \n",
    "                # Extract Location Text\n",
    "                job_location = loc.get_text()\n",
    "            \n",
    "                while True:\n",
    "                    # Force Indeed to give us the job description\n",
    "                    try:\n",
    "                        soup_job = BeautifulSoup(urllib.request.urlopen(base_url + str(title_element.get('href'))), 'html.parser')\n",
    "                        break\n",
    "                    except (URLError, HTTPError) as e:\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "                    break\n",
    "                \n",
    "                # Extract Job Description\n",
    "                job_desc = soup_job.find(class_=class_id).get_text()\n",
    "            \n",
    "                if job_location is not None and job_title is not None and job_desc is not None:\n",
    "                    # This job has a location, title, and description. Add it to our data\n",
    "                \n",
    "                    jobs.append({\"job_location\": job_location,\n",
    "                                \"job_title\": job_title,\n",
    "                                 \"job_description\": job_desc,\n",
    "                                \"sponsored\": False})\n",
    "                    # Update progress bar\n",
    "                    printProgressBar(num_done, num_pages*20, prefix='Progress:', suffix='Complete', length=50)\n",
    "                    num_done += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
