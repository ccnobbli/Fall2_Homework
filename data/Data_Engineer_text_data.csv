,job_description,job_location,job_title,sponsored
0,"ContractSr AWS Data EngineerRemote project3 months plusRate: DOEExperience:Excellent communication skill requiredData lake storage and analytics experience on AWS is mustStrong AWS experience with certification is requiredThanks,AnuraNextPhase systems6508876642Job Type: ContractExperience:AWS: 5 years (Required)Data engineer: 10 years (Preferred)License:AWS (Preferred)","Maui Meadows, HI",AWS Data engineer,True
2,"At ISM Connect, we believe that success is achieved through teamwork. We empower team members to make decisions and create an environment that makes everyone on the team better.Join the analytics team that works in all of our business lines. Hone your skills in an exciting environment, and have a measurable impact on a well-funded and rapidly expanding business. Our venture-backed firm delivers innovative solutions to clientele from professional sports to some of the nation’s largest retailers.We are looking for a Data Engineer to join our analytics team to help design, build, and maintain a cloud-based ETL pipeline feeding a cloud-based data warehouse, which will support our analytics and data science efforts as our volume of data grows. The Data Engineer will likely help out in other areas of the data science stack as well. An engineer who is passionate about his or her work, skilled at collaborating with coworkers, and a positive and supportive presence in the office will be a particularly good fit.Specific Duties And ResponsibilitiesHelp build and maintain a cloud-based, scalable ETL pipeline that ingests data from heterogeneous sourcesAssist with design of a cloud-based data lake and data warehouseEnsure high data quality and availabilityTest the ETL pipelineContribute to other aspects of the data science workflowComplete ad hoc data work as neededStay up-to-date with ETL advancements and best practicesEducation And ExperienceRelevant data engineering/data science work experience in a corporate environment or similarPractical experience with ETL and database technologiesExperience with Amazon Web Services analytics and ETL tools, or similar cloud-based toolsRelational databases and columnar storageData lake and data warehouse designExperience in other data science technologies (R, Python, etc.) and capabilities (data analysis, machine learning, data visualization, etc.) is helpfulExperience in a higher performance language such as C++, Go, etc. is a bonusUndergraduate or graduate degree in a technical field preferableSkillsAbility to work and collaborate in a distributed team environmentWork with multiple internal teams to create deliverables for clients or for internal usageEffectively communicate project status, blockers, and questions to team membersSelf-motivated and requires minimal managementAbility to learn new technologies and be involved in multiple projectsTechnologyAmazon Web Services analytics/ETLGlueAthenaKinesis FirehoseAuroraRedshiftRedshift SpectrumSpark (PySpark)SQLMongoDBPythonRGitBashBenefitsMeaningful work on world-class and industry-changing productsAmazing culture and a flat organizational structure100% covered premiums for medical, dental, life, disability401(k) PlanUltra-casual dress work environmentFlexible scheduleDevice stipendISM Connect is an Equal Opportunity Employer M/F/V/DJob Type: Full-timeExperience:Amazon Web Services Analytics: 1 year (Required)Practical Data Science/Data Engineering: 2 years (Required)ETL and Database Technologies: 1 year (Required)Education:Bachelor's (Preferred)","Doylestown, PA",Data Engineer,True
3,"Data EngineerDoers Wanted. Dreamers Encouraged.Our Data Engineer performs a wide range of job duties utilizing technical know-how and an exceptional, pro-active customer service approach, ensuring high levels of customer satisfaction. Maintains a very hands-on focus for technology matters combined with an affinity for solving complex technical issues and delivering projects on time and within budget.Foresight culture is that of spirited team players in an environment energized by innovation and continuous improvement. We truly believe it takes an entire team united behind something big. So together, we work hard, we have fun, we brainstorm, we love ideas and we give high-fives in the hallway. Foresight Intelligence employees are encouraged to take a high degree of ownership and improve continuously. We work in an environment where your voice matters and where your actions have a direct positive impact on the team and the customer.REPORTS TO: Vice President of OperationsJob Duties and Responsibilities:Identify, evaluate and recommend appropriate technologies and strategies for building products and delivering services, from a database perspective.Assist in design and development of database systems.Secure the database and ensure its integrity.Establish the needs of users and monitor the user-access model; control access permissions and privileges.Develop databases functions, scripts, stored procedures, etc. to collect, process and present data.Monitor performance and manage parameters of the database and provide fast query responses to front-end users.Synchronize the conceptual design with the actual database.Enhance and refine the logical design of the database.Ensure appropriate system storage requirements.Update the database by installing and testing new versions of the DBMS.Develop and manage backups and construct recovery plans.Create physical and logical database models according to business needs or requirements.Provide technical assistance to sort out database related issues, identify errors and then resolve in a proper way.Prepare documentation related to the database.Work with DBAs to manage the company databases effectively.Performs other duties and responsibilities as required or assigned.Minimum Requirements:Bachelor’s degree in computer engineering or related field, or equivalent knowledge, skills and abilities in software engineering.5+ years of technical experience in a database engineering role.Excellent oral/written communication skills and interpersonal skills, with the ability to articulate ideas to both technical and non-technical audiences.Expert with MS-SQL; familiarity with C# programming concepts.Innovative, self-motivated and self-directed with keen attention to detail, exceptional service orientationAbility to work directly with customers, understanding and fulfilling their needs in a competent manner.Ability to discern user requirements and develop specifications.Experience with Microsoft system administration and web server configuration.Knowledge of internet protocols, database management systems, revision control systems, information security vulnerabilities and risk management.Possessing a business understanding of the underlying data.Ability to follow projects through to completion.Ability to learn new technologies quickly, step outside your own comfort zone and handle unfamiliar challenges enthusiastically.Enjoys work (overcoming obstacles is fun), loves to help people and can work well independently or in groups.Job Type: Full-timeExperience:data engineering: 5 years (Preferred)technical: 5 years (Preferred)C# Programming: 1 year (Preferred)MS-SQL: 5 years (Preferred)Education:Bachelor's (Required)Location:Scottsdale, AZ (Required)Work authorization:United States (Required)","Scottsdale, AZ",Data Engineer,True
4,"Product Delivery EngineerClearsense is building an end-to-end data platform for healthcare companies based in the fastest growing markets around the world. Our Product Delivery team works directly with our customers to help solve their biggest challenges. As a Product Delivery Engineer (PDE), you’ll help solve these challenges by building applications and configurations using the core platform and cutting-edge technologies.ResponsibilitiesWork with engineering to write platform configuration code for new featuresLead the development, execution and review of manual and automated test scriptsDevelop tools that provide customers with unprecedented transparency into their operationsBuild customer specific applications to streamline data integrationDetermine the best approaches to productize platform configurationAuthor and maintain platform configuration documentationWrite SQL queries to support product implementationsProficient programming skills to make minor modifications to existing Java applications as neededRequirementsBachelor's degree in a technical field or equivalent 4 years of experience.Programming experienceStrong attention to detail, able to take high level requirements and implement in a self-directed daily basis.Motivated, positive and constructive attitude—seeing challenges as opportunitiesAbility to operate in an agile startup environment with evolving requirementsStrong SQL programming experience, can be across any of the various RDBMS such as Oracle or SQL Server, but this is a must have for this position.Java Programming – competent proficiency is required. GO is a plus.Comfortable using a shell to interact with Linux or WindowsPrevious experience working in an Agile environment is a plus.Strong communication skills.EducationBachelor's degree in a technical field or equivalent 4 years of experience.LocationJacksonville, FLJob Type: Full-timeEducation:Bachelor's (Preferred)Location:Jacksonville, FL (Required)Work authorization:United States (Required)","Jacksonville, FL",Healthcare Data Engineer,True
5,"In this role, the candidate will be responsible for performing data engineering duties such as planning, developing, Testing, maintaining and monitoring systems. The candidate will report to the Senior Database Engineer and work collaboratively with other teams to achieve business, department, and organizational goals. The individual will also be responsible for developing, implementing, and overseeing secure database policies and procedures to ensure the integrity and availability of database systems.

Data Engineer Responsibilities:
As part of the Data Engineering team, provide support to Java engineers for basic database administration needs such as tables, indexes, functions, procedures, etc.
Daily maintenance of database infrastructure, mainly checking daily / nightly scheduler jobs, backup/recovery and replication
Understand business objectives and design services that couple business logic with code components for scalability and reusability
Proficient in designing efficient and robust ETL workflows
Create, or support creation of, required reports in response to business user needs
Monitor and report for critical production data systems
Support multiple data Systems in a production environment, including DSS, OLTP, NOSQL and Big data services
Able to work with Cloud Computing environments
Proactively monitor as well as troubleshoot problems escalated by the business / QA / Analytics / Development team in a timely manner
Respond to and resolve SQL database access and performance issues
Candidate will be on-call and maybe required to work over weekend at times, and will be part of a team in automating daily tasks using automated out-of-box solution or Shell/Perl scripting.

Data Engineer Requirements:
3-4 years’ experience related to ORACLE DB Administration on Unix/Linus in a mid-to-large-scale computing environment
Strong understanding of database structures, concepts, principles, and practices
Experience in AWS or Google Cloud Computing Environment
Strong Working knowledge of any NoSQL systems such as Cassandra, MongoDB or DynamoDB and Elasticsearch
Working knowledge of Big Data system such as Hadoop, MapReduce, Hive or Spark along with Resource management using YARN or Mesos
Proficient in Python, Java and/or R
Experience in migrating from RDBMS to NoSQL systems
Strong SQL (ANSI or other standard SQL) writing skills is a must
Understanding of UNIX/Linux/Perl or bash Shell scripting language
Ability to architect highly scalable distributed systems
Hands-on experience with PySpark or PyTorch
Working knowledge of Apache Kafka or AWS SQS with Kinesis Data Firehose
Working knowledge of in-memory computing systems such as Redis, NuoDB, VoltDB or GridGain","New York, NY",Data Engineer,True
6,"Data Engineer / DeveloperLooking to be part of an amazing team that is solving challenges, growing, and attracting attention? Plus, do you want to work with data you can explain to your mom? TravelPass Group is a close-knit group of technical, marketing, and business experts who are disrupting the travel industry.We are creating cutting-edge travel technology and data systems. We uncover travel opportunities to help people travel more by saving money and booking easily. Since 2008, we have helped people book more than 8 million hotel rooms. We have been recognized locally and nationally among the fastest growing companies and as a top workplace. Here’s a sample: Inc. 5000 List of America’s Fastest-Growing Private Companies, Utah Business Magazine Fast 50 (#9), UV50 Fastest Growing Companies in Utah Valley (#9), Mountain West Capital Network’s Utah 100 (#22), Deloitte Technology Fast 500 (#155), and The Salt Lake Tribune Top Workplace.Our data services team is looking for a mid- to senior-level Data Engineer/Developer with a passion for creating elegant yet simple data systems. You’ll have opportunities to grow and be responsible for building and maintaining data pipelines, making sure data is available and accessible, and helping drive new innovation and insights via data management. The ideal applicant will have strong development and systems skills and will provide the company with essential data.Key Responsibilities Curate business critical data sets using our own data as well as external data setsProvide ETL solutions to populate and enhance our data storesArchitect data pipelines to transform and validate dataMaintain data quality and integrity across our systemsAssist in data governance processes, planning, security and executionQualifications Demonstrated strength in data modeling, ETL development, and data warehousingExtensive experience writing complex, highly-optimized queries across large data sets using SQL, Postgres, MySQL, or something similarStrong experience in Python or similar languagesExperience with various AWS services including RDS, S3, EMR and RDSExperience consuming and cleansing data from third-party APIs and other sources3+ years of experience building software and working with dataExperience with Big Data technologies such as Hive, Hadoop, or SparkMotivated, passionate, and self-improvingStrong organizational and analytical skills with an ability to extract meaning from dataExperience in our current technologies a bonus: AWS Cloudsearch, Redshift, Athena, Golang, RBenefits Health insurance, PTO, Paid Holidays, 401k with matchingCompetitive compensation with quarterly bonusesFlexible work schedule with remote daysFully stocked break room with game room and lounge areaFun and casual company culture based upon respect and resultsGreat “friends and family” travel booking site that can save you and your friends a ton of cash! We want you to experience travel.Job Type: Full-time","Salt Lake City, UT",Data Engineer,True
7,"Job SummaryWe are hiring a Senior Data Engineer for a new purpose-driven, VC-backed data and AI startup founded by Adam Bly (adambly.com/bio). We invite you to learn more about the vision and thesis for the company and the problems we're setting out to solve here: https://www.linkedin.com/pulse/why-im-starting-new-ai-company-adam-blyResponsibilities and DutiesYou will:Architect and build key components of our first product, working with large amounts of heterogeneous data and tackling high-impact technical challengesBuild products that help advance data science and machine learningCollaborate with experts in data science and ML and partners at top research labsLeverage best practices in continuous integration and deliveryBe part of the founding engineering team of the company and help shape our engineering culture, values, and ways of workingQualifications and SkillsYou have:A proven record of personally taking large data projects from ideation to implementationExpertise working with high volume, heterogeneous data using distributed systems such as Hadoop, BigTable, and CassandraExpertise architecting, building, and operating large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and DataflowStrong knowledge about data modeling, data access, and data storage techniquesExperience with Agile developmentPreferably worked on graphs and open source data-related projectsYou are:A systems thinker who is naturally predisposed to connecting dotsA scientifically-minded individual who generates hypotheses from observations, conceives creative ways to test hypotheses, presents arguments supported by data, and changes your mind based on new dataMission-driven and passionate about the problems raised in the post shared aboveReady to build something big and ambitious from the ground upJob Type: Full-timeWork authorization:United States (Required)","New York, NY",Senior Data Engineer,True
8,"InternshipPosition Summary:
The New York County District Attorney's Office has immediate openings for Spring Internships in its new Law, Technology, and Innovation Unit (LTI). LTI is charged with developing technology solutions to enterprise-wide problems related to investigations, evidence processing, and case management. Interns will brainstorm problems and potential solutions with bureaus, develop prototypes, and contribute to existing data science projects and applications. The LTI intern will have latitude to apply his or her unique skillset to an evolving set of projects and tasks.

Responsibilities:
•
Work with those in your unit to understand case workflow, and automate and streamline investigative steps wherever possible
•
Develop technological infrastructure for your unit, in the form of internal web applications or otherwise
•
Develop tools to identify trends, behaviors, and patterns related to cases in your unit
•
Work on cases directly wherever programming can speed up an investigation or bring in new insights
•
Collaborate with others throughout the office to work on DANY-wide applications and projects

Qualifications:
•
Knowledge of Python, and preferably R and SQL
•
Experience developing (internal) web applications via Flask, Django, Shiny, Bokeh, HTML, CSS, Javascript, etc.
•
Experience working with databases
•
Experience with data science
•
Ability to make sense out of virtually any type of incoming file, and extract and analyze the relevant data
•
Ability to communicate effectively with anyone - other programmers internally or externally, assistant district attorneys, investigators or paralegals
•
Driven to find ways to use technology to improve the way cases are brought in and investigated within your unit

Educational Requirements:
•
Currently enrolled in an undergraduate or graduate program in Computer Science or a related field.

Additional Requirements:
•
Cover Letter
•
Unofficial transcript
•
Previous data science project or portfolio
•
Interest in law enforcement, criminal justice, or civic technology

Commitment:Applicants must commit to a complete term:
•
Summer Term (June - August, ï»¿10 weeks, 35 hours/week)
•
Fall Term (September - December, 12 weeks, minimum of 12 hours/week)
•
Spring Term (February - May, 12 weeks, minimum of 12 hours/week)
The New York County District Attorney's Office is an Equal Opportunity Employer","New York, NY",Data Engineer Internship,False
9,"To be based in the South West we are looking for experienced Data Engineers/Installers of Cat 5, 6 and fibre optic cabling systems.
LOCATION: South West

TYPE: Full-time / Sub-contractor

RATE: Negotiable Salary, depending on experience

You must be a qualified installer of Cat5, Cat6 and Fibre. The role is geared around carrying out new installations and fault diagnosis / repair work on existing infrastructure in our customer data centres.

The range of installations we undertake, in which you would be a part of:

Single and multi-site solutions
Design and installation
Contract work and retained service
Voice and data projects
Cat5e, Cat6, Cat6a, Cat7 specifications
Fibre optics, including blown, splicing, and pre-termination
Switches and routers
This role has become available due to expansion. We have a close-knit team of Engineers coupled with a high retention of employees.

Offers of employment are normally subject to satisfactory references and any other required pre-employment checks. In certain roles, successful applicants will need to satisfy security requirements in order to gain access to our customer sites.

All candidates will undergo a Criminal Reference Check (basic disclosure).",United States,Data Engineer,False
10,"$50,000 a yearJob Description

No Computer Science degree? No problem. We love self-taught developers!
LaunchCode offers paid, data engineering apprenticeships at one of our employer partners that include Mastercard, Anheuser-Busch, Boeing, Carnival Cruise Lines, and many more! More than 4 of 5 apprentices are offered a full-time position with an average starting salary of $50,000.
We’re looking for applicants who are tenacious, driven, and know how to work a problem. Always digging deeper to learn how things work? Eager to squeeze some learning out of every experience? We’re looking for you!
We only accept applications through our website at
https://bit.ly/2OQizEY

Qualifications

Passion, drive, and aptitude to succeed in technology
Skills to analyze data using one or more of the following tools:
Python with bonus points for data science libraries such as pandas, NumPy, and scikit-learn, or some experience with R
SQL including comfort with multi-table joins, views, and stored procedures
Data warehousing or ETL process design
Visualization tools such as Tableau, Chartio, PowerBI, or chart-making libraries such as ggplot2
AWS
The ability to work full-time in the United States and a desire for your apprenticeship to become a full-time job
We only accept applications through our website at
https://bit.ly/2OQizEY
Additional Information

If you want to land a job in technology with LaunchCode, but don’t yet have the required skills, visit www.launchcode.org/get-started to see how we can still help!
Our stats:
Over 4 out of 5 LaunchCode apprentices get offered full-time employment.
On average last 84 days (longer or shorter depending on the company).
Average starting salary after apprenticeship period $50,000","Tampa, FL 33612",Entry Level Data Engineer,False
11,"Book of the Month is looking for a detail-oriented software engineer to oversee our data warehouse and reporting infrastructure. Our user base generates millions of rows of data daily, and our new Data Engineer will play a critical role in the way the company uses this information to make business decisions. Working closely with our Director of Engineering and Director of Data and Analytics, you’ll build out suite of internal tools that touch all parts of our business—including operations, marketing, product, and customer experience—while scaling and maintaining our data infrastructure.
What you’ll do here
ETL design, development, and management
Build new ETL pipelines using Python
Scale out existing ETL architecture to support our growing business (and data), including developing best practices for pipelines supported by batch vs. streaming services
Maintain and improve the reliability of the backend infrastructure and data quality, e.g. build tools and alerts to monitor ETL job status
Data warehouse design, development, and management
Oversee our data warehouse and BI servers
Optimize data warehouse architecture to handle both automated data processing and ad hoc reporting
Develop best practices for data storage
Work closely with our Analytics team to understand company reporting and optimize pipelines for reporting needs
Work with Analytics team to ensure data quality and reporting availability
What you bring to the table
1–2 years experience developing software
Interest in data engineering and developing reporting platforms
Highly proficient in writing complex SQL queries to transform raw user action data into business data
Experience with Redshift, Athena, S3, Google Big Query, Vertica, Postgres, or MySQL databases
Experience with Python, R, bash, or similar scripting languages
Experience developing ETL jobs
Bonus points for (but not required): experience creating reports and data visualizations in a BI tool like Microstrategy, Looker, Domo, or Chart.io; understanding CI/CD deployment pipelines; experience with e-commerce subscription data
A little bit about us
Book of the Month uncovers the best new books and sharing them with 10 million readers every month. Founded in 1926 and relaunched in 2015, Book of the Month is an early-stage startup with a rich history and loyal user base—and we’re looking for the best people to join us.
Our benefits and perks
Unlimited vacation days with a get-your-work-done policy
Competitive compensation
Medical, vision, dental, and 401(k) plans
A dog-friendly office with an open floor plan
Free books! snacks! beer! seltzer!","New York, NY 10001 (Chelsea area)",Data Engineer,False
13,"You will have the opportunity to work as part of a team of highly technical engineers to design, develop and maintain data acquisition, pressing, and management software. The successful candidate will be highly motivated with a passion for technology and possess a strong computer science background. In this role, you will support the unique, data driven research of our financial analysts. This is a terrific opportunity for a skilled engineer to become a key contributor for our investment research business.

Qualifications

3-5 years of work experience designing and architecting big data systems using spark / Hadoop / map-reduce
Advanced Degree in Computer Science / Engineering or related field
Experience building data models for normalizing/standardizing varied datasets for machine learning / deep learning
Experience working on cloud infrastructure – AWS
Job Expectations

Independent, self-starter capable of architecting and implementing systems end to end
Design standardized data model for over 10 data sources
Build scalable connectivity pipelines to data sets and toolsets for easy definition of data cleaning and normalization to data model
Email resume to info@mscience.com and include position for which you are applying.","New York, NY",DATA ENGINEER,False
14,"Job Description
We are a startup within one of the fastest growing and most strategic parts of Amazon. Marketing Insight owns the product, technology and deployment roadmap for advanced analytics and insights products across our advertiser success team. Advertiser success is core to Amazon’s growth, as it helps our suppliers drive awareness, consideration, and purchase of their products by hundreds of millions of consumers around the world, and generates revenue which helps us lower prices and invest in improvements to our customer experience. We are a highly motivated, collaborative and fun-loving team with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.


We are looking for passionate data engineers to develop a flexible data model and optimize the consumption of massive data sources we require to generate unique insights. Data is at the center of every product we will develop as we create brand new systems that serve the needs of our large and growing base of advertisers. You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence. Members of this team will be challenged to innovate using big data technologies. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive profitability. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.
Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience working with AWS big data technologies (EMR, Redshift, S3)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Masters or PhD in computer science, mathematics, statistics, economics, or other quantitative field","New York, NY",Data Engineer,False
15,"Job Description
-Thinking Big-
Do you want to help shape the future of television using your data skills? Amazon Studios' unique approach is disrupting the entertainment industry with fan and award favorites like The Marvelous Mrs. Maisel, Jack Ryan, The Man in the High Castle and so many others.

With a passion for all things television and transformative data insights, Amazon Studios Research has built and continues to evolve a robust platform that predicts, models and tracks success at every stage of production: from concept, through release and beyond. The Research team is uniquely situated at the crossroads of creativity, data-driven decisions, customer experience and statistical analytics.

The tools you build will help drive and define the future of television and movie content for millions of customers around the world.
-Ownership: Insisting on the Highest Standards-
As a Data Engineer, you will be architecting, building and supporting the analytic technologies that give internal customers timely, flexible and structured access to their data as a means to optimize marketing, scheduling and show development. You will interact with team members and internal customers to gather requirements and build robust and secure data flows. You will also own the design, creation, and management of extremely large datasets. You will be designing, implementing, and operating stable and scalable solutions.

Excellent written and verbal communication skills are a must as the candidate will work closely with teams of diverse skills, mindsets and backgrounds. Amazon Data Engineers are capable of much more than just thinking in terms of lines of code.


The position is located in beautiful Santa Monica, California.
Basic Qualifications
3+ years of data engineering or related experience (Data Science, Business Intelligence)
Firm grasp of relational database models, aggregate and analytic functions, as well as database and query optimization methods
Ability to design, develop and automate scalable ETL and reporting solutions that transforms data into accurate and actionable business information
Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets
Expertise in the design, creation, management, and business use of extremely large datasets
Comfort with command line operations including authoring, executing and maintaining bash scripts
Preferred Qualifications
Experience with primary research and/or entertainment data is a plus but by no means expected or required

Direct experience with Big Data technologies such as Spark, Hive, Python

Experience with AWS tools including Redshift, S3, Elastic MapReduce (EMR), Elastic Cloud Compute (EC2)

Familiarity with AWS administrative tasks including managing access, permissions and roles

Experience with security methodologies related to encryption, authentication and threat modelin","Santa Monica, CA",Data Engineer - Amazon Studios Research,False
16,"Description:
The New York Yankees Baseball Operations department is accepting applications for an experienced data engineer with a focus on data quality analysis. This position reports to our senior Baseball Operations executives and will assist in the development and maintenance of our data processing pipelines.
Primary Responsibilities:
Prepare, clean, format analytical datasets for processing by data scientists
Become an expert in our datasets, their strengths and weaknesses and write code to pull and verify data in response to data scientist requests
Using R, visualize complex, multi-source data to pinpoint data quality issues
Build automated pipelines for processing and cleaning data
Conduct database feature engineering to support ongoing quantitative research
Work with developers to create and deploy systems for anomaly detection
Interface with data scientists, software developers, and other baseball operations staff as needed
Design department-wide principles and workflow for data quality management
Serve as the main point-of-contact for questions about data structures, definitions, and quality


Qualifications and Experience:
Bachelor’s degree in Computer Science or related field
3+ years of experience developing in SQL (preferably T-SQL)
2+ years of experience with data profiling, data modeling, and data pipeline development
2+ years of experience developing in R (or a similar statistical programming language), including experience with data manipulation and visualization in that language
Ability to write succinct code with optimal performance and simplicity
Excellent communication and problem-solving skills – must be able to break down a complex task and put together an execution strategy with little guidance
An understanding of typical baseball data structures, basic and advanced baseball metrics, and knowledge of current baseball research areas


This description is intended to describe the type of work being performed by a person assigned to this position. It is not an exhaustive list of all duties and responsibilities required by the employee. The New York Yankees is an Equal Opportunity Employer. The company is committed to the principles of equal employment opportunity for all employees and applicants for employment.","Bronx, NY","Data Engineer, Baseball Operations",False
17,"ContractNo OPT's.GC/USC preferable.Long term contract roleLooking for only 2-8 years experience candidates.Experience with Spark, Scala, Hadoop & Java skillsJob Type: Contract","New York, NY",Data Engineer,False
18,"$30,000 - $37,500 a yearThis is an apprenticeship opportunity. Candidates will be hired based on attitude and aptitude. The successful candidate will be provided the required technical training and certification to grow into the role and launch a successful career in data engineering.We’re looking for a Cloud Data Engineer Apprentice to contribute to growing data and analytical needs. Be part of the team that looks to build out next-generation hybrid data platform, leveraging new technologies and techniques to maximize the value out of data assets and empower employees to innovate.A requirement of the application process is to complete the following assessment:https://survey.harrisonassessments.com/dzwy-bw68-yv6rResponsibilities:Act as champion to identify and respond to data needs for business users to use the analytics environmentWork with data architects to design and implement solutions to ingest, transform, connect, store and expose Homesite’s data to range of usersIncorporate new data sources by building pipelines for automated and semi-automated data ingestion and data refreshMaintain existing scripts and develop new scripts in SQL, Python, and other languages as appropriate to parse, clean, transform, and load dataDevelop and maintain methods to match external data sources to Homesite data using deterministic and probabilistic methods.Develop and implement tests to ensure data quality and proper governance across all integrated datasetsQualifications:Bachelor’s degree in Computer Science, Engineering, or equivalent work experience.Candidates will have an understanding and some practical experience in the following areas:Python and SQL in Windows and Mac/Linux environmentRelational databases including PostgreSQL and Microsoft SQL ServerColumnar data structures like Apache Parquet and columnar databases like RedShiftDistributed SQL query engines like Presto DB and AthenaAmazon Web Services including Redshift, S3, Kinesis, Glue, and DynamoDBWriting shell scripts for process automationTraining and development will be provided to develop skills.Job Type: Full-timeSalary: $30,000.00 to $37,500.00 /yearExperience:Redshift: 1 year (Preferred)PostgreSQL: 1 year (Preferred)DynamoDB: 1 year (Preferred)Apache: 1 year (Preferred)Shell Scripting: 1 year (Preferred)Education:Bachelor's (Required)Work authorization:United States (Required)","Boston, MA",Cloud Data Engineer - Apprentice,True
20,"ATNi is looking for an experienced LAN/WAN Data Engineer that is a self-starter with exceptional attention to detail and is a skilled multi-tasker to join the team!
You will be a key member of our wide area network deployment team with focus on insuring our data core which carries our entire customer base traffic. Our wireless network is complicated with 2G/3G/4G technologies running in multiple countries and your job will be to ensure that our wide area network data core which carries all of our customer’s traffic is as efficient and as fault tolerant as possible. Our customers like their data service to be fast and always on and it will be your responsibility to make sure that they are delighted with our service. A big part of this job is troubleshooting at the routing and switching level, and the ability to come up with creative solutions so if you like wearing your thinking cap this job might be for you.
BASIC QUALIFICATIONS:
Bachelor’s degree in related field and 2 years of experience; or Associates Degree and at least 3 years work experience in related field or equivalent related work experience.
Working knowledge of Juniper networks gained through relevant experience, vendor training, and/or college or technical/vocational school coursework.
Proficient in understanding of TCP/IP and routing protocols to include BGP and MPLS.
Strong computer skills; proficient in MS Office.
Strong analytical skills to resolve problems.
Strong oral and written communication skills to coordinate repair efforts and prepare reports.
Strong interpersonal skills to coordinate efforts effectively between multiple groups.
Ability to work on call support as needed.
PREFERRED QUALIFICATIONS:
Previous experience as Data Network Engineer.
Experience with SNMP
Experience with Mobile IP, and L2TP
Certifications: (JNCIA, JNCIS, JNCIP, etc)
Working exposure to wireless data network deployment.
Experience with some of the following standards & protocols: MEF, Ethernet, BGP, IBGP, OSPF, QoS, CoS, MPLS, VRF, VLANS, and VPNs
Experience with mobile wireless protocols, specifically CDMA, GSM, LTE, or IS-835

JOB DUTIES (include but are not limited to):
Design and implementation of core to edge network architecture and routing protocols on Juniper platforms.
Operational Support of Data Network including troubleshooting routing and switching issues
LAN/WAN Responsibilities including IP address management and documentation
Engineering for growth and sustainability of the network.
Education and training of other individuals within the company regarding network routing.
Engineering and support of other systems supported by the Data Network Organization.
Analysis for root cause determination of issues including recommendations for improvements.
Write, review, and implement Methods of Procedure.
Working directly with equipment vendors
Other duties as assigned by management.","Castle Rock, CO",LAN/WAN Data Engineer,True
21,"To analyze data and informational requirements and provide the best data solutions to internal Vanguard clients.

Duties and Responsibilities
1. Creates sophisticated data solutions to business problems.
2. Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports using team procedures and tool selection guidelines. Ensures that all code is developed in a well structured manner - includes sufficient documentation - and is easy to maintain and reuse.
3. Continues to discover and implement new data sources and data integration techniques that are beneficial to clients who self-provision data.
4. Leads all phases of development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members. Tests code thoroughly for accuracy of intended purpose.
5. Understands and enforces team development guidelines. Mentors staff with less experience. Resolves issues elevated from staff with less experience.
6. Partners with internal clients to enhance understanding of business functions and informational needs. Maintains a broad understanding of Vanguard’s technologies, tools, and applications, including those that interface with business area and systems.
7. Designs and conducts training sessions on tools and data sources used by the team and self provisioners. Provides supporting documentation to team members and business partners as needed.
8. Understands, complies with and enforces team policies and procedures, especially those for quality and productivity standards that enable the team to meet established milestones. Understands, complies with and enforces all Information Security policies and procedures, and verifies deliverables meet Information Security requirements. Abides by Risk Controls as defined in the team CAF Documentation.
9. Manages and/or participates in special projects and performs other duties as assigned.

Qualifications
Undergraduate degree in a related field or the equivalent combination of training and experience.
Master degree preferred.
Minimum of five years analyst, developer, or DBA experience.
Strong SQL and query performance tuning skills.
Extensive experience using one or more of the following data sources: Enterprise, Data Warehouse, VAST, VISTA, Adobe or Tea Leaf
Advanced knowledge of one or more of the following: COGNOS Report Studio, SAS, SQL, AWS, Python, UNIX, db2, and Java, Tag management Systems, Adobe and Tea Leaf.
Strong problem solving, planning, and communication skills.
Knowledge of the Retail and Institutional WEB Site preferred.
Vanguard is not offering visa sponsorship for this position.","Malvern, PA 19355",Data Engineer,True
22,"Job Field: REDE - Research & Development
Location: Tarrytown, NY, US
Company: BASF Corporation
Job Type: Standard
Job ID: EN_US_1803973

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, age, citizenship, color, religion, sex, marital status, national origin, disability status, gender identity or expression, protected veteran status, or any other characteristic protected by law.
Description
At BASF, we create chemistry through the power of connected minds. By balancing economic success with environmental protection and social responsibility, we are building a more sustainable future through chemistry. As the world’s leading chemical company, we help our customers in nearly every industry meet the current and future needs of society through science and innovation. We achieve this through our commitment to continuous improvement and operational excellence, which gives us the opportunity to set and deliver on ambitious, long-term goals.


We provide a challenging and rewarding work environment with a strong emphasis on process safety, as well as the safety of our employees and the communities we operate in and are always working to form the best team—especially from within, through an emphasis on lifelong learning and development. This allows for our employees to innovate and generate new ideas, put them into action, and gain insights from them to further advance our collective expertise.


Data solutions form a core contribution to innovation. Data solutions comprise data ingestion, data architecture, as well as data visualization for all kinds of materials, systems, and processes at BASF. We support the live cycle of a material from ideation in research to quality management in production.

The mission of the global Data Solutions team is to set up 21st century data architectures and build interfaces around them, thereby making information accessible for and speeding up innovation in BASF’s materials science research. The Data Solutions Team is recognized within BASF’s research organization as the core competency center regarding data management, in particular relational and graph databases and big data architectures (Hadoop).



At BASF, we are constantly striving to become an even better place to work. BASF has been recognized by Forbes Magazine as one of America’s Best Employers in 2017. We strongly support the spirit of collaboration through effectively involving team members and colleagues from other relevant units when developing and executing strategies and projects. Come join us on our journey to create solutions for a sustainable future!


Data Engineer (1803973) – Tarrytown, NY or Wyandotte, MI (This role can be based in either location)


Where the Chemistry Happens
We are searching for a professional like yourself to play a key role in the execution of data solutions to accomplish project objectives. Supported by our data architects, you will populate IT architectures optimally suited for capturing, storing, integrating, and provisioning primary lab data generated in our R&D organization. Through application of intelligent data solutions, you will enable researchers to access huge amounts of data swiftly. Your contribution allows for faster and more data driven decisions and shall help increasing efficiency of our R&D efforts. You will closely cooperate with R&D, Engineering, central functions and business units.



You will be relied on to provide knowledge and expertise in data management, data connectivity and visualization.



Formula for Success



Leveraging your academic background in natural sciences combined with a pronounced IT affinity or experience in IT with a strong interest in natural sciences, you will contribute to a research direction that provides innovation and breakthrough advantages for the business by delivering tailored data solutions.
Your strong communication skills help to bridge the gap between IT and real-world applications. You will engage directly with chemical and biological labs, analyze existing lab processes and develop a common understanding of the best data management solutions.
Relying on your strong programming skills (preferably in Python, R and/or Java, as well as interfacing environments based on these such as R-Shiny or Dash), you will contribute to the development of applications that make data architectures accessible to both lab technicians and business partners.
Your familiarity with different data base architectures, both relational and NoSQL (e.g. graph databases, document stores, etc.), will be critical to set up lasting data infrastructures tailored to the labs’ needs.
Create Your Own Chemistry: What We Offer You


Adding value to our customers begins with adding value to you. You@BASF is the suite of benefits, perks, programs and unique opportunities we offer to support you—the whole you—in all stages of your life and career. With you@BASF, you create your own chemistry.


The total rewards that you receive as a BASF employee go way beyond a paycheck. From competitive health and insurance plans, to robust retirement benefits that include company-matching contributions, to making sure you never stop learning, we believe investing in you is investing in our success. Working for a large, global organization, you’ll have a chance to grow professionally and personally, expand your network and build a rewarding and dynamic career.




BASF provides interesting and challenging learning and development opportunities to help you make the most of your talents and your job.




Qualifications - BASF recognizes institutions of Higher Education which are accredited by the Council for Higher Education Accreditation or equivalent","Tarrytown, NY 10591",Data Engineer,True
23,"$75,000 - $80,000 a yearEarthDefine is seeking a GeoSpatial Data Engineer who is equal parts geospatial analyst, data scientist and Python hacker. The successful candidate will build solutions for developing new geospatial products, solve GIS problems and provide GIS support to internal teams.Who you areYou are someone who brings a good mix of geospatial, programming and data science skills. You are excited by the prospect of learning and implementing new algorithms to improve classification workflows. Your geospatial and coding kung-fu is strong and you enjoy building new skills. You’ll be right at home here if you are organized, cultivate strong relationships, and push yourself and your team to the next level. We expect our Data Engineer to:Embrace technology: You have a proven ability to quickly learn and use new technologies.Be organized and detail oriented: You will help plan and successfully deliver projects on time and within budget. You are excellent at prioritization, follow-through, and time management.Be a natural problem-solver: You anticipate challenges and opportunities within your work as it relates to our team and EarthDefine’s goals and priorities, and quickly generate creative solutions to meet our evolving needs.Be collaborative. We want to hear your opinions. You will support the growth of your colleagues and help them create better solutions. You have great interpersonal, oral and written communication skills.Thrive on change and ambiguity: You can fully embrace a fast-moving organization. You have a strong sense of urgency. Deadlines, changing priorities and new projects don’t scare you. Instead, they inspire you to rise to the occasion. Be willing to step up to the plate. You’re eager to contribute and learn, willing to provide solutions before we even know there are issues. You are resourceful and acquire skills independently.Job ResponsibilitiesDevelop data workflows leveraging best-in-class machine learning algorithms to classify ground cover from remote sensed imagery and Lidar.Write maintainable Python code for spatial data processing.Research, identify and consolidate geospatial data over large geographical areas from multiple sources for input into classification workflows (imagery, Lidar, demographic data, road networks etc.).Develop algorithms and software (e.g. ArcMap AddIns, scripts, tools etc.) to process geospatial data.Maintain and improve existing Python codebase.Learning new geospatial platforms, programming languages, tools as needed.Research new approaches to address geospatial data development challenges.Manage small and large image classification projects.Provide training and support to other team members.Understand and deliver on client expectations of our products and services.What We Are Looking For MS/ Ph.D. in GIS/Remote Sensing/Data Science disciplines with strong analytical skills and 3+ years of hands-on experience working intimately with geospatial data.Proficiency with Python.Experience with machine learning, data science, and deep learning.Experience implementing machine learning models in common ML frameworks (TensorFlow, Torch, Theano, etc.)Experience working with multispectral imagery and Lidar data.Experience with object-based image classification and spatial modeling.Familiarity with open source geospatial libraries (GDAL, OGR, OTB etc.).Familiarity with scientific computing libraries like numpy, scipy, sklearn, pandas etc.Strong technical aptitude, including the ability to quickly and efficiently learn applications, systems, policies, and procedures.Proven ability to lead and manage multiple projects.Strong organizational, interpersonal, and communication skills.A good sense of humor and comfort with ambiguity.Job LocationRedmond, WABefore You ApplyApplicants must be currently authorized to work in the United States without the need for additional visa sponsorship. To apply for this position, please send in your resume and a cover letter describing your experience working with Python, GeoSpatial data (collating, fixing, preprocessing, normalizing raster/vector data etc.), machine learning and any other experience that would make you a good fit for this role.About UsEarthDefine is a growing mapping company transforming vast amounts of earth sensor data into consumable geospatial information products. We have mapped millions of acres of high-resolution land cover and tree data across the United States and continue to grow this unique data archive. We have embraced big data challenges by building a unique technology platform capable of integrating large volumes of imagery, vector, and point cloud datasets. To learn more about us, please visit http://www.earthdefine.com.Job Type: Full-timeSalary: $75,000.00 to $80,000.00 /yearExperience:GIS: 3 years (Preferred)Python: 2 years (Required)Machine Learning implementation: 1 year (Preferred)Education:Master's (Preferred)Work authorization:United States (Required)","Redmond, WA",GeoSpatial Data Engineer,True
24,"MS or BS in Computer Science or Software Engineering REQUIRED5+ years of data engineering operations experience, especially in the cloudDirect experience in building large scale data platform, including Hadoop, Spark, Kafka, Flink/Storm, Hive/Presto or Druid/VerticaPassionate about scalability and efficiency, details and process orientedMust be local in San Francisco Bay AreaKubit is a venture backed early stage startup building a SaaS analytics platform for machine learning based diagnostics. We currently have 5 engineers and are hiring more to grow.About UsFounder has 20 years of software experiences, mostly in startups. Recent 7 years as CTO of Smule, scaling engineering team 10x and operations 1000x. https://www.linkedin.com/in/zhonglingli/Join an elite team focusing on execution, build amazing products from 0 to 1, then scale and grow with the companyNo red tape, staying lean and mean. A flat organization where everyone is hands on and get things doneGenerous stock options and benefits (including medical/dental/vision, 401K and FSA)H1B and Green Card sponsorshipOffice is based in Belmont CA with ample parking, walking distance to CaltrainPlease don’t apply if you: Are looking for work-life balance or work from home/remote arrangementsHave an Inbox often with hundreds of unread emailsDidn’t read any technical or startup books in the last six monthsWant inflated titles and can’t do hands on workWhat we are looking for: Self-motivated individuals with a strong urge to make a difference and take chargeEntrepreneur mindset, effective contributors with strong leadershipStrong analytical and critical thinking skillsAlways learning, especially from failures. Humble and objective, team playerExcited about the enormous potentials of data and analyticsJob Type: Full-timeExperience:Machine Learning: 1 year (Preferred)Hadoop: 1 year (Required)Kafka: 1 year (Required)Spark: 1 year (Required)Education:Bachelor's (Required)Location:San Francisco Bay Area, CA (Required)","San Francisco Bay Area, CA",Data Engineer - Presto/Flink/Kafka/Spark - Early Stage Start...,True
25,"Data Engineer – Claims Data Science
Description
The Hartford Financial Services Group is seeking an energetic and passionate Data Engineer. As a member of the Claim Data Science team, this candidate will be responsible for designing, developing and implementing data assets for a wide range of operational initiatives supporting the Claims department.
We seek candidates with a solid understanding of data architecture and principles of ETL and Data Warehousing, coupled with strong analytical, communication, interpersonal, and business skills. This position will be a key member of a cross-functional Business, IT, and Data Science team developing deep insights using predictive models and artificial intelligence to enhance claim operations and outcomes. This role will combine business and technical skills involving interaction with business customers, data science partners, internal and external data suppliers and information technology partners.
Responsibilities
 Identify and analyze internal and external data sources for availability and quality.
 Perform data analysis to ensure accuracy of the data and determine applicability.
 Identify areas of opportunity to improve or enhance existing data processes.
 Support data assets and perform adoption activities including pseudo production, maintenance, quality measures, documentation, use case development and consulting activities.
 Partner with data scientists, business partners, and data suppliers to create innovative solutions and define business requirements.
 Relate the data to the business processes and communicate information regarding the availability, quality, and other characteristics of the data to a diverse audience.
 Perform cost benefit analysis of concepts and solutions to drive prioritization.
 Evolve and maintain a position as a developing subject matter expert and data consultant.
 Create and follow a strategic plan and report progress to management.

Qualifications

Qualifications
Experience & Skills
Candidates must have the technical skills to transform, manipulate and store data, the analytical skills to relate the data to the business processes that generates it, and the communication skills to disseminate information regarding the availability, quality, and other characteristics of the data to a diverse audience.
•
 Experience accessing and retrieving data from large data sources
•
 Experience with data modeling, data warehousing tools and databases (e.g. ETL, ORACLE, Teradata, SQL Server)
•
 Experience in Python and/or R
•
 Experience in creating and tuning SQL Queries
•
 Ability to analyze source systems and provide business solutions
•
 Self-starter with a willingness to become a data expert and to learn new skills
•
 Results oriented with the ability to multi-task and adjust priorities when necessary
•
 Ability to work independently or in a team environment with internal /external customers
•
 Ability to determine business solutions and translate into actionable steps
•
 Experience with indexes and basic database design is a plus
•
 Knowledge of insurance claims, operations and analytics is a plus
•
 Experience with medical and healthcare data is a plus
•
 Bachelor degree or equivalent experience in related field required
Behaviors at the Hartford
•
 Deliver Outcomes – Demonstrate a bias for speed and execution that serves our shareholders and customers.
•
 Operate as a Team Player – Work together to drive solutions for the good of The Hartford.
•
 Build Strong Partnerships – Demonstrate integrity and build trust with others.
•
 Strive for Excellence – Motivate yourself and others to achieve high standards and continuously improve.

Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age

** NO AGENCIES PLEASE **


Job Function
: Data Engineering
Primary Location
: United States
Schedule
: Full-time
Job Level
: Individual Contributor
Education Level
: Bachelor's Degree (±16 years)
Job Type
: Standard
Shift
: Day Job
Employee Status
: Regular
Overtime Status
: Exempt
Travel
: No
Job Posting
: Oct 8, 2018, 1:45:54 PM
Remote Worker Option : Yes",United States,Data Engineer,False
26,"Temporary, InternshipRiot Games was established in 2006 by entrepreneurial gamers who believe that player-focused game development can result in great games. In 2009, Riot released its debut title League of Legends to critical and player acclaim. As the most played PC game in the world, over 100 million play every month. Players form the foundation of our community and it's for them that we continue to evolve and improve the League of Legends experience.

We're looking for humble but ambitious, razor-sharp professionals who can teach us a thing or two. We promise to return the favor. Like us, you take play seriously; you're passionate about games. We embrace those who see things differently, aren't afraid to experiment, and who have a healthy disregard for constraints.

That's where you come in.

Intern with the Data Discipline:
The Data challenges at Riot Games are extensive and you can expect to level up in all data domains. This includes software engineering, distributed systems, data modelling and machine learning.

As a Data Engineer intern, you may:

Contribute to engineering efforts in stream processing platforms aimed to unlock clean analytic data for all Riot teams.
Learn the internals of building a robust end-to-end data pipeline on all levels especially in software development, databases, infrastructure, data modelling and data processing.
Contribute code to production data systems and gain strong software engineering practice.
Gain hands-on experience in distributed data processing technologies: Kafka, Spark, and Flink.
Learn techniques for high volume distributed data processing and storages.
Ensure the integrity, availability, and confidentiality of data, database systems and supporting services.
Be involved in the data processing of hush-hush, unannounced titles that are being worked on in our skunkworks lab (""Research and Development!"").

While our engineers come in all shapes and sizes and work with many technologies, we expect every engineer at Riot to be:


Player-focused: you're a gamer whose passion for games (especially League of Legends) helps you stay focused on initiatives that make the difference to players in and out of the game
A threat to convention: bored by what's considered traditional, you constantly push past limits until the status is no longer quo; you don't ""think outside the box"" because, hey, there's no box
Focused on team: you find, shape, and cultivate teams that don't just swing for the fences, they jack that (metaphorical) baseball beyond the stars; you help Rioters develop the tools and creative atmosphere to shine, but ultimately hold them accountable for making smart calls and delivering capital-V value
Seriously playful: you work hard but always leave time for pentakills; whether you're grabbing a game of League in our onsite PC Bang or rapidly sharing cat GIFs, you make time for daily play in all of its wonderful forms
Humbitious: you're ambitious but humble, a state of being summed up by fans of portmanteau as ""humbitious;"" always shooting for the stars, you never forgo rounds of feedback from teammates, players, and partners who keep you from drifting off into space

To be eligible for an internship within the data discipline you should:


Be enrolled full-time in a computer science or relative program for a minimum of 1 school term following your internship with Riot
Be returning to university following your internship as a Junior or Senior with only 1-2 years left before completing your program
Have completed at least 1 competitive software development focused internship

We receive a lot of applications, but we'll notice a fun, well-written intro that shows us you take play seriously. Apply below and don't forget to include a resume and cover letter.

---------

To apply:
---------


Please read through the internship page ( https://www.riotgames.com/university-programs ) before applying. It includes helpful information that'll increase your chances at landing the gig.
Your application must include a cover letter and resume. In your application pitch, share your Summoner Name and experience with League of Legends, why you're interested in Riot, what you want to learn and what we can learn from you. Help us understand why you're right for this team by sharing what you've done that's truly above and beyond and how that may relate to this internship opportunity. There are 99 ways to impress us, but an average pitch ain't one.

The application deadline for Summer 2019 internships is December 31st, 2018. Applications received after this date won't be eligible for a Summer 2019 internship. We recognize that a great application takes time and effort, so we promise to return the favor of your effort with a response by January 15th, 2019.","Los Angeles, CA 90064","Data Engineer Intern, Summer 2019",False
27,"$100,000 - $110,000 a yearWe are redefining the guest experience at hotels worldwide. Our mission is to create a whole new way to travel, taking advantage of cutting-edge mobile technologies to deliver a superior hotel stay. We put guests in charge of what, when and how they want their hotel to meet their needs.We are building the next-generation hardware and software platform for the hospitality industry to meet these needs and are looking for brilliant and versatile team members who know the technology landscape and can craft simple and creative solutions.We are currently looking for a mid level to senior data engineer to assist our engineering and product teams in designing and implementing a new data pipeline and data solution to provide our business teams and our customers with valuable business insights based on the usage of our platform. This is an opportunity to join a dynamic team and help define our data strategy.Job ResponsibilitiesManage our existing analytics solutions and improve them to satisfy business and product requirementsArchitect data pipelines to transform and validate dataMaintain data quality and integrity across our systemsCommunicate goals and progress to teams and stakeholders.Assist in data governance processes, planning, security and executionJob RequirementsExperienced in data modeling, ETL development, and data warehousingExperience with various AWS services including RDS, redshift and S3Experience consuming and cleaning data from third-party APIs and other sourcesExperience with Big Data technologies such as Hive, Hadoop, or SparkStrong organizational and analytical skills with an ability to extract meaning from dataStrong communication skills to interact with stakeholders and customersStrong experience in Python or similar languages is a plusJob Type: Full-timeSalary: $100,000.00 to $110,000.00 /yearExperience:data modeling: 1 year (Required)API integration: 1 year (Preferred)Hadoop,Spark, Hive: 1 year (Required)python: 1 year (Preferred)AWS services: 1 year (Required)","Los Angeles, CA",Data Engineer,False
28,"Company DescriptionNovantas Solutions is a division of Novantas, Inc., a New York based consulting firm that provides advisory services and decision support software to the financial services industry. The Firm focuses mainly on issues of revenue strategy, which we describe as ""customer science”; this may include work focused on segmentation, product design, pricing, distribution management, payment services, sales execution effectiveness, branding, market mix management, and customer experience.The Novantas Solutions division delivers data-driven software products and services to accelerate revenue growth and increase profitability in the financial services industry. Our solutions leverage the models and insights accumulated through years of consulting to top banks and financial institutions. Our team members have the business expertise, analytical skills and know-how to implement creative, practical solutions for our clients.Job DescriptionWe find that the most successful candidates for the Data Engineer position are natural and relentless problem solvers who are passionate about working with data to solve business problems. These individuals demonstrate ease with quantitative analysis, can work well as part of small multi-disciplinary teams, have a keen interest in software engineering and are passionate about developing leading edge analytical business applications.Our Data Engineers are part of the engineering team responsible for data architecture, backend development and maintenance of our proprietary decision support software applications. They work closely with product management and the front end development team to ensure that our products are constantly improving and have leading edge functionality.Responsibilities: Collaborate with Product Management and other engineers on the team to make product improvements and develop new featuresHands-on SQL Server development (Stored Procedures, Functions etc)Migrate complex data processing from SQL Server to Big Data using Spark, Scala in the near future.Create and maintain detailed documentation and functional design specificationsDesign data architecture and ETL processes on our next-generation Big data stackPerform ongoing backend Database maintenance of existing applicationsProvide technical information to assist in the development of client facing product documentationAuthor and participate in software design and code reviewsAdhere to change management protocols and version controlPresent demonstrations of new features to internal product teams as well as high level firm leadership and partnersDesired Skills and ExpertiseAspiring candidates should have the following background, skills and characteristics:Bachelors degree, preferably in computer science, or engineering field3+ years of experience in database development or software engineeringFamiliarity with the design, development and maintenance of best-in-class BI capabilities, including data warehouse data structures and data pipelines spanning Spark/Hadoop and RDBMS worldsExpert-level database development experience in SQL Server, preferably for reporting data marts and business intelligence solutions, including writing stored procedures for complex business logic in T-SQLFamiliarity of architectural design patterns for micro-services leveraging relational and big data technologies is an added plusFamiliarity with Agile development process, SVN or other change management protocols would be a plusHands-on development experience with Big Data technologies such as Hadoop, Impala, Spark, Scala would be a plusProven track record of academic and/or professional successExceptional analytical thinking ability, ease with quantitative analysis, and excellent problem solving skillsSelf–discipline and willingness to learnAbility to work well with others in a high-pressure environmentExcellent verbal and written communication skillsOnly local candidates are encouraged to apply. We offer a competitive salary along with a complete benefits package.We are proud to be an Equal Opportunity Employer. All candidates must possess work authorization which does not require sponsorship for a work visaPlease visit www.novantas.com for more information.Job Type: Full-timeLocation:New York, NY 10017 (Preferred)Work authorization:United States (Required)","New York, NY 10017 (Midtown area)",Data Engineer,False
29,"We are Farmers!

Job Summary:

The Data Rotation Program will allow recent, college graduates to rotate through a 12- month program, exploring different areas within the Chief Data Officer’s (CDO) Group. Individuals will rotate through 4 assignments of approximately 3 months duration, determining where and how to make the strongest impact. Potential areas of placement include Business Intelligence/Data Delivery, Business Unit Data Engineering, Financial Data Engineering, Customer Master Data Engineering, Operational Data Engineering, Emerging/Big Data Engineering, Data Quality, and Data User Support.

We are seeking the December 2018 graduate; this opening will offer a start date of January or February of 2019.

Essential Job Function:
Actively contribute to Farmers Insurance data endeavors to manage core business.
Drive customer centricity while developing breadth of skill and knowledge across the data domain.
Understand, design, and utilize data structures and content via established and emerging technologies.
Gain technical and subject matter expertise through professional development and exposure to senior leaders and visible assignments.
Build and develop competencies thru a combination of on-the-job learning, formal and informal mentorship, and training that will enable the potential to take on significant ownership roles in the organization upon completion of the rotation program.

Education Requirements:
Four year college degree in Science, Technology, Engineering, Statistics, Mathematics.

Experience Requirements:
Prior internship or relevant professional experience in IT/ Data is a plus.


Physical Actions: Required job duties are essentially sedentary work consisting of occasional walking, standing, and lifting and or carrying 10-lbs maximum.
Physical Environment: Required job duties are normally performed in a climate-controlled office environment.


(Internal Title: Sr. Data Management Analyst)


Farmers is an equal opportunity employer, committed to the strength of a diverse workforce.


#dice

Schedule: Full-time

Job Posting: 10/17/2018","Woodland Hills, CA",Data Engineer I (seeking the December 2018 graduate; start d...,False
30,"$60,000 - $65,000 a yearTHE AGENCYThe Department of City Planning (DCP) plans for the strategic growth and development of the City through ground-up planning with communities, the development of land use policies and zoning regulations, and sharing its perspectives on growth and community needs with sister agencies in collaboration with the Office of Management and Budget (OMB).DCP?s six strategic objectives include: (a) catalyze long-term neighborhood improvement through integrated planning and targeted accompanying public investments; (b) encourage housing production, affordability, and quality; (c) promote economic development and job growth; (d) enhance resiliency and sustainability of neighborhoods; (e) ensure integrity, timeliness and responsiveness in land use reviews; and, (f) supply objective data and expertise to a broad range of planning functions and stakeholders.Central to its mission, DCP supports the City Planning Commission in its annual review of approximately 450 land use applications. The Department also works closely with OMB in developing the Ten-Year Capital Strategy, and helping administer the Neighborhood Development Fund, geared toward ensuring that growing neighborhoods undergoing rezoning have accompanying infrastructure investments.The New York City Department of City Planning is a great place to work ? cultivating intellectual inspiration, professional development and creativity. Visit our website at www.nyc.gov/planning to access the full listing of job opportunities and to learn more about our great agency.THE DIVISIONInformation Technology Division (ITD) is responsible for supporting the agency?s technology footprint, including technology infrastructure across five boroughs, as well as workflow applications and databases for analytics and decision-making. The division is comprised of 50+ inter-disciplinary staff with specialties in desktop support, server engineering, telecom, application development, database maintenance, data processing, data visualization and mapping, amongst many others. The division provides technology support for agency staff across five boroughs.Within ITD, the Enterprise Data Management (EDM) section is responsible for creating and implementing the agency?s data strategy and data governance policy; updating and maintaining core citywide data sets in support of 911 dispatch and the operations of city agencies; and improving the creation, use, and availability of geospatial data sets within the agency.THE ROLEITD?s Enterprise Data Management (EDM) section is seeking a highly-motivated, innovative Data Engineer to modernize data set development by automating processes and ensuring procedures are reproducible and transparent. He/she will be involved in the design and implementation of the entire data pipeline, from capturing and storing disparate data sources to processing that data and making that data available to DCP staff. The Data Engineer will work across the agency to understand data needs, and create systems that provide consistent and complete information to help solve business problems. He/she will work to increase data literacy throughout the agency, provide guidance on best practices, collaborate with stakeholders, write blogs, and participate in both formal and informal data information sessions. This work supports analyses used to inform important decisions, such as the allocation of the Neighborhood Development Fund, designation of areas subjected to special zoning regulations, and siting of new facilities.Data Engineering is a new unit within EDM responsible for modernizing and improving some of the citywide data sets distributed on BYTES of the BIG APPLE?, including PLUTO/MapPLUTO, Zoning Tax Lots, and the Facilities Database. It works with agency planners to automate the creation of additional high value data sets that require frequent updates. Responsibilities will include: Develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources; Work with agency staff to understand their business requirements and gather technical requirements for new data sets or updates to existing data sets; Collaborate with other engineers to design and implement EDM?s next generation data warehouse system; Implement new product features and performance improvements to existing data products; Write and disseminate technical documentation and metadata; Help drive optimization, testing and QA/QC to improve data quality across the product; and Perform other related tasks.

Minimum Qual Requirements

1. A baccalaureate degree from an accredited college and two years of experience in community work or community centered activities in an area related to the duties described above; or 2. High school graduation or equivalent and six years of experience in community work or community centered activities in an area related to the duties as described above; or 3. Education and/or experience which is equivalent to ""1"" or ""2"" above. However, all candidates must have at least one year of experience as described in ""1"" above.

Preferred Skills

Strong SQL experience (PostgreSQL / PostGIS) Experience developing and analyzing spatial data Familiarity with Python, Bash, and JavaScript Comfort with source control (GitHub) and working in a Linux environment Interest in urban planning and knowledge of NYC geography and data Strong analytical, quantitative, problem-solving, and critical thinking skills Excellent interpersonal, verbal, and written communication skills

To Apply

Click on ?Apply Now? at the bottom of the postingPlease be advised only candidates under consideration will be contacted.Appointments are subject to Office of Management and Budget (OMB) approval.The candidate selected for this position must be a resident of the City of New York or become a resident within 90 days of appointment.Authorization to work in the United States is required for this position. Sponsorship is not available for this position.THE DEPARTMENT OF CITY PLANNING IS AN EQUAL OPPORTUNITY EMPLOYER AND A COPY OF THE EQUAL OPPORTUNITY PROGRAMS IS AVAILABLE IN THE HUMAN CAPITAL DIVISION. THE DEPARTMENT MAKES AVAILABLE ACCOMMODATIONS FOR DISABLED APPLICANTS.

Residency Requirement

New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.","Manhattan, NY",Data Engineer,False
31,"An E-Discovery Data Engineer in the Advanced Data Services Group at ProSearch supports the custom data transformation needs of ProSearch clients. E-Discovery Data Engineers at ProSearch support quality and data integrity by:

Engineer and implement solutions and workflows for clients and internal teams
Perform both standardized and ad-hoc data analysis, reporting, and modifications via SQL
Create, implement, and support custom Event Handlers
Creatively utilize and combine existing scripts, recipes, and applications (both proprietary and third-party) to automate tasks and workflows
Contribute to solution and script ownership, including providing training and guidance on use and supporting enhancements and troubleshooting
APPLY","Los Angeles, CA",E-Discovery Data Engineer,False
32,"ContractSr. Data Engineerwith Tableau Administration:Location: San Jose/Sunnyvale* Must have Tableau Administration related to application, user admin and not necessarily servers, and Data Visualization experience* Experience in at least one ETL solution(SSIS, Informatica, Talend, etc)* Data engineering, User training/consulting, Software/Product architecture* Scripting/development experience (Python, SQL) is preferred* Understanding of ACL, Load balancer, Proxy and other Network configurations* Verbal & Written communication, customer focus, teamwork/collaboration, eye for automationJob Type: ContractExperience:Data Visualization: 2 years (Preferred)Scripting: 3 years (Preferred)SQL: 6 years (Preferred)Administrative: 3 years (Preferred)Informatica: 5 years (Preferred)","San Jose, CA",Data Engineer,False
33,"$20,000 - $80,000 a yearPart-time, InternshipOur Big Data Internship Programs include the Big Data Project Development and Big Data Training Courses which guide the Intern student how to finish the real cases in Gosvea Inc. Our aim is to establish the undergraduate and graduate students with characteristics unique to Silicon Valley. Now, we are looking for internship candidates (CPT/OPT) who are interested in Big Data field.Responsibility:Accept one-month training program in Big Data.Organize events and marketing activities.Create contents for the website and social media.Develop marketing channels \APP \Web Site (just one choice is ok).Requirements:BA/BS degree required at least.OPT/CPT are Welcomed, sponsor h1b for excellent people.Computer science, Business, Marketing or related major is our best choice.Candidates must be fluent in both English and Chinese.Job Types: Full-time, Part-time, InternshipSalary: $20,000.00 to $80,000.00 /yearEducation:Bachelor's (Preferred)","San Jose, CA",Big Data Engineer Internship,False
34,"Overview
Engineer will provide RAMS/LCC system/product demonstration and validation so that the performance of the system is measured and formally documented. The engineer manages the Environmental, Fire, and Smoke requirements of developed and purchased products. These activities are primarily for embedded electronics and mechanical products developed for Railway brake and coupler systems.
Responsibilities
Target Responsibilities:
Derive, validate and document failure rates, modes and effects at product level.
Derive, validate and document maintainability/LCC data for products.
Provide detailed failure rate analysis on systems, subsystems and components or equipment level - reliability prediction based on standard model, test data and field data.
Perform qualitative & quantitative analysis with focus on failure rate allocations.
Support impact analysis of design change requests on RAMS/LCC targets.
Setup and schedule interaction with customers to collect reliability data in a systematic and structured manner resulting in improved and updated understanding of field performance.
Track/Analyze field and test data for contractual compliance, producing reliability reports and updates to RAMS database.
Review and understand the Environmental, Fire and Smoke (F&S) industry regulations and customer requirements for the railway market.
Support RAMS/LCC Systems Engineer with the development of Environmental and F&S documentation.
Manage and update the F&S database, checking validity and expiration date.
Qualifications

Target Qualifications:
BS Engineering, minimum 1 years of experience in RAMS analysis.
Working knowledge in industry standards and analysis techniques (IEC 61508, EN50126/8/9, MIL 756).
Experience with Reliability and Maintainability demonstrations.
Experience with conformity to Flame/Smoke/Environmental regulations for product design.
Understanding of problem solving, root cause analysis and experience with segregating reliability failures from manufacturing defects and misdiagnosis.
Background with development of product maintainability and reliability prediction through test data, field data analysis, standard model calculations.
Working knowledge of PHA, SHA, SSHA, OSHA, FMECA, FTA, Reliability and Maintainability Analysis is preferred.
Demonstrate Engineering design skills with RAMS/LCC focus.
Good writing and verbal communication skills related to technical items and reporting.
Functional knowledge of Reliability, Maintainability (RM) processes and tools, and skilled with graphical presentation, databases, spreadsheets and word-processing software.
Knowledgeable of RAMS tools and techniques.
Preferred working engineering knowledge of pneumatics, embedded control electronics, mechanical systems.
Preferred experience in transit or rail industry.
US Citizenship.
Code: IN-123

#LI-123","Duncan, SC 29334",Reliability Data Engineer,True
36,"At Brivo, we are on a journey to transform the physical access control industry. A critical piece of this transformation is a robust data infrastructure. As a data engineer, you will work with a brilliant team to architect a big data platform that is real-time, stable and scalable in order to support data analytics, reporting, data visualization and machine learning. You will help to build the data product with cutting-edge technology and best practices. Your input and contribution will have direct impact on our data strategy and overall product roadmap. Please join us to re-shape how data is processed and analyzed in the security industry!

Responsibilities

Design and develop ETL for the new big data platform with open source technologies such as Kafka, Spark, and Presto etc.

Implement and support streaming technologies such as Kafka, Spark, and Kinesis to move data efficiently

Refactor the existing data model into an easy-to-maintain data solution across the organization

Work with agile teams to perform code reviews and participate in planning and design sessions

Work with QC team to assure product quality

Work with operations team to automate build and deployment from DEV to PROD

Required Qualifications

5+ years of work experience with ETL and data modeling

4+ years of experience with open source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.)

3+ years of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale

2+ years of experience with AWS - EMR, Athena, Kinesis, S3, EC2

Expert in at least one SQL language such as T-SQL or PL/SQL.

Understanding of modern data structures and business intelligence reporting tools and track record of applying those on the job

Ability to manage numerous requests concurrently and be able to prioritize and deliver

Good communication skills and dynamic team player

Preferred Experience

Experience in real-time analytics applications.

Experience in both batch and stream processing technologies

Experience with Java or Scala programming languages.

Machine learning experience with Spark or similar

Education

Bachelor’s Degree in computer science, computer engineering, data science or related fields; Master’s and above preferred

About Brivo

Brivo is the original innovator of cloud-based physical security solutions for commercial buildings. Currently serving over ten million users, Brivo offers a unified security platform including access control, mobile credentials, mobile administration, video surveillance, identity federation, visitor management, and elevator control. As a SaaS company, Brivo also offers a complete API platform service that empowers partners to build custom integrations and vertical market offerings. Our mission is to make the world a safer place by providing a subscription-based service for securing buildings using reliable, convenient, scalable, cyber-hardened technology.

What’s it like to work at Brivo

Unlimited PTO

Casual and open work space

Flexible work schedules

Motivated coworkers in a collaborative and entrepreneurial environment

Fun team environment with games, potlucks, parties and happy hours

Agile/scrum development

Company sponsored team outings","Bethesda, MD 20814",Data Engineer,True
37,"ContractJob Title: Big Data Engineer

The Retail Distribution Analytics (RDA) team's mission is to dramatically transform Client's current retail distribution strategy and performance around the globe, by enabling the sales team to deepen and expand their relationships with Financial Advisors & Home Offices by leveraging data to provide thoughtful analytics and by delivering cutting edge technology tools & applications to the sales force. The team currently consists of ~50 people primarily located in New York, Princeton and San Francisco.

We are seeking a talented programmer/analyst with strong data management and ETL development skills to join our team. The candidate will work on all aspects of the data management process including the acquisition, onboarding, attribution, and reporting of newly acquired data sets. Opportunities to migrate existing data acquisition and onboarding processes to new / strategic sources and protocols will also be a priority. The work is expected to combine some project management and basic reporting in support of the team's overall goals to provide high-quality data that supports analytics and the sales process in a timely fashion.

Top technical / programming skills – in particular SQL, Python, Hadoop, Java and Excel – are a must. Given the highly execution-focused nature of the work, the ideal candidate will roll up their sleeves to ensure that their projects meet deadlines and will always look for ways to optimize processes in future cycles. Experience in project management and laying out and communicating a project schedule are also important.

Drive the onboarding and attribution of newly acquired data sets in to our database environment; manage all aspects of the loading process; monitor attribution throughout the process and identify ways to optimize the process each cycle Identify, investigate, and resolve data discrepancies by finding the root cause of issues; work with partners across various cross-functional teams to prevent future occurrences Proactively look for opportunities to optimize the data loading structure and develop new approaches to improve the onboarding and integrity of the data Provide basic reporting and respond to inquiries asking for insights about the data sets from senior stakeholders, in a timely fashion Maintain a project management tracker and assist with the communication of the team's status and deliverables on a regular basis

Skills:
Bachelor's Degree, preference for Economics, Finance, or Computer Science or related fields preferred

4-9 years of experience developing in the HortonWorks Hadoop stack.

Python, Hadoop, pySpark using Apache Spark, SQL Server SQL; Sybase

1-4 years of experience in the financial services sector, preferably at a top-tier asset manager, investment bank, or management consulting in a data-driven role, or at a technology firm Strong cross-functional experience in data management, onboarding and attribution of large data sets Ability to interpret data to help in strategic decision making Demonstrable problem-solving, quantitative, and analytical skills; strategic and creative thinking Track record of strong performance History of effective multi-tasking (manage long complex projects alongside urgent high-priority tasks), expectation setting, escalation of issues, where appropriate Experience with financial markets, working knowledge of the asset management and/or financial services industry, preferred Solid proficiency in all Microsoft Office applications; expert Excel skills Strong organizational, planning, and coordination skills Ability to clearly articulate and present ideas both in writing and verbally; to senior management as well as outside audiences Experience in working on long-term projects and iterative development cycles where constant improvement is expected Exceptional project management skills with attention to detail Collegial orientation; relationship-builder who is solutions-oriented Comfortable interacting with all levels of management Able to work effectively under pressure and in a rapidly changing environment in order to meet deadlines Focused attention to detail and high standards for quality and accuracy in his/her work product Professional, positive demeanor Collaborative, team-oriented, service-oriented Strong interest / curiosity about asset management industry

Keywords:
Education:
Bachelor's Degree, preference for Economics, Finance, or Computer Science or related fields preferred

Skills and Experience:
Required Skills:
MULTITASKING

ASSET MANAGEMENT

PROBLEM-SOLVING

DATA MANAGEMENT

MICROSOFT EXCEL

Additional Skills:
DATA ACQUISITION

DATABASE

APACHE SPARK

MANAGEMENT CONSULTING

ENGINEER

EXCEL

FINANCIAL SERVICES

HADOOP

JAVA

MICROSOFT OFFICE

PROJECT MANAGEMENT

PYSPARK

PYTHON

QUANTITATIVE

SQL

SQL SERVER

SYBASE

DATABASES

ETL

FINANCE

FINANCIAL MARKETS

MS SQL SERVER

RETAIL

RETAIL DISTRIBUTION

RETAIL MARKETING

SALES TEAM

#indny","Princeton, NJ",Big Data Engineer,True
39,"Req ID: 32804

At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.

NTT DATA Services currently seeks a Senior Big Data Engineer to join our team in Durham, North Carolina (US-NC), United States (US).

The Senior Data Engineer will be responsible for finding trends in datasets and developing workflows and algorithms to help make raw data more useful to the enterprise. He or she will also be responsible for creating data acquisition strategy and develops data set processes.

You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform.

Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus.

Primary Responsibility
As a Senior Data Engineer, you must be an expert with Big Data technologies like Hadoop, Hive, Hbase, Spark, and various AWS technologies.
You will architect and drive the build out of next generation data platform.
You will build reusable code, with the ability to scale with very large data volumes.
Everything you build will need to scale and perform.
Define and lead the frameworks for compliance with data management standards for emerging technologies (streaming platforms, cloud integration, etc.)
You will drive the build out of next generation data platform.
General Responsibilities
Be part of a core team leading migration to new data technologies for unstructured, streaming and high volume data.
Overall accountability of your work and adopt established data management frameworks to prevent data lakes from becoming data swamps.
Provide senior level technical consulting to application development teams during application design and development for highly complex or critical projects
As a hands-on engineer, you will influence all architecture decisions.
Job Functions:
Provide technical leadership to build and implement data and big data solutions
Articulate pros and cons of various technologies, platforms and tools.
Demonstrated work experience with distributed, scalable Big Data programming model and technologies such as Hadoop, Hive, Pig, etc.
Demonstrated hands-on experience with at-least one of the major Hadoop distributions (Preferrably Cloudera).
Deep technical Expertise in Hadoop eco system components.
Experience designing, developing and implementing online-machine learning libraries.
Minimum Experience:

8+ years’ experience in dimensional data modeling, ETL development, and Data Warehousing.
Business Consulting-ETL Processes: 5+ years
BIG Data ( BD)-Apache Hadoop (HDFS)-Base/Hive/Pig/Mahout/Flume/Scoop/MapReduce/Yarn: 3-5 years
Cloud Dev and Migration-AWS-Analytics/DW/Redshift : 1+ year

Travel: Need locals
Degree: Bachelors in Computer Science or equivalent work experience

Desirable Skills:
Master’s degree in Information Systems or a related field.
Hadoop/Cloud Developer Certification
Experience deploying applications in a cloud environment; ability to architect, design, deploy and manage cloud based Hadoop clusters
Exposure to cloud infrastructure
Experience with Redshift and other AWS services.

This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.

About NTT DATA Services

NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.

NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.

NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.","Durham, NC",Senior Big Data Engineer,True
40,"Job Details
Description
We are looking for experienced Data Engineers to work on building, operating, and scaling next generation data platforms and tools that will power data-driven capabilities throughout the entire organization, spanning areas such as business intelligence and reporting, data science, and data analytics.

Here at Ultimate Software, we truly put our people first. We strongly believe in teamwork, and we encourage and trust our people to reach higher, learn more, and live up to their potential. Ultimate is ranked #1 on Fortune's “Best Places to Work in Technology” for 2018 and #3 on the “100 Best Companies to Work For” list in 2018. Ultimate is also ranked #1 on the Fortune’s “100 Best Workplaces for Millennials” for 2018 and #3 on its ""Best Workplaces for Diversity” list for 2017.

Primary/Essential Duties and Key Responsibilities:
You will be responsible for hands-on development of frameworks and applications for large-scale data processing
You will be expected to influence technical direction for the team, leveraging your prior experiences and helping evaluate emerging technologies and approaches
You will help bring engineering maturity to a growing team that is at the center of a lot of critical initiatives for the company
Required Qualifications:

Significant experience doing hands-on development
Understanding of distributed systems driving large-scale data processing and analytics
Familiarity or expertise with technologies like Hadoop (and related ecosystem), Spark, Kafka
Experience shipping production code and working on real running systems at scale
Experience building operational data pipelines
Ability to work both collaboratively and autonomously
Ability to communicate effectively (listening, presenting, and questioning)
Strong organizational, written, and communication skills
Preferred Qualifications:
Exposure to large-scale stream processing systems
Deep expertise with one of the major Hadoop distributions (Hortonworks, Cloudera, etc.)
Experience working in hybrid private / public cloud environments
Experience working with Java-based technologies and frameworks
Development experience with one or more of Java, Scala, Python
Experience working with data science
Experience working with BI and data warehousing tools
Experience working with enterprise data, where security is paramount and data governance is critical
Experience with event-driven architectures
Experience working with Agile methodologies
Degree in Computer Science or a related technical field involving coding (e.g. physics or mathematics), or equivalent practical experience
Physical Requirements:
No unique physical demands are required for this job.
Travel Requirements:
Limited travel upon request (less than 5%)


This job description has been written to provide an accurate reflection of the current job and to include the general nature of work performed. It is not designed to contain a comprehensive detailed inventory of all duties, responsibilities, and qualifications required of the employees assigned to the job. Management reserves the right to revise the job or require that other or different tasks be performed when circumstances change.

Ultimate Software will reasonably accommodate employees with disabilities as defined by the Rehabilitation Act of 1973, the Americans with Disabilities Act (ADA) and other appropriate statutes.","San Francisco, CA",SENIOR DATA ENGINEER,True
41,"Job Description
Amazon Legal is looking for an outstanding, analytical, and technically skilled BI Engineer to join our Legal Technology team. This position will be responsible for building and supporting business analytics and reporting for the Amazon Legal department as a whole.

This role requires an individual with excellent statistical and analytical abilities, deep knowledge of business intelligence solution and, data engineering practices as well as outstanding business acumen and an ability to work with a variety of teams across Amazon Legal. The successful candidate will be a self-starter comfortable with ambiguity, have a strong attention to detail, an ability to work in a fast-paced environment, and be driven by a desire to innovate in this space. In this role, the right person will be able to revolutionize the department’s monthly reporting processes and develop insightful and meaningful metrics and reports that enable decision making at all levels.

Primary Responsibilities
Support a platform providing secure access to departmental reporting across all areas of legal practice.
Interface with business customers and delivering complete BI solutions.
Model data and metadata to support ad-hoc and pre-built reporting.
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
Continually improve ongoing reporting and analysis processes, automating or creating self-service options.
Develop analytical tools to provide transparency into spend
Create operational scorecards based on key metrics, leveraging multiple different data sources to build a cohesive story
Synthesize and translate complex findings into relevant and actionable insights
Basic Qualifications
5+ years of relevant experience in a business intelligence role, including data warehousing and business intelligence tools, techniques and technology, as well as experience in diving deep on data analysis or technical issues to come up with effective solutions, experience in analytics, business analysis or comparable consumer analytics solutions.

Bachelor’s degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline or equivalent industry experience.

Excellent knowledge and Expertise with SQL, OLAP, and Relational, NoSQL & Multi-dimensional Databases.

Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets.

Knowledge and direct experience using business intelligence reporting tools like QuickSight or Tableau.

Proven ability to look at solutions in unconventional ways and see opportunities to innovate.

Excellent verbal and written communication and interpersonal skills to convey key insights from complex analysis in summarized business terms and an ability to effectively communicate with technical teams.

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation
Preferred Qualifications
Experience with legal related data, such as litigation, legal billing, patents, IP, or related fields.

Knowledge of AWS Infrastructure, Redshift, NoSQL databases & associated technology.

Candidates with a Ph.D. or MS degree in a relevant field are preferred.","Seattle, WA",Data Engineer (Legal),False
42,"$95,000 a yearFluz Fluz is aiming to disrupt the global retail shopping experience through various consumer touch points.We are a FinTech company sitting in the intersection between blockchain, social media, payments and retail.We are a fast-moving company that has experienced rapid growth.Over the past year our team has grown from 5 members to over 45 and we are operating internationally.We are about to launch our US consumer cash back ap.We are looking for a Data Engineer that will be responsible to set up the systems that will manage all of our transactional and user data.They will own the process of designing, implementing and maintaining the systems used to collect and organize all data touch points.This position will report directly to the CEO and the company’s technical advisors.ResponsibilitiesWork with management and operations team to architectConnect all data points to a centralized point and configure all automated processes around thatOwn the process of selecting and implementing the reporting toolsIntegrate all new data touch points in the the centralized reporting toolsConfigure customer data into enterprise grade CRM toolsCreate custom triggers with datasets to automate certain processesMonitor flow of data from various touch-points to central system and upkeep systemsHelp to monitor the production systems on a daily basis and respond immediately to any breakagesRequirementsExperience building out automationExperience with CRM system configuration and customizationExperience with Tableau or a similar systemLight understanding of distributed cloud and micro service architectureExperience with AWSExperience with NoSQLAbility to breakdown and articulate solutions to complex problemsKnack for writing clean, readable and maintainable codeWillingness to learn and teachDesire to be involved in product definitionA love for automationStrong sense of urgency in a professional settingCompensation$95,000 plus annual bonus and other benefits: Competitive Salaries, Team Socials, Paid vacation days, Free coffee, tea and snacks, Dog friendly office and Insider accessYou Will Work WithMaurice Harary, Co-Founder. Maurice is one of Fluz Fluz’s Co-Founders. He leads business development efforts and product view. A businessman at his core, he loves to build & grow companies. His personal hobby is building the Fluz Fluz company.Andreas Antrup, Board Advisor. Andreas is one of Fluz Fluz’s board advisors with a technical focus. When he is not advising Fluz Fluz, he is managing a team of 400+ AI developers at Europe’s largest fashion retailer, Zolando. Andreas is focused on bringing the same best practices to Fluz Fluz.Ragha Srinivasan, Board Advisor. Ragha acts as an involved board advisor for Fluz Fluz. He supports the technical infrastructure development for the application. When he is not advising Fluz Fluz he is leading the AI team at Youtube.Eric Johnson, Product Manager. Eric has been with Fluz Fluz since its inception. He has witnessed it evolve into everything it is today and has a clear vision of where the company will be going. He helps share that vision with all members of the team and implement the best user journey throughout the business.EEOCAll qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.Learn more about us at FluzFluz.comSeniority LevelMid-Senior levelIndustryMarketing and AdvertisingFintechMobile ApplicationsEmployment TypeFull-timeJob FunctionsData mangementJob Type: Full-timeSalary: $95,000.00 /yearExperience:Database Management: 1 year (Preferred)Tableau: 1 year (Preferred)Data Engineering: 1 year (Preferred)CRM Systems: 1 year (Preferred)","New York, NY 10036",Data Engineer - Fintech Start Up,False
43,"Position Summary:
The New York County District Attorney's Office (DANY) has an opening for a Data Engineer in its Human Trafficking Response Unit (HTRU). HTRU is responsible for the investigation and prosecution of sex and labor trafficking cases in Manhattan. It consists of specially-trained ADAs, analysts, investigators, and social workers who focus on developing proactive evidence-based prosecutions, connecting victims to services, and targeting individuals and businesses that permit trafficking to occur. HTRU works closely with other areas of the Office, law enforcement agencies, and non-profit partners to target the business mechanisms of trafficking and disgorge trafficking-related proceeds. In this position the Data Engineer is responsible for providing high-level confidential technical investigative support to HTRU.

Responsibilities include but are not limited to:

Work with those in your unit to understand case workflow, and automate and streamline investigative steps wherever possible.
Develop technological infrastructure for your unit, in the form of internal web applications or otherwise.
Develop tools to identify trends, behaviors, and patterns related to cases in your unit.
Work on cases directly wherever programming can speed up an investigation or bring in new insights.
Collaborate with others throughout the office to work on DANY-wide applications and projects.
Partner with outside agencies or entities on data-driven projects.
Perform other related and necessary tasks as needed.

Qualifications:

Knowledge of Python (required)
Proficiency in Microsoft SQL or other RDBMS experience.
Proven experience writing high-quality code, demonstrated through academic projects, work experience, and/or an active GitHub account.
Experience developing web applications via Flask, Django, HTML, CSS, and Javascript.
Experience working with complex datasets.
Ability to communicate effectively with a broad spectrum of internal and external partners.
Motivation to find ways to use technology to improve the way cases are brought in and investigated within your unit.

Educational Requirements:

Bachelor's degree required.
Graduate degree in related field is preferred.

Commitment:

One (1) year commitment to hiring unit.

The New York County District Attorney's Office is an Equal Opportunity Employer","New York, NY",Data Engineer,False
44,"Job Description

The Enterprise Asset Management (EAM) Team is building the world’s most comprehensive and effective EAM program and asset management database. This is a huge challenge considering the considerable asset inventory and rapid growth of Amazon. To accomplish our goals, we are interested in finding top candidates that are ready to take on challenges, obsess over customers, and lead change. The EAM Program/Team administrates the EAM Program via our software platform, master data management strategy, configuration and solutions, deployment, BI Reporting and KPI development, training and documentation. The EAM program is being deployed in the Fulfillment Centers, Data Center, and EU networks across many stakeholder groups.

We are looking for a successful and outstanding MSSQL DBE/ DE to help manage database support and operations for our highly complex and mission-critical systems. These systems are critical to the Enterprise Asset Management (EAM) program and other related critical Amazon applications. The DBA will be well versed in MSSQL, Data Warehouse, and integration technologies and will perform administration and engineering for multiple production databases. The ideal candidate should have experience in the architecture, design, and implementation of large production systems with high transaction volumes. The candidate will also be responsible for fast-paced complex distributed database environments supporting with large databases and complex integrations. Successful candidates will have the ability to rapidly troubleshoot complex technical problems under pressure.


Basic Qualifications
Bachelor's degree and/or Master's degree in Computer Science with 5+ years of industry experience
5+ years of experience as a MSSQL DBA or DE in a high traffic, transactional environment.
Experience with high-volume OLTP & OLAP database systems.
Performance tuning of MSSQL, PLSQL, SQL processes and queries.
Strong knowledge of systems architecture, loosely coupled and distributed systems
Day to Day Experience supporting highly scalable, distributed, service oriented systems
Ability to work cooperatively with software engineers and system administrators
Experience with system integrations, ETL, data warehouse, queries and stored procedures
Strong understanding of fundamental relational database design, data warehouse and Business Intelligence best practices, methodologies and terminology.
2 Years working experience on Infor EAM software package, including: software configuration, dashboard development using Cognos, Business Objects or SSRS reporting tools.
One full cycle of software implementation experience is required.
Superior communication and analytical skills, including strong ability to identify and solve ambiguous problems.
Strong knowledge in project development methodology, clear verbal and written communication skills, has the ability to handle daily activities in a dynamic environment and drive deliverables.
Preferred Qualifications
Master’s Degree in Computer Science or related field with 10+ years industry experience
4 Years’ Experience working with Infor EAM, Ion, and other related EAM Packages
High attention to detail and proven ability to manage multiple, competing priorities simultaneously.
5 years’ experience in developing BI solutions using Microsoft SQL Server Analysis Services (SSAS) and Reporting Services (SSRS).
Ability to build and maintain strong working relationships with DBA and application teams
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation","Seattle, WA",Data Engineer - EAM,False
45,"DATA ENGINEERAbout UsERGO Interactive is a market leader in data-driven, hyper-personalized, adaptive algorithmic email journey design and deployment. It’s a mouthful, but very cool stuff that is driving enormous returns for our blue-chip clients. The best way to explain it would be to envision you’re reading your favorite magazine, and with every page you turn and every issue you read, it becomes more relevant to you. We call it Content Science™, and we’re looking for an exceptional Data Engineer to join our team.Please apply with your resume and a thoughtful cover letter telling us why you're the best fit for the job.DescriptionThe Data Engineer is responsible for developing, maintaining, testing and evaluating data solutions systems in order to load, transform and query large data sets from a variety of sources.Essential FunctionsWork with our Data Scientists to create data-driven insights and reports for senior managementAssist Data Scientists in developing processes to migrate data from various formats and data sources(Oracle, MySQL, Sybase, flat files, etc.) to database architectureDesign and implement tools to analyze very large data set of raw dataProcess unstructured data into a form suitable for analysisPerform ad hoc data updates and other follow-on data servicesAutomate recurring analytics reports for clientsQualificationsBachelor’s degree or higher in Computer Science, Information Systems or related fieldHighly proficient in SQL (MySQL, Postgres)Expertise with relational databases (implementation, queries, modeling)Working knowledge of distributed data stores (Cassandra, Redshift, Hadoop, HBase)Proficient in scripting language of choice (Python, R, PHP, Ruby)Expertise with optimizing query performanceFamiliarity with NoSQL technologies (Mongo DB, DynamoDB)Deep understanding of data structures and schema designDetail-oriented, proactive problem solving skillsPerksA comprehensive career development program and trainingGym membershipBagel FridaysSummer FridaysOffice bar cart, produce, healthy and unhealthy snacks, NespressoPool tableCompetitive vacation allowanceCompany-paid office shut-down between Christmas and New Year’s DayHealth, vision & dental benefits401k planReferral bonus programPet-friendly officeFun company outings and eventsVery cool industrial-style Greenwich Village studio spaceThe best perk? Our people. If you’re ready to join us, apply now!Job Type: Full-time","New York, NY",Data Engineer,False
46,"The role:

We’re looking for someone to join our data team who relishes the opportunity to work in datasets with millions to billions of records. At RISC Networks we are passionate about solving complex problems, transforming data into information, and creating real value for our customers. We are looking for a full-time Junior Data Engineer located in our downtown Asheville, NC office. In this role you will get to work with some of the largest organizations in the world, helping them understand their IT environments, and bringing context to the decisions they are making.

Gotta have it:

SQL Database experience
You’re a natural problem solver comfortable facing new challenges, especially within IT
You have the ability to meet deadlines and adjust to changing priorities
You’re a technologist who stays current with trends
Love it:

Programming experience with scripting languages, Perl, Python, shell
familiarity with version control systems like Git
experience with computer networking
Like it:

Experience AWS Glue and other services
Experience with automated unit testing
Culture

You have a can-do, action-oriented approach to your work
The idea of working for a small but growing company is appealing to you
You love to learn new things
Your meme game is strong
About RISC Networks:

Founded in 2007, RISC Networks strives to unlock business potential by delivering more meaningful data analysis. Created by engineers, we understand the needs of IT leaders. We endeavor to provide actionable intelligence that IT professionals can use to plan for IT change.

RISC Networks' mission is to improve the performance of our partners' and customers' businesses and make a difference in their lives by developing technology and research that supports an actionable approach and perspective for their IT business.

We’re nestled in downtown Asheville, NC in the heart of the Blue Ridge Mountains. There is a lot of great beer, food, and music as well as mountain biking, kayaking, hiking, etc. Asheville consistently ranks in national polls of top 10 places to visit.

Compensation:

This is a salaried position. Compensation is based on experience and skill set.

401k, 100% health care, and Ping-Pong table provided.","Asheville, NC 28801",Junior Data Engineer,False
47,"$20,000 - $80,000 a yearPart-time, InternshipWe are seeking instructors of Big data and Deep Learning! Inspire beginner data scientist and deep learning engineers to commit to building their career! You’ll lead technical workshops teaching the basics of data science or deep learning!Responsibilities:- Lead great events as an instructor or speaker- Collaborate with TAs that will help you deliver a great experienceRequirements:- 0-5 years experience as a data scientist or deep learning- Real project experiences - OPT/CPT students are welcomed, sponsor H1B for excellent candidates - Fluent in both English and ChinesJob Types: Full-time, Part-time, InternshipSalary: $20,000.00 to $80,000.00 /yearExperience:CPT Coding: 1 year (Preferred)","San Jose, CA 95112 (Downtown area)",big data engineer,False
48,"Fusion is currently seeking an entry-level Data Engineer to join our fast-growing software development team. The data engineering team is responsible for developing and maintaining the data processes for our clients. Tasks will include import and export of data, data migration from legacy systems, interface development, reporting development, as well as maintenance and support for all our existing data projects.

About Us:

Fusion was founded in 2006, and has since become a major disruptor in the Corrections and Public Health sectors of government. Recognized by INC magazine as one of the fastest growing private companies in the United States, Fusion is looking to expand its hand-picked team to include a candidate through this job placement.

From a company culture perspective, we are a vibrant and young group who have come together to be leaders in healthcare IT and software for government agencies. The office provides open working spaces, several meeting areas as well as a café & gym on premise.

Because of the niche Fusion belongs in as well as the business model we operate with, we are looking for not only skilled and qualified candidates, but also candidates who have an outgoing personality and fit well with our other team members.

To date, Fusion has a phenomenal retention of our team members. Our fundamental belief is that employee satisfaction is critical to achieving our mission, so we provide competitive compensation, professional development, career advancement opportunities, and a supportive team-based atmosphere. We also provide a full range of health related benefits, including medical, dental, vision and 401K. And we offer work-life enhancements like flexible hours, business casual dress code, and an easy-going corporate structure.

Fusion has been recognized by Inc. 5000 List of Fastest-Growing Private Companies – thanks to the tireless efforts of our team. If you are a talented professional and our mission speaks to you, please speak to us!

Job Roles:

Communicate with partner companies to develop and support bi-direction data communication.
Migrate data for new clients from old systems to new systems.
Create meaningful insight with data to help our customers meet compliance standard.
Develop reports to allow customers to view their data in the format they request.
Meet with government clients to understand their environment and work with Project Managers to determine the optimal solution for their needs.
Work with Project Managers to create and execute a technical implementation plan for larger client roll outs.
Work closely with product management to understand current and new product features so they may be implemented correctly.


Required Experience:

Javascript
SQL (Plus if it is SQL Server)
Crystal Reports or any comparable reporting tool
C#
Windows Network Experience
Familiarity with most common structured or delimited file formats (CSV, XML, JSON, etc)
Familiarity with data transfer methods (sFTP, HTTP, TCP, SOAP, REST, etc)
Qualifications:

Bachelor’s Degree in Computer Science or any IT-related field.
Working Hours:

Standard hours for this role are M-F start between 8 and 9, expected to put in 8 hours.
Willingness to provide weekly on-call coverage, rotationally.
Additional Notes:

It is not expected that applicants have any familiarity with Fusion’s proprietary applications, GE Healthcare software, or Corrections/Public Health business processes. Qualified candidates will be able to demonstrate experience in this role as well a demonstration of working well with the Fusion team.
This is an On-Site, Full-Time salaried position.","Woodbridge, NJ 07095",Junior Data Engineer,False
49,"Data Analyst


Who We Are
Murmuration seeks meaningfully improved education outcomes for kids by providing information, infrastructure, and support for education-related public advocacy and community building efforts.
One of Murmuration's key initiatives, m{insights, is an online platform that allows users to aggregate data from a wide variety of sources such as partner organization’s member data, publicly available data, consumer data, voterfile data, and more. This data can then be used by partners to activate and expand their supporter bases effectively and mobilize them for sustained political change. We also use this data to build predictive machine learning models that improve our partners’ efficiency. Murmuration has the unique opportunity to build, with partners, a massive dataset that can leverage creative data science to profoundly change the way that advocacy efforts and service providers engage their bases.
About the Position
The Data Analyst has two main responsibilities. First, listening to our partners and staff to understand the questions and problems they have, that can be answered with data. Second, sourcing the answers and solutions to those questions and problems and presenting them in a clean, concise way for both technical and non-technical audiences to understand. To do this you must be a creative multi-disciplinarian who will work with all parts of the organization; from analyzing and visualizing the data with other analysts, to sourcing new data with the data manager and data engineer and sitting in on partner calls with our partnerships team.
If you are interested in learning more about data, education, or politics, this is a great chance to gain hands-on experience with a small but quickly growing organization.
The Data Analyst will:
Work with the partner solutions team to understand the key questions and problems facing our partners, in particular those that could be answered/tracked with data
Work with each part of the Murmuration team to identify internal reports that could improve how we do business
Create analytic reports, graphs and dashboards for Murmuration and for partners answering their key business questions
Be conversant in basic statistics and be able to perform our most common analytic operations using tools such as Python, SQL, Tableau, Excel
Become an expert in data visualization solutions and learn when and where to apply each solution
Build and maintain a style guide for Murmuration’s data visualizations and reports
Document business requirements across all Murmuration’s products
Work with the Data Manager to craft data acquisition strategy for any key data we are
missing that lets us address the above questions and problems

Candidate Profile
The Data and Analytics teams are highly collaborative, friendly and hard-working, and we are looking for a Data Analyst who embodies those values.
The ideal candidate is:
Data-driven
Creative
Flexible
A self-starter
A constant learner
A problem-solver
An effective communicator
Team-oriented and collaborative
Able to multi-task and prioritize work effectively
Responsible and able to meet deadlines
Passionate about education and/or politics (or interested in learning more)
*The ideal candidate will have a Bachelor’s degree in political science, sociology, mathematics, statistics, education or other related field.

We are relying on you to be our translator, storyteller, and problem solver. You should always be listening and reading between the lines to try and identify problems our partners and staff are having, which are often different from the problems they are describing. You are comfortable being highly reactive to solving the questions of today while also proactively keeping an eye on longer-term questions of strategic interest. We don’t expect you to solve every problem, but you should be able to identify them, and then work with the rest of Murmuration to craft a solution. You will need to walk the fine line of knowing enough of the technical details of our solutions to be factually correct without spending so much time on it that it detracts from the primary goal of presenting these solutions as clearly and cleanly as possible.

Location
This position will be based in New York, NY, and requires minimal travel.
Compensation
The Data Analyst position is a full-time, salaried position with a comprehensive benefits package. Compensation for this position is commensurate with experience.

An Equal-Opportunity Employer with a Commitment to Diversity
Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","New York, NY 10005 (Financial District area)",Data Analyst,False
50,"ArcBlock is looking for an experienced Data engineer who is passionate about building great products in the context of a diverse, multi-functional and independent team.

Our systems need to meet remarkably high standards of quality, performance and reliability, operating around the clock on a massive scale. If you are a talented, detail-oriented and enthusiastic professional who is passionate about new technologies including big data, computer vision, and machine learning, then this is the right team for you.

In this Data Engineering role, you will work closely with product engineering teams and data scientists to tackle problems in on-chain/off-chain data mining, anomaly detection, personalization, search, ranking, etc. You will build end-to-end data solution for the company, from analytics event definition, data collection, ETL jobs, all the way down to key metrics visualization and matching learning model serving. You will work closely with backend, client and product/business team to delivery calibrated internal metrics/learnings and best user experience for our customers and users.

Who we are?

ArcBlock Inc.is a global leader in the blockchain revolution. We are building the first-lever Blockchain 3.0 ecosystem designed to reimagine how blockchain apps and services are built. ArcBlock is combining current and next-generation blockchain technologies with cloud computing and token economics that remove today’s technological limitations and deliver a complete framework to developers that empowers them to build blockchain decentralized applications service. We’re excited to announce that the company is growing and we are looking to add to our industry-best talent and are looking for people who are excited about the future.
Why work here?

A fast-growing team of passionate people and a tech-first / user-centric blockchain business
Autonomy and end-to-end ownership
A unique culture that is driven by our common principles
Competitive pay,Token Incentive Plan, full medical, catered lunch, and your choice of hardware
Big opportunity for internal growth (weekly BBLs, monthly retrospective, bootcamp every 6-8 weeks, tech conferences,opportunity to work from US and China, and travel around the world etc.)
Essential Duties and Responsibilities

Primary focus on delivering end-to-end solutions
Build and work with various data pipelines
Build the team capability for continuously multi-var A/B testing
Build machine learning models and deliver the models to Backend team
Work closely with Backend team to build anomaly detection engine for both customers and our own infrastructure
Basic Qualifications

BSc/MSc in Computer Science or related field
3+ years of industry experience, at least two of which in a data infrastructure related role
Proven track record of building and operating scalable, flexible and always-on data pipelines
Strong knowledge of Scala and Apache Spark
A passion for shipping production quality code with good test coverage
Ability to quickly evaluate and make trade-off decisions on adopting emerging technologies
An understanding of supervised and unsupervised machine learning methods
Preferred Qualification

Familiarity with blockchain concepts and algorithms
Familiarity with Amazon AWS tools and technologies
Demonstrated leadership abilities in an engineering environment in driving operational excellence and best practices
The ability to take raw product requirements and develop software solutions and designs, to bring them to life
Excellence in technical communication with peers and non-technical cohorts","Bellevue, WA",Data Engineer,False
51,"Data EngineerJoin our seasoned team of data engineering professionals to take your career to the next level. As a Data Engineer, you will face the most complex and up-to-date challenges by helping our clients address their complex data management, reporting and analytical challenges using Big Data Technologies (Hadoop, Columnar/ NoSQL DBs, Python) and Cloud Services (AWS, Azure, GCP)Minimum Requirements5+ years of Experience in working with global clients in data management and business intelligence using Python and Hadoop for data managementExperience in writing efficient data management code to support high performance/ parallelization requirements in complex environments such as: distributed and high performance environments, cloud and enterprise solutionsExperience in working closely with clientsBachelor’s Degree in technology disciplinePreferred RequirementsStrong Linux and Windows administration skillsCertifications in cloud platform (AWS/ GCP/ Azure)Strong Python programming and object oriented programming skillsExperience in Hadoop based data managementMaster's level educationSkills & CapabilitiesWriting complex and high performance data pipelinesAbility to automate end-to-end production data pipelines using automation technologies such as Airflow, AWS EMR, Hadoop, AWS Lambda etcUser, data and security administration including integration with enterprise directories and data encryptionStrong understanding of cloud server environmentsStrong understanding of SQL (2+ years), programming languages and other scripting languages (e.g., Python, Perl)Strong communication and project management skillsAbility to work effectively in a global teamJob DutiesCreate high performance data pipelines to support complex data integration workflowsDevelop automation programs, shell scripts and other utilities to support overall goal of end to end automationProduces a weekly status report documenting project health and progressSupport process flow analysis and ETL process redesignDocument and gain approval for customer requirements definitionParticipate in completion and implementation solution documentationParticipate in user acceptance testing efforts as neededParticipate in training design, documentation and delivery efforts in concert with other project team membersParticipate in internal projects as requiredJob Type: Full-timeJob Type: Full-timeExperience:Hadoop: 1 year (Preferred)AWS: 2 years (Required)Shell Scripting: 4 years (Required)Python: 2 years (Preferred)SQL: 4 years (Required)","Research Triangle Park, NC",Senior Data Engineer,True
53,"Job Description Summary
The Data Engineer contributes to the vision, development and administration of Hadoop enabled infrastructure as a core service to all business functions.
Job Description
Responsibilities
Support the administration of on premise and cloud based Hadoop clusters.
Make information available to large scale, next generation, predictive analytics applications.
Work with senior staff to build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process).
Qualifications
Bachelor’s degree in computer science, software engineering, or related field.
One year of ETL experience with Hive/Impala – Hue, ETL and advanced SQL programming.
Understanding of the Hadoop eco-system (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive).
Understanding of data warehousing and BI concepts, including hands on experience building ETL/ELT data pipelines.
Preferred Qualifications
1+ years of Java and/or Python development experience.
1+ years’ experience in the data warehouse space.
1+ years’ experience in custom ETL design, implementation and maintenance.
1+ years’ experience in writing SQL statements.
Background working in cloud platforms such as Azure or AWS.
Behavioral & Leadership Competencies
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Communication skills including the ability to identify and communicate data driven insights.
Ability in managing and communicating data warehouse plans to internal clients.
Attention to detail and results oriented, with a strong customer focus.
The ability to work within a team environment.
Problem-solving and communication skills.
Our Culture
At Transamerica we promote a Future Fit mindset. What is a Future Fit mindset?
Acting as One fosters an environment of positive collaboration.
Accountability allows us to own the problem as well as the solution.
Agility inspires new ideas, innovation and challenges the status quo.
Customer Centricity encourages an above and beyond approach to our customer.
Working Conditions
Office environment.","Denver, CO 80221",Data Engineer,True
54,"ContractData processing/ big data engineer -14376 We’re a science and technology company with a very human mission. Our client is looking for a Big Data Engineer for their San Jose, CA Location. This is a contract position.Responsibilities: Build real-time big data pipelineNot doing analytics (not front end. This is BE)Required Skills:Back end JavaSpark - plusEMR- plusstream processing -plus4 -10 yrs. big data processing experienceDistributed systems experienceDatabase can be Hadoop, Cassandra, mongo and any NoSQL dB - does not matter which oneimportant note - the technology does NOT matter.looking for someone who has worked in building data platforms. ie : ingesting, housing data the data ----( so that other people can use it for analytics)Position logistics: 6-12 Month ContractWhat’s in it for You: Competitive pay for contact with excellent stability and opportunity for growth.Great opportunity to enhance or solidify your skills with an excellent, diverse and experienced team.Work on multiple projects allows you to get experience with a variety of technologies and teams.About Maxonic: Since 2002 Maxonic has been at the forefront of connecting candidate strengths to client challenges. Our award winning, dedicated team of recruiting professionals are specialized by technology, are great listeners, and will seek to find a position that meets the long term career needs of our candidates. We take pride in the over 5,000 candidates that we have placed, and the repeat business that we earn from our satisfied clients.Interested in Applying?We can’t wait to see your resume. Please apply below with your most current resume and anything else you’d like us to know about you – commute preferences, desired work environments, etc. We promise to get back to you within 24 hours. Or you can feel free to contact Mani at 408-739-4900 x 125.Key Words: Data Pipeline, Big Data, Spark (Plus), Back endJob Type: Contract","San Jose, CA",Data Processing/ Big Data Engineer,True
55,"$101,000 - $138,000 a year (Indeed Est.) ContractData processing/ big data engineer -14376 We’re a science and technology company with a very human mission. Our client is looking for a Big Data Engineer for their San Jose, CA Location. This is a contract position.Responsibilities: Build real-time big data pipelineNot doing analytics (not front end. This is BE)Required Skills:Back end JavaSpark - plusEMR- plusstream processing -plus4 -10 yrs. big data processing experienceDistributed systems experienceDatabase can be Hadoop, Cassandra, mongo and any NoSQL dB - does not matter which oneimportant note - the technology does NOT matter.looking for someone who has worked in building data platforms. ie : ingesting, housing data the data ----( so that other people can use it for analytics)Position logistics: 6-12 Month ContractWhat’s in it for You: Competitive pay for contact with excellent stability and opportunity for growth.Great opportunity to enhance or solidify your skills with an excellent, diverse and experienced team.Work on multiple projects allows you to get experience with a variety of technologies and teams.About Maxonic: Since 2002 Maxonic has been at the forefront of connecting candidate strengths to client challenges. Our award winning, dedicated team of recruiting professionals are specialized by technology, are great listeners, and will seek to find a position that meets the long term career needs of our candidates. We take pride in the over 5,000 candidates that we have placed, and the repeat business that we earn from our satisfied clients.Interested in Applying?We can’t wait to see your resume. Please apply below with your most current resume and anything else you’d like us to know about you – commute preferences, desired work environments, etc. We promise to get back to you within 24 hours. Or you can feel free to contact Mani at 408-739-4900 x 125.Key Words: Data Pipeline, Big Data, Spark (Plus), Back endJob Type: Contract","San Jose, CA",Data Processing/ Big Data Engineer,True
56,"Fetch Rewards is a fast growing technology company that is innovating and changing how consumers fulfill their grocery needs. We are headquartered in the heart of downtown Madison with another office in the Chicago Loop area. Our company currently provides two mobile applications in the mobile grocery market - Shop Fetch and Fetch Rewards - in both the Android and iOS platforms. The mission of Fetch is to provide easy access to the information and technology necessary for people to enjoy every stage in the life of food (inspiration, planning, purchasing, preparation, and of course ... eating). We are looking for a Data Engineer to help us change the grocery shopping experience.This Data Engineer position will be a unique opportunity to gain experience with end to end spectrum of Data & Analytics, ranging from:1. Manage and augment the entire data pipeline, from Raw database to relevant structures in Data Warehouse2. Partner with the Client Service teams to help create Analytical Solutions that help our clients get rich insights from our unique data repositories that include Fetch app usage data, shopper transaction data and other shopper / product dimensionsThis person will be a key member of the analytics team and report to the VP of Analytics.ResponsibilitiesMaintain and implement systems that ingest, transform, organize, and expose data insightsAutomate database processes to reduce admin time and human error ensure structures that are relevant for analytics purposesContribute to upgrading/deploying/building of reliable database solutionsIntegrate and clean multiple data sources; backup, restore, recovery, troubleshooting and configuration management;Integrate security best practicesWrite complex stored procedures and optimize execution efficiencyParticipate in code and design reviewsDeploy releases to multiple environments and support other teams during releasesWork closely with application teams and business owners and develop a strong understanding of business logic and processesResearch and evaluate third-party tools to increase efficiencyRequirementsExperience with Big Data Platforms like Spark, AWSExperience with PostgreSQL / RedshiftUnderstanding of database design/architecture and operationsExpertise in data modeling, query design and optimization, stored proceduresExperience with a major BI technology set like Tableau, PowerBI etc.Experience with very large databases and distributed database environmentsAbility to work with and provide technical leadership to other team membersInnovative thinker who is positive, proactive and readily embraces changeSuperior conceptual and analytical abilities, identifying opportunities for improvement through analysis and creative thinkingExperience with Python a plusExperience with Java a plusJob Type: Full-timeExperience:Spark: 1 year (Required)Redshift or PostgreSQL: 1 year (Required)","Chicago, IL",Data Engineer,True
57,"$100,000 a yearBI Data Engineer

Up to $100K per year

DC58748717

Pinnacle Partners is assisting our client in their search for a BI Data Engineer to add to their team located on north of Indianapolis. Our client is seeking a strong analyst who can be business facing and have a technical foundation they can build upon with evolving tool sets. This successful resource will be part of a team of engineers who are able to work with stakeholders business request, digest and understand them, pull data and put in to a visualization tool and them assist the business in understanding what the results are. This is an excellent opportunity to lead Bl modernization.

RESPONSIBILITIES


Detect, design, and execute internal process improvements such as automate manual testing, re-design infrastructure, optimizing data delivery, and etc.
Form infrastructure required for loading of data, transformation, and extraction from a wide variety of data sources using SQL and cloud based technologies.
Build analytic tools that can offer actionable insights into customer retention and acquisition.
Collaborate with stakeholders to support data infrastructure needs and assist with data-related technical issues.
Design data tools for analytics and data scientist team members that aid them in optimizing and building the product.
Work in collaboration with analytics and data experts to seek greater functionality within data systems.

REQUIREMENTS


At least 3 years of data engineering experience in the context of BI and data warehousing.
Expert knowledge of SQL and working experience with writing queries and working with relational databases.
Working experience performing root cost analysis on external and internal data processes to answer specific business questions and identify ways to improve.
Form processes supporting data structures, data transformation, dependency, metadata, and workload management.
Demonstrated experience processing, manipulating, and extracting value from large datasets.
Able to visualize data preferably experience with Tableau.

TERMS

This is a direct hire opportunity with a salary up to $100K based on experience. They offer excellent benefits including full medical benefits, 401K match, PTO, and great work life balance.","Carmel, IN",Data Engineer,False
58,"$60,000 - $75,000 a yearContractRecent Graduates are welcome to apply! CPT/OPT acceptedWe provide free on-boarding training for entry-level employees.Job Responsibilities: Design and create Data Model based on various business requirement.Create ERD to the proposed database.Create database objects such as tables, views, UDFs and UDPs etcDesign packages related to the extraction, transformation and loading (ETL) process using Microsoft SQL Server 2012 or laterResponsible for data profiling as well as SQL and database tuning / optimizationAnalyze the client requirements and design technical solutions based on those requirements.Develop testing and implementation strategy, conduct appropriate functional and performance testing to identify and resolve process bottlenecks and data quality issuesReviewing query performance and optimizing codeDesigning and coding database tables to store the application’s dataData modeling to visualize database structureWorking with application developers to create optimized queriesCreating table indexes to improve database performance and apply appropriate index maintenanceEnsure code security and prevent SQL InjectionParticipate in development and creation of data warehouseQualifications: Master’s Degreein Computer Science (CS), Information System (IS), Management of Information System (MIS), Statistics, Analytical Finance, etc.0-1 years of practical experience with Microsoft SQL Server platform and ETL as well as T-SQL (Transact SQL) stored procedures and triggersInterest in working with complex data sets and leading edge analytic technologies such as machine learning and Power BIAbility to write & troubleshoot SQL code & design stored procedures, functions, tables, views, triggers, indexes, & constraints.Knowledge of MS SQL Server 2012 or laterKnowledge with SSRS\SSIS\T-SQLExcellent communication and analytical skillsAbility to work in team environment and client interfacing skills.Work Authorization Status: We accept US citizens, GC, EAD, etc. We will sponsor H1b for the right candidates.JOINING Beaconfire Solution : We welcome students on OPT/EAD/H1B/GC/US CITIZENS.H1-B Visa Sponsorship for Students on OPT.Competitive salariesInterview guidance from experienced professionals.*BeaconFire Solution is an equal opportunity employer and an E-verified company.Job Type: Full-timeSalary: $60,000.00 /yearJob Types: Full-time, ContractSalary: $60,000.00 to $75,000.00 /yearEducation:Master's (Preferred)","Township of Hamilton, NJ",Junior BI/SQL Developer / Data Analyst / Data Engineer ( All...,False
59,"Zeus - Flawless Stays.

Founded in November 2015 and based in San Francisco, California, Zeus is shaking up the $12b corporate housing industry through its unique strategy of leasing unfurnished, privately-owned homes and converting them into expertly appointed, full-service corporate housing units for today’s global professional. We're passionate about delivering world-class customer service and becoming the most trusted provider of corporate housing on Earth. With more than 350 homes in the San Francisco Bay Area and Los Angeles and over 100,000 nights booked, we are well on our way to realizing our vision. Zeus has raised $14.1M in financing from Initialized Capital, Google Ventures, Bowery Capital, and Floodgate. The Zeus founders have all had previous exits and have worked in real estate and technology for over 10 years.

The Role

The data engineer is responsible for designing and developing robust, scalable solutions for collecting and analyzing large data sets. At Zeus, you'll be creating and maintaining the data pipeline and set the foundation of data engineering in terms of technical choice and process.
What You Will Do
Architect, develop and own data pipelines at enterprise level
Writing robust programs for realtime acquiring data from various sources
Work closely with data scientists to launch and maintain new pricing models, build frameworks for model deployment
Automate existing processes, ensure stability and accuracy
What We Are Looking For
Exceptional coding skills, particularly in Python/Ruby
2+ years of experience in a data engineering or data analytics role
Expertise in database design and data warehousing concepts
Proactive, lead complex projects with a “can-do” mentality
Benefits
100% medical, dental, and vision coverage for employees and their dependents
Parking, commuter pass, and education or certification expense coverage
20 days paid time off per year
Catered daily team lunches
401K with matching
Relocation package
Zeus is an equal opportunity employer and does not discriminate on the basis of race, color, religion, gender, gender expression, age, national origin, disability, marital status, sexual orientation, or military status in any of its activities or operations.

We do not accept calls from 3rd party recruiters.","San Francisco, CA",Data Engineer,False
60,"Are you interested in products that make lives better for those most in need and help reshape healthcare in the US? We are looking for an energetic and highly motivated Data Engineer to help launch customer implementations and analyze our database of patient medical record data. You will be joining a fast-growing VC-backed company, with a unique team of seasoned professionals with a vast combined expertise in both the technical and the healthcare space.
Key Responsibilities:
Launch new customer implementations of our population health and value based contracting products
Analyze customer patient medical record data to mine a complete patient profile from Electronic Health Records
Develop quality measure analytics that provide insights about patient populations
Enhance our data lake and data pipelines as we continue to normalize new data sources and leverage our Big Data assets
Qualifications and Experience:
Bachelor Degree in computer science preferred
Big Data technologies in the cloud such as Hadoop, Pig, Hive, etc.
Traditional relational DBMS as well as No-SQL
Extract, Transformation and Load (ETL) tools and practices
Data and object modeling
General software engineering principles such as OOP, SOLID, etc.
Quick and eager to learn new technologies
Strong individual contributor who is able to thrive in a dynamic and fast paced environment
Competitive salary and benefits provided. This position is located in New York City.","New York, NY",Data Engineer,False
61,"Job Description
Interested in Amazon Alexa? Come work on it. We’re building the speech and language solutions behind Amazon Echo and other Amazon products and services. We’re working hard, having fun, and making history!

We are looking for candidates who want to help shape the future of human-computer interactions. Specifically, we are looking for an outstanding Data Engineer who is looking to work in a new space to help define how we use data to understand customer behavior and satisfaction. In this role, you will develop and support the analytic technologies that give our teams flexible and structured access to their data, including implementation of a BI platform, defining metrics and KPIs, and automating reporting and data visualization.

The successful candidate will be an expert with SQL, ETL (and general data wrangling) and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.

Responsibilities

You know and love working with business intelligence tools, can model multidimensional datasets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:

Design, implement, and support a platform providing ad hoc access to large datasetsInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQLManage AWS ResourcesModel data and metadata for ad hoc and pre-built reportingInterface with business customers, gathering requirements and delivering complete reporting solutionsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisionsRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersParticipate in strategic & tactical planning discussions, including annual budget processes
Basic Qualifications
Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.
Relevant work experience in analytics, data engineering, business intelligence, market research or related field.
Experience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reports
Experience using SQL, ETL and databases in a business environment with large-scale, complex datasets
Preferred Qualifications
Graduate degree in computer science, business, mathematics, statistics, economics, or other quantitative field
Both technically deep and business savvy enough to interface with all levels and disciplines within the organization
Demonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, marketing, finance, and operations
Knowledge of Advanced SQL and a programming language
Experience with data visualization using Tableau or similar tools
Experience with large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Proven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.","Seattle, WA",Data Engineer - Alexa,False
62,"THE OPPORTUNITY

Scholastic is seeking Associate Data Analysts and Data Engineers to provide data-driven insights and execute recommendations as part of the Analytics and Data Engineering teams. The ideal candidate is highly analytical, passionate about hypothesis-driven problem-solving, and excellent at developing data solutions. The role serves a range of functions across Scholastic’s businesses, covering financial analysis, customer analytics, data engineering, web analytics, and analysis of offline activities.

The candidate's individual strength and skill set will determine whether the position is a Data analyst or a Data engineer.

YOUR RESPONSIBILITIES

Manage enterprise data across Scholastic's business unitsInterface with other developers, product owners and business analysts to understand data needsDevelop and maintain Scholastic's data infrastructure to drive efficient and reliable processesIdentify opportunities to improve Scholastic operations and its supply chainUtilize data to analyze the effectiveness of Scholastic’s marketing efforts across channelsGenerate insights and recommendations that are aligned with business realities and Scholastic’s mission and visionDevelop dashboards and improve reporting measures


HOW YOU CAN FIT

Undergraduate/Graduate degree in business, computer science, math, engineering or other quantitative disciplinesStrong SQL skills for purposes of data extraction, transformation, cleaning and analysisAbility to collaborate with data and software engineers as well as non-technical business ownersExceptional problem-solving ability, logical reasoning, creative thinking and quantitative aptitudeStrong Excel, SAS, R or Python skills in analysis, statistics and charting

Preferred Skills and Knowledge:

Familiarity with BI and data visualization tools like Tableau and LookerFamiliarity with web analytics tools such as Google Analytics and Adobe Analytics (a.k.a. Omniture)Experience in Python, Scala or Java is a plusExperience using big data technologies like Hadoop, Hive and Spark is a plus


ABOUT SCHOLASTIC

Scholastic Corporation (NASDAQ: SCHL) is the world's largest publisher and distributor of children's books, a leading provider of print and digital instructional materials for pre-K to grade 12, and a producer of educational and entertaining children's media. The Company creates quality books and ebooks, print and technology-based learning programs, classroom magazines and other products that, in combination, offer schools customized solutions to support children's learning both at school and at home. The Company also makes quality, affordable books available to all children through school-based book clubs and book fairs. With a 98-year history of service to schools and families, Scholastic continues to carry out its commitment to ""Open a World of Possible"" for all children. Learn more at Scholastic.com/AboutScholastic.

About the Associate Program

Scholastic’s Technology Associate Program is designed to identify, train, and promote the next generation of leaders. Each new-hire class receives comprehensive training and is quickly given responsibility to deliver on business goals using industry-leading tools, partners, and technology – all while working in an agile, iterative model that emphasizes collaboration, transparency, and goal-oriented development.","New York, NY",Associate Data Analyst/Engineer,False
63,"Imagine what you could do here. At Apple, new ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.
The Fraud, Engineering, Algorithms and Risk group is responsible for combating fraud and abuse for Internet Software and Services at Apple. In this role, you will be tasked with building mission-critical, robust and scalable distributed systems that can keep pace with data across a number of high-profile and large-volume Apple cloud properties. You will chip in to building the next-generation libraries, platforms, and data pipelines to empower us to rapidly build and deploy complex models to production.

Key Qualifications
MS or BS in Computer Science or related field
3 or more years experience building large-scale distributed systems
Exceptional analytical and programming skills
Experience in Scala or Java
Superior knowledge with at least two of the following: Spark, MapReduce, HDFS, Cassandra, Kafka
Description
We engineer high-quality, scalable and resilient distributed systems that power data exploration, model building and production models. Our core systems need to work seamlessly across different execution contexts (real-time, near real-time and batch) You will support diverse data analytics stacks such as Spark, Hadoop, Kafka, Cassandra and beyond.
We work at an unusual intersection of huge data volumes and adversaries that are continuously adapting, which means we are operating at and beyond the limits of conventional alternative data systems. On our team you can be sure that every commit you make will come with the satisfaction that you are helping protect and improve the user experience of hundreds of millions of users.
This role requires in-depth knowledge with cutting-edge data analytics technologies. Tuning, troubleshooting and scaling these big data technologies are a key part of our work, where having a curiosity with the internal workings of these systems is key to being successful. This is a hard-core software engineering role, where a large part of an engineer's time is spent writing code with the remainder being spent on designing and architecting systems, tuning and debugging alternative data systems, supporting production systems and supporting our data scientists.

Education
BS in Math, Computer Science, or equivalent experience
Apple is an equal opportunity employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.","Santa Clara Valley, CA",Data Engineer,False
64,"When will rush hour start? How long will it take for a traffic jam to clear up? How will my route be affected by a nearby ball game? We believe these questions are important to you, and we strive to answer them for you.
We are the Traffic team at Apple and we are looking to hire a Data Engineer to help us answer questions like these. We process anonymized location data from a vast number of iOS devices and combine it with other data sources to extract valuable insights such as speed information.

Key Qualifications
Our team's main responsibility is to measure and predict speeds for every road in the world, and all in real-time. These speed values heavily influence routing and ETAs, as well as traffic display on the map. Your work will impact millions of people’s lives every single day.
Excellent Java scripting skills
Strong product sense and ability to empathize with user needs
Strong experience in data analytics and meaningful tools
Bias for action
Understanding of location data
Front-end experience a plus
Description
You will be a key member of our team crafting, implementing, and evaluating the system that expertly processes a massive stream of GPS and sensor data. At Apple, you will help make contributions towards the quality of our data pipelines, the testing methodology and testing automation.
This is a timely opportunity to work on exciting and challenging problems. You are self-motivated and hands-on. You love using concrete approaches to develop metrics for product quality and measure improvements, and you have a strong focus on user needs.
You also speak Java and know how to deeply impact products.

Education
BSc or MSc in Computer Science or a quantitative field
Additional Requirements
WHILE THE FOLLOWING SKILLS ARE NOT NECESSARY, THEY MAY BE HELPFUL TO BEING SUCCESSFUL IN THIS ROLE:
- Understanding of Location Data
- Knowledge and experience with Scala","Santa Clara Valley, CA",Data Engineer - Maps Traffic Team,False
65,"ContractWe are looking for an BigData Engineer one of Direct client in NYC NYIf interested please reply back with your word format resume,rate and availability. We need to close this position ASAP, appreciate your immediate response.Job Title: BigData Engineer ( 2 positions )Locations: NYC NY & Raleigh NCDuration : Long termComplete DescriptionCloudera Java Scala Spark are Key skills· 5+ years of distributed system design and development, data analysis and warehousing· Strong hands-on programming using Java, Python, RDBMS and SQL (Scala)· 2+ years of deep understanding and programming on Cloudera and Hortonworks· Expertise in distributed file systems and streaming applications (Spark, Kinesis, Kafka)· Expertise in design of non-trivial ETL applications using Flume, Sqoop, Oozie, Pig etc· Expert in NoSQL data solutions (key-value, columnar, document, graph, time series)· Experience configuring, managing, monitoring, tuning, and debugging Hadoop clusters· Expertise in search technologies (Elastic search, Splunk) is a huge plus· Expertise in high performance REST based web service frameworks· Familiarity with Agile development using SCRUM and XP methodologiesNice to have· Certification in Cloudera and Hortonworks ecosystems· Certification in Apache Spark 2.0Job Type: Contract","New York, NY",Big Data Engineer,False
66,"Job Description

Camp4 is seeking a creative and experienced bioinformatics data engineer, passionate about joining a team that utilizes integrated and diverse datasets, proprietary and external, as a key component to its Gene Circuitry PlatformTM for drug discovery and development. This role is responsible for developing and implementing Camp4’s data architecture, including data storage, management, processing, and retrievability. The ideal candidate should have a solid bioinformatics and computer science training, extensive experience with large-scale genomic data management, familiar with open source bioinformatics tools and databases, Next Generation Sequencing (NGS) analysis, annotation and visualization tools. The successful candidate will collaborate closely with data analysts, as well as experimental biologists to ensure Camp4’s Gene Circuitry PlatformTM realizes its full potential to discover and develop drugs with higher probability of success. Reports to the Head of Data Sciences.
Key Responsibilities

Oversee organizing, processing, quality assurance, and visualization of internal and external datasets
Manage data storage solutions and scientific computing resources (local & cloud)
Build data access portals and visualization of omics data
Perform NGS data processing, analysis and interpretations
Design and assemble hypothesis-specific datasets for machine learning purposes
Develop processes, contribute to compliance policy setting and present solutions to diverse internal audiences
Qualifications
Ph.D. in Bioinformatics, Computational Biology, Genomicsor a relatedfield, OR M.S. with prior experience (+3 years) in Computational biomedical research.
Proficient in R, Python and SQL, with experience in Linux environment and cloud-based computing
Solid expertise in NGS pipeline development and workflow management
Extensive experience in processing RNA-seq, ChIP-seq and ATAC-seq
Hands-on experience in interactive visualization tools (R-Shiny or D3)
Strong organizational skills and interest in exploring new technologies and platforms
Knowledge of transcriptional regulation, epigenetics, signal transduction is preferred
Detail-oriented, team player mentality, good communication and troubleshooting skills
Passionate about realizing the potential of fundamental scientific discoveries to the bettering of patients’ lives and health
About Camp4
Founded in 2016 and focused on a core mission of realizing a world with ‘an effective treatment option for every patient,’ CAMP4 Therapeutics evolved from seminal discoveries made by company founders Dr. Richard Young and Dr. Leonard Zon, characterizing the ways in which dynamic cell signaling networks control the expression of genes. Operating at the intersection of genomics, computational biology and data sciences, CAMP4 has extended this foundational work, creating a unique Gene Circuitry PlatformTM to amplify the value of cellular and genetic insights to better understand how genes are controlled by signaling pathways in specific disease states. By generating proprietary 4D maps, CAMP4 can identify de-risked druggable targets, produce actionable insights and improve therapeutic predictability, potentially addressing hundreds of diseases and benefiting millions of patients globally.
Find out more at www.camp4tx.com.","Cambridge, MA",Bioinformatics Data Engineer,True
68,"Are you a never give up problem-solver? Do you thrive in a dynamic environment? As a Data Engineer, you will help deliver faster decision-making within R&D to allow for faster launch timing (go-to-market) on initiatives and enable cost-savings efforts. We believe data modeling, wrangling, mapping, and formatting to enable advanced modeling and analytics will be the foundation of your role.

What will I do?
At this job, you will balance multiple projects at different stages of development at a time. In addition, you will need to maintain high level of curiosity and creativity to learn daily on many fronts. You should be self-motivated and able to drive technical insights into actions that improve business results.

We offer you:
Truly significant work from the beginning
Mentorship, coaching, training, and guidance
Work with world-renowned technologies in some interesting ways
The chance to influence the direction and future of our leading products","Cincinnati, OH 45249",Research and Development Data Engineer,True
69,"About the Company:
------------------

Clarifai is an artificial intelligence company that excels at visual recognition. We do not sell an abstract, futuristic technology - we sell a solution that people can use today to solve real-world problems. We believe that the same AI technology that gives big tech companies a competitive edge should be available to developers and businesses. That's why we build products to make it easy, quick, and inexpensive for them to innovate with AI, go to market faster, and build better customer experiences. We make ""teaching"" AI just as accessible as we make using AI, which is why our technology is the most personalized, unbiased, accurate solution in the market.

We have secured $40M in funding up to date, backed by Menlo Ventures, Google Ventures, USV, NVIDIA, Qualcomm, Osage, Lux Capital, LDV Capital, and Corazon Capital. To continue to succeed, we need people like you to join the team!

Clarifai is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce.

Your Impact:
------------

As our first Data Engineer at Clarifai, you will own the existing data stores as well as work closely with other engineering teams to deliver a future vision for data at Clarifai. Your expertise in managing ever growing data sets will be put to immediate use in simplifying, optimizing, and consolidating our existing data stores.

Your Opportunity:
-----------------

You will be focus on creating and maintaining systems to ensure Clarifai's internal systems run correctly. You will be ensuring internal data is properly measuring external usage, devising and implementing new ways to utilize existing data, and working closely with data analytics to provide insight into business decisions. You will be reporting to the Head of Infrastructure, but will often operate cross-functionally by working with our Business Intelligence and Data Strategy teams. You will be working out of our New York office. As part of the infrastructure team, you will help them maintain responsibility for the overall availability and reliability of the products we provide. You will work with the other engineering teams to ensure they have the tools and resources they need to deliver the best product for our customers.

What You Bring:
---------------


2-4 years of experience as a Data Engineer or a similar role.
You program in Python, Golang, C++, and/or Java.
You are well versed in observability, reliability, and security best practices.
You are an expert at optimizing usage of common cloud datastores (both SQL and NoSQL) and their open source alternatives.
You have experience with automating via infrastructure-as-code both cloud and physical database deployments.
Ability to complete a task with little supervision, organized, ability to work on multiple projects at once.
Experience working with remote people/working remotely
Experience working in a tech startup
We prefer experience with container orchestration (e.g. Kubernetes, Mesos, EKS, GKE, et al.)
Experience with distributed storage (e.g. Ceph, EFS, HDFS, et al.) is also a HUGE plus.
Must be able to start as soon as possible, but no later than November.

Objectives:
-----------

In your first month, you will start off by learning the ropes. You will:


Develop an understanding of our current data stores and use cases
Scale existing data warehousing practices
Work with internal customers to identify potential new data services

3 months later, you will use your understanding of Clarifai's infrastructure to find the critical areas to address. You will:


Improve reliability of existing data pipelines
Develop insights as to a more efficient subscription and billing system
Programmatically ensure consistency across multiple data stores for existing data as scale increases

6 months down the road, your understanding of current and future projects will allow you to proactively work to scale our products and our engineering practices. You will:


Scale our data stores to meet the unique requirements for our internal Data Strategy Team
Deploy and maintain a scalable subscription and billing system

In 12 months, your deep understanding of our product and infrastructure will be critical in determining the future vision for our infrastructure. You will:


Continue to scale our existing billing system and data stores for a rapidly growing engineering organization and product suite.
Identify large future data initiatives the infrastructure layer should be focusing on, ensuring we remain ahead of any scaling bottlenecks and product requirements.

In the future, you'll continue to ensure our products scale to meet growing customer demand and our engineering team has the tools and platforms they need to deliver new products faster than before.","New York, NY 10010 (Gramercy area)",Data Engineer,False
70,"Make things that matter, at Plated! Our teams are making healthy and tasty eating part of everyday living. What we build as a team has a major impact on our customers, and we're passionate about doing what's best for them.

As a member of our data science team, you'll help build features on our distributed data platform that enable our teams and our customers make better decisions. You'll work collaboratively with analysts and data scientists to shorten the time between insight and action (and farm to table!) while being able to explore innovative technologies and methodologies for producing data and decisions at scale.

What you'll do with us:

Develop and maintain data pipelines and applications in collaboration with internal teams, such as marketing, culinary, operations, and product, and external teams at Albertsons
Productionize machine learning models and automated decision making processes at scale and across companies
Abstract and accelerate the model development process through engineering tools and processes
Create new features for our distributed data workflow and processing systems
Collaborate with internal teams to translate new analytics requirements into features for our analytics applications and data warehouse
Maintain and improve the reliability of the backend infrastructure and data quality
Develop and maintain data environments that help us meet our financial and regulatory requirements

What you're bringing to us:

Understanding of security and privacy and how related policies manifest themselves in code
Curiosity for the unknown and unknowable
Eagerness to learn and teach new skills and test new technologies
Ability to self manage, organize, and prioritize while collaborating across multiple work streams
Aptitude for understanding and untangling complex code
A sense of community, collaboration, and flexible communication style to accommodate an onsite and remote team
Understanding of data modeling and data architecture design best practices

What you've worked on before:

3+ years hands-on development experience working on full life cycle information management or data science projects; including data warehousing, business intelligence, machine learning, or ETL (extract, transform, load)
Undergraduate degree in Computer Science, Information Systems, or other quantitative/engineering field
Experience in SQL and Python, specifically for data applications
Experience with ETL process design and maintenance (preferably open source tools like Airflow)
Experience working with various data storage mechanisms; RDBMS, No-SQL, and column-oriented DBMS

Working at Plated

Plated brings together exceptional food, unparalleled creativity, and innovative technology to redefine the dinner experience. Our team is solving big-picture problems in a collaborative, data-driven environment we've built together—and we're looking for the best people to join us.

Our benefits and perks (expanding all the time!):

The best part: a free weekly Plated box!
Unlimited vacation days with a ""get your work done"" policy that respects your time
Competitive compensation
Customizable medical, vision, and dental plans and a 401(k) plan
Annual education stipend for your career development
Newly renovated office in Chelsea, Manhattan with an open floor plan, beautiful demo kitchens, and adorable office dogs.
Daily breakfast, afternoon snacks, and Test Kitchen samples
Company events, cultural clubs, and food-related activities

","New York, NY",Data Engineer,False
71,"Part-timeWho are we?

Since 2011, General Assembly has transformed tens of thousands of careers through pioneering, experiential education in today's most in-demand skills. As featured in The Economist, Wired, and The New York Times, GA offers training in web development, data, design, business, and more, both online and at campuses around the world. Our global professional community boasts 40,000 full- and part-time alumni — and counting.
In addition to fostering career growth for individuals, GA helps employers cultivate top tech talent and spur innovation by transforming their teams through strategic learning. More than 21,000 employees at elite companies worldwide have honed their digital fluency with our corporate training programs. GA has also been recognized as one of Deloitte's Technology Fast 500, and Fast Company has dubbed us leaders in World-Changing Ideas as well as the #1 Most Innovative Company in Education.

GA has a remote-friendly culture with offices around the world. If you prefer the office, our headquarters are located in New York City. Twice a year, the entire Product team gets together in New York for a week of team building, workshops, lightning talks, urban adventures, and an epic hackathon.

As part of the Engineering team, we collaborate with application engineers and business owners to build and manage a wide variety of data sets, deliver near-real-time monitoring of key business metrics, model events, as well as provide longer term trend analysis. We are proud of using new tech and always researching alternatives. We are super happy with our new open source data pipeline and need more engineers to grow it.

Responsibilities include

We are looking for a Data Engineer that loves data. Your responsibilities include…


Development of data marts and rollup tables
API feeds automation
Work on the data pipeline that feeds our data warehouse from a variety of sources
Assist with the development of a BI strategy to support current and future needs through technology
Support and advise all parts of the global business on their data needs
Write robust, well-tested, production code in support of the business intelligence systems
Develop and maintain monitoring, alerting, and anomaly detection services
Understand what data is needed and how it will be presented in visualization tools like Looker

Must have:

3+ years experience in the data field
ETL/Data manipulation in Python, Java, Perl or any language. Python preferred
Strong SQL skills. Experience creating data marts and/or star schemas
Be able to translate business requirements into deliverables
Comfortable writing code that pulls data from API's and storing in database
Excellent communications skills

Nice to have:

Familiar with visualization tools like Tableau, Looker. Looker preferred
Event modeling
Experience with a columnar datastore, preferably Redshift
Familiar with any monitoring system like New Relic
Machine Learning experience

Benefits:

Remote / flexible working hours
Highly Competitive Salary
Generous parental leave
Annual Education Allowance
Gym Allowance
Apple Macbook Pro + External Monitor

USA Specific


Flexible PTO
401k Retirement plan
Health, Dental & Vision Insurance
Company iPhone

Who can apply?

You are living in or willing to self-relocate to...


The USA in any of these states: CA, CO, CT, DC, FL, GA, IL, KS, MA, NY, NC, TX, VA, WI and/or WA

",North Carolina,Data Engineer,False
72,"Position Description
The Boston Bruins Hockey Operations department is accepting applications for an experienced data engineer. This position reports to our Director of Hockey Analytics, and will assist in the development of our database systems and infrastructure, as well as the creation and maintenance of our data processing pipelines.

Responsibilities
Build automated pipelines for acquiring, processing and cleaning data from different sources and providers; manage data flow into centralized databases
Conduct database feature engineering to support departmental research
Prepare, clean, and format analytical data sets for processing by analysts
Develop processes for monitoring and testing data quality across multiple sources; diagnose and resolve data quality issues to ensure accuracy
Use and create tools for data manipulation, visualization, reporting
Define storage, security and backup procedures; serve as main resource for departmental support and data maintenance
Take ownership of database structure - manage with long-term stability in mind while delivering short-term results
Interface with analytics and other hockey operations staff, and execute exploratory research and analysis as needed

Qualifications
Bachelor’s degree in Computer Science, Data Science, Engineering, IT, or related field
Preferred post-graduate education or 2-4 years related work experience
 3+ years of experience developing in SQL or AWS Redshift
2+ years of experience with data profiling, modeling, and data pipeline development
2+ years of experience developing in Python, R, or similar language
Familiarity with APIs and machine learning a plus
Experience manipulating large and complex data sets
Excellent written and verbal communication ability – desire to be part of a group and to put the needs of the team first.
Problem-solving skills – must be able to assess tasks and react to requests independently, if necessary
Ability to take initiative, work in a fast-paced environment, and consistently meet deadlines
A knowledge and passion for working in sport. Hockey knowledge is a plus, but candidates should have an understanding of typical data structures and research areas in sport.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","Boston, MA 02114 (Back Bay-Beacon Hill area)",Data Engineer - Hockey Operations,False
73,"Job Description
The Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Echo Look and Echo Show, Fire TV and Amazon Dash. Our team is serious about great scalable design and redefining best practices with a cloud-based approach to scalability and automation.

What will you help us create?

Amazon's Devices Technology team is looking for a talented Data Engineer with a strong background in ETL, Data warehousing and interest in linking data to key business trends. You will work closely with the business and technical teams to analyze many non-standard and unique business problems and use creative-problem solving to deliver actionable output. Your work will have a direct impact on the day-to-day decision making in the Operations & Supply Technology business domain.
Basic Qualifications
Bachelor’s degree or higher in an engineering or technical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
3+ years in with and detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures.
3 years of demonstrated quantitative and qualitative data experience
3+ years of experience in data management & data quality control experience
Preferred Qualifications
Advanced knowledge and expertise with Data modelling skills, Advanced SQL with Redshift, Oracle, MySQL, and Columnar Databases
Knowledge of Enterprise reporting tools like Tableau
Experience with AWS
A track record of problem solving using software systems, and the desire to create and maintain data warehouse systems.
Proficient in the composition of Advanced SQL (analytical functions)
Proven track record of identifying metric variances and delivering solutions to address the changes
Ability to effectively communication with both business and technical teams
A self-starter who loves data and who enjoys spotting the trends in it!
Lab126 is part of the Amazon.com, Inc. group of companies and is an Equal Opportunity-Affirmative Action Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation
#d2ctech tag","Sunnyvale, CA","Data Engineer, Amazon Devices",False
74,"JOB SUMMARY
We are looking for a Data Engineer to join our growing team of analysts and developers. The data engineer will lead efforts to expand and optimize our data infrastructure as well as optimize the data flow for our Business Intelligence team. The ideal candidate will have hands-on development experience using industry standard ETL tools combined with experience with standard database technologies. The individual will be expected to work with team members across multiple areas of the Cavaliers Operating Company’s organization often playing several different roles within a project life cycle. Excellent requirements gathering and communication skills are a must. This role will reside between the Cavaliers Operating Company’s Business Intelligence and Information Technology teams and will provide an opportunity to gain end-to-end experience in delivering best of breed Business Intelligence solutions. Cavaliers Operating Company’s Business Intelligence practice encompasses data strategy, data transformation, database technologies, business intelligence, predictive analytics, descriptive analytics, targeted digital marketing solutions and more.

RESPONSIBILITIES
 Assemble large, complex data sets that are analysis-ready
 Design, implement and continuously optimize the organization’s customer data strategy
 Serve as the technical lead for data warehousing/integration projects
 Provide thought leadership and lead efforts to design data integration and governance architecture and implement extract, transform and load (ETL) jobs/processes, detailed data warehouse models and data mappings
Provide consultation on best practices and standard practices to internal team members and third-party partners
 Lead design and build-out of infrastructure in conjunction with third-party to support information management and data integration
 Provide strategic and tactical guidance with respect to customer data best practices and attention to quality documentation within Cleveland Cavaliers and partnering organizations
 Perform performance optimization and tuning on new and/or existing data warehouse implementations. Provide detailed documentation, end user training and knowledge-transfer services to end users and partners of the Cavaliers Operating Company
 Develop and maintain expertise in advanced and/or emerging data management and analytical information technologies such as data warehouse, data lake, Big Data, data warehouse appliances, and data virtualization. Continue professional and technical growth through corporate and personnel initiatives
 Must possess solid organization skills with the ability to communicate effectively with internal stakeholders and vendors

QUALIFICATIONS
Demonstrated ability in data modeling, ETL development, and data warehousing
 Experience building data products incrementally and integrating and managing data sets from multiple sources
 Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist, Business Analyst) with a track record of manipulating, processing, and extracting value from large data sets
 Experience with a DW technology (Redshift, SQL Server, etc) and relevant data modeling
Coding proficiency in at least one modern programming language (e.g. SQL, Python)
 Experience processing large amounts of data, in various formats and processing data in batch mode and streaming mode
 Exposure and knowledge of Security, encryption and Data Governance
 Experience with scalable service architecture and design
 Experience working with AWS technologies – Redshift, S3, EC2, Data-pipelines, Lambda, etc.
 Experience working with Microsoft and Azure technologies – SQL Server, Blobs, Functions, Dynamics CRM, etc.
 Experience with automation and deployment (e.g. Azure DevOps, Cloudformation)
Knowledge and direct experience using business intelligence reporting tools. (Tableau, SSAS Cubes, PowerBI, etc.)
 Experience building flexible data APIs that consumers use to power other parts of the business
 Awareness of best practices to secure data and processes from unauthorized access
 Strong interpersonal skills with the technical and business community
 Passion for using data to drive business decisions
Ability to work under minimal supervision
Excellent written and verbal communication skills
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","Cleveland, OH",Data Engineer,False
75,"Your consulting projects will include integrating data in a virtual manner for operational and/or informational purposes - Integration of 100+ data sources for a Customer Service Multichannel IT Infrastructure; implementation of Logical Data Warehouses and Virtual Datamarts to enable modern Business Intelligence solutions, Integration Layers for Hadoop-based Data Lakes, and support for Agile Operational Reporting on a diverse Big Data infrastructure are just a few flavours of your future projects.

Be part of an elite team in a rapidly growing international software product company. Your career with us will combine cutting edge technology, exposure to worldwide clients across all industries (Financial Services, Automotive, Insurance, Pharma, etc.), exciting growth path for technical, product and customer-facing roles, direct mentorship, and access to senior management as part of a global team. Your mission is to help our clients and prospects to realize their full potential through accelerated adoption and productive use of Denodo's data virtualization capability in many solutions.

Location New York, NY
Duties & Responsibilities

As a Data Engineer Virtualization (f/m) you will successfully employ a combination of high technical expertise, client communication and coordination skills between clients and internal Denodo teams to achieve your mission.

Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.
Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.
Contribute to knowledge management activities and promote best practices for project execution.
Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Location
New York, NY
Function
Engineering

Qualifications
Desired Skills & Experience
Experience Range: 2-5 years (Fresh Graduates must be top-ranked and exceptionally qualified)
University Degree relating to information systems or computer science (Bachelor or Master degree)
Understanding of Data Integration flavors
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. Good knowledge of software development and architectural patterns.
Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems (e.g. SVN, git).
Basic experience in Big Data, NoSQL, and InMemory environments is welcome.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Personal and Relationship qualities: Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker.
Communications: Good written/verbal communication skills in English (other international languages a plus) are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.
Willingness to travel.
Employment Practices
We are committed to equal employment opportunity. We respect, value and welcome diversity in our workforce.","New York, NY",Data Engineer - SQL / Big Data / Java,False
76,"Help build technology that saves lives!

LeanTaaS is a fast growing healthcare predictive analytics company that uses sophisticated math and lean principles to make healthcare providers more efficient.


Our technology helps millions of people wait less at hospitals and specialty clinics across the country.
Our customers include some of the nation’s largest hospitals including Stanford, UCSF, NewYork-Presbyterian, the University of Texas MD Anderson Cancer Center, and more
Our team includes veteran executives and the brightest minds from Google, McKinsey, Stanford, MIT, Duke, Berkeley, UIUC, and more.
We are a Series B company backed by multiple prominent investors in the healthcare space.

You will work in a small data ingestion team that will focus on:

Understanding EHR data models and develop and refine EHR queries to fetch the data we need. We work with several EHRs such as EPIC, Cerner, Meditech, McKesson Paragon, etc. This requires research and intuition - you’ll have to navigate your way through complex models.
Working with data teams at customer sites to ensure that the data is complete, accurate, and aligned with the structure we expect to see for the product. This requires a lot of data reverse engineering - you’ll have to figure out what data should look like and what may have gone wrong and get to the bottom of it. This is perhaps the most fascinating part of this job.
Working with engineering and infrastructure teams to make our predictive models work at scale. This requires refactoring sophisticated algorithms and leveraging tools like Hadoop and Spark to improve efficiency and performance.

Must Have:

B.S. or M.S. (preferred) with major in Statistics and/or Computer Science.
1-2 years professional experience as a data engineer. No experience is also OK if you can demonstrate exceptional skills/abilities.
Strong R or Python skills and data analysis skills - many times, you will need to reverse-engineer the data: there’s typically not good documentation available for the EHR data models you will work with, however, you’ll need to find out what each column means and how to read different dimensions and values.
Strong data analytics skills - you should love to work with data and solve challenging data problems patiently.
Good communication skills - you will work directly with customers and therefore need to communicate well.

Nice to Have:

Experience with data analysis tools such as Tableau.
Experience with python data frameworks such as numpy, etc.
Experience working with EHRs.
Experience working on ETL.

","Santa Clara, CA",Data Engineer - Santa Clara,False
77,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
Data EngineerOur New York Data Science team is building critical prototypes that serve our growing business. Our systems serve billions of requests and process terabytes of raw data in a single day. We are looking for problem solvers who embrace challenges with complex requirements and build prototypes that can validate business goals. We need engineers who are ready to make an impact and deliver quality software.Responsibilities: - Work with different teams to design technical data storage and processing solutions- Work with data scientists to create efficient and scalable data pipelines in AWS for data modeling- Design, develop, and test data-driven workflows- Follow updates in different FreeWheel backend components and contribute to the long-term roadmap for FreeWheel's data strategy- Drive the optimization, testing and tooling to improve data quality- Improve performance, availability and scalability of our backend systemsAbout You: - Bachelor's or Master's Degree in Computer Science or similar field of study- 2+ years of experience building large scale big data applications- Skilled with Scala/Hadoop and database general concepts- Proficient with Linux commands and environments- Proficient with 2-3 programming languages including Scala/Python. Fast to pick up new languages- Hands-on experiences with data processing or analysis systems is strongly preferred- Experiences With Hadoop/Spark/Kafka/Presto/etc. are Strongly Preferred.- Experiences with AWS or other Cloud technology are strongly preferred- Known for being a smart, analytical thinker who approaches their work with logic and enthusiasm- Detail oriented, flexible and can work well in a global team-oriented environment
Comcast is an EOE/Veterans/Disabled/LGBT employer","New York, NY 10001 (Chelsea area)",Data Engineer,False
78,"ContractThe Candidate is expcted to have 5-6 years of good experience with Proven experience with Hadoop technologies - Spark, HBase, Hive, Pig Good knowledge in back-end programming, specifically java, JS, Node.js and OOAD Writing high-performance, reliable and maintainable code Translate complex functional and technical requirements into detailed design Good knowledge of database structures, theories, principles, and practices Ability to write Pig Latin scripts Hands on experience in Spark data processing, HiveQL Familiarity with data loading tools like Flume, Sqoop Knowledge of workflow/schedulers like Oozie Analytical and problem solving skills, applied to Big Data domain Good aptitude in multi-threading and concurrency concepts","New York, NY",Big Data Engineer,False
79,"Specific information related to the position is outlined below. To apply, click on the button above. You will be required to create an account (or sign in with an existing account). Your account will provide you access to your application information.

Should you have a disability and need assistance with the application process, please request a reasonable accommodation by emailing BB&T Accessibility or by calling 866-362-6451. This email inbox is monitored for reasonable accommodation requests only. Any other correspondence will not receive a response.


Regular or Temporary:
Regular

Language Fluency: English (Required)

Work Shift:
1st shift (United States of America)
Please review the following job description:

Primary job responsibilities involve supporting ingestion and transformation pipelines that handles data for analytical or operational uses across broad line of business needs areas and enterprise data domains. The data engineer often works as a dedicated member of support teams, focused on providing production stability data processing workflows that will be used by analytics groups and data scientists who are interrogating information for predictive analytics, machine learning and data mining purposes. In many cases, the support engineer also works with business units and departments in proactively identifying data quality issues, and coordinating with the development groups to ensure data accuracy to business analysts, leadership groups, and other end users to aid in ongoing operational insights.


An independent & self-motivated lead support engineer must be versed in broad approaches to data architecture and applications, and will develop components/applications by studying operations and designing, and developing, reusable services and solutions that support the automated ingesting, profiling, and handling of structured and unstructured data.


Essential Duties and Required Skills:
Following is a summary of the essential functions for this job. Other duties may be performed, both major or and minor, which are not mentioned below. Specific activities may change from time to time.
5-10 years software development/ programming or support experience in enterprise/ web/ cloud applications.
3-5 years of experience in data modeling, data design and persistence (e.g. warehousing, data marts, data lakes).
Exposure to functional, imperative and object-oriented languages and methodologies.
Experience with supporting BIG DATA and Hadoop.
Experience with Big Data approaches and technologies including: Hadoop, Cloudera utilities, Spark, Kafka, Hive, Oozie (experience with Angular JS/HTML5/Node JS are big plus).
Experience with SQL (MySQL, Postgres) and NoSQL(MongoDB/HBase/ReDis) database is expected.
Proficiency with Linux operating systems especially troubleshooting and log-handling of applications deployed in Linux
Exposure to programming languages/tools including: C#, Java, Python, Ruby, Scala, SQL and scripting (Java, Python, Spark, SQL, Hive, JavaScript, Shell). Scripts
Experience supporting large-scale web services (RESTful APIs)
Has led, or been directly involved with, the investigation and resolution of incidents and complex data issues in a production setup.
Experience working in an agile environment.
Explores, examines and interprets large volumes of data in various forms and recommends additional sources of data for improvements.
INDBBTIT
Experience in data management best practices, real-time and batch data integration, and data rationalization.
Ability to prioritize well, communicate clearly, have a consistent track record of commitment and accountability for Level0, Level1 and Level2 support, as well as excellent troubleshooting skills.
Understand the relationships across business information and units of data; collaborate with business and other departments to identify data usage patterns and to formulate business names, definitions and data quality rules for data elements.
Understand database performance factors and trends pertaining to very large database design and collaborate with DBAs for resolving performance bottlenecks;
Pursue continuous improvements based on lessons learned and industry best practices.
Understand the goals and risks associated with the business and technical requirements, and offer counsel on risk mitigation and the alignment of data solution with objectives.
Coach and mentor support team members.
Demonstrate a team orientation by working closely and effectively with business partners, development teams and outside services.
Ability to apply systems thinking for solutions by considering broad potential alternatives and impact areas.
Ability to travel as needed, occasionally overnight.

Desired Skills:
Knowledge of, and experience working in, DevOps environments is desirable. Previous experience in the financial services industry is a plus. Experience with performance tuning and documenting changes. Exposure to container technologies (Docker or similar) and orchestration is a plus. Experience with metadata capture, management, Informatica Big Data Management and platforms.","Raleigh, NC",Big Data Engineer,True
81,"Company Overview: We are Visionaries, Focused and determined to disrupt the enterprise software management industry. In affiliation with InSite Group and its core values, Univers will deliver the next generations top notch IT platforms to the enterprise management of construction companies and many other industries.Purpose: Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The Senior Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.Essential Functions: Create and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS relational and big data technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data science team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Qualifications: BS Computer Science or Information Technology or equivalent work experience.Minimum 7 -10 years developing and designing RDBMS and Big Data architectures.2 years experience designing NoSQL data stores such as AWS Redshift Spectrum, Cassandra, Hadoop, MongoDB, etc.Experience with cloud deployments and CI/CD pipelines is a plus.Experience developing for microservice architectures is a plus.Experience in building data stores for REST APIs.Strong understanding of Object Oriented Design principlesExperienced with Agile methodologies such as KanBan.Capable of performing structured testing on features and doing research in production issues.We offer: Your Voice Matters! Our unique Open Source culture gives you the opportunity to impact our continuously evolving company.Explore our Hotels! Associates receive hotel and restaurant discounts & perks at any of our properties.Continued Leadership Education Program! We offer an ongoing Insite Leadership Series (ISLS, Weekly Webinars, e-learning courses).Enjoy the Holidays! We offer Seven (7) paid holidays throughout the year.Employee Social Events! Quarterly Themed Luncheons.Exceptional benefits including elective Medical, Dental, Vision, and Short-Term Disability coverage.Company paid Basic Life Insurance, AD&D, Long-term Disability, and Employee Assistance Program (EAP).Paid Time Off (PTO)Celebrate You! Choose one (1) paid day of personal celebration each year to use however you choose.Job Type: Full-timeEducation:Bachelor's (Preferred)","Fort Lauderdale, FL",Senior Data Engineer,True
82,"Overview


At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient is on a mission to help the Healthcare industry take advantage of modern data and analytics architectures, tools, and patterns to improve the quality and affordability of care. This is an excellent opportunity for the right individual to assist Perficient and its customers to grow the capabilities necessary to improve care through better use of data and information, and in the process take their career to the next level.

Perficient currently has a career opportunity for a Data Engineer located in Boston, MA.

Job Overview:

As a Data Engineer you will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment you will work with solution architects and developers on interpretation/translation of wireframes and creative designs into functional requirements, and subsequently into technical design.

Responsibilities
Lead the technical planning & requirements gathering phases including estimate, develop, test, manage projects, architect and deliver.
Serve as a technical lead and mentor. Provide technical support or leadership in the development and continual improvement of service.
Develop and maintain effective working relationships with team members.
Demonstrate the ability to adapt and work with team members of various experience level.
Qualifications
Passionate coders with 3-5 years of application development experience.
Proficiency with Spark with Python/Java a must.
Expert knowledge of developing in Cloudera environments using Spark and Cloudera tools such as Navigator and Manager.
Cloudera Certification highly preferred.
Knowledge of Hadoop tools such as Flume, Sqoop and Oozie.
Knowledge of data formats and ETL and ELT processes in a Hadoop environment including Hive, Parquet, MapReduce, YARN, HBase and other NoSQL databases.
Experience in dealing with structured, semi-structured and unstructured data in batch and real-time environments.
Experience with working in AWS environments including EC2, S3, Lambda, RDS, etc. Familiarity with DevOps and CI/CD as well as Agile tools and processes including Git, Jenkins, Jira and Confluence.
Client facing or consulting experience highly preferred.
Skilled problem solvers with the desire and proven ability to create innovative solutions.
Flexible and adaptable attitude, disciplined to manage multiple responsibilities and adjust to varied environments.
Future technology leaders- dynamic individuals energized by fast paced personal and professional growth.
Phenomenal communicators who can explain and present concepts to technical and non-technical audiences alike, including high level decision makers.
Bachelor’s Degree in MIS, Computer Science, Math, Engineering or comparable major.
Solid foundation in Computer Science, with strong competencies in data structures, algorithms and software design.
Knowledge and experience in developing software using agile methodologies.
Proficient in authoring, editing and presenting technical documents.
Ability to communicate effectively via multiple channels (verbal, written, etc.) with technical and non-technical staff.
Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.
#LI-DC1","Boston, MA",Data Engineer,True
83,"Re-envision Patient Care: As a Call9 team member you will operate at the nexus of clinical excellence, engineering innovation and progressive enterprise in a culture that fosters ingenuity and innovation. By joining the Call9 team you’ll not only be a part of the future of healthcare you’ll be integrally involved in shaping it.About Call9:Call9 is recreating the patient experience by bringing care to the patient. Our innovative model connects off-site physicians with on-site emergency medicine trained first-responders via a proprietary telemedicine platform born of Silicon Valley and improved, every day, by our physician and product teams. Call9 provides a spectrum of emergency, follow-up, and palliative care services at the patient’s bedside in skilled nursing facilities (SNFs). Our physician team prevents harmful and unnecessary transfers to the hospital, delivering high quality, value-based care, that improves outcomes, saves money, and provides a superior experience for the patient, their family, and our physicians.ResponsibilitiesMaintaining, scaling and improving our data warehousing system using Rails and SQL.Building systems that will integrate with external Nursing Facility EMR’s in a reusable, scalable way.Verifying system security and integrity is being maintained.Working with product engineers to determine how to structure data needed to meet business requirements.Testing reliability of our external data pipelines to ensure data is extracted and processed correctly.Mentoring and growing other engineers.Who You AreYou like to dig into the details, and you enjoy finding the reason behind requests.You love mentoring and growing other engineers, and you have experience in seeing projects fail and succeed.You care about why you're building software and want to work on data that impacts healthcare directly.Technologies We UseRuby on RailsLooker/SQLPythonHL-7 InterfacesRequirementsBS degree in computer science or equivalent5+ years of software development experience3+ years working on a data warehouse, ideally in an architect/senior positionCall9 provides equal opportunity in employment for all qualified persons and prohibits discrimination in employment on the basis of race, color, religion, creed, sex, sexual orientation, perceived sexual orientation, gender identity, marital status, national origin, ancestry, age, veteran status, disability unrelated to job requirements, genetic information, military service, or other protected status as may be mandated by applicable Federal, State and local law. All personnel actions, including but not limited to those relating to compensation, benefits, termination, training and education, are based on the principle of equal employment opportunity.Job Type: Full-timeExperience:Software Development: 5 years (Required)Data Warehouse: 3 years (Required)Education:Bachelor's (Preferred)","Brooklyn, NY",Senior Data Engineer,True
84,"ContractJob SummaryJob Title: Data Engineer IIILocation: Menlo Park, CADuration: 12+ MonthsResponsibilities and DutiesDuties:Apply proven expertise and build high-performance, scalable data warehouse applicationSecurely source external data from numerous global partnersIntelligently design data models for optimal storage and retrievalDeploy inclusive data quality checks to ensure high quality of dataOptimize existing pipelines and implement new ones, maintenance of all domain-related data pipelinesOwnership of the end-to-end data engineering component of the solutionCollaboration with the program’s SMEs, data scientistsQualifications and SkillsSkills:competence with relational databases (Oracle, MySQL, Vertica)experience working with Tableau.coding and scripting experience with Python, SQLJob Types: Full-time, ContractExperience:tableau: 4 years (Required)","Menlo Park, CA",Data Engineer,True
85,"Job Description
Are you ready to own all the data infrastructure and pipelines for a machine learning product? Amazon Payment Products is a growing business with an established ML team. We are looking for a technical leader to join our team of machine learning scientists, software developers, research scientists. This technical leader will lead our data engineering team and own all data infrastructure for our machine learning, launching new capabilities and experiences for Amazon customers.

In this role, you will work closely with machine learning scientists, product managers and SDEs on launch decisions. You will drive high value customer actions and influence senior management decisions and metrics. You will provide proactive, deep insights into metrics driving the business on a regular cadence. The right candidate will be passionate about working in with large datasets and emerging businesses.

Over time, you will lead the development of statistical and other machine learning models. You will work closely with our engineering teams to influence the product roadmap and implement solutions designed to improve operations and controls reporting.

Outstanding leadership, data engineering expertise and a keen interest in machine learning are required for this role. Strong interpersonal and communication skills are key.

Basic Qualifications
Bachelor’s degree in math, computer science, engineering, finance, statistics, or a related technical field5+ years of professional experience and passion for working with large data sets plus deep experience in statistical analysis, advanced modeling techniques, data mining and business analysisAgile project management experience and ability to drive successful end-to-end project executionAbility to thrive in an environment that is tasked with providing data-driven decision support and business intelligence that is timely, accurate and actionableHands on experience with AWS data products including EMR, S3, Redshift as well as SQL, Excel, data mining, data visualization software
Preferred Qualifications
Ability to think big, understand business strategy, provide consultative business analysis, and leverage technical skills to create insightful, effective BI solutionsHands on experience with data extraction, manipulation, statistical analysis and predictive modeling.Experience in machine learning (decision trees, multivariate and logistic regression, etc.)Proven track record of strong verbal/written communication & data presentation skills, including an ability to effectively communicate with both business and technical teams
Amazon is an equal opportunity employer.","Seattle, WA","Data Engineer, Payments ML",False
86,"HiHope you are doing great!Please review the below job description and let me know if you would be interested. If yes, please share me a copy of your latest resume and let me know the best time to connect with you.Job title: Data EngineerJob Location: NYC, NYDirect hirePrimary Skill: Redshift, Python, R, Glue, Crawler, Kinesis, Kinesis Firehose, HadoopSecondary Skill: Snowflake, H2O, Jupyter, Neptune, AlationJob Type: Full-time","New York, NY",Data Engineer,False
87,"What You’ll Do
 Design and deliver automated transformation of large data sets influencing MapReduce, streaming, and other new technologies
 Use HBase, Elasticsearch, etc. to ingest transformed data at scale
 Collaborate with security experts to deliver high-impact web-based APIs
 Implement high-volume data integration solutions
 Analyze, monitor, and optimize for performance
 Produce and maintain high-quality user documentation
Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.
Who You Are
 Recent graduate or on your final year of studies toward a Bachelor's degree in Computer Science or a related technical field
 Minimum of a 3.0 GPA or equivalent
 Track record of developing technology to enable large scale data transformation
 Strong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.
 Possess knowledge of software engineering standard methodologies
 Real passion for solving hard problems and exploring new technologies
 Excellent communication and user documentation skills
Why Cisco
At Cisco, each person brings their own rare talents to work as a team and make a difference.
Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.
 We connect everything – people, process, data and things – and we use those connections to change our world for the better.
 We innovate everywhere - From launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.
 We benefit everyone - We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.
Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us! #WeAreCisco
#univsoftwarejobs #STO #SoftwareEngineer #ComputerScience #Security @WeAreCisco
This position is available to Bachelor's level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions.
Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.","San Jose, CA",Big Data Engineer - Bachelors (Full Time) – United States,False
88,"About Kraken

Kraken is one of the largest and most successful bitcoin exchanges in the world and we’re growing faster than ever. We’re looking for people who constantly push themselves to think differently and chart exciting new paths in a rapidly growing industry. Kraken is a diverse group of dreamers and doers, we truly believe our success depends on having both in spades. Join us and the movement to change the way the world thinks about money.
Learn more about us:

Read our reviews on Glassdoor
Follow us on Twitter
Catch up on our blog

About the Role

This role is fully remote.

Our Engineering team is having a blast while delivering the most sophisticated crypto-trading platform out there. Help us continue to define and lead the industry.
Responsibilities
Build scalable and reliable data pipeline that collects, transforms, loads and curates data from internal systems
Augment data platform with data pipelines from select external systems
Ensure high data quality for pipelines you build and make them auditable
Drive data systems to be as near real-time as possible
Support design and deployment of distributed data store that will be central source of truth across the organization
Build data connections to company's internal IT systems
Develop, customize, configure self service tools that help our data consumers to extract and analyze data from our massive internal data store
Evaluate new technologies and build prototypes for continuous improvements in data engineering
Requirements
5+ years of work experience in relevant field (Data Engineer, DW Engineer, Software Engineer, etc)
Experience with data warehouse technologies and relevant data modeling best practices
Experience building data pipelines/ETL and familiarity with design principles
Excellent SQL skills
Proficiency in a major programming language (e.g. Java, C++, etc.) and/or a scripting language (Javascript, Python, etc.)
Experience with business requirements gathering for data sourcing
Check out all our open roles at https://jobs.lever.co/kraken. We’re excited to see what you’re made of.

We’re powered by people from the around the world with their own unique backgrounds and experiences. We value all Krakenites and their talents, contributions, and perspectives.",Remote,Data Engineer,False
90,"PMC: Data Engineer

Fast-growing media company Penske Media Corporation (PMC) seeks a Data Engineer to support our product, editorial, and sales teams by expanding, scaling and managing our data architecture and data lake.

Why work with us?
Work on some of the largest, trusted brands in entertainment and fashion news, including Variety, WWD, and Robb Report.
Own your work. High responsibility and high autonomy.
We’re a stable, growing company with a startup mentality.
Agile scrum environment with small, focused teams.


As our Data Engineer your role is critical in expanding the data-driven culture at PMC. Working side by side with the data science and data engineering teams, you will utilize BigQuery, Airflow, and machine learning to build data products to service the entire business. This position requires strong interpersonal and communication skills and the ability to interact effectively with people at various levels of the organization, our SaaS partners, and with our hundreds of data providers/sources. You are a quick learner and hungry to make sure you have a deep understanding of the meaning behind all measures, dimensions, and KPIs of the business. You are also ready to expand your data skills into the data science field to go beyond data architecture and help the business gain competitive advantages in the marketplace.

This position will report to PMC’s Director of Data Architecture.

Job Duties:
Write articulate, compelling summaries to communicate results of your hypothesis iterations, analysis, model development and which inform how the business should act.
AI multivariant testing
BigQuery, ML, Tensorflow, and user behavior algorithms
Anomaly and pattern detection
Airflow and Kubernetes development
Mentor other data engineers
Plan, direct, and build analytics solutions with a team of data science, other big data engineers, consultants, and visualization experts.


Job Requirements and Experience:
Strong SQL, Python and Java
Excellent communication with non-technical staff members to optimize their data workflows and uncover hidden optimizations.
Experience with Airflow and complex DAGs
Predictive modeling and applied statistical analysis
Machine learning with clickstream or online behavioral data a plus (GA, DFP, Krux, Programmatic)
Experience in digital media or content publishing is a plus


About PMC
Penske Media Corporation (PMC) is a leading digital media and information services company whose award-winning content attracts a monthly audience of more than 180 million and empowers more than 1 million global CEOs and business thought-leaders in markets that impact the world. Our dynamic events, data services, and rich content entertain and educate today’s fashion, retail, beauty, entertainment and lifestyle sectors. Headquartered in New York and Los Angeles with additional offices in 11 countries worldwide, Penske Media is the way global influencers are informed, connected, and inspired. To learn more about PMC and its iconic brands, visit www.pmc.com.","New York, NY 10017 (Midtown area)",PMC: Data Engineer,False
91,"Job Description
This opportunity is for a Senior Data Engineer (DE) within Audible’s Data Science group. In this role, you will be a member of an interdisciplinary team of Data Scientists, Data Engineers and Software Development Engineers. This team will be responsible for modeling many business areas such as Customer Engagement, Marketing, Content Management, Recommendations and more.

You have a passion for Big Data and its supporting technologies and platform components. You will be an enabler for cutting edge modeling efforts, passionate about accessing, moving, processing and managing terabytes of data in a performant, scalable, fault-tolerant way, and will display your strong problem-solving ability in different environments.

You will be able to thrive in the powerful, advanced environment that makes Audible such a unique company in the area of data-processing technology, and will be offered a perfect mix of responsibility and mentoring. You will also be exposed to modeling tasks further downstream of the Data Science value creation process.

KEY RESPONSIBILITIES
Apply business understanding and a technology know-how to cutting edge Data Science problems
Play a leading role in architecture design and implementation of next generation BI solutions
Play a leading role in optimally acquiring, crunching, and understanding for Machine Learning (ML) and modeling by Data Scientists
Effectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflicts
Peer review work. Actively mentor more junior members of the team, improving their skills, their knowledge of our systems and their ability to get things done

HOW DOES AMAZON FIT IN?

We're a part of Amazon, they are our parent company and it's a great partnership. You'll get to play with all of Amazon's technologies like EC2, SQS and S3 but it doesn't stop there. Audible is built on Amazon technology and you'll have insight into the inner workings of the world's leading ecommerce experience. There's a LOT to learn!

If you want to own and solve problems, work with a creative dynamic team, fail fast in a supportive environment whilst growing your career and working on a platform that powers web applications used by millions of customers worldwide we want to hear from you.
Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, Physics, or a related field
5+ years of building large scale data-processing systems with experience in Big Data technologies such as MapReduce, Hadoop, AWS EMR, Spark, Kafka, AWS Kinesis or equivalent
Expertise in Database technologies such as AWS Redshift with proficiency in SQL
Proficiency in Java, Scala or Python on Linux Platforms
Proficiency in shell scripting
Preferred Qualifications
Experience working with Data Science teams
Experience with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena
Experience working with Agile methodologies in a Data Engineering environment
Strong problem-solving skills with the ability to navigate highly complex and ambiguous situations
Strong interpersonal skills and the ability to work effectively across teams
Great communication skills - ability to think creatively and adapt the message to the audience. Can provide information to technical and non-technical stakeholders alike and guide them to confidently informed decisions
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision


Audible is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation","Newark, NJ",Sr Data Engineer II (Data Science),False
92,"CommonBond is building a values-driven, customer-centric financial services company. Our mission is to change the way people think about student loans. We accomplish this with lower interest rates, a state-of-the art technology platform, and caring customer service. We also are the only financial services company with a 1-for-1 social program: For every loan funded, we fund the education of a child in the developing world. CommonBond has been named to the World's 50 Most Innovative Companies by Fast Company, the 50 Best Places to Work by Inc., and the Forbes Fintech 50. We are backed by great investors, have an awesome team, and are looking for our next great team member.

CommonBond is seeking a self-guided, flexible Database Engineer, who will be responsible for identifying database requirements by analyzing our lending applications, business systems, and operations; evaluating existing data systems and designing and implementing new systems. Additional responsibilities include (but not limited to):


Work with software developers to optimize data architecture and data access patterns
Develop data extract, transform, load (ETL) and migration tools
Design and operate and maintain our core business data warehouse
Ownership of operational stability of production transactional and warehouse Postgres and RedShift databases
Installing revised or new systems by proposing specifications and flowcharts; recommending optimum access techniques; coordinating installation requirements
Manage database schema maintenance and migration
Guide data analysts in adopting best practices with their SQL queries
Monitor production databases, troubleshoot and resolve incidents
Preparing users by conducting training; providing information; resolving problems
Maintain quality service by establishing and enforcing organization standards

Desired Qualifications
----------------------

Minimum qualifications:

4+ years' experience with database management, security, data maintenance
Knowledge of Postgres, RedShift, ETL and java or python or go
Knowledgeable about data modelling, data access and data storage techniques
You care about agile software processes, data-driven development, reliability, and responsible experimentation
Demonstrated ability to lead great performing (direct and indirect) cross functional teams
A deep affinity for data, expert level analytical skills and an intense customer focus
Professional but not stuffy – we're sharp and competent but also fun
Experience in financial services is great, but more important is your ability to learn fast
Team player – you want to be relied upon and you understand how others make your goals achievable
Sense of urgency – you want to ""go, move, make things happen, and shake things up!""
Nimble – able to handle changing tasks and priorities and adapt quickly

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","New York, NY",Data Engineer,False
93,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Do you like working with big data? Do you want to use data to influence product decisions for products being used by hundreds of millions of people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.


This is a full time position based in our office in New York.
RESPONSIBILITIES

Inform, influence, support, and execute our product decisions and product launches

Manage data warehouse plans for a product or a group of products.

Interface with engineers, product managers and product analysts to understand data needs.

Partner with Product and Engineering teams to solve problems and identify trends and opportunities.

Build data expertise and own data quality for allocated areas of ownership.

Design, build and launch new data extraction, transformation and loading processes in production.

Support existing processes running in production.

Define and manage SLA for all data sets in allocated areas of ownership.

Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS

BS/BA in Technical Field, Computer Science or Mathematics.

4+ years experience in the data warehouse space.

4+ years experience in custom ETL design, implementation and maintenance.

4+ years experience working with either a Map Reduce or an MPP system.

4+ years experience with schema design and dimensional data modeling.

4+ years experience in writing SQL statements.

Ability to analyze data to identify deliverables, gaps and inconsistencies.

Communication skills including the ability to identify and communicate data driven insights.

Ability in managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS

4+ years experience using Python or Java","New York, NY","Data Engineer, Analytics",False
94,"Founded by The Allstate Corporation in 2016, Arity is a data and analytics company focused on improving transportation. We collect and analyze enormous amounts of data, using predictive analytics to build solutions with a single goal in mind: to make transportation smarter, safer and more useful for everyone.
At the heart of that mission are the people that work here—the dreamers, doers and difference-makers that call this place home. As part of that team, your work will showcase both your intelligence and your creativity as you tackle real problems and put your talents towards transforming transportation.
That’s because at Arity, we believe work and life shouldn’t be at odds with one another. After all, we know that your unique qualities give you a unique perspective. We don’t just want you to see yourself here. We want you to be yourself here.


Job Description

Data Science incorporates techniques across many disciplines – including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing – with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions. Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions. Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve. The Big Data Engineer job family is accountable for end to end engineering of data solutions which includes designing and building systems for data storage and analytics that enable Allstate analysts to make better decisions to achieve Allstate’s goals.
This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics. This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.
This role contributes to the structured and unstructured Big Data / Data Science tools of Allstate from traditional to emerging analytics technologies and methods. The role is responsible for assisting in the selection and development of other team members.



Job Responsibilities

Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Partners closely with team members on Big Data solutions for our data science community and analytic users.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive “real time decisions” analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e. Java, C#, Python, and Perl).
Trains more junior engineers.



Job Qualifications

3 - 4 Years of experience or equivalent skills & ability
Master’s or PhD preferred in a quantitative or scientific field such as computer science, computer engineering or equivocal experience.
Experience in using software development to drive data science & analytic efforts.
Experience with database & ETL technologies.
Experience with various data types (e.g. Relational, Unstructured, Hierarchical, and Linked “Graph” Data).
Experience in developing, managing, and manipulating large, complex datasets.
Experience in working with statistical software such as SAS, SPSS, MatLab, R, CART, etc.
Proven ability to code and develop prototypes in languages such as Python, Perl, Java, C, R, SQL, and XSLT.
Ability to communicate and present advanced technical topics to general audiences.
Understanding of predictive modeling techniques a plus.




That’s the day-to-day, now let’s talk about the rest of it. As we mentioned, Arity was founded by The Allstate Corporation. But you’ll be working for—and at—Arity. It’s the best of both worlds. You’ll get access to the full suite of Allstate benefits and work in a fast-paced startup culture. That’s more than just free breakfasts, brain breaks and ping pong. It’s a culture that encourages you to be you.
Sound like a fit? Apply now! We can’t wait to meet you.
Arity.com Instagram Twitter LinkedIn


Allstate generally does not sponsor individuals for employment-based visas for this position.

Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.

For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

It is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era.","Chicago, IL 60601 (Loop area)",Arity - Big Data Engineer,True
95,"$100,000 - $140,000 a yearWorking directly with the two founders of this profitable 30 person startup, whom Bivium has placed multiple engineers to find a great Data Integration Engineer.

Target salary in the 100-130k ish range + other great benefits  this role is a Data Engineer  a software engineer with the flair to do heavy data engineering, but not a pure data scientist. This role is about engineering things

The Founders had a prior, very profitable sale of their last startup, in the same data space.

You must know Python beyond academics or school projects and have a flair for data.

Should have great communication skills, positive attitude and real ""smarts"".

HQ in the heart of Boston's Financial district in top grade office space. Lunch every day and a gym on their floor. Health insurance premiums fully covered by the company. Competitive compensation with an opportunity to earn equity in a profitable startup.


Some of things you'll be working on:

Design, build and document (or find & use) scalable, robust tools to manage and store data
 Interact with outside data vendors to create file formats that contain required data
 Migrate and optimize existing data processing to updated formats
 Provide transparency on the state of the data processing workflow
 Implement automated testing on existing and new components
 Create tools to automatically compare data sets and output differences concisely

* * *

To learn more - scott@biviumgroup.com

About Me - The Bivium Group: Scott Dunlop on LinkedIn

I am the recruiter people work with who don't work with recruiters!

For over 20 years Ive been Bostons leading software engineer/software developer Recruiter (with nearly 100 public LinkedIn recommendations) - I'm exclusively focused on the Software Engineering market in Boston  Im always looking to expand & build strong, long-term relationships with exceptional clients and talent.

* * *






python, software engineer, data engineer, data software engineer, data science, devops, unix, scott dunlop, bivium group","Boston, MA","Data Software Engineer, Data Engineer Python Software Engine...",True
96,"The data quality engineer will write testing plans and routines, manage continuous integration and regression testing processes, and be the point person for clearing data for release according to delivery specifications. The technician must be able to function effectively in a high-volume, rapid delivery environment in which publicly available data must meet data, security and accessibility specifications without fail. Additional responsibilities: Work with product managers and development leads to create testing strategies Work with data engineers to develop automated tests Design, monitor and maintain QA reports, KPIs & quality trends for the internal data systems Create test plans and test cases Execute and automate test cases, and perform bug tracking Help create and implement quality processes and requirements Knowledgeable about industry data compliance strategies and practices, such as continuous integration, regression testing and versioning. Familiarity with triple stores and related technologies such as RDF, SPARQL, SHACL and Linked Data is a strong plus. Ability to read/write SQL queries is a plus Experience working in a Big Data environment, dealing with large diverse data sets Familiarity with Python is a plus Bachelors in Computer Science or Information Systems or 2+ years’ experience with corporate data management systems in high-compliance contexts.
The Data Quality engineer provides continuous automated and manual testing of data sets for use in internal data systems and for delivery from internal systems to clients within Disney. Such data sets can be in several formats, depending on supplier systems and client requirements, including JSON, XML, CSV or RDF. The data quality engineer works in the data technology team with the senior staff data engineer and the data engineer. 588469","Burbank, CA",Data Quality Engineer,True
97,"About the Opportunity
An asset management company in New York City is actively seeking a new Big Data Engineer for a promising position with their growing staff. In this role, the Big Data Engineer will be responsible for working on collecting, storing, processing, and analyzing of huge data sets to design the optimal software solutions. Apply today!

Company Description
Asset Management Company

Job Description
The Big Data Engineer will be responsible for:
Working closely with Financial Analysts to develop new algorithms to automate the investment process
Processing unstructured data into a form suitable for analysis
Supporting business decisions with ad hoc analysis, as needed

Required Skills
8+ years of strong Programming experience in Python or C++; 6+ years of strong SQL experience, including designing data warehouse schemas and tuning performance of very complex SQL queries
Bachelor's Degree in a related field
Experience processing large amounts of structured and unstructured data at scale (including writing scripts, web scraping, calling APIs, writing SQL queries, etc.)
Ability to clean and scrub noisy datasets
Ability to build custom software tools
Experience with AWS/EMR and other web services
Great interpersonal skills
Excellent communication skills (written and verbal)
Strong attention to detail
Highly organized
Able to multitask efficiently and effectively

Desired Skills
Experience with statistical analysis, data mining, machine learning, natural language processing, or information retrieval
Experience with Spark and MapReduce
Experience with Data Visualization (Tableau)","New York, NY",Big Data Engineer,True
98,"At TMP Worldwide, we ask ourselves important questions to find the big story behind the data. We're a robust team of analysts, problem solvers, learners and idea-churners working to solve complex problems for our clients.

The Sr Data Engineer 's role is to bridge the gap between our business and client needs, and the Data Engineers. This includes participation in requirements gathering, analysis, planning, and developing solution. This individual will have a passion for data and experience using tools to interact and analyze it.
RESPONSIBILITIES
What does a great Sr Data Engineer do?

Gathers requirements, analyzes, creates design documents, and performs impact analysis.
Partner with senior leaders from product, engineering, and Performance Media Analysts to identify opportunities for improvement and drive decision-making.
Transform raw data into meaningful and impactful analysis characterized by strong data governance, technique transparency and aggressive documentation.
Builds processes to validate and ensure data integrity.
Creates test plans, test cases, test scripts and performs testing of the related environments.
Act as an internal consultant to help business users develop, enhance and maintain the metadata layers, reports, report definitions and dashboards.
Raise the skill level of entire analyst team through the creation of exemplary work, mentorship and the introduction of better practices, processes and tools.
Assist users with problems and resolves issues independently.


QUALIFICATIONS
Requirements for consideration
3+ years of experience writing queries against traditional relational and dimensional databases.
2+ years of experience building interactive dashboards with Tableau.
An in-depth knowledge of data visualization theory and technologies strongly preferred.
Demonstrated advanced query development and design using SQL.
2+ years of experience using Microsoft SQL Server or similar technologies
2+ years of Experience working with non-traditional databases including more than one of the following systems: Spark, Hadoop, Cloudera, MongoDB, Teradata, Hive, Azure, Cassandra, etc. a plus.
2+ years of experience with data warehouses, data marts, and OLAP technologies.
Experience in Google Cloud Services strongly preferred.
Experience with web services integration architectures, including SOAP and REST, a plus
Experience with Agile methods a plus

Join the global leader in talent acquisition technologies that's committed to finding new ways to leverage software, strategy and creative to enhance our clients' employer brands - across every connection point.
We're looking for unconventional thinkers. Relentless collaborators. And ferocious innovators. Talented individuals who are ready to work towards solutions that transform the way employers and job seekers connect.

#GDMediaAnalytics
#GDDataAnalytics APPLY","Chicago, IL",Sr Data Engineer,True
99,"Data Engineer
As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core data warehousing tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.
Who are you?
You’re a smart, collaborative person who is passionate about technology and driven to get things done.
You’re not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
What technologies will you be using?
Everything. It’s about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We are tool and technology agnostic, so we work across the board.
Qualifications:
Bachelor’s degree in Computer Engineering, Computer Science, or related discipline
5-7+ years relevant experience
Understand different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
4+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
2+ years of experience working on AWS, GCP, or Azure
Experience working with relational databases
Strong analytical problem-solving ability
Great presentation skills
Great written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and cloud-specific data pipeline tools such as Airflow, Glue, and Dataflow
What does our recruitment process look like?
﻿Our process is highly personalized. Some candidates complete their process in one week, others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless how long or short the process may be for you, the most important thing is that you find your dream job.

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.","Los Angeles, CA",Data Engineer,False
100,"$100,000 - $200,000 a yearAre you an experienced Data Engineer who is seeking an opportunity in a Java focused data engineering organization?We have partnered up with a financial, Serie A funded company based in Atlanta, Georgia who is seeking to expand their development team of data engineers. The data engineer will partner with other businesses in developing analytical tools to improve market efficiency. This position will consist of analyzing data, identifying trends, and identifying technological opportunities for the business.This company is looking for those who have a supreme understanding of Java and can apply those understandings into the development of their unique artificial intelligence. Ideally, you should have experience in machine learning and be able to recognize when something is optimal or obsolete.Your opportunity lies here!What will you be doing?Working on complex data applicationsUsing high performance code Java and ScalaLeading project developments and experiments in data scienceWorking with a team of 5-10 other engineersDesign and develop new features for a company with 80+ employeesRequired SkillsBachelor’s degree or Advanced degree (Master’s, PhD) in the field of computer science, engineering, or mathematics3+ years of experience in software engineering, including testing and deploying software systems2+ years Professional experience in Java or ScalaUnderstanding of a variety of Data science algorithmsExceptional analytical and big-data skillsWhat will I get: Market-leading salary plus bonuses401kFlexible vacationQuarterly incentivesFlexible working hoursHealth, dental, vision coverageDon’t miss out on this life-changing opportunity! Apply now!Job Type: Full-timeSalary: $100,000.00 to $200,000.00 /year","Atlanta, GA",Data Engineer,False
101,"InternshipFacebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Would you like to work with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll have the opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

This is an internship position based in Menlo Park, Seattle, or New York.
RESPONSIBILITIES

Architect, implement and deploy new data models and data processes in production.

Perform data analysis to generate business insights.

Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs.

Build data expertise and own data quality for allocated areas of ownership.

Manage data warehouse plans for a product or a group of products.

Support critical data processes running in production.
MINIMUM QUALIFICATIONS

Pursuing a BS, MS or PhD degree in one of the following areas: Computer Science, Mathematics, Physics, or related technical field, preferred

Programming expertise in a language of your choice

Knowledge of SQL

Knowledge of database systems

Curious, self-driven, analytical and excited to play with data

Ability to thrive in a fast paced work environment

Returning to at least one semester of school at the end of internship","Seattle, WA","Data Engineer, Analytics, Intern",False
102,"Auth0, a global leader in Identity-as-a-Service (IDaaS), provides thousands of enterprise customers with a Universal Identity Platform for their web, mobile, IoT, and internal applications. Its extensible platform seamlessly authenticates and secures more than 1.5B logins per month, making it loved by developers and trusted by global enterprises. Auth0 has raised more than $110 million to date and continues its global growth at a rapid pace. We are consistently recognized as a great place to work based our outstanding leadership and dedication to company culture, and are looking for the best people to join our incredible team spread across more than 35 countries!

Auth0 is loved by developers and trusted by global enterprises. More than 500.000 unique users visit Auth0.com each month due to our compelling content and the functionality of our identity platform. Our mission is to simplify developers' lives, improve security, and reduce identity TCO for our enterprise customers, by making identity simple, secure and extensible. We strive to maintain a welcoming and inclusive culture built on the principle of ""No B.A.P."" (No BS, No A-holes, No Politics).

As Data Engineer, you will convert the data we process everyday from our users into actionable information to better understand our customers and give them an even better service every day.
Responsibilities:
Transform data from different data sources to integrate with our Data Warehouse in Redshift.
Create ETLs to transform that data into usable formats.
Design and develop reports to help make business decisions.
Design infrastructure components.
Integrate to other external systems like Stripe, Zendesk and others.
Skills and Abilities:
Experience as a Data Engineer.
Desire to learn about Auth0's business.
Experience with R and R Studio.
Experience with SQL, Python and Bash.
Experience with BI reporting tools like Tableau

Auth0 is an Equal Employment Opportunity employer. Auth0 conducts all employment-related activities without regard to race, religion, color, national origin, age, sex, marital status, sexual orientation, disability, citizenship status, genetics, or status as a Vietnam-era special disabled and other covered veteran status, or any other characteristic protected by law. Auth0 participates in E-Verify and will confirm work authorization for candidates residing in the United States.",Remote,Data Engineer,False
103,"About Us
Healthline.com is the fastest-growing health information site on the planet. Every month, over 80 million people count on our talented teams to support, guide and inspire them toward the best possible health outcomes for themselves and their families. We create authoritative content that’s highly relevant, approachable and actionable. And we complement that with a culture of genuine compassion. In short, we’re changing the consumer health information business and we need exceptional people like you to help us do it. If you share our vision for a stronger, healthier world please explore Healthline Media and let’s talk.

Healthline has also recently become a certified ""Great Place to Work"" You can check out our review and get a better idea of what it's like to work @ Healthline here: http://reviews.greatplacetowork.com/healthline

Location
San Francisco, CA

About the Team
The Healthline date engineering team is committed to creating a positive contribution to the health and wellness of our users by using data. We have an opportunity to build a data platform, with collaboration with the product and business intelligence team. We are using some of the leading edge cloud technologies from Google to process all the data that is generated by our users. We strive to be a fast-moving team that supports each other along the way, which requires each of us to be committed to the team and aligned with our goals/value.

About the Role
As a Data Engineer, you will be working closely with the Sr. Director of Data Engineering as one of the very first member data team. It’s an opportunity to have significant input into the technical direction and architecture of our new data platform. You will also work closely with product management and various business owners to build an infrastructure this is critical to the success of the company.
What are your Qualifications
Proficiency with Python and SQL required
Proficiency with MySql
Experience with BigQuery
Experience with GCP and it’s services
Nice to have
Knowledge of statistics
AWS Data Pipeline
Data Studio
Tableau
Adtech and/or CDP experience a big plus
What You’ll Do and How You’ll Make an Impact
Build and support our data pipelines to help power our dashboards, reports and analytics teams
Work together with the engineering and product team come up with better and more efficient ways to collect and process the data generated
Collect and process data from our ad, event aggregation, and various 3rd party systems
Reasons Why You Should Apply
You are psyched (as opposed to wary) to take the lead on making the platform a central part of the product offering.
You want to be a part of creating a culture within a small team
You strive for simplicity even for complex problems
Why You’ll Love Working at Healthline …
We are the fastest growing health information site on the planet, and the 2nd largest health site in the US (per comScore)! Over 80 million people count on our talented teams to support, guide, and inspire them toward the best possible health outcomes for themselves and their families.
We are a purpose-driven organization with a vision to create a stronger, healthier world.
We work together to achieve our mission with humility and genuine respect for each member of our team.
We are smart, innovative, and inspiring changemakers.
Transparency and honesty earn us the trust of each other and our users.
We feel with our users and are committed to being their true ally in their lifelong pursuit of health and well-being.
We’re all in for having fun, living well, and promoting good health. We’re proving that you can work hard and be happy.
We’ve got cozy canine companions in the office. Pooches make Healthline HQ fun!
A smart, experienced leadership team that wants to do it right and is open to new ideas.
We offer competitive compensation packages and comprehensive health benefits.
Does this sound like it was written for you? Excellent! Please apply and let’s explore this together.

Healthline Media is committed to a policy of Equal Employment Opportunity and will not discriminate on any legally recognized basis, including but not limited to race, color, religion, sex, sex stereotyping, pregnancy (which includes pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We also provide reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act as amended and applicable state and local law. No person shall be excluded from participation in, be denied the benefits of, or be subjected to discrimination on the basis of these factors. If you require an accommodation in the application process, please advise your recruiter.","San Francisco, CA",Data Engineer,False
104,"ContractTitle: Data EngineerLocation: San Jose, CA (95110)Duration: 6+ MonthsDuties: Design, build, and manage complex analytics data models in Hive/Hadoop for GTM Analytics team across all customer journey from Acquisition, Engagement, and Retention. The analytics data marts will be used by data analysts in GTM Analytics and other team to do deep dive analysis, build analytics dashboard, or other data science project.Design, build, deploy, and maintain new data models ETL pipeline with SQL query, Python, Oozie, and other script language and create/maintain workflow using Oozie.Ensure overall data quality.Skills: Querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred)Experience with Hadoop/big data environments to synthesize and analyze data.professional experience in the data warehouse spaceGood attention to detail and ability to QA multiple data sourcesExperience working on building scalable ETL pipelines, data warehousing and schema modelingExperience working with Oozie WorkflowExperience with script language such as Python2 - 3 years of relevant experienceEducation: Degree in Computer Science/Computer Engineering or a related fieldJob Type: ContractExperience:Hadoop: 2 years (Required)Python: 2 years (Required)SQL: 2 years (Required)Hive: 1 year (Required)Data Engineer: 3 years (Required)Oozie: 1 year (Preferred)","San Jose, CA",Data Engineer,False
105,"Junior Data EngineerMSI is working with one of the world’s largest, fastest growing, best companies in the world to hire top tier individuals with an education or career background in data consolidation, import and export of data, and Excel. The Junior Data Engineer will be working as part of a dynamic team and will play a critical role in a best in class, Fortune Top 20 company.OverviewThe qualified candidate needs to have data analytic skills who can sort through 2000 components, 100 PDF files, and 100 excel spreadsheet with five thousand lines effectively.The majority task of this job is to sort through equipment data and navigate through the various internal and external web portals.Create and compile equipment bill of material tracker.Create and maintain metrics and dashboards.Document review; comparison of data from one source to another; identifying missing data elements; data entry; maintaining spreadsheets; submit applications and requests via online web portals.Engage with manufacturers to obtain equipment test reports.Document meeting notes and follow up with action items.Required skillsExpertise in Excel or other data analytic programming language skill.Ability to create Pivot tables.Importation and Exportation of data from multiple data sources and consolidating into one data source/repository.Being careful about comparing data from one source to another and being able to clearly identify missing data elements; efficient data entry; maintaining spreadsheets; attention to detail.Ability to learn and master at different online web portals.Web interface / web design skills.Strong administrative and project management skills with the ability to schedule meetings via Microsoft Outlook and communicate/meet with stakeholders.Preferred skillsAbility to understand compliance regulations.Electrical equipment knowledge.Job Type: Full-time","Seattle, WA",Jr Data Engineer,False
106,"LendKey is solving a complex challenge – to improve lives with lending made simple – by helping financial institutions compete in the digital age and provide a delightful customer experience, while providing borrowers with the simple, transparent, digital borrowing experience they have come to expect and desire. LendKey works with hundreds of credit unions and banks to conduct their education finance and home improvement loan programs.
We are looking for a data engineer to help us build our data pipeline, data warehouse and technical data structure across the firm. Our data capabilities and culture are still in the early stages, so this is an opportunity to build a data platform from the ground up.
What you’ll do:
Partner with the product and engineering teams to develop scalable, extensible systems
Be the driving force behind the roadmap to normalize all of our transactional data, disparate systems, and transfer of data in way that creates a flexible and scalable data solution
Develop data governance policies & appropriate structures to ensure adherence to those policies
Ensure that solutions work well within our current code environment, that technical data initiatives are aligned effectively with development staff, and work to understand how other tiers in the technology stack influence data quality including APIs, ORMs (Object Relational Mapping) and User Interface
Responsible for managing the full life-cycle of the data pipeline and warehouse solution; including the architecture, design, development, implementation, and support of the data warehouse
Participate in creating the data design of all transactional data stores across business units and technology stacks
Work with end users to translate business questions and requirements into applications that employ the appropriate reporting tools
Assist in monitoring and troubleshooting system performance, reliability, availability, and recoverability of all data stores
Requirements
What we’re looking for:
Culture Fit:
Strong desire to work for a mission-based organization that emphasizes the importance of providing exceptional customer service and aligned with our core values: Truthful at all times; Helpful to teammates, clients, and customers; Present, committed & engaged to their teams and work; Driven to be courageous to make an impact; and Diligent & conscientious in executing every element of work.
Technical/Business Experience
Bachelor’s degree in Computer Science or related field
5+ years overall experience working in development and enterprise data architecture
Experience and background in building data platforms for financial services
Minimum 3+ years of experience with enterprise data architecture/design
Minimum 3+ years of experience in software development with deep experience in Object-Oriented, Functional, Object-Functional Language and one of the major SQL relational datastores (we are a SQL Server and MySQL shop)","New York, NY",Data Engineer,False
107,"$60,000 a yearFreightWaves, a trucking financial instruments and market data analytics startup is looking for a Data Engineer.

Founded by a group of successful transportation technology executives- FreightWaves is focused on helping transportation companies mitigate market price volatility and providing a real-time supply/demand map of the US transportation market.

JOB DESCRIPTION:

Build Relational Databases, Tables, Views, Stored Procedures, Batch Jobs, and Reports

Develop procedures and code to monitor data from vendors, our own ETL process, and data readiness in production.
Proactively find data problems early, and resolve batch job failures, even outside normal business hours.
Run ad-hoc queries to support analysis in editorial, sales, marketing, and data science departments.
Solve database performance issues via indexing, normalizing, hardware, replication

Manage data retention to prevent loss, while minimizing storage cost

Education, Skills & Experience:

Minimum of a Bachelor degree in computer science, engineering, other scientific/quantitative field
Required 3+ years of experience with relational database operations and administration

Strong quantitative orientation with excellent research and analytic skills that will support in depth analysis

Ability to quickly learn about new data, and design an appropriate storage schema
Understanding of the difference between performance of high-read versus high-write database designs

Strong communication skills, both written and verbal

Transportation industry experience is preferred but not required

Document Store / NoSQL experience is preferred but not required

Programming experience in R, Python, or other non database languages preferred but not required

WE OFFER:

An excellent start-up work environment, flat hierarchies and short decision paths.

Competitive salary and bonus potential packages.

A generous benefits package including 100% employer paid health, dental, life and vision insurance, unlimited vacation plan, workout benefits, industry education and company-paid travel to industry events, student-loan reimbursement, pet insurance, family bank account, merchant discount program, and much more!

Stock options

Training programs and career development opportunities.

Flexible working hours and work-life-balance options","Chattanooga, TN 37408",Data Engineer,False
108,"$55 - $57 an hourContractData EngineerJob Description7 or more years of relevant industry/ 5 years of domain experienceProvide Systems Engineering and Development as required with specific emphasis on Kafka, RabbitMQ, and Big Data Middle Tier frameworks. Provide System Enhancements and Codebase Deliveries as needed.This position is responsible for onboarding, QA and analyses of data sets. This role will also interface with business and product owners to understand requirements and ensure that data sets are accurate, timely and available to support the business needs.Responsibilities include: Ability to interpret data sets and define process and procedures to ensure data is received to spec and in a timely and accurate mannerIdentify and address issues with data setsEnsure appropriate procedures are in place to meet SOC 3 compliance proceduresEnsure excellent communication procedures are in place viadaily/monthly/weekly reporting to advise status of data sets and that end users are informed of data statusWork with business and product owners to develop procedures, methods, code to provide efficient tools and products to meet the needs of the end usersSupport all internal teams as needed to address questions regarding data quality, status, ETL and other platform related questions and concerns that may arise Qualifications and Education RequirementsStrong working knowledge of SQL, Python and other database query related languagesExperience in application development, “big data” analytics and ability to deliver customer focused products that are simple and easy for end users to interpretJob Types: Full-time, ContractSalary: $55.00 to $57.00 /hourExperience:Kafka: 1 year (Required)Data Engineering: 7 years (Required)Big Data: 3 years (Required)Education:Bachelor's (Required)Location:Hicksville, NY (Preferred)Work authorization:United States (Preferred)","Hicksville, NY",Data Engineer (T),False
109,"Acara Solutions (formerly Superior Group) is helping our Redmond, WA client to find a Data Engineer to join their electronics plant that is assembling a gaming device. This is a direct salaried position with full benefits. The Manufacturing Data Engineer position will be the subject matter expert on our manufacturing Execution System (MES), leading efforts to maintain the system, manage data quality, report/analyze production data and lead the effort to maximize proper use of the MES systems. Critical emphasis on integration and communication between different systems. You will slice and dice data sets, move data between different locations and develop programs/scripts to automate the transactions of data to and from many different systems. Once the data is in the proper place, you’ll develop quick, simple and easy user interfaces used by office staff, executives and factory floor personnel doing their daily jobs. This position requires a lot of customer-facing interaction. -Become the subject matter expert on the configuration and data structures of our Manufacturing Execution System (MES), assisting engineers with knowledge on how the system is used to create NPIs, processes, reports and other tasks -Work with manufacturing/engineering teams to analyze production operations data, understand daily KPIs, document daily manufacturing reports -Understand and help develop manufacturing processes in order to create the proper data required -Develop and maintain end-user programs, web applications, and reports -Develop and maintain back-end server services to integrate data between different systems and databases; This includes SQL Server databases, XML formatted data, CSV, .NET interfaces, COM interfaces, web services and plain text formats -Data mining and harvesting; maintain database queries, data modules, bug fixes/JIRA -Develop and maintain Microsoft SQL Server code, including views, stored procedures, triggers, SSIS, DTS, replication, linked servers, indexing and maintenance plans -Re-factor existing legacy software programs and code when appropriate -Relational database design and optimization -Application security -Documentation from a technical and end-user perspective -While this position is very much a data engineering job, you must have an appreciation for other IT roles. Their small technology group is an integrated team; Sys Admins sometimes write simple code and coders sometimes assist with Sys Admin tasks. You must be used to wearing multiple hats.
Required Skills / Qualifications:
Bachelor’s Degree in Computer Science, Data Engineering or related discipline, or 10 years general software/data engineering experience, or equivalent combination of education and experience
Min 5 years of data engineering experience
Minimum 2 years of manufacturing, industrial or ecommerce experience
Min 2 years of ETL or data integration experience
Min 1 years of parametric data pass/fail experience
Min 2 years of experience with at least 3 of the following: ASP.NET C#, COM interfaces and T-SQL
Min 1 year of experience with web application frameworks, such as MVC, Webform or others
Min 1 year of experience with scripting languages, such as Javascript, batch or PowerShell
Min 1 year of experienc with source control and code versioning (SVN, Git, or other) experience
Min 3 years of experience with MS Office Suite (Advanced Excel, Access, Outlook, PowerPoint, etc)




Preferred:
Experience with advanced analytics platforms such as Tableau, Microsoft Bl or others

Go Beyond. www.superiorjobs.com.
EEO Employer - Minorities / Females / Disabled / Veterans / Sexual Orientation / Gender Identity.","Redmond, WA",Data Engineer (Manufacturing Gaming Plant),True
111,"TITLE: Senior Associate, Data Engineer

DUTIES: DFS Corporate Services LLC seeks Senior Associate, Data Engineer in Riverwoods, Illinois to work with business clients to design, develop, test and deliver solutions that meet customer requirements. Develop data driven solutions with current and next generation big data technologies to meet evolving business needs. Develop Greenfield capabilities leveraging open source next-generation technologies. Code and integrate open source solutions into an enterprise Hadoop ecosystem. Utilize multiple development languages/tools such as Python, SPARK, Hive, R, and Java. Participate in prototype solutions by integrating various open source components. Operationalize open source data-analytic tools for enterprise use. Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and data providers. Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes. Ensure proper data governance policies are followed by architecting and building data lineage, quality checks, and data classification systems and frameworks. Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes
Skills:
REQUIREMENTS : Master’s degree or foreign equivalent in Computer Science, Mathematics, Computer Engineering or a related field and one (1) year of experience in job offered or related position: coding and integrating open source solutions into an enterprise Hadoop ecosystem; developing real-time data ingestion and stream-analytic solutions using technologies including Kafka, Apache Spark, NIFI, Python, HBase and Hadoop; utilizing multiple development languages and tools including Python, SPARK, Hive, R, Java; and utilizing technologies including SAS, SQL and Tableau.

QUALIFIED APPLICANTS: Please apply directly through our website www.mydiscovercareer.com for Job ID 45361 by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.","Riverwoods, IL","Senior Associate, Data Engineer- IHM",True
112,"As a Master Data Engineer you will provide engineering leadership to create and enhance data solutions enabling seamless integration and flow of data across our data ecosystem which includes AWS. Additionally you will provide senior level technical consulting to peer data engineers during design and development for highly complex and critical data projects. Some of these projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as NiFi, Lambda, Step Functions, Java, Python, Spark, etc. Additionally, you will work on real time processing solutions using tools such as Flink, Storm, Kafka, and Kinesis. You will deploy application code using CI/CD tools and techniques and provide support for deployed data applications and analytical models.
This role will sit in our state of the art, downtown Chicago location at 350 N. Orleans.
Responsibilities:

Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.
Ability to quickly identify an opportunity and recommend possible technical solutions.
Utilize multiple development languages/tools such as Python, SPARK, HBase, Hive, Microsoft R, Java to build prototypes and evaluate results for effectiveness and feasibility.
Work heavily within the AWS ecosystem, using AWS services
Operationalize open source data-analytic tools for enterprise use.
Develop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase, Kinesis, and EMR.
Custom Data pipeline development (Cloud and locally hosted)
Work heavily within the Hadoop ecosystem and migrate data from Teradata to Hadoop.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.
Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes.
Skills:
Skills Required:

AWS data services (Lambda,Glue, EMR, Kinesis, Step Functions, Data Pipeline)
Deep understanding of the Hadoop technology stack
Building custom NiFI processors
Data pipeline development
Experience in developing Python / R applications
Spark application coding in Scala / Python (pySpark)
Deep knowledge and very strong in SQL, and Relational Databases
Strong in Unix / Shell scripting
Experience in creating very efficient HiveQL and SparkQL queries and can educate peers on the topic
Bachelors degree in Computer Science, related field, or equivalent work experience
Leadership Skills:

7+ years of experience of being a lead engineer and able to coach/provide guidance to peer and junior engineers.
Excellent written and verbal communication, presentation and professional speaking skills
Collaborative individual who excels in working within a team and with business partners to identify, develop and deliver innovative data solutions
Influencing skills/ability. Must be able to work with effectively with different levels of management and all business areas.
Ability to demonstrate leadership to managers, and peer level staff.
Ability to build and leverage external relationships.
Decision making abilities while gathering information and then put your decisions into action.
Passionate learner who enjoys education through class room training and self-discovery on a variety of emerging technologies
#LI-KE
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.","Chicago, IL",Master Data Engineer,True
113,"About Your Role:

As a Data Engineer on the Commerce Team, you will integrate data from a variety of sources and surface it in our reporting platform so that users can make data-driven decisions and derive insights. You will also provide data analysis, reports and dashboards to your team and Dotdash as a whole. As a Data Engineer, you will facilitate the free and open use of data across the organization and encourage data culture through weekly meetings, outreach and technical consulting.

About Your Contributions:
Analyze raw data to answer ad hoc questions
Map business rules onto our raw data to enrich customer reporting.
Build reports and dashboards to drive insights
Work collaboratively to solve data challenges for your team
Build and maintain data integrations
Find ways to use new and exciting technology to solve data challenges
About You:
Proficient writing SQL queries to analyze data
Experience with a BI reporting platform (Looker, QuickSight, Tableau)
Strong analytical and problem solving skills
Experience building data integrations with Python, Spark or Java
Comfortable with the Linux CLI
Looker and LookML experience a plus
About Us:
For more than 20 years, Dotdash brands have been helping people find answers, solve problems, and get inspired. We are one of the top-20 largest content publishers on the Internet according to comScore, a leading Internet measurement company, and reach more than 30% of the U.S. population every month. Our brands collectively have won more than 20 industry awards in the last year alone and, most recently, Dotdash was named Publisher of the Year by Digiday, a leading industry publication. Our brands include Verywell, The Spruce, The Balance, Investopedia, Lifewire, TripSavvy and ThoughtCo.
Dotdash embraces inclusivity and values our diverse community. We are committed to building a team based on qualifications, merit and business need. We are proud to be an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","New York, NY",Data Engineer,False
114,"What You’ll Do
 Design and deliver automated transformation of large data sets influencing MapReduce, streaming, and other new technologies
 Use HBase, Elasticsearch, etc. to ingest transformed data at scale
 Collaborate with security experts to deliver high-impact web-based APIs
 Implement high-volume data integration solutions
 Analyze, monitor, and optimize for performance
 Produce and maintain high-quality user documentation
Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.
Who You Are
 Recent graduate or on your final year of studies toward a Master's degree in Computer Science or a related technical field
 Minimum of a 3.0 GPA or equivalent
 Track record of developing technology to enable large scale data transformation
 Strong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.
 Possess knowledge of software engineering standard methodologies
 Real passion for solving hard problems and exploring new technologies
 Excellent communication and user documentation skills
Why Cisco
At Cisco, each person brings their own rare talents to work as a team and make a difference.
Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.
 We connect everything – people, process, data and things – and we use those connections to change our world for the better.
 We innovate everywhere - From launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.
 We benefit everyone - We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.
Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us! #WeAreCisco
#univsoftwarejobs #STO #SoftwareEngineer #ComputerScience #Security @WeAreCisco
This position is available to Master's level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions.
Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.","San Jose, CA",Big Data Engineer - Masters (Full Time) – United States,False
115,"At Pandora, we're a unique collection of engineers, musicians, designers, marketers, and world-class sellers with a common goal: to enrich lives by delivering effortless personalized music enjoyment and discovery. People—the listeners, the artists, and our employees—are at the center of our mission and everything we do. Actually, employees at Pandora are a lot like the service itself: bright, eclectic, and innovative. Collaboration is the foundation of our workforce, and we’re looking for smart individuals who are self-motivated and passionate to join us. Be a part of the engine that creates the soundtrack to life. Discover your future at Pandora!
Job Description
Data engineers at Pandora are responsible for the services and infrastructure capable of processing and making available the extensive volume of data produced on its platform each day. Data Engineers build the infrastructure needed to enable analysts and scientists to query and author data products that operate against our largest collections (billions of events per day).
Responsibilities:
At Pandora the data team supports a variety of business functions including our science, marketing, product, finance and sales teams. You should have a solid understanding of Java software development, and take personal responsibility for testing the code you write. You should have strong academic credentials and a degree in Computer Science or a related field. You should be enthusiastic about learning new technologies and skills. You must be capable of managing your time well and working collaboratively. Excellent communication skills, both written and verbal, are required.
Qualifications:
3+ years development experience of which 1-3 years are focused on data or analytics engineering working with big data technologies (Hadoop: i.e. MapReduce, HDFS, Tez, Hive, Spark)
Experience with one of the following distributed databases: MySQL, Postgres, Redis, NoSQL or NewSQL
Experience developing in one of the following: Java, Scala, C/C++, or Python
Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server side software for deployment
Experience developing SQL applications of significant complexity
Experience developing service oriented architectures/orchestration including the support of data science
Experience with API design/development (i.e. RPC, REST, JSON, XML, SOAP)
Significant experience unit testing with frameworks i.e. JUnit
“We're considering candidates for multiple positions and levels. Successful candidates will be placed at appropriate level depending on their qualifications or experience.”

Pluses:
Experience collaborating with data scientists, exposure to machine learning algorithms and/or statistical modeling methods.
Experience with recommender, or search systems.
Experience with Apache Spark and Kafka.
Experience working across the full technology stack
BA/BS or above in Computer Science or a related field
Pandora is committed to diversity in its workforce. Pandora is an equal employment opportunity employer and considers qualified applicants without regard to gender, sexual orientation, gender identity, race, veteran or disability status. Women and people of color are encouraged to apply.
Pandora is also a VEVRAA federal contractor. Pandora requests priority referrals of protected veterans from each ESDS, as required by regulation.
If you believe you need a reasonable accommodation in order to search for a job opening or to apply for a position, please contact us by sending an email to disability@pandora.com This email box is designed to assist job seekers who require a reasonable accommodation to the application process. A response to your request may take up to two business days.
In your email, please include the following:The specific accommodation requested to complete the employment application.The location or office to which you would like to applyThe subject of the email should read ""Request for Reasonable Accommodation"".","Oakland, CA",Data Engineer,False
116,"Named as one of LA's Best Places to work in 2018, Prodege, LLC, parent company of rewards community Swagbucks.com & cash-back shopping sites MyPoints.com & ShopAtHome.com has awarded over $500 million to its members. Our business solutions brands, ProdegeMR (market research), ProdegeDR (direct response) & ProdegeVN (video network) allow our partners to reach, influence and acquire consumers online.

Join Prodege and help us ""Create Rewarding Moments"" for consumers around the world!
We are currently looking for a Data Engineer to join our team. As a key member of our Technology and Business Intelligence team, this position will:
Design, build and maintain data loading routines using a variety of tools including open source ETL tools, stored procedures, scripting and programming languages such as Java, C, Python, etc.
Improve the performance and scale of our data operations to deliver information on a real-time basis.
Design, build and maintain data QA routines to ensure that our systems deliver quality information to our customers.
Contribute to the design and architecture of our Enterprise Data Infrastructure.
Work closely with end-users and technical team members on data ingestion projects, investigating and solving data and reporting problems.
Skills and experience required:
5+ years development experience using MySQL, Oracle or other relational database software experience. Experience writing and optimizing complex SQL queries.
5+ years developing back end data processing routines for data warehouses or any backend data system using open source tools, and/or high level languages.
5+ years experience developing database related software using Java, C, C++, etc.
Excellent verbal and written communication skills.
B.S. Computer Science, Math, Physics or equivalent experience.
Knowledge of BI Analytical tools a plus.
Our Accolades:
Los Angeles Business Journal – Best Places to Work, August 2018, 2017, 2016, 2014 & 2013
The 2016 & 2017 Career Launching Companies, Wealthfront
Inc 500 – Fastest Growing Private Companies
Deloitte’s Technology Fast 500
Los Angeles Business Journal – Fastest Growing Companies
Los Angeles Business Journal– Best Places to Work
President Josef Gorowitz - Ernst & Young 2014 Entrepreneur of the Year: Los Angeles, Advertising Category
President Josef Gorowitz - SoCal Tech Top 50 Executives Award 2013
CEO Chuck Davis – Los Angeles Venture Association (LAVA) Hall of Fame Honoree
CFO Brad Kates - Los Angeles Business Journal: 2014 CFO of the Year, Winner
CTO Shane O’Neill - Los Angeles Business Journal: 2014 CIO of the Year, Finalist","El Segundo, CA",Data Engineer,False
117,"THE ROLE

As a Data Engineer on the Business Intelligence team, you will lead data collection and performance improvement efforts across multiple data sources and platforms.

RESPONSIBILITIES


Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python and AWS
Facilitate the transfer of data from internal sources to external tools and platforms
Facilitate the transfer of data from external sources to our analytics database
Identify gaps in our data collection and collaborate with product and engineering on data capture specifications
Compile and reconcile data from multiple sources
Develop a vision to extend/improve our existing analytics and reporting technology stack
Stay up to date with the latest technologies and trends in analytics

REQUIREMENTS


Strong understanding of SQL and relational databases
Experience in database administration is strongly preferred
Experience building and maintaining batch processes, using AWS Data Pipeline, Airflow, Luigi, cron, quartz, etc.
Experience inspecting, moving, and transforming data on the *nix command line using shell scripts, Unix tools such as sed, awk, cut, sort, etc.; or higher level languages like Python/Perl/Ruby
3+ years technology experience working in an Engineering/Development/Data Warehouse team (or alternatively, organization)
A solid foundation in computer science fundamentals with particular expertise in data structures, algorithms, and design
Experience building systems that must deal with the complexities of integrating with third parties

","New York, NY","Data Engineer, Business Intelligence",False
118,"Are you a talented, quant developer/data engineer that wants to get closer to the business? Are you passionate about data and how it can be used in investing? If so, we need people like you to help us:

Build solutions to collect, enrich and automate data for use in customer experience enhancement, and alpha generation
Creation of new capabilities and modules in our data pipeline
Develop reporting and data visualization solutions, as well as looking to build out a dynamic platform

A brand new group within the Investment Bank's Corporate Client Solutions (CCS) business based around Big Data, Machine Learning and Artificial Intelligence. The team will leverage in-house cutting edge knowledge and data to develop predictive analytics models and research to systematically identify traditional investment banking opportunities across Advisory and Capital Markets. The team is embedded within the Business and will partner with different teams across CCS Advisory and Capital Markets globally to develop innovative client and transaction targeting solutions and expand the department into new areas.

You have:

Bachelor’s degree in STEM or finance related disciplines
5 to 10 years experience programing with multiple technologies and software design within Investment Banking or Hedge Fund industries is essential
Experience working in a multi-layered distributed architecture is essential.
Experience with working in Hadoop and distributed computing frameworks is a plus
Understanding of data science, machine learning, and AI is a plus.
Strong analytical and problem solving skills (data analysis and requirement documentation)
Outstanding data skills
Excellent project management skills and ability to prioritize issues
Excellent oral and written communication, organizational and client facing skills
Strong interest in working in investment banking and financial services

You are:

ready to join a fast growing team in a dynamic and challenging environment
relentless in pursuing new ideas and self-improvement
Ability to self-manage and prioritize workload
full of energy and able to persevere through technical issues
a team player willing to learn / share solutions and best practices from your colleagues
performance and detail-oriented

Expert advice. Wealth management. Investment banking. Asset management. Retail banking in Switzerland. And all the support functions. That's what we do. And we do it for private and institutional clients as well as corporations around the world.

We are about 60,000 employees in all major financial centers, in more than 50 countries. Do you want to be one of us?
Together. That’s how we do things. We offer people around the world a supportive, challenging and diverse working environment. We value your passion and commitment, and reward your performance.

Keen to achieve the work-life agility that you desire? We're open to discussing how this could work for you (and us).

Why UBS? Video
Are you truly collaborative? Succeeding at UBS means respecting, understanding and trusting colleagues and clients. Challenging others and being challenged in return. Being passionate about what you do. Driving yourself forward, always wanting to do things the right way. Does that sound like you? Then you have the right stuff to join us. Apply now.
UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.","New York, NY",Quant Developer/Data Engineer - Investment Bank,False
119,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile computing. Create groundbreaking technology for large-scale systems, spoken language, big data, and artificial intelligence. And work with the people who built the intelligent assistant that helps millions of people get things done — just by asking. Join the Siri team!
Key Qualifications
Strong data analytical skills
Excellent coding skills in one or more of following languages — Python, Shell scripts, java etc
Experienced in big data processing, tools development and/or web crawling
Outstanding communication skills
Gradings tasks and tools preferred: e.g. for transcribing speech audio, labeling tasks, language translation, etc.
Knowledge in the field of language technologies/NLP, such as language modeling, machine translation, text mining, parsing, etc.
Multilingual speaker
Description
We are looking for highly motivated engineers to fulfill our data needs and drive the product quality to the next level. You will be working closely with a group of talented researchers and engineers on the groundbreaking machine learning technologies. Your responsibilities include data creation, data management, processing pipeline and tools development. The role also involves the collaborations with other multi-functional teams on data related projects.
Education
BS or MS in Computer Science or equivalent experience","Santa Clara Valley, CA",Siri - Data Engineer,False
120,"WHAT’S YOUR NEW ROLE ABOUT?
We’re hiring a data engineer to join our growing data science and analytics team at Opta. This engineer will work on a growing multidisciplinary team that is responsible for the development and implementation of advanced sport metrics. The analytics team builds data science models, logic, and metrics based on sport data. Those data and metrics are then sold through Opta’s API and user interfaces to external customers in the media and professional sports.
On the job you will have a major influence over the new frameworks and pipelines we are building to productize our models and metrics. We are looking for someone who relishes the design process and can work with high autonomy. This is an individual contributor role that will have opportunities to work on high impact and new projects.
HERE’S YOUR ROLE BROKEN DOWN (NOT ALL OF IT, JUST THE MOST IMPORTANT STUFF)
Projects you will work on:
message based near real time data pipelines
distributed message based near real time data pipelines
batch analytics based data pipelines
machine learning model storage, serving, versioning, and monitoring
model feature storage
simple front ends
Technologies you will touch:
rabbitmq
kafka
exadata
hdfs
spark
redis
python
airflow
During the first month you will:
meet the rest of the team, and our internal stakeholders
learn about the data science models and logic that have been developed, and those currently in development
learn about the current implementations of our models
learn about Opta and Perform’s current data infrastructure and future plans
begin work on some of our current projects
During the first three months you will:
develop a close working relationship with the data scientists and analysts on the team
take ownership over a project that’s in its development phase
set the team’s best practices with regards to data engineering
contribute to an inquisitive, respectful, and fun work environment
During the first year you will:
take at least one project into production
see the outputs of your work being sold in the market
blog or speak about the pipeline you took ownership on
take a mentorship role with the data scientists with regards to data engineering
learn machine learning and statistics concepts to supplement your work
DO YOU HAVE THESE ESSENTIALS?
3+ years of data engineering with major contributions to the projects you work on
experience designing solutions to complex data problems
experience with simple message based systems
experience with batch ETL
experience with data warehousing
experience coding in python
an interest in data science and machine learning
IT WOULD BE GREAT IF YOU HAD THESE DESIRABLES TOO
experience with kafka and spark
experience with scala
experience with orchestration frameworks like airflow or luigi
HERE’S A LITTLE MORE ABOUT US
At Perform Group we like to consider ourselves a progressive, dynamic, fun and fast-paced global sports media broadcasting company. We are passionate about what we do and good at it too! We have over 2,800 employees with an extensive network of freelance specialists. We have a strong team with experts from Netflix, Prime Video, BBC iPlayer, YouView and Now TV. We’re growing rapidly and launching into new global markets every year.
DAZN is our sports streaming service that allows fans to watch their sport, their way, live or on-demand. With access to the world’s best sports, fans can watch their favourite teams, leagues and players anytime, anywhere, for one simple, affordable price and with no long-term contract. DAZN has over 8,000 live events a year and features the widest array of live sports ever offered on one TV service. DAZN can play, pause and rewind anytime with no commercial interruptions and no long-term commitments.
DAZN is currently available in Canada, Germany, Austria, Switzerland and Japan on most connected devices including Smart TVs, smartphones, tablets and games consoles. In 2018, we’re soon to be launching in the US and Italy.
Find out more at www.performgroup.com and http://media.dazn.com

THE BENEFITS YOU WILL ENJOY WHEN YOU JOIN WILL INCLUDE…
25 days’ annual leave (increasing by 3 days after 3 years), single cover Aviva Private medical insurance*, Life Assurance (4x annual salary), matching pension contributions up to 5%, Travel Loan, Cycle to Work Scheme* and more.
And there’s more…… you’ll have access to the Perform/DAZN online learning portal (MindTools) and be part of our Career Deal, which aims to support your continued professional development. We also have a structured management development programme and a financially rewarding ‘Refer a Friend’ scheme. If you fancy a move abroad, Perform are currently seeking the best talent in the world.
Please Note – some of these benefits will be available to you upon successful completion of your probation.
Apply here","New York, NY",Data Engineer (US 0365),False
121,"$60,000 - $85,000 a year-New Grads Welcome!We are looking for a Database Engineer to join the Data Engineering team. The role will be responsible for designing, developing and improving our data technology for our enterprise-class applications. The ideal candidate will have experience with ETL process, database architecture and a solid understanding of data processing using a combination of python, shell scripting, and PHP.In this position, you'll be responsible for:Will be required to write “clean”, well-designed codeAbility to analyze client feeds for accuracy and standardsIdentifies and maintains company databases, including data sources, data structures, data organization, and data optimizationTroubleshoot, test and maintain the core product software and databases to ensure strong optimization and functionalityContribute in all phases of the development lifecycle and other various projectsDevelop and deploy new features using Agile and similar methodologiesAssist in the review of data-system performance and the development and implementation of improvementsSupport production processing by troubleshooting and resolving issues that arise around data processing, performance and set-upDesign, develop and maintain efficient and robust ETL workflows which produce data extracts and process feeds between various internal and third-party partnersBecome a subject matter expert for the internal ETL processesYou might be a good fit if you have:At least 3+ years of experience with database design, optimization, and tuningAt least 2+ years of experience using GithubAt least 1+ years of experience in continuous integration and development methodologies tools such as Jenkins2+ years of experience in an Agile development environmentKnowledge of AWS or similar cloud computing platformsBS/MS degree in Computer Science, MIS, Engineering or a related subject or equivalent experienceData integration and a good understanding of ETL, data warehousing, and data mart conceptsMust have a solid understanding of relational databases and SQLGit or other version control toolsAutomation and continuous integrationComfortable with shell scripting in a Unix environmentBackground in healthcare data is a plusStrong organizational and communication skills (both written and verbal)Job Type: Full-timeSalary: $60,000.00 to $85,000.00 /yearExperience:using Github: 2 years (Preferred)Internship: 2 years (Preferred)Location:Lyndhurst, NJ (Required)Work authorization:United States (Required)","Lyndhurst, NJ 07071",Data Engineer,False
122,"ContractJob FunctionsBuild an understanding of data sources and downstream systemsLiaise with key stakeholders to understand requirements, business definitions and the potential value of different data Sets.Support, design, implement and document solutions for loading, piping and exposing data from multiple sources.Support and build well-engineered data systems to support analytical needs using AWS .Assure accuracy of data processing and outputs through consistently high software development skills, adherence to best practice, thorough testing and peer reviews.Habitually approach problem solving with creativity and resourcefulness; carefully evaluate risks and determine correct courses of action when completing tasks.Skills/Abilities:Demonstrable professional experience designing, building, and maintaining data systems and processes using AWS Big Data PlatformDemonstrable, hands-on professional Data Analytics skills using Python, Sql. Java is a plus.Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutionsDemonstrated willingness and ability to effectively work with various team members when gathering requirements, delivering solutions, and eliciting suggestions and feedbackExtremely quick learner both in terms of new technical skills and acquiring domain knowledgeExperience:Experience working in Python development environments for Data wrangling and analytics.5+ years as Data Engineer or equivalent roleExpertise using cloud-based systems and services to acquire and deliver data via APIs and flat filesExtensive hands-on experience working with data using SQL.Extensive hands-on experience working with AWS S3, Redshift. [Glue a Plus]Education:Bachelor’s Degree in Computer Science or closely related disciplineJob Type: ContractExperience:Cloud/AWS: 5 years (Required)Python: 5 years (Required)Education:Bachelor's (Required)License:W-2 (Required)","New York, NY 10036",Data Engineer,False
123,"ContractOur client is seeking a Big Data Engineer for a 3 month contract to hire opportunity in Plano TX. Qualifications:
8 + years of professional experience
3+ years of experience with Big data technology and analytics
3+ years of experience in ETL and ELT data modeling
Experience working with traditional warehouse and correlation into hive warehouse on big data technologies
Experience setting data modeling standards in Hive
Developing automated methods for ingesting large datasets into an enterprise-scale analytical system using Scoop, Spark and Kafka
Experience with streaming stacks like Nifi and PySpark
Understanding of Big Data tools (e.g., NoSQL DB, Hadoop, Hbase) and API development consumption.
Proficiency in using query languages such as SQL, Hive
Understanding of data preparation and manipulation using Datameer tool
Knowledge of SOA, IaaS, and Cloud Computing technologies, particularly in the AWS environment
Knowledge of setting standards around data dictionary and tagging data assets within DataLake for business consumption.
Experience in one or more languages (e.g., Python or Java, Groovy)
Experience with data visualization tools like Tableau
Identifying technical implementation options and issues
Partners wand communicates cross-functionally across the enterprise
Ability to explain technical issues to senior leaders in non-technical and understandable terms
Foster the continuous evolution of best practices within the development team to ensure data standardization and consistency
Experience in agile software development paradigm (e.g., Scrum, Kanban)
Strong written and verbal communication","Plano, TX",Big Data Engineer,True
124,"ContractJob SummaryTitle: Data Engineer Location: Seattle, WADuration: 6 Months with Possible Extension or ConversionMust have: Previous experience in SQLETL developmentRequired Skills: A desire to work in a collaborative, intellectually curious environment. Degree in Computer Science, Engineering, Mathematics, or a related field and 2 years industry experience. Demonstrated ability in data modeling, ETL development, and data warehousing. Data Warehousing Experience with Oracle, Redshift, Teradata, etc. Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Preferred skills:  Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets. Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc). Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets. Experience building data products incrementally and integrating and managing datasets from multiple sources. Query performance tuning skills using Unix profiling tools and SQL. Linux/UNIX including to process large data sets. Experience with AWSRegards,SandeepLead Recruitment650 249 3775Job Type: ContractExperience:ETL: 3 years (Required)Hadoop, Big Data, Hive Hbase: 2 years (Required)Data Engineering: 5 years (Required)","Seattle, WA",Data Engineer,True
125,"As a Lead Data Engineer, you have a solid understanding of both the business and the technical aspects of BI in relation to digital media business. You will drive the completion of projects within the established scope, while simultaneously planning for and managing unknown future BI requirements in a dynamic environment.Responsibilities Include: Design, model and develop data sets to support reporting and analytics in a cloud environment.ETL development: perform all aspects of programming assignments and assist with systems design.Develop and maintain a technical metadata framework and repository of data events and ETL operations.Manage and administer Analytics tools and tag management systems.Help plan and maintain the technical infrastructure, its configuration, performance, and storage requirements, with consideration of tiered data and data archiving.Generate ad-hoc queries and reports based on business requirements.Provide ongoing evaluations of technology solutions and capabilities to ensure alignment with business objectives, identify areas of risk, while monitoring the current environment and potential improvement areas.Work with business stakeholders to gather, analyze, and translate requirements in BI reporting area – either recommending an existing solution, developing a solution, or synthesizing delivery requirements to engineering teams for development.Actively question and challenge customers to understand their requirements and reach the best solutions, near and long term.Understand and adhere to development and documentation standards, database design andstorageSuccessfully implement process improvements impacting own work and work of others.On-call application support is required.*Qualities / Experience We're Seeking: 5+ years of top-tier Data engineering experience; at least 2 years on cloud Infrastructure.Working knowledge of digital media ecosystem, including how digital video streaming, ad servers, DSPs, SSPs work.Experience working in a mix of Cloud and Enterprise data environments with real world implementation of data collection and processing on AWS environment.Knowledge of web technologies and online advertising systems.Experience with real-time Big Data analytics.Experience with Hadoop, MapReduce, Spark, Flink and/or other Big Data processing platforms.Excellent knowledge of OLAP concepts.Familiarity with columnar databases like Redshift, Vertica etc.Programming language such as Java, and scripting languages like python, ruby and Unix shell scripts.Experienced working in a fast-paced, high-tech environment (preferably software development) and comfortable navigating conflicting priorities and ambiguous problems.Experience with data visualization tools such as Looker, Tableau.Great communication and collaboration skills across technical and non-technical stakeholders.A Bachelor’s degree in Computer Science or equivalent preferred.Job Type: Full-timeEducation:Bachelor's (Preferred)","Los Angeles, CA",Lead Data Engineer,True
126,"Senior Hadoop Engineer/ArchitectThe Position: Avalon Consulting, LLC is seeking a Senior Hadoop Engineer/Architect to act as a consultant with clients through the design, development, installation, and securing of Big Data solutions. Preferred locations include , Washington, DC, Dallas, Austin, Chicago, Denver or Minneapolis. However, Avalon is open to candidates in all major US cities. Candidates will travel to various client sites based on needs (~50% travel). Summary Description: As a Senior Hadoop Engineer/Architect, you will solve problems for clients using Big Data and Hadoop ecosystem technologies. You will help them select the appropriate Hadoop, NOSQL and Search components to use, design a solution to solve their business and technical problems, and then present your solution to the client. Furthermore, the consultant will be responsible for deployment or conversion of the solution with aligned security controls.Job Duties: Perform structured data analytics on Hadoop using various methods for SQL on Hadoop including Hive, Impala and Spark SQL - experience with other Spark components is a plusIntegrate Hadoop with existing RDBMS systems to implement two-tiered EDW & ETL solutions offloading data and processing to achieve the best cost-benefit for the overall systemImplement and train machine learning algorithms using big data to help understand and use large datasetsConsult with client to address data governance and security requirementsPerform performance tuningPerform POC deployments and conversionsInstall technical security controlsSelect the right approach of technology depending on data type and latency requirements for batch, interactive and streaming data analyticsTrain clients in usage and features of big data technologies, through in-person training sessions, speaking at conferences, and blog postsAdminister solutions for clientsRequired Experience: Bachelor’s degree in Computer Science, Information Systems or other related fieldA minimum of ten years’ work experience designing, developing and deploying application and systems using multiple paradigms and development methodologiesA minimum of three years’ work experience developing systems using Hadoop, NOSQL and Search technologiesRequired Technical Skills: Deep Hadoop development and administration experienceDeep Java or Scala development experienceKnowledge of scripting languages (Python, Bash, Ruby, Perl) and at least one SQL variantKnowledge and use of ETL tools and methodsAbility to install and configure components in the Unix/Linux environmentExperience following the Software Development Lifecycle, preferably agileUse of version control systems such as Git or SVNBasic comprehension of distributed systems and how they workBasic understanding of networking and data centersPossess a one or more technical certifications such as:Certified Information Systems Security Professional (CISSP)International Information Systems Security Certification Consortium (isc)2Microsoft Certified Technology Specialist (MCTS)Windows 7 AdministrationHortonworks Apache Hadoop 2.0 Certified DeveloperHortonworks Certified Hadoop 2.X AdministratorCloudera Certified Professional (CCP)CCP Data EngineerCloudera Certified Associate (CCA)CCA Spark and Hadoop DeveloperCCA Data AnalystCCA AdministratorCloudera Certified Administrator (CCA)Knowledge, Skills and Abilities: Intellectual curiosity and demonstrated critical thinking and creative problem solving abilityTrack record of learning new technologies and methods quicklyAbility to architect designs and solutions based on client problemsAbility to program a solution based on provided designAbility to see pros/cons for approaches and reason about themProven experience explaining complex topics to others and communicating effectively with clientsAbility to work independently and as a part of a team of other consultants and/or client representativesExperience working in a consultative role with external or internal clientsAbility to travel up to 50%Process of Candidacy: After reading this Job Description, if you believe your skills and experience are a good initial match for this position and our firm, we invite you to send your resume using the link below.The search will be conducted in a thoughtful, thorough, and consistent manner, with a conscious effort to preserve the confidentiality of all candidates. The talent acquisition team is committed to offering each potential candidate the same consideration throughout the process. There is a strong sense of urgency to fill this vital role.We offer market-competitive compensation, performance incentives, and a full benefits package that includes health, dental, vision, and life insurance; holidays & vacations. All inquiries will be kept confidential. Apply now to explore our opportunities!Job Type: Full-timeEducation:Bachelor's (Preferred)Location:Washington, DC (Preferred)Work authorization:United States (Required)","Washington, DC",Sr. Hadoop Engineer,True
127,"ContractA cutting-edge telecommunication analytics company in Los Angeles, CA is looking for a Senior Data Engineer (AWS) to join their team for an immediate contract position. As a Senior Data Engineer, you bring marketing analytics and data technologies together to deliver best-in-class insights.
Senior Data Engineer Responsibilities:
Lead adoption of new data technologies by creating easy to use scalable frameworks that address needs of data processing operational team.
Automate processes by reverse engineering and optimizing data flows and analyst experience.
Apply data engineering to analytical data pipelines.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Recommend and implement process improvements.
Maintain specifications and meta data; follow and develop best practices.
Coach and technically train data analysts.
Senior Data Engineer Qualifications:
Strong data management skills: accessing and reading raw data files, creating data structures, data processing, merging, handling missing values, programming and debugging, data validation.
Sound knowledge of relational, structured, semi-structured and unstructured data systems, and the experience to know when to use each type of system.
Experience working with a range of data systems (e.g., Spark, Hive, Postgres, MySQL).
Must have strong SQL/SparkSQL and Python/PySpark programming skills.
Must have proven experience with python data type projects (list, tupal, dict).
Must have reporting experience using KPI's.
Experience working in AWS is required.
Strong data management skills: accessing and reading raw data files, creating data structures, data processing, merging, handling missing values, programming and debugging, data validation.
Analytical and BI reporting experience desirable.
Bachelor's Degree in Computer Science, Mathematics, Economics, or experience in related analytical field.
About Profiles
An award-winning Marketing and Creative Technology staffing agency, Profiles places the highest caliber candidates in Fortune 500 companies and successful organizations across the country. Our experienced recruiters focus on candidates drawn from the top 20% of job seekers nationwide. Profiles professionals are available for contract, contract-to-hire, and direct hire positions. Headquartered in Baltimore, MD, Profiles has regional offices in Philadelphia, Richmond and Washington D.C.
Have you considered a contract position? Profiles offers the following benefits: competitive salary; 401(k) plan; weekly paycheck and bonus pay; health, vision and dental insurance; online software and soft skill training.
New job opportunities are listed daily – www.careerprofiles.com.
INDML","Los Angeles, CA",Data Engineer,True
128,"ContractLead Data Engineer
Primary Location – Merrimack, New Hampshire
V-Soft Consulting is an end-to-end recruiting and staffing solution provider known for our ability to provide highly qualified consultants for any project at any scale.
What makes us different? Our expertise is derived from over 20 years of delivering world-class IT staffing, consulting, engineering and managed services to Fortune 1000 and mid-market companies in the U.S., Canada, and Asia. V-Soft is a trusted partner with experience across diverse technology stacks to help business get IT done.
Like what you hear? Apply with V-Soft today!
V-Soft Consulting is currently hiring for a Lead Data Engineer for our premier client in Merrimack, New Hampshire. This is a 6+ month contract position in the financial services industry.

WHAT YOU’LL NEED:
Education and Experience »
15+ years of experience defining data architecture solutions and establishing common data capabilities for enterprises
6+ years of experience building distributed solutions in Spark, MapReduce and other MPP system with associated data models and datastores (i.e. Redshift, Cassandra, HBase, Parquet)
2+ years of experience working with AWS Cloud data engineering stack, including EC2, S3, EMR, Kinesis, Glue and other AWS Services
Proven experience creating actionable data and analytics strategies for Compliance, Risk, and Financial Intelligence business functions
Solid experience optimizing the Hive queries using Partitioning and Bucketing techniques
Hands-on experience with Apache Ni-Fi, Kafka, Python and Spark, preferably on AWS
Experience defining technology blueprints and roadmaps, collaboratively defining solutions, and enabling architecture capabilities
Experience in tool selection, conducting rapid PoCs, and recommending use case appropriate technologies
Experience writing high quality code in Python and one another OOP language (Java, Scala, C++, Go, etc.)
Experience with structured/unstructured/semi-structured data ingestion and processing
Experience with automation and deployment (Jenkins, CloudFormation, Chef, etc.)
Experience working with RDBM systems, particularly familiarity with SQL
Experience in working with UNIX shell scripts


WHAT YOU’LL DO:
Job Responsibilities »
Design, build and implement scalable streaming data pipelines and ETL frameworks to increase data access and decrease analysis and decision times across the organization
Own software throughout the entire development life cycle – design, code, test, automate and deploy
Share ideas to improve our client’s product and processes, and provide feedback


Interested?
Qualified candidates should send their resumes to sksingh@vsoftconsulting.com
V-Soft Consulting Group is headquartered in Louisville, KY with strategic locations in India and across the U.S., including Madison, Chicago, Denver, Harrisburg and Atlanta. Known as an agile innovative technology services company, we were recently rewarded the Large Business of the Year award from Louisville Business First, and were recognized among the top 100 fastest growing staffing companies in North America. V-Soft has a wide variety of partnerships across diverse technology stacks, and holds such titles as MuleSoft Certified Delivery Resource, Oracle Gold Partner, ServiceNow Partner, Microsoft Partner and Cisco Registered Partner, amongst many others.
For more information or to view all our open jobs, please visit www.vsoftconsulting.com or call (844) 425-8425.","Merrimack, NH 03054",Lead Data Engineer,True
132,"-----------------------------------------------------
RAPP Dallas is looking for a Junio Data Engineer
-----------------------------------------------------

--------------------------------------------------
to join our rapidly growing Technology Team.
--------------------------------------------------

ABOUT RAPP

Our purpose

We are the agency absolutely, utterly, fiercely focused on the individual. We use our data, technology and creative smarts to make meaningful, connections with every single person a brand knows.

Our family

We are part of the DAS group, which is in turn a part of Omnicom. This group also includes DDB, BBDO, OMD, PHD and other well-known acronyms.

Our home

A hop, skip and jump from DFW Airport and a quick Uber to all the sights, sounds, BBQ and tacos of the Dallas-Fort Worth Metroplex.

Our clients

From opening a checking account to buying a sweet new ride to biting into a Big Mac®, we provide smart solutions for companies like Toyota, McDonald's, TXU Energy and more.

ABOUT YOU

As a junior data engineer, you define solutions for the use, extraction and manipulation of data - driven by the balanced combination of business needs and consumer preferences. You work in close collaboration with multidisciplinary teams to provide the data needed, in the optimal format for making critical, real-time decision.

You can communicate through a project with ease and understanding and you know your tools like the back of your hand.

You are experienced with writing complex SQL statements and mapping relational database structure. You strive in designing & continuously improving data workflows using enterprise campaign management solutions such as Adobe Campaign, Unica, Eloqua, Marketo and/or Salesforce Marketing Cloud.
You may have experimented in the past with BI tools such as Tableau, Domo or QlikView, and have R/Python in your radar

Most importantly not only can you do the job, you can explain it to a relative and they actually understand. You are self-directed, highly motivated and have fun while working hard in a fast-paced environment.","Dallas, TX",Junior Data Engineer,False
133,"ContractDuties:Manage data warehouse plans for a product or a group of products.Interface with product managers, analysts, and engineers to understand data needs.Design, build, optimize, launch and support new and existing data models and ETL processes in productionSupport existing processes running in production.Define and manage SLA for all data sets in allocated areas of ownership.Work with data infrastructure to triage infra issues and drive to resolution.Build data expertise and own data quality for allocated areas of ownership.Skills:2+ years hands-on experience in custom ETL design, implementation, and maintenance.2+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language. (Python preferred)2+ years’ experience in the data warehouse space with schema design and dimensional data modeling.Ability to write efficient SQL statements.Excellent communication skills and proven experience in building data-driven projects from definition through interpretation and execution.Experience with large data sets, Hadoop, and data visualization tools such as Tableau, Qlik View etc.Experience initiating and driving projects to completion with minimal guidance.Job Type: Contract","Menlo Park, CA",Data Engineer,False
134,"About us: We are an international company with offices in the UK, Portugal, andUS. The Virtual Forge works with organisations to create digital and technology platforms to drive transformation, develop capabilities and deliver digital and transactional experiences that buildbusinessaround the world.We are looking for a talented Data Engineer to work with us on a dynamic project for one of our international clients.You will be based on our cool offices in Doylestown, PA.Role Overview: We are seeking a Data Engineer to join our growing team. This person will be responsible for working across several Data Warehousing, Reporting, and Data Integration projects, assisting with the development of data best practices and governance, building reporting infrastructure, creating ad-hoc reports, and helping to optimize data flow and collection.The ideal candidate has knowledge of, and is excited to learn about all aspects of data from multiple complex sources and domains, and who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our developers, database architects, data analysts, and data scientists, and will ensure optimal data delivery architecture is consistently present across projects. They will also support non-technical colleagues in the collection and appropriate use of data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by data,cross systemintegration, traditional and non-traditional forms of ETL.Duties and Responsibilities: Data Modeling – evaluate structured and unstructured data, determine the most appropriate schema for new tables, fact tables, data marts, etc.Data Integration – incorporate new business and system data into client Data Warehouses while maintaining enterprise best practices and adhering to data governance standards.ETL and Reporting – apply business rules to data to migrate from source to target using ETL tools or scripting languages. Validate data to ensure quality. Collaborate with colleagues across the enterprise to scope requests. Extract data from various data sources, validate results, create relevant data visualizations, and share withrequester. Develop dashboards and automate refreshes as appropriate.Governance / Best Practices – adhere and contribute to enterprise data governance standards. Assemble large, complex data sets that meet business requirements.KPI Development - Develop analytics thatutilizedata resources to provide actionable insights, operational efficiency and other key business performance metrics.Technical Support - Work with stakeholders including client and internal company teams to assist with data-related technical issues and support their data infrastructure needs.Essential Skills: Bachelor’s degree in computer related field2-3 years of Business Intelligence/Data Warehousing experience, preferably across multiple domains and in-depth in at least 1Proficient at integrating predictive and prescriptive models into applications and processes.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong communication and organizational skills.Strong analytic skills related to working with structured and unstructured datasets.RDBMS - strong skills in SQL andMySQL .Scripting - Python proficiency.Preferred Skills: Experience with Big Data and HadoopExposure to stream-processing systemsExposure to object-oriented/object programming: Python, Java, etcExposure to visual analytics tools: QlikSense, Tableau, Power BI etc.Data Science / Machine Learning experienceFamiliarity withAgilemethodology for developmentJob Type: Full-timeEducation:Bachelor's (Preferred)","Doylestown, PA",Data Engineer,False
135,"Job Description

The Opportunity
Your consulting projects will include integrating data in a virtual manner for operational and/or informational purposes - Integration of 100+ data sources for a Customer Service Multichannel IT Infrastructure; implementation of Logical Data Warehouses and Virtual Datamarts to enable modern Business Intelligence solutions, Integration Layers for Hadoop-based Data Lakes, and support for Agile Operational Reporting on a diverse Big Data infrastructure are just a few flavours of your future projects.

Be part of an elite team in a rapidly growing international software product company. Your career with us will combine cutting edge technology, exposure to worldwide clients across all industries (Financial Services, Automotive, Insurance, Pharma, etc.), exciting growth path for technical, product and customer-facing roles, direct mentorship, and access to senior management as part of a global team. Your mission is to help our clients and prospects to realize their full potential through accelerated adoption and productive use of Denodo's data virtualization capability in many solutions.

Location New York, NY
Duties & Responsibilities

As a Data Engineer Virtualization (f/m) you will successfully employ a combination of high technical expertise, client communication and coordination skills between clients and internal Denodo teams to achieve your mission.

Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.
Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.
Contribute to knowledge management activities and promote best practices for project execution.
Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.

Qualifications

Desired Skills & Experience
Experience Range: 2-5 years (Fresh Graduates must be top-ranked and exceptionally qualified)
University Degree relating to information systems or computer science (Bachelor or Master degree)
Understanding of Data Integration flavors
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. Good knowledge of software development and architectural patterns.
Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems (e.g. SVN, git).
Basic experience in Big Data, NoSQL, and InMemory environments is welcome.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Personal and Relationship qualities: Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker.
Communications: Good written/verbal communication skills in English (other international languages a plus) are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.
Willingness to travel.
Additional Information

Employment Practices
We are committed to equal employment opportunity.
We respect, value and welcome diversity in our workforce.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.","New York, NY",Data Engineer - SQL / Big Data / Java,False
136,"SmartAsset is a financial technology company that empowers people with automated personalized financial advice. Its proprietary technology, up-to-the-minute research and Automated Financial Modeling software simulate the impact of different decisions on people's personal finances, enabling millions of people to make smart financial decisions. For more information, visit www.SmartAsset.com. To learn more about SmartAdvisor, visit https://SmartAsset.com/Financial-Advisor/.

We are looking for a talented Data Engineer to join our growing Data Engineering team and help us design and implement the next version of our integrated data platform. We take a wholistic view of the firm's data, from cloud storage to sales contracts, and provide centralized and actionable views to build research, monitoring, and reporting tools. For this you'll need to be a tenacious and creative problem solver with the ability to quickly prototype solutions. You'll also need to be proactive and have a desire to mingle with the product and business facing groups to understand what drives their processes, and turn that insight into great tools to help grow the SmartAsset business.

Candidates should have


2-4 years professional development experience with at least 1 year of Python
Familiarity with ETL concepts
Working front end knowledge - HTML/CSS/jQuery
Solid SQL skills
Good knowledge of Pandas and some hands on data experience
Knowledge of Linux environments - bash, ssh, crontab etc.
Working knowledge of statistics
A desire to learn about all facets of the SmartAsset business

Ideal to have


4-6 years of professional development experience with 1-2 years Data Engineering experience
ETL/data pipelining experience
Experience with BI tools - Looker, Tableau etc.
Full stack experience
Math/Stats background
Experience with AWS services/EC2
Experience using Cloud storage solutions
Data Science experience

","New York, NY",Data Engineer,False
137,"The Data Intelligence Group at Balyasny Asset Management is seeking a Data Engineer to help build out a Data Acquisition Platform that empowers our Product and Data Science teams to deliver timely and accurate data. The data acquired by this platform drives investment decisions across the firm. The optimal candidate will have some hands-on development experience with an excellent technical background. The candidate will have a penchant for building maintainable and scalable systems.
In the role of Data Engineer, the employee will be responsible for the following:
Designing, building, and administering our Data Acquisition Platform to ensure smooth day-to-day operation over hundreds of data sets
Working directly with Product and Portfolio Managers to understand our customers’ data needs and provide end-to-end data solutions that address requirements
Investigate data issues, working with data providers to ensure seamless interconnection between different data sets
Full lifecycle development including requirement, architecture, implementation, testing, and release
Develop written documentation of infrastructure including processes and procedures
Help to develop and test robust business continuity/disaster recovery processes
Perform with minimum supervision, exercising sound judgment
Help identify and automate manual processes


QUALIFICATIONS & REQUIREMENTS:

In order to effectively represent the Company and communicate with clients, the employee must be someone who has:
Degree in Computer Science or closely related field, or Code School graduate
0-2 years professional development background
Experience with Python, Java, C# or similar language a plus.
Unix (Linux) and database experience (SQL, MySQL, PostgreSQL, redis)
Familiarity with object oriented development approach
Analytical skills – Ability to troubleshoot and logically assess problems and determine solutions
Documentation skills – ability to represent ideas, requirements, and problems in clear and concise documents","New York, NY",Data Engineer,False
138,"job description
Who We Are:
HBC is a diversified global retailer, focused on driving the performance of high quality stores and their all-channel offerings, growing through acquisitions, and unlocking the value of real estate holdings.
Founded in 1670, HBC is the oldest company in North America. Our portfolio today includes formats ranging from luxury to premium department stores to off price fashion shopping destinations, with more than 480 stores and over 66,000 employees around the world.
Our leading banners across North America and Europe include Hudson’s Bay, Lord & Taylor, Saks Fifth Avenue, Saks OFF 5TH, Galeria Kaufhof, the largest department store group in Germany, and Belgium’s only department store group Galeria INNO.
We have significant investments in real estate joint ventures. HBC has partnered with Simon Property Group Inc. in the HBC Global Properties Joint Venture, which owns properties in the United States and Germany. In Canada, HBC has partnered with RioCan Real Estate Investment Trust in the RioCan-HBC Joint Venture.
A truly global corporate citizen, HBC is committed to responsible business practices to bring about positive change, and we work hard to shape a sustainable future for people and the planet. Our philanthropic initiatives help create healthy families, strong communities, and sport excellence in the cities and countries in which we operate around the world, while striving to create innovative programs and resources that provide flexibility for work-life balance in order to maintain a positive working environment.

What This Position Is All About:
The Data Engineer is responsible for database architecture, ELT process development, Data Management policy development, and support and QA activities of data management developments. Also responsible for coordinating with the various IT and functional system owners to design and deliver integrated Data Management solutions toward operational efficiency and a high data quality standard in the provisioning and distribution of enterprise data.

Who You Are:

You get things done by engaging in high level teamwork and flexing your interpersonal skills.
You are keenly interested in telling stories and making cases via data, analytics, trend graphs, and metrics.
You have a proven ability to multi-task in a fast paced environment. You have been described by past peers and managers as having “excellent interpersonal, verbal, and communication skills”.

You Also Have:

Bachelor of Science degree in Computer Science, Mathematics or Engineering, plus 5 years of relevant experience in database development, software engineering or related occupation.
Experience providing management support, business intelligence, and data-warehouse or data-focused systems integration solutions using proven methodology framework.
Solid SQL/PLSQL skills and strong understanding of Oracle database, data structures, data quality and cleansing strategies.
Experience with ETL processes and tools.
Proficiency in metadata management, master data management, data governance and data solutions management.
Ability to produce quality technical analysis covering data, data mapping and process mapping.
Demonstrated knowledge in all areas of product development life cycle.
Broad knowledge of IT systems, processes and controls.
Experience with office automation tools including Word, Excel, Visio.

As Data Engineer, You Will:
Build & enhance data models and database architecture for Enterprise data Model.
Help define data standards and implement best practices working with the other members of the Data Management team.
Evaluate, design and develop functional specifications through detailed technical requirements document.
Evaluate MDM Applications, developments or models and provide recommendations for improvements.
Evaluate existing database structures and recommend improvements.
Build and/or maintain ELT processes in support of the enterprise model.
Evaluate and help select various data warehousing tools and components.

Your Life and Career at HBC:
Be part of a world-class team; work with an adventurous spirit; think and act like an owner-operator.
Exposure to rewarding career advancement opportunities, from retail to supply chain, to digital or corporate.
A culture that promotes a healthy, fulfilling work/life balance.
Benefits package for all eligible full-time employees (including medical, vision and dental).
An amazing employee discount.

Thank you for your interest with HBC. We look forward to reviewing your application.

HBC provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, HBC complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
HBC welcomes all applicants for this position. Should you be individually selected to participate in an assessment or selection process, accommodations are available upon request in relation to the materials or processes to be used.","New York, NY",Data Engineer,False
139,"Code Pilot is replacing the crusty old resume with a data science based portfolio for engineers, and we're looking for a Data Engineer with data integration expertise and a passion for changing the data landscape. If you are interested in working for a data driven company on the edge of industry disruption, we'd love to talk with you. And if we ask you about bulk batch high volume data processing and your eyes light up and your brain goes into overdrive, we're talking to the right person.

-------------------

We are looking for:
-------------------

A new team member to support the data development & processing for one of our hiring partners.

------------------------------
What will keep you challenged?
------------------------------


Design, develop, automate, monitor and maintain Extract Transform Load (ETL) data movement applications using our preferred ETL tools and techniques.
Performance tune ETL applications to manage high volume batch data transfer to and from internal and external system locations.
Troubleshoot data issues, recommend, test and implement solutions.
Develop and document technical requirements and solutions.
Participate in design and code reviews.
Troubleshoot issues making recommendations and delivering on those recommendations.
Engage in project planning and delivering to commitments.
Participate in daily stand-up meetings, planning meetings and review sessions (using Scrum / Agile methodology).
Interact with cross-functional teams to ensure complete delivery of solutions.
Assist with configuration of applications software.

----------------------------------------
Which traits contribute to your success?
----------------------------------------


Passion - for software development (algorithms, design patterns, clean code, agile processes, data structures).
Humility - Outstanding engineers display inverse proportions of talent to humility.
Mission Oriented - A dogged commitment to getting the job done is crucial.
Highly Competent - Software Engineering at the top level requires highly motivated individuals who are constantly learning and adapting.
Highly Adaptive - Mike Tyson said it best, ""Everyone has a plan 'till they get punched in the mouth."" Developing software at the top level requires incredibly adaptability and agility. Nothing ever goes the way you want, and anything valuable probably doesn't exist the way you want it.
Ship or Quit - High performance software teams have one unifying quality, they ship, a lot. So if you're a fellow Ship talker, you'll love it here.

-------------------------------------------

Some technologies you will be working with:
-------------------------------------------


ETL tools (ie, Ab Initio, Pentaho, Talend, SSIS)
Programming languages Java (Groovy), JavaScript, SQL, or PL/SQL
SQL and relational platforms (PostgreSQL, Oracle)
Revision Control (e.g., Subversion)
NoSQL databases (Cassandra)

---------------
Why Code Pilot?
---------------

Code Pilot is a new startup and relatively unheard of. We're currently 4 people, all co-founders, based in Austin, TX. Given the rigor and process we adopt to hire engineers, you might be asking yourself, why invest your time and effort to simply apply?

We're hoping if you've reached this part of the posting, you've already noticed the care and attention we've invested in how we hire engineers. We're not interested in the instant gratification someone can provide by being able to ""code"", and frankly, as an engineer who is passionate about the craft, you shouldn't be either. We're passionate about the discipline of engineering, and what it takes to develop world class engineers. To that end, we don't care if you've got 1 year or 20 years experience, we don't care if you went to MIT or are self-taught, hell, we don't care if your favorite programming language is Ruby (although we will help you forget everything you know about it pretty quickly). What we care about is your insatiable desire to be pressure tested in a credible engineering environment that brings out your best.","New York, NY",Data Engineer,False
140,"Data EngineerJob Location: US-TX-ArlingtonDuration: PermanentOverviewWe are expanding our efforts into complementary data technologies for decision support in areas of ingesting and processing large data sets including data commonly referred to as semi-structured or unstructured data. Our interests are in enabling data science and search based applications on large and low latent data sets in both a batch and streaming context for processing. To that end, this role will engage with team counterparts in exploring and deploying technologies for creating data sets using a combination of batch and streaming transformation processes. These data sets support both off-line and in-line machine learning training and model execution. Other data sets support search engine based analytics. Exploration and deployment of technologies activities include identifying opportunities that impact business strategy, collaborating on the selection of data solutions software, and contributing to the identification of hardware requirements based on business requirements. Responsibility also includes coding, testing, and documentation of new or modified scalable analytic data systems including automation for deployment and monitoring. This role participates along with team counterparts to develop solutions in an end-to-end framework on a group of core data technologies.ResponsibilitiesJOB DUTIESContribute to the evaluation, research, experimentation efforts with batch and streaming data engineering technologies in a lab to keep pace with industry innovationWork with data engineering related groups to inform on and showcase capabilities of emerging technologies and to enable the adoption of these new technologies and associated techniquesContribute to the definition and refinement of processes and procedures for the data engineering practiceWork closely with data scientists, data architects, ETL developers, other IT counterparts, and business partners to identify, capture, collect, and format data from the external sources, internal systems, and the data warehouse to extract features of interestCode, test, deploy, monitor, document, and troubleshoot data engineering processing and associated automationQualificationsKnowledge2+ years of hands-on experience with SQL, data modeling, and relational databases such as Oracle, DB2, and Postgres1+ years of experience with software engineering to include Java, Scala, and PythonExperience with processing large data sets with Kafka, RabbitMQ, Flume, Hadoop, HBase, Cassandra and/or Spark or similar distributed systemExperience with NoSQL data stores such as MongoDB, Cassandra, HBase, Redis, Riak or other technologies that embed NoSQL with search such as MarkLogic or Lily EnterpriseBachelors or higher degree in computer science or other quantitative discipline or equivalent work experienceExperience or familiarity with ETL and Business Intelligence technologies such as Informatica, DataStage, Ab Initio, Cognos, BusinessObjects, or Oracle Business IntelligenceSkillsAbility to quickly prototype and perform critical analysis and use creative approaches for solving complex problemsExcellent written and verbal communication skillsJob Type: Full-timeExperience:IT: 5 years (Required)SOLR: 1 year (Preferred)HBase: 2 years (Required)Spark: 2 years (Required)","Arlington, TX",Data Architect,True
142,"You get a kick when mining through heaps of data, and understand the underlying technologies of working with big-data. Your primary responsibility will be to assimilate data from various sources across the stack, and build a system from ground-up, so that it is easy to query and visualize key business metrics. You should be proficient in a few of these (or similar) technologies: MongoDB/SQL, Hadoop, MapReduce, Redshift, Python, R.","Redmond, WA",Data Engineer,False
143,"Come join Workfront, one of the hottest companies in cloud computing as recognized by Forbes magazine! Located in the heart of Silicon Slopes, Workfront is the leader in enterprise work and project management. Meeting our mission to become the authoritative source for work, hundreds of thousands of enterprise users leverage Workfront's SaaS solution to make their work faster and more efficient taking projects to a higher level.

As part of the Analytics/BI team, the Data Engineer helps leaders and analysts throughout the organization get the data they need to further accelerate our growth. The team helps the business define analytics needs and translates that into actionable data models that are both robust and easy for business users to understand. This position will work closely with business groups to understand and solve problems and questions that will help them understand our business and contribute to a data driven culture. Our data engineers build clean and easy to understand data models that the rest of the business can use to provide insights.

Job Responsibilities


Work with business stakeholders to understand analytics needs and architect solid data models which will provide the insights they need
Improve overall robustness and efficiency of existing data marts
Design and build robust data models in Redshift which combine multiple data sources
Build data models that are easy to use and understand by business stakeholders
Clearly document and demonstrate the functionality and power of data models to the business
Provide accurate and actionable data to the organization

Experience


Expert SQL programmer
At least 4 - 5 years designing and building actionable data models for use in Business Intelligence tools
Strong understanding of star schema and Kimball methodology
Experience building slowly changing dimensions
Working knowledge of code repositories (preferrably Git)
Transforming data to meet business needs
Exposure to Tableau, Power BI, Clik, Quicksight, or other BI tools
Strong experience working with multiple business groups to meet their analytics needs
Experience validating and performing quality assurance on data as it flows from the sources to data models and visualizations

Bonus Skills


Experience with AWS Redshift, Vertica, Greenplum, Netezza, or other MPP databases
Past experience building sales funnel and marketing attribution data models
Understanding of Salesforce, Marketo, Netsuite, and other enterprise systems

","Lehi, UT",BI Data Engineer,False
144,"ContractExcellent opportunity with Established retail Domain clientLocation- SunnyvaleMy direct client is looking for a Data engineerSkills-SQL, ETL, Teradata, Spark, hadoop3 or more years of progressively complex related experience.Has strong knowledge of large scale search applications and building high volume data pipelines.Experience building data transformation and processing solutions.Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Interested candidates please submit resume through indeed.Bhavya ll Technical Recruiter ll FlextonJob Types: Full-time, Contract","Sunnyvale, CA",Big Data Engineer,False
145,"The Data Engineer is responsible for defining, implementing, configuring and maintaining the tools and systems deployed to continuously monitor and gather data on the performance and availability of highly-demanding VOIP and Unified Communications systems and applications that Continuant manages for its customers.Job Duties: Setup and configure monitoring systems to ensure continuous monitoring of critical aspects and parameters of the managed UC equipmentPropose, evaluate and participate in selection of new monitoring tools and products to be deployed in the customer’s environmentDevelop extended monitoring functionality, such as custom tests and scripts targeted to specific attributes of Unified Communications equipment.Perform manual data gathering and extraction from multiple raw data and SQL repositories; automate the data extraction process where possibleLoad data in Continuant’s central data repositoryPerform systems administration and maintenance on existing monitoring tools and systemsRequirements: Linux systems administrationKnowledge of SNMPScripting and programing in Bash and/or PythonSQL knowledge will be considered an advantageKnowledge of or experience with any VOIP/UC system by the following manufacturers will be considered a strong advantage: Cisco, Microsoft (Lync/Skype for Business), Avaya, Siemens, NortelITIL knowledge or experience will be considered an advantageAt least 2 years’ experience in IT systems monitoring/event management or related positionBachelor’s Degree in Computer Science or related field or equivalent experienceBenefits: Medical, dental, and vision insuranceWellness reimbursementMatching 401(K) programPaid time offTraining creditOn-site gym and massageReady to meet us? Send your resume to our Talent Team using the apply link.Learn more at www.continuant.com.Continuant takes pride in having a dynamic, diverse workforce. We are an Equal Opportunity Employer. Drug test and criminal background check are required at the time of employment.Continuant is a Drug Free Work Environment. Any candidate that tests positive for marijuana or ANY controlled substance and/or alcohol during a pre-employment drug/alcohol screening will not be eligible for hire.Job Type: Full-timeLocation:Tacoma, WA (Preferred)Work authorization:United States (Required)","Tacoma, WA 98424",Data Engineer,False
146,"About Convene

At Convene, we're changing the way the world works by transforming the way businesses work. By partnering with the largest landlords in commercial real estate, we design and service the next generation office building – one that feels more like a full-service, lifestyle hotel. Our integrated ""workplace-as-a-service"" platform gives building tenants and enterprise clients access to a growing network of premium meeting and event spaces, flexible workspaces, hospitality services, and curated experiences for users, all connected by Convene's proprietary technology platform.


Founded in 2009
Locations in New York City, Boston, Philadelphia, and Washington, D.C.; Opening in LA in 2018 and plans to expand globally.
Launched Convene Workplace in 2017
Ranked #11 Workplace in New York in 2017 by Fortune Magazine
Ranked #30 in 2017 Linkedin Annual List of Industry Disruptors

About the Role

We are looking for a Data Engineer to join the Convene team where you will apply your enthusiasm in working with data along with your curiosity and ingenuity to develop a data infrastructure and solutions that will enable Convene to transform the workplace. As a part of the core team's early formation, you will be at the center of creating the data platform and tools that will fuel the data-driven products and features built by developers, and that will power the solutions that enable analytic and business teams to answer questions and gain insights that inform new strategies. You will be in a pivotal role creating the foundation for a robust, reliable and efficient data architecture that scales with Convene's continued growth.

About You


Care deeply about transforming data into business value with speed while also establishing high data quality, and scalability.
Analytic-minded self starter with an eye for detail and resourcefulness to create new innovative data solutions, services and reusable components.
Collaborative, adaptive, and comfortable owning and driving ambiguous projects to successful completion.
8+ years of hands-on data engineering experience in
data modeling and database design, building flexible schemas, analytic data models and warehouses
working with relational databases, column stores, NoSQL databases; experience with AWS services strongly preferred (S3, Redshift)
coding in Python, Java and writing complex SQL
developing and maintaining scalable data pipelines, ingesting data from a variety of source systems including CRM, ERP, and others.
consuming and writing API's
workflow management, preferably knowledgeable using a platform such as Airflow
profiling and analysis of source system and ingested data
leveraging and modifying open source libraries to build custom frameworks
establishing data governance and management practices
familiarity with BI tools including Looker and Tableau, multiple analytic disciplines, and also machine learning techniques
Able to communicate clearly with technical and business audiences at executive and team levels.
You have fun while you work and are a pleasure to work with.
You inspire others and want to be inspired by being a part of the ground level of a rapidly growing and successful company.

What You'll Do


Build our data infrastructure, including ingest data from a variety of internal and external sources into a data lake and build a data warehouse, flexible data models and other reusable data solution components to be leveraged by product, analytic teams and for advanced analytic models.
Work with internal subject matter experts, product and development teams to understand business data objectives, and design and implement scalable data pipelines and ELT/ETL frameworks to increase data access and decrease analysis and decision times.
Own data quality throughout data life cycles, including acquisition, cleaning, processing and validation, providing requirements feedback for identified data variability and gaps.
Share ideas to improve data solutions and processes, make decisions that have a significant impact and provide recommendations that continually enhance data solutions for increasingly complex product and analytic solutions.
Explore new technologies and have fun applying new tools and better ways to turn data into products, intelligence and advanced analytic solutions.

","New York, NY 10178 (Midtown area)",Data Engineer - Business Intelligence,False
147,"The Senior Software Engineer (Data Engineer) will be part of a dynamic, multi-disciplinary, competitive and collaborative team with motivation and purpose to create tools to leverage the latest advances in data science, machine learning and deep learning to impact healthcare across a spectrum of focus areas, including, medical imaging, diagnostics, clinical informatics and population health. To this end there is a wealth of raw medical record data that includes over 6 Petabytes of data that can be mined in the pursuit of building AI models.

The incumbent will successfully demonstrate the ability to work independently, as necessary, identify solutions where impediments have previously existed, and bring an energy, drive and determination to pull the team on a forward trajectory, and deliver technological advancements to usher next generation imaging diagnostics into the forefront.

PRINCIPAL DUTIES AND RESPONSIBILITIES:

Relevant activities include, but are not limited to the following:
· Write clean, maintainable performance code ensuring data is flowing smoothly between source and destination
· Transform, normalize and merging multiple sources of data in both batch and streaming environments with a solid level of comfort
· Build pipelines that feed data scientists with data: Develop and manage extraction tools, wrap the data, send it forward in the data pipeline. Correct, transform and enrich the data. Quickly and efficiently load bulk data
· Work tightly with the broader date science and software team to identify optimum pathway to a successful product
· Take responsibility for strengthening the team by facilitating the adoption of processes that will allow us to work faster and hire exceptional team-mates
· Use the MGH and Radiology values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat one another: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration



SKILLS & COMPETENCIES REQUIRED:

· Strong sense of urgency and proactiveness
· Ability to function effectively and independently in a fast-paced environment, organize and prioritize work independently, and meet tight deadlines
· Self-motivated, with an entrepreneurial mindset and ability to learn quickly
· Strong analytical, planning, organization and time management skills with a strong attention to detail
· Excellent interpersonal skills to effectively communicate with technical teams, cross-functional teams, and staff at all levels of the organization including both technical and non-technical personnel
· Ability to successfully negotiate and collaborate with others of different skill sets, backgrounds and levels within and external to the organization
· Ability to effectively conduct meetings and lead and facilitate large working sessions with all levels of staff and across various stakeholder groups
· Demonstrates strong evidence of algorithmic and structured thinking, with an intuition for logic, pattern matching, what-if analysis, problem decomposition and synthesis.
· Demonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively


Working Conditions:
· Working with our team on site as well as traveling to meet collaborators at multiple sites in the local Boston area
· Occasional travel (<1 month/year) to professional seminars and conferences.


Qualifications
Bachelor’s degree or equivalent combination of education and demonstrated front-end experience required. Computer science, engineering, or equivalent undergraduate and graduate degrees are preferred, though not strictly required necessary

EXPERIENCE:

Required:
· A minimum experience of 3+ years in Data Engineering
· Clear, demonstrable evidence of exceptional productivity and performance in competitive environments

Preferred:
· Familiarity with Linux (Ubuntu) system administration, docker, VMWare, NFS mounts, and large scale data management strongly preferred.
· Familiarity with DICOM & DICOM WEB strongly preferred.
· Expert knowledge of python and git. Expert knowledge of database software (SQL + variants) are strongly preferred.
· Familiarity with Flask micro service with containers and .NET are strong pluses
· Familiarity with common tools for machine learning (i.e. TensorFlow, PyTorch and SkLearn) and general scientific computing are pluses
· Knowledge of software team management philosophies (e.g. Agile, Scrum) and various product management/software development tools are pluses
· Experience with software development for healthcare products as well as familiarity with common clinical scenarios, regulatory and quality standards, payer and provider considerations, are beneficial but not necessary
 EEO Statement
Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.

Primary Location
: MA-Boston-175 Cambridge - MGH
Work Locations
:
175 Cambridge - MGH
175 Cambridge Street
 Boston 02114
Job
: IT/Health IT/Informatics-Engineer
Organization
: Massachusetts General Hospital(MGH)
Schedule
: Full-time
Standard Hours : 40
Shift
: Day Job
Employee Status
: Regular
Recruiting Department : MGH Radiology Research
Job Posting
: Oct 16, 2018","Boston, MA",Data Engineer - Breast Imaging Research Center,False
148,"Fusion is currently seeking a mid-level Data Engineer to join our fast-growing software development team. The data engineering team is responsible for developing and maintaining the data processes for our clients. Tasks will include import and export of data, data migration from legacy systems, interface development, reporting development, as well as maintenance and support for all our existing data projects.

About Us:

Fusion was founded in 2006, and has since become a major disruptor in the Corrections and Public Health sectors of government. Recognized by INC magazine as one of the fastest growing private companies in the United States, Fusion is looking to expand its hand-picked team to include a candidate through this job placement.

From a company culture perspective, we are a vibrant and young group who have come together to be leaders in healthcare IT and software for government agencies. The office provides open working spaces, several meeting areas as well as a café & gym on premise.

Because of the niche Fusion belongs in as well as the business model we operate with, we are looking for not only skilled and qualified candidates, but also candidates who have an outgoing personality and fit well with our other team members.

To date, Fusion has a phenomenal retention of our team members. Our fundamental belief is that employee satisfaction is critical to achieving our mission, so we provide competitive compensation, professional development, career advancement opportunities, and a supportive team-based atmosphere. We also provide a full range of health related benefits, including medical, dental, vision and 401K. And we offer work-life enhancements like flexible hours, business casual dress code, and an easy-going corporate structure.

Fusion has been recognized by Inc. 5000 List of Fastest-Growing Private Companies – thanks to the tireless efforts of our team. If you are a talented professional and our mission speaks to you, please speak to us!

Job Roles:

Communicate with partner companies to develop and support bi-direction data communication.
Migrate data for new clients from old systems to new systems.
Create meaningful insight with data to help our customers meet compliance standard.
Develop reports to allow customers to view their data in the format they request.
Meet with government clients to understand their environment and work with Project Managers to determine the optimal solution for their needs.
Work with Project Managers to create and execute a technical implementation plan for larger client roll outs.
Work closely with product management to understand current and new product features so they may be implemented correctly.


Required Experience:

Javascript
SQL (Plus if it is SQL Server)
Crystal Reports or any comparable reporting tool
C#
Windows Network Experience
Familiarity with most common structured or delimited file formats (CSV, XML, JSON, etc)
Familiarity with data transfer methods (sFTP, HTTP, TCP, SOAP, REST, etc)
Qualifications:

3+ Years of professional Software Development experience
Bachelor’s Degree in Computer Science or any IT-related field.
Working Hours:

Standard hours for this role are M-F start between 8 and 9, expected to put in 8 hours.
Willingness to provide weekly on-call coverage, rotationally.
Additional Notes:

It is not expected that applicants have any familiarity with Fusion’s proprietary applications, GE Healthcare software, or Corrections/Public Health business processes. Qualified candidates will be able to demonstrate experience in this role as well a demonstration of working well with the Fusion team.
This is an On-Site, Full-Time salaried position.","Woodbridge, NJ 07095",Data Engineer,False
149,"$115,000 - $170,000 a yearJob Summary: This opportunity is within the Data Engineering group working with technology platforms and datasets that enable systems and people to uncover new insights and fine-tune operations to meet business goals. They need your help with designing and building these.KEY RESPONSIBILITIESApply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to design systems that meet business needsPlay a leading role in building systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practices.Analyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specificationsDemonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelinesEffectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflictsPeer review work. Actively mentor more junior members of the team, improving their skills, their knowledge of our systems and their ability to get things doneBasic QualificationsA Bachelor's degree or higher in computer science with industry experience.8+ years of building large scale data-processing systems with 4+ years in Big Data technologies such as MapReduce, Hadoop, Spark, Kafka or AWS equivalentsExpertise in Database technologies such as AWS Redshift with proficiency in SQLExperience designing and coding in Python on Linux Platforms.Hands on experience in Data Warehouse (DWH) environment with data integration/ETL of large and complex data sets.Data modeling skills such as Star, Snowflake schema design for DWH.Expertise in performance tuning and scalingFamiliarity with Business Intelligence (BI) and Visualization platforms such as MicroStrategy and AWS QuicksightPreferred QualificationsExtensive knowledge of BI and Visualization platforms such as MicroStrategy and AWS QuicksightExperience with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, AthenaExperience working with Agile methodologies in a DWH, BI environmentAbout the Company: Our client is a seller and producer of spoken audio entertainment, information and educational programming on the Internet. They are looking for creative developers and engineers to help build technologies that continuously improve the listening experience. Teams at the company have the freedom to utilize the technologies that best suit their needs. Whether those are open source, internal, or third party, teams are empowered to make their own decisions about what works best for them. Our client also encourages the use of open source software and contributions back to projects. Join us in helping to bring inspiration and entertainment to our growing base of global listeners.Salary and Benefits: Salary range: $115K - $170K a yearMedical benefits (starting day 1), 401(k) match, Disability / FSA23 days of PTO + 9 paid holidaysTuition reimbursementFree catered lunchFree ParkingParental leave$160/mo stipend for public transportation$500/mo stipend to put towards rent/mortgage IF residence is in NewarkRelocation is available – Depending on the level the candidate is hired for, depends on the amount of relo available. There are lump sum and assisted move options available. It starts at $10,000 for lump sum.Work Life Balance, what does that mean to you?Flexibility – work from home, appointments, etc.Casual + friendly environmentCan arrive as early as you’d like, or come in as late as 9:30 (just get permission from manager) – the earlier employees come in, earlier they can leaveJob Type: Full-timeSalary: $115,000.00 to $170,000.00 /yearExperience:building large scale data-processing systems: 8 years (Required)Python on Linux Platforms: 2 years (Required)Data Warehouse (DWH) environment: 2 years (Required)Data modeling skills such as Star or Snowflake schema: 1 year (Required)AWS Redshift: 2 years (Required)Big Data technologies: 4 years (Required)Education:Bachelor's (Required)","Newark, NJ",Data Engineer,False
150,"Position Details
The Boston Bruins Hockey Operations department is accepting applications for an experienced data engineer. This position reports to our Director of Hockey Analytics, and will assist in the development of our database systems and infrastructure, as well as the creation and maintenance of our data processing pipelines.
Responsibilities
Build automated pipelines for acquiring, processing and cleaning data from different sources and providers; manage data flow into centralized databases
Conduct database feature engineering to support departmental research
Prepare, clean, and format analytical datasets for processing by analysts
Develop processes for monitoring and testing data quality across multiple sources; diagnose and resolve data quality issues to ensure accuracy
Use and create tools for data manipulation, visualization, reporting
Define storage, security and backup procedures; serve as main resource for departmental support and data maintenance
Take ownership of database structure - manage with long-term stability in mind while delivering short-term results
Interface with analytics and other hockey operations staff, and execute exploratory research and analysis as needed
Qualifications
Bachelor’s degree in Computer Science, Data Science, Engineering, IT, or related field
Preferred post-graduate education or 2-4 years related work experience
3+ years of experience developing in SQL or AWS Redshift
2+ years of experience with data profiling, modeling, and data pipeline development
2+ years of experience developing in Python, R, or similar language
Familiarity with APIs and machine learning a plus
Experience manipulating large and complex data sets
Excellent written and verbal communication ability – desire to be part of a group and to put the needs of the team first.
Problem-solving skills – must be able to assess tasks and react to requests independently, if necessary
Ability to take initiative, work in a fast-paced environment, and consistently meet deadlines
A knowledge and passion for working in sport. Hockey knowledge is a plus, but candidates should have an understanding of typical data structures and research areas in sport.
More Information
To be considered for this role, please submit responses to the below questions. You may attach your answers as a cover letter to your application.
How did you hear about this job?
Describe your experience writing in SQL, including years experience.
Describe your experience writing in R, or a similar language. What packages do you use the most?
Describe your experience developing data pipelines.
Describe briefly what steps you would take to identify data biases or inconsistencies in a new or unfamiliar data set.
Have you ever worked with data sets from hockey, or another sport, before? If so, please describe the data and how it was used.
Tell us why you love sports and want to be part of a team environment.","Boston, MA","Data Engineer, Hockey Operations - Boston Bruins (Full Time)",False
151,"Shutterfly is seeking an experienced Senior Data Engineer with Software Engineering skills to join the Data Warehouse Development team. You will own, manage and drive end-to-end solutions and data infrastructure. You will work with analytics and business partners to deliver data solutions in support of insights and analysis of a multi-million customer ecommerce business with both internal and external data. If you like applying your expertise with data-warehousing technical concepts, CS fundamentals and data and system architecture to multi-terabyte, multi-source data, come join the Shutterfly Data Warehouse Development team!


Responsibilities
Build data expertise and own data quality for the pipelines you build
Architect, build and launch new data models and data marts that provide intuitive analytics to your customers
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) into and out of the Shutterfly Data Warehouse
Design and develop new systems and tools to enable folks to consume and understand data faster
Use your coding skills across a number of languages including Python and Java
Have a clear understanding of the reports/analyses/insights to be driven by data and build data solutions to optimally support the analytics needs
Integrate third party data to enrich our data environment and enable new analytic perspectives
Become fully immersed in the context of Business Development and Partner business initiatives
Work across multiple teams in high visibility roles and own solutions end-to-end
Work with program managers, business partners and other engineers to develop and prioritize project plans
Qualifications
5+ years of experience with implementing big data business solutions at productions scale
Expert knowledge of SQL
History of building, maintaining and automating reliable and efficient ETL, ELT jobs
Strong CS fundamentals and experience developing with object-oriented programming (Java, Python)
Expertise with dimensional warehouse data models (star, snowflake schemas)
Experience with Cloud Data Warehouses, like Google BigQuery and Amazon Redshift, is preferred
Experience with multi-Terabyte MPP relational databases, such as Teradata, and concepts
Understanding of Hadoop or Spark, including manipulating data with with Pig, Hive and potentially with Java or Python
Understanding of streaming technologies and concepts used with data warehouses, is preferred
Understanding of automation and orchestration platforms such as Automic (formerly UC4) and Airflow","Redwood City, CA 94061",Senior Data Engineer,True
152,"ContractFull Benefits – Medical, Dental, Vision, 401k and more!Are you a Senior Java Engineer with strong data experience including expertise in Big Data technologies like HDFS, Pig, Hive, Oozie HBase and Spark? Have you worked with real-time data pipelines using technologies such as Kafka and Storm?HighLight Group seeks a Senior Data Engineer to join our team in supporting a Fortune 50 global leader in digital and mobile on a full-time, contract basis.The ideal candidate will have 5+ years’ of software development experience with a focus on Java, Big Data and real-time data pipelines.Duties and Responsibilities: Work on development initiatives as part of the client's growth marketing team in an agile environment.Closely interact with our stakeholders (Product Owners/Managers, Business Analysts, others) for clarity on sprint items and for verification of developed solutions.Participate in team activities such as sprint grooming sessions, project or product discussions, brown bags.Follow appropriate coding standards and best practices as applicable. Document your work well.Participate in code reviews for your peers. Collaborate with your peers for finding solutions to complex problems. Share knowledge with your peers and also learn from them as required.Work on operational and production support for the applications we build and maintain.Work towards quarterly team and organizational goals that should be result oriented and measurable.Qualifications 5+ years of overall experience in software development.Strong demonstrable experience working on real-time data pipelines using technologies such as Kafka and Storm.Strong data engineering experience with demonstrable skills building data pipelines from structured and semi-structured data sources, data cleansing, formatting and storing data into reporting tables.Strong scripting experience using Python/Perl/Shell.Strong programming experience with Java.Strong experience with relational database systems such as Oracle, MySQL.Strong experience working on BI tools such as Tableau, Microstrategy, Looker etc.Preferred QualificationsExperience with big data technologies - HDFS, Pig, Hive, Oozie, HBase, Spark etc.Experience working with RESTful APIJob Type: Full-timeExperience:software development: 5 years (Required)Relational database: 3 years (Required)Storm: 3 years (Required)Kafka: 3 years (Required)Scripting (Python, Perl, Shell): 3 years (Required)Java: 5 years (Required)Big Data: 5 years (Required)Spark: 3 years (Required)Location:Bay Area, CA (Required)Work authorization:United States (Required)","San Francisco Bay Area, CA",Senior Big Data Engineer,True
153,"$130,000 - $150,000 a yearData Engineer - highly sophisticated cloud analytics platform

Data Engineer for a small team that develops a highly sophisticated cloud based analytics platform that helps their customers (big and small) make critical investment decisions. Lots of new development and a great opportunity for the Data Engineer to impact design and selection of technology as the new data platform continues to expand.

Looking for an experienced Data Engineer with a production development background in both Linux and Windows enterprise environments. Strong relational database management systems experience (MySQL, MS SQL, Oracle) and NoSQL databases and tools (Cassandra, ElasticSearch, Hadoop, Hbase, Mongo, Redis, etc.) Strong coding in chops in either Python, Java or Scala is required. Interest in financial data and/or machine learning is a plus.

Great opportunity for a mid-level data engineer or senior data engineer looking to take on a technical leadership role, as this highly impactful team continues to grow.

Located in Boston. Base salary is highly competitive, and compensation will also include strong bonus and equity.

For more info contact Bivium Founder Jamie LeBlanc - jamie@biviumgroup.com
-

The Bivium Group is a leading recruitment firm in Boston that specializes exclusively in the software engineering market in Boston  for the past 16+ years.

Bivium founder Jamie LeBlanc has a degree in Mechanical Engineering and has been a key member of several startups - a technical software recruiter who understands software and the technologies used to build it. Jamie is a top ranked technical recruiter in Boston with 80+ written Recommendations on LinkedIn that has been expertly recruiting and placing Software Engineers for nearly 20 years.

Visit our website to view more of Boston's Best Software Engineer jobs Here
You can also sign-up for our job alert email to be automatically notified of any new jobs meeting your criteria Here

computer science, data engineering, mathematics, data structures, big data, aws, etl, data transformation, data platform, hadoop, cloudera, mapr, emr, python, scala, java, boston software jobs, jamie leblanc, bivium, #jamiejobs","Boston, MA",Data Engineer - highly sophisticated cloud analytics platfor...,True
154,"Data Engineer Job Description
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
S3, Redshift, ETL tools knowledge is required.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
We are looking for a candidate with experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with AWS cloud services: EC2, EMR, RDS, Redshift","New York, NY",Data Engineer,False
155,"Quest Analytics is a mission-driven healthcare data company trusted by the nation's largest health plans and over 500,000 doctors. We are dedicated to providing consumers with convenient access to an adequate network of doctors and hospitals and an up-to-date, accurate directory of providers. Our culture is collaborative, pragmatic and fast-paced.We're looking for talented, entrepreneurially minded and data-driven people who also have a passion for helping people - and having a ton of fun while they're at it.Join us to revolutionize healthcare data!About the role: Are you looking for a Data role where you can have a high degree of freedom, a huge impact on a product’s future, and work with an amazing team? We may have the right opportunity for you!We’re searching for a passionate Data Quality Engineer - someone who lives and breaths data, desires and produces high-quality data sets. We want someone who makes data-driven decisions and loves running experiments to deliver value to our customers.Your Responsibilities: We are building a Data Operations team to tackle BetterDoctor’s diverse data challenges. You will analyze and optimize data in our data pipeline, which intakes, validates and distributes provider data to our clients. You will provide data to both our data scientists and product teams, refining and improving our data quality.*Some projects you will work on: Anomaly detection in data sources by identifying the root causes of data integrity issues, and creating corrective processes and systems to prevent reoccurrence.Participate in our data release process, and partner with teams to iterate on and improve existing data pipeline.Lead definition for data quality and build a reliable audit process that ensures and improves public facing data quality.Develop specifications for data integrity checks that need to be enforced across the data pipeline.Work with the Data Science team to convert specifications into an automated process.What we expect from you: Bachelor’s Degree or higher in Computer Science or a related field*3+ years of experience in a relevant industry.Experience writing and executing complex SQL queriesExperience managing and optimizing SQL databasesExperience with development in one or more of the following Python, R, Scala, SQLExperience with data processing frameworks and data warehouses such as Hadoop, Spark, RedshiftBonus points for: Experience working with healthcare dataExperience with Looker, Tableau and other BI toolsExperience with DataBricks analysis platformExperience with building and operating data pipelinesExperience with machine learningJob Type: Full-timeExperience:Software Development: 2 years (Preferred)Work authorization:United States (Preferred)","San Francisco Bay Area, CA",Data Engineer,False
156,"Job Description
The Data Engineer (DE) will be a key member of the technical team at Amazon Lending, contributing to a new and rapidly growing line of business for Amazon. This person will take the lead on optimizing our data architecture for the heavy analytics needed for this business.
The role presents a significant intellectual and technical challenge with enormous opportunity for business impact. Amazon is expanding into lending services and has launched with the small business segment. We believe we are in a unique position to serve our customers with exceptional value due to our deep understanding and insight into our base, coupled with our data-grounded analytic and customer-focused approach to building products.

We are looking for a talented and passionate data professional that can design a high-quality and scalable analytic infrastructure that can automate our decision-making processes. A successful candidate will be innovative, technically versatile and be able to interact with customers to gather requirements and deliver the right set of data to support business growth.

The key strategic objectives for this role include:
a) Innovating to deliver mission-critical near real-time data feeds to optimize the business.
b) Delivering a robust, flexible, and scalable data analytics environment to support Amazon Lending as it grows deeper in its existing lines of business and expands to new geographies, customer segments, and products.
c) Collaborate with technical teams, legal, finance, operations, and product managers to drive process improvement and track progress against goals.
Basic Qualifications
We are looking for Data Engineers who have a passion for supplying their clients with meaningful and trustworthy data. You know and love working with analytic tools, can write excellent SQL and Unix scripts, can partner with customers to answer key business questions, and you are an advocate for your customers. You are analytical and creative, and you don’t quit.

You should also have the following skills or experiences:
Bachelor's degree in CS or related technical field
5+ years experience in data warehousing
Strong data modeling and SQL skills
Excellent troubleshooting and problem solving skills
Passion for learning and using new technologies
Effective communication and strong collaboration skills
Preferred Qualifications
Experience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies
Proficient in Big Data processing (e.g. Hive, Scala)
Expertise in MPP and NoSQL persistent storage solutions
Experience integrating with 3rd Party data providers","Seattle, WA",Data Engineer - Lending,False
157,"Qualifications
University student in their final year in a Computer Science, Engineering or equivalent program; expected graduation date of December 2018 or May 2019
Or, currently completing your first year in a non-business, 2-year masters program and have less than 2 years of work experience
Experience in multiple database technologies such as Distributed Processing (Spark, Hadoop, EMR), Traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan)
Ability to structure and analyze data in appropriate framework (e.g., Excel, Access, VBA, SQL)
Passion for data tools and processes to drive business insights
Strong analytical thinking, process management and quality control
Ability to quickly understand and appreciate underlying business context, problems and objectives of analytical projects
Clear communication skills to run well defined analyses and produce reports
Excellent time management skills
Who You'll Work With
You’ll work with a technology team in one of our North American offices to conduct full life cycle analysis of data sets, including implementing data acquisition, cleansing, transformation and upload activities.
You will work closely with Data Science, Product Management, Product Design and Scrum Master team members. Data Engineers are staffed on engagements or product teams and are expected to leverage expertise to solve some of the most pressing and complex issues at clients.
Please review the team options and locations below to clarify your application preferences.
McKinsey Analytics (Geospatial - Waltham; Healthcare Analytics & Delivery- New York, Waltham; Ingenuity- New York, Waltham; Pharma and Medical Products- New York; Public and Social Sector- New York, Waltham, Washington DC). You will apply an analytical, entrepreneurial mindset to foster innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work
Digital McKinsey- (New York City) You will work across industries and functions on projects spanning from IT modernization and strategy to agile, cloud, cybersecurity, and digital transformation
What You'll Do
You will provide analysis, analytical modeling, and/or visualization of data sets using relevant data tools.
In this role you'll identify possible trends and patterns in the data, and communicate relevant findings that help support problem solving, insight generation and decision making. You'll apply your understanding of client data sets and intended use to effectively capture, validate, cleanse, transform and upload data. You'll perform quality checks to ensure consistency, integrity and robustness of data. You'll proactively identify potential issues (e.g., inconsistencies) with data quality, collection or reporting. You'll support solution delivery team, often remotely, in managing client data.
You'll leverage standard reporting to clearly articulate findings to solution delivery team or client team. You will be responsible for coding and scripting (SQL) for automating data transformation and loading specified by delivery team. You will perform impactful data enrichment, based on understanding of the sector/industry and types of data that are available in the space.","New York, NY 10022 (Midtown area)",Data Engineer - University Students,False
158,"A Data Engineer opportunity is currently available at The Lebermuth Company. Lebermuth specializes in essential oils for fragrance and flavor applications. Our business segments include resale of purchased raw materials, and the development and sale of manufactured fragrance and flavor blends. We’re seeking an experienced person who can support our continued growth.The Data Engineer contributes to Lebermuth strategic initiatives and operational efficiency by transforming data from internal and external sources into information critical to business decisions.Responsibilities:Experience in at least two of these databases (SQL, NoSQL, DB2 and Big Data)Lead Data Modeling, Data Architecture and Data Governance projectsLead Data migration, integration and consolidation activitiesProvide graphical representations of data trends and statisticsProvide ongoing reports, in part through development of dashboardsWork closely with Lebermuth IT personnel and vendors to ensure on-going technical support for the OrganizationCommunicate project status updates to Lebermuth team members on a regular basisCollaborate with Lebermuth Management to prioritize information reporting needsProvide research and analytical services to Lebermuth team membersCollect/Mine data to be studiedFilter and ""clean"" data to remove errors and ensure consistency; develop strategies to optimize data qualitySummarize data findings in formats understandable to end usersIdentify, analyze and interpret trends or patterns in complex data setsRecommend opportunities for business process improvements to Lebermuth decision makersIdentify drivers in data and focus on solutions in fulfilling project requestsRemain current on the various options available for data analytics tools, software/hardware updates, system upgrades, and other IT enhancementsQualified candidates have a Bachelor’s degree, preferably in mathematics, economics, computer science, information management or statistics, and 5 to 8 years’ experience in a Data Development, Data Modeling, Data Architect or similar role; or equivalent combination of education and experience from which comparable knowledge, skills and abilities have been achieved. Knowledge base representative of the position includes technical expertise with data models, database design, data mining, and data extraction and transformation with reporting and statistical packages for analyzing large data sets. Advanced knowledge of databases (i.e. DB2 and SQL Server), and advanced skill set with Microsoft Excel. Some programming experience is preferred, to aid in understanding of business processes, and to correct errors or repair data. Knowledge of manufacturing, inventory control and order processing software (i.e. ERP, CRM) is also preferred.For consideration of this opportunity, please submit your resume through Indeed.We invite you to explore Lebermuth at www.lebermuth.com. eoe m/f d vJob Type: Full-timeExperience:Data Engineering: 5 years (Required)Education:Bachelor's (Preferred)","South Bend, IN 46628",Data Engineer,False
159,"$80,000 - $150,000 a yearContractData Engineer Java, Python, C++, GoAdvanced knowledge of Java. Familiarity of PythonFamiliarity with Hadoop stack, Spark, AWS Glue, AWS Athena etcDiverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.)Job Type: ContractSalary: $80,000.00 to $150,000.00 /yearExperience:DynamoDB: 1 year (Preferred)Spark: 1 year (Preferred)Go: 1 year (Preferred)C/C++: 1 year (Preferred)Java: 1 year (Preferred)","Walnut Creek, CA 94598",Data Engineer,False
160,"At Toptal, we measure everything and always rely on data to guide all of our initiatives, including both our long-term strategy and our day-to-day operations.
As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts, and support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity and solutions that will have significant impact on our velocity. We, in turn, will give you autonomy and freedom to turn your ideas into reality.
This is a remote position that can be done from anywhere. However, we do things like rent out hotels in Africa or mansions in Thailand, and you will certainly be invited to come work with us.
Responsibilities:
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Develop batch & real-time analytical solutions, prototypes, and proofs of concept for selected solutions.
Implement complex analytical projects with a focus on collecting, managing, analyzing, and visualizing data.
Build frameworks and tools to empower our data scientists and analysts.
Be in constant communication with team members and other relevant parties and convey results efficiently and clearly.
Requirements:
Working experience with Python, Pandas. Prior experience with Luigi is a plus.
Working experience with Scala and Airflow is a big plus.
Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
Working experience with Dimensional Modeling and Rails is a plus.
Familiarity with the basic principles of distributed computing and data modeling.
Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures. Familiarity with functional programming concepts is a plus.
Outstanding communication and interpersonal skills.
Be excited about collaborating daily with your team and other groups while working via a distributed model.
Be eager to help your teammates, share your knowledge with them, and learn from them.
Be open to receiving constructive feedback.
You must be a world-class individual contributor to thrive at Toptal.",United States,Data Engineer,False
161,"InternshipPalo Alto Networks is the fastest-growing security company in history and a six-time Gartner Magic Quadrant leader for our innovation and ability to execute. Named best place to work by the Silicon Valley Business Journal, we offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job may be for you!
Description
Palo Alto Networks® is the next-generation security company, leading a new era in cybersecurity by safely enabling applications and preventing cyber breaches for tens of thousands of organizations worldwide. If you are motivated, intelligent, creative, hardworking and want to make an impact, then this job is for you!

Our Summer Internship Program (May-August) or (June-September) provides you:

1:1 mentorship
Fun and engaging events that inspire your intellectual curiosity
Opportunities to expand your knowledge and work on challenging projects
Connections to other interns, recent grads, and employees across the company as well as our leaders.

Data Operations at Palo Alto Networks is a back bone for Marketing Data Science team. Our mission is Data readiness from where it is to where it needs to be with greater precision, speed and quality. We eat, drink and sleep Data that helps Data Scientist discover predictions for Sales and Marketing. We integrate, automate and optimize various systems with API, design and develop and build Data warehouse in Big Data architecture. We are looking for a talented and motivated intern that can turn into a full-time member of our team after they graduate.

Learning Opportunities:

You will learn Data integration in a production environment.
You will be exposed to API Calls integrating 3rd Party information.
You will get a chance to work on latest Big Data technology in Hadoop.

Skills sets:

The successful applicant should…
Have string PL/SQL skillsComfortable with Database architecture, Data Warehouse conceptsUnderstanding of Hadoop or similar cloud DB architectureKnowledge on ETL is requiredExperience with Python, scripting languageDevelop Data Visualization for data quality dashboardsGood Communication and documentation skillsAbility to deal with ambiguity & work independently

Requirements – To apply, you must be pursuing a 4-year Undergraduate Degree, a 2-year Master’s Degree or a Doctorate degree and returning to school in the fall. You must have authorization to work within the United States.

Palo Alto Networks is the fastest-growing security company in history and a six-time Gartner Magic Quadrant leader for our innovation and ability to execute. Named best place to work by the Silicon Valley Business Journal, we offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job may be for you!
Description
Palo Alto Networks® is the next-generation security company, leading a new era in cybersecurity by safely enabling applications and preventing cyber breaches for tens of thousands of organizations worldwide. If you are motivated, intelligent, creative, hardworking and want to make an impact, then this job is for you!

Our Summer Internship Program (May-August) or (June-September) provides you:

1:1 mentorship
Fun and engaging events that inspire your intellectual curiosity
Opportunities to expand your knowledge and work on challenging projects
Connections to other interns, recent grads, and employees across the company as well as our leaders.

Data Operations at Palo Alto Networks is a back bone for Marketing Data Science team. Our mission is Data readiness from where it is to where it needs to be with greater precision, speed and quality. We eat, drink and sleep Data that helps Data Scientist discover predictions for Sales and Marketing. We integrate, automate and optimize various systems with API, design and develop and build Data warehouse in Big Data architecture. We are looking for a talented and motivated intern that can turn into a full-time member of our team after they graduate.

Learning Opportunities:

You will learn Data integration in a production environment.
You will be exposed to API Calls integrating 3rd Party information.
You will get a chance to work on latest Big Data technology in Hadoop.

Skills sets:

The successful applicant should…
Have string PL/SQL skillsComfortable with Database architecture, Data Warehouse conceptsUnderstanding of Hadoop or similar cloud DB architectureKnowledge on ETL is requiredExperience with Python, scripting languageDevelop Data Visualization for data quality dashboardsGood Communication and documentation skillsAbility to deal with ambiguity & work independently

Requirements – To apply, you must be pursuing a 4-year Undergraduate Degree, a 2-year Master’s Degree or a Doctorate degree and returning to school in the fall. You must have authorization to work within the United States.
 Learn more about Palo Alto Networks here and check out our fast facts","Santa Clara, CA 95054",Intern-Data Engineer,False
162,ContractJob SummaryUrgent Needed Data EngineersImmediate interviews within 24 HrsJd is provided upon RequestJob Type: Contract,"Fremont, CA",Senior Data Engineer,False
163,"This position will require re-location to client locations after appropriate training. As a data engineer, you should be familiar with and have hands-on experience with all aspects of big data engineering from data ingestion of various types of sources and common data cleansing and transformation techniques. Alternatively, you must have 3-4 years of strong Java/J2EE, database and data warehouse (ETL, data modeling, analytics) experience. Ability to develop, debug and deploy applications using common IDEs in a Linux environment is required.Required experience:

Big Data engineer: 1+ years
Java/J2EE developer: 3+ years
Required education:
Minimum of Bachelor’s Degree in Engineering, Master’s preferred",United States,BIG DATA ENGINEER/TRAINEE,False
164,"$100,000 - $140,000 a yearOptomi, in partnership with a leading global asset management and financial services company is seeking a Data Engineer for their Owings Mills, MD locations.The Data Engineer will be joining a team that will help create, maintain and optimize a new cloud-based data architecture that will support multiple investment business lines and will make architectural and design recommendations that will help facilitate the determination Buy/Sell investment decisions in a highly dynamic team environment.What the right professional will enjoy!!Help establish best practices in big data technologies and data warehouse to collect and analyze large volumes of data advancing the state of the art in efficiency optimization, predictive analytics and BI self-reporting tools.Join an emerging technical team of data engineers delivering a wide array of big data and self-service reporting solutions that use cutting edge technologiesHelp guide the organization in efficient data and resource management best practices with cloud based big data, Data Warehouse and Reporting platforms and servicesApply today if your background includes: You have a Bachelor’s Degree in Computer Science (or related field) and have 5+ years of relevant work experience in business intelligence, data engineering, data warehousing, or a similar field.You remain active in learning emerging practices and technologies in the BI/DW and data science space.You are comfortable and confident in your knowledge of multiple database programming languages and your knowledge of relational and columnar databases.You have experience integrating data from multiple data sources and have processed large amounts of unstructured data.You have experience with AWS, Big Data Components (Apache Spark, Hadoop, Presto, etc.), and Continuous Integration tools (TeamCity, Octopus, Jenkins, etc.) is desired, but not required.Data Engineer Job Responsibilities: Provide significant contributions to the growth of our Data infrastructureDesign, develop, test and deploy business intelligence solutionsDesign and document data structures/models and data flowsInteract with various business units and technical teams to gather requirementsReview, optimize and document current ETL processesEnsure data quality, completeness, and accuracyCollaborate with team members in code reviews, discovering better practices and patterns and continuous improvementsJob Type: Full-timeSalary: $100,000.00 to $140,000.00 /yearLicense:US Citizen or Green Card (Preferred)","Baltimore, MD",Data Engineer,False
165,"The Data Engineer is responsible for data acquisition strategies and integration scripting and tools, data migrations, conversions, purging and back-ups; fulfills data acquisition strategy requirements. They work with product, financial control, analysts, users and other stakeholders to understand business requirements and supports data architecture to translate into data acquisition strategies.


The Data Engineer will author artifacts defining standards and definitions for storing, processing and moving data, including associated processes and business rules. Additionally, the Data Engineer will map the details within these their artifacts to business processes, non-functional characteristics, QA criteria and technical enablement. The role is responsible to be constantly thinking through the needs of the business to support efficient and error free processes


The Data Engineer will be responsible for finding trends in datasets and developing workflows and algorithms to help make raw data more useful to the enterprise. He or she will also be responsible for createing data acquisition strategy and develops data set processes.
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning and data destruction (scripts, programs, automation, assisted by automation, etc).
Ensures quality of technical solutions as data moves across Healthfirst environments
Provide insight into the changing data environment, data processing, data storage and utilization requirements for the company and offer suggestions for solutions
Ensures managed analytic assets support Healthfirst’s strategic goals by creating and verifying data acquisition requirements and strategy
Develop, construct, test and maintain architectures
Align architecture with business requirements and use programming language and tools
Identify ways to improve data reliability, efficiency and quality
Conduct research for industry and business questions
Deploy sophisticated analytics programs, machine learning and statistical methods
Prepare data for predictive and prescriptive modeling and find hidden patterns using data
Use data to discover tasks that can be automated
Create data monitoring capabilities for each business process and work with data consumers on updates
Aligns data architecture to Healthfirst solution architecture; contributes to overall solution architecture
Help maintain the integrity and security of the company data
Minimum Qualifications:
Bachelor’s Degree in Computer Engineering or related field
7+ years’ experience in a data engineering
10+ years’ experience in data programing languages such as java or python
4+ years’ experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc.
3+ years’ experience working in a production cloud infrastructure


Preferred Qualifications:
Proven track record of success directing the efforts of data engineers and business analysts within a deadline-driven and fast-paced environment
Hands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment
Demonstrated experience working in an Agile environment as a Data Engineer
Hands on work with Amazon Web Services, including creating Redshift data structures, accessing them with Spectrum and storing data in S3
Knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval.
Proven results using an analytical perspective to identify engineering patterns within complex strategies and ideas, and break them down into engineered code components
Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry
Experience developing, prototyping, and testing engineered processes, products or services
Proven ability to work in distributed systems
Proficiency with relational, graph and noSQL databases required; expertise in SQL
Must be able to develop creative solutions to problems
Demonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes
Excellent interpersonal skills with proven ability to influence with impact across functions and disciplines
Ability to work independently and as part of a team
Ability to manage multiple projects/deadlines, identifying the necessary steps and moving forward through completion
Skilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio

WE ARE AN EQUAL OPPORTUNITY EMPLOYER. Applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected Federal, State/Province or Local status unrelated to the performance of the work involved.

If you have a disability under the Americans with Disability Act or a similar law, and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services.


EEO Law Poster and Supplement","New York, NY",Data Engineer,False
166,"Junior Data Engineer
We build products that help people protect their privacy and security on their mobile phones, and we are looking for new team members to help fuel the growth of our innovative suite of mobile apps.
RoboKiller blocks robocalls then gets revenge on telemarketers, while TrapCall unmasks blocked calls, protecting users from all kinds of phone harassment. SpoofCard lets consumers change their Caller ID to keep their number secure, and TapeACall allows users to record calls on their iPhones. If these are products you are excited about, we'd like to learn more about you.
We are looking for an enthusiastic Data Engineer to join our organization. In this role, you will work closely with our engineers, analysts, and product owners to develop and optimize data infrastructure and tools that empower advanced analytics. The right candidate will be knowledgeable and excited about continuously improving the data management practice by implementing data technologies that power our analytics efforts.
Responsibilities:
Design, administer, and maintain an analytical data warehouse and data lake.
Build a batch and real-time data pipeline to ingest and process data from various sources such as mobile applications, product backends, and third-party partners.
Build internal tools and services that will boost our data operations.
Assist product-marketing, analytics, and engineering teams with data-related technical issues to support their data infrastructure needs.
Facilitate data governance practice to document, profile, and manage data across multiple products and platforms.
Qualifications:
0 - 2 years of data or relevant engineering experience developing scalable data pipelines.
Fluency in SQL and programming languages (Python, Java, JavaScript, Go, etc.).
Hands-on experience with relational database and non-relational data stores.
Familiarity with creating ETL processes for large scale data warehouses.
Understanding of fundamental software engineering principles.
The ability to work effectively in a team environment with a positive “get stuff done” attitude and a passion for data engineering practices.
Pluses:
Experience with various cloud services like GCS, GKE, PubSub, Dataproc, etc.
Knowledge of dimensional data warehouse modeling.
Knowledge of big data ecosystem and related technologies such as Hadoop, Spark and Cassandra.
Familiarity with relevant frameworks and tools such as Airflow, Pandas, Spark and more.
Previous experience on data management practice is a plus.
Active involvement in open source software development is also a plus.
What you can expect:
Opportunities to grow
An awesome, collaborative, fun place to work
Co-workers who will make you feel like family, but challenge you to be your best
In Headquarters:
A well-stocked kitchen with breakfast and lunch options almost every day
An on-site gym facility with a wellness program
A game room and lounge so you can relax and recharge as needed
We want “A” Players, who are driven to achieve, but know that teamwork is the key to their own success. We are a successful, self-funded company, with a fantastic Silicon Valley style office and a culture-first personality. If you think you are a good fit, please first take a close look at our RoboKiller, TrapCall, TapeACall and SpoofCard products and tell us how your story and experience can help shape TelTech's future.","South Amboy, NJ",Jr. Data Engineer,False
167,"ConsenSys is a venture production studio and the leading technology firm in blockchain globally. We deliver products, solutions and platforms built using blockchain technology to transform how business is done in complex network of buyers, suppliers and consumers.

Our teams are busy at work building the future of identity, financial markets, commerce, the music industry, security, and infrastructure, and more. To accomplish this we've built out a flat organizational structure which we call the ConsenSys Mesh: a network of individuals & teams working autonomously and towards the same goal. Our mission is to use these decentralized solutions to fundamentally reshape the economic, social, and political operating systems of the planet. If you are someone that thrives in a fast-paced environment where being self-directed, determined, and resilient are a requirement, we would love for you to join us.

About the role:
ConsenSys is looking for an experienced Analytics/Data Engineer who will play an integral part in building out the company's data infrastructure. They will be responsible for integrating data from various sources (both third party and first party) and ensuring its availability for reporting in SQL databases and Looker. This role requires a strong understanding of data infrastructure, integration, ETL processes and Looker.

As the first member of the team, the Analytics/Data Engineer will be working closely with the Analytics Lead in driving the data strategy and roadmap of the company.

The ideal candidate will be enthusiastic, well-organized and motivated by creative problem-solving. Candidates should generally be interested in and passionate about the technologies poised to transform the way we live and be open to learning deeply about blockchain topics.

Responsibilities:

Integrate data from various third party and first party data sources, including but not limited to Google Analytics, YouTube, Medium, Facebook, Twitter, LinkedIn and individual spoke data
Code fix and maintain MapReduce data transformation jobs
Design, build and launch new data extraction, transformation and loading pipelines and processes in production
Build SQL Scripts for processing and analysis of data
Data analysis and data quality checks for completeness and accuracy
Maintain reporting systems
Define and manage SLA for all data sets

Requirements:

3+ years experience in the data warehouse space
3+ years experience in custom ETL design, implementation and maintenance
3+ years experience with schema design and dimensional data modeling
Deep knowledge of Blockchain Technology
Passion for Ethereum and its potential
Experience with Snowplow, PostGres, BigQuery, Redshift, Looker and LookML
Experience with data warehousing, dimensional modeling and ETL development
Proven ability to work cross functionally
Previous experience and successful track-record of learning new tools and technologies
Working knowledge of AWS components and services
Strong programming, shell scripting and deep SQL skills
Knowledge of MapReduce jobs and ETL scripts
A keen eye for detecting data defects and anomalies
Comfortable juggling multiple technologies and high priority tasks
You are a self-starter who enjoys learning new technologies
Ability to analyze data to identify deliverables, gaps and inconsistencies
Ability to manage and communicate data warehouse plans to internal clients

Here are some of the perks of being part of a unique organization like ConsenSys:


The forefront of a revolution. At ConsenSys we fundamentally believe that a next generation of technologies presents the opportunity to create a more just and equitable society.
A dynamic startup environment. ConsenSys is a thought leader in the blockchain space and we are absorbing a significant portion of the mindshare. This is both exciting and challenging, as we learn to scale our organization while adhering to the principles of decentralization.
Continuous learning. You'll be constantly exposed to new languages, frameworks and ideas from your peers and as you work on different projects -- challenging you to stay at the top of your game.
Deep technical challenges. This entire ecosystem is about 10 years old. Ethereum itself is still a toddler. There is much work to be done before these platforms can scale to the order of millions or billions of users. ConsenSys is building the technology platforms that can get us to those next thresholds of scale.

","New York, NY",Data Engineer - Marketing,False
168,"We're seeking an experienced Data Engineer to join our growing data team. You will team up with our data engineer to build, scale and improve the core of our data stack. In addition, you will be working closely with our analytics team to productionalize statistical and machine learning models. We are looking for someone who's excited to help us ask the right questions, dig deep into the answers, and build solid systems that address them.

A little about us

The Paperless Post Data team plays a crucial role in our product's success. We get to dive into advanced data work—from machine learning to visualization, classifier creation to ETL pipelines—that solve complex business problems and increase the functional capabilities of everyone throughout the company. On the reporting side, we help ensure that all teams have the information they need to make sound business decisions. Above all, we're a team that is excited about what we do.

What you'll do here


Build efficient and reliable data infrastructure and tools.
Develop production code for real-time data processing, ETLs and machine learning services.
Analyze the data we collect to generate important insights with our analytics team.
Deploy and monitor production systems.

About you

As an ideal candidate, you have a good blend of programming, analysis, and communication skills.

This describes you


You are an expert in SQL, Python, and/or JavaScript.
You have an expert knowledge with AWS (Data Pipeline, Lambda, Redshift) and best practices.
You have experience with modern ETL and data modeling technologies.
You have a solid understanding of both relational and NoSQL database technologies.
You have a strong understanding of data security and protection.
You have great communication skills, both written and verbal.
You have the desire and willingness to take on new challenges and learn new things.

This is a bonus


You have experience with Hadoop / MapReduce or Spark.
You have experience working on and productionizing machine learning models.
You have experience with real-time data processing.
You have experience with database architecture.

Company-wide we enjoy an amazing ecosystem of an even gender split and a healthy balance of engineers and designers. Because Paperless Post isn't supported by ad revenue, we can focus our efforts on building and improving on the ideal version of our platform, product, content, and partnerships for our users.

We are proud that Paperless Post helped over 30 million people connect in the real world last year. Our product is global, and we are committed to being a company where everyone belongs. We encourage people of all backgrounds, races, genders, and abilities to apply!","New York, NY",Data Engineer,False
169,"ContractJob SummaryFounded in 2007, InterSources Inc is an Small Business Enterprise, Minority Business Enterprise & Women Owned Small Business certified Company specializing in providing IT Consulting, IT Staffing solutions and Software solutions. We have been recipients of Various Awards under ""Fastest Growing IT Consulting and Software Company "" and ""Excellence in Technology Services ""Responsibilities and DutiesData Engineer you will: Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our businessHelp us build a world class data lake/data warehouse, by building data pipelinesDesign and develop new systems and tools to enable folks to consume and understand data fasterUse your expert coding skills across a number of languages from Python, Java, C++, Go etc.Work across multiple teams in high visibility roles and own the solution end-to-endDesign, build and launch new data extraction, transformation and loading processes in productionWork with data infrastructure to triage infra issues and drive to resolution.Job Type: ContractLocation:San Mateo, CA (Preferred)","San Mateo, CA",Data Engineer,False
170,"SemanticBits is looking for a Data Engineer/Wranger who is an effective technologists, self-motivated, and able to source and develop data models to fuel the analytics, developed by analysts and data scientists. You will deliver data acquisition, transformations, cleansing, conversion, compression, and loading of data into data and analytics models. Work in partnership with data scientists and and analysts to understand use cases, data needs, and outcome objectives. You are a practitioner of advanced data modeling and optimization of data and analytics solutions at scale. Expert in data management, data access (big Data, traditional data marts, etc.), advanced in programming (Python, Shell scripting, Java, and SQL), advanced database modeling, familiarity with analytic algorithms and applications (like machine learning).
SemanticBits is a leading company specializing in the design and development of digital health services, and the work we do is just as unique as the culture we’ve created. We develop cutting-edge solutions to complex problems for commercial, academic, and government organizations. The systems we develop are used in finding cures for deadly diseases, improving the quality of healthcare delivered to millions of people, and revolutionizing the healthcare industry on a nationwide scale. There is a meaningful connection between our work and the real people who benefit from it; and, as such, we create an environment in which new ideas and innovative strategies are encouraged. We are an established company with the mindset of a startup and we feel confident that we offer an employment experience unlike any other and that we set our employees up for professional
success every day.
Requirements
Bachelor’s degree in computer science (or related) and 2 to 4 years of professional experience
Strong knowledge of computer science fundamentals: object-oriented design and programming, data structures, algorithms, databases (SQL and relational design), networking
Demonstrable expertise with Python, Elasticsearch, and Spark, wrangling of various data formats - CSV, XML, JSON.
Experience with the following technologies is highly desirable: R, AWS cloud computing, Apache NiFi, Apache Kafka, Kibana, Node.js
Experience with Agile methodology, using test-driven development.
Excellent command of written and spoken English
Self-driven problem solver
Benefits
Generous base salary
Three weeks of PTO
Excellent health benefits program (Medical, dental and vision)
Education and conference reimbursement
401k retirement plan. We contribute 3% of base salary irrespective of employee's contribution
100% paid short-term and long-term disability
100% paid life insurance
Flexible Spending Account (FSA)
Casual working environment
Flexible working hours",Remote,Data Engineer,False
171,"ABOUT REFLEKTIVE'S ENGINEERING TEAM

Reflektive is seeking a Data Engineer to deliver our talent development platform to the world's best places to work. Reflektive is a rapidly scaling company making this the best environment to take on ownership as well as learning how to grow a company.

You'll join a lean, prolific team where everyone, including you, is active in the product defining and development process (where deploying new features every 2 weeks is common). You'll know the customers we're talking to, and the needs of each one. As a result, you know where your initiative and drive can best make a difference (and be recognized!)

Our engineering team consists of developers from a wide array of backgrounds. Our data team primarily focuses on Java, SQL and Python. Our team is a tight knit, friendly group of engineers that are dedicated to learning from and teaching to each other. Team members regularly contribute to and optimize our engineering practices and processes. Our team wants to make software engineering fun, easy, and fulfilling, so we've come up with a set of values that we apply to our software every day: Simple, Flexible, Consistent, Predictable, Efficient, and Pragmatic.
Responsibilities
Working on Reflektive's Data & Infrastructure Projects
Experience or demonstrated interest in Big Data Technologies
Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data
Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack
Help promote best practices for Big Data development at Reflektive
Act as a bridge between the infrastructure and application engineering teams
Provide infrastructure support with a focus on cloud based computing
Build and support visualization and exploration capabilities around our Data Sets
Work with the Data Extraction and Data Science engineers on normalization and analytical processes
Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries
Help design and implement disaster recovery efforts
Analyzes requirements and architecture specifications to create detailed design
Research areas of interest to the team and help facilitate solutions
Desired Skills and Experience
3+ Years of professional experience as a Data Engineer or a Backend / Full Stack Software Engineer looking to move into Data Engineering
Skills required: Java, SQL
Great if candidate has experience in any of the following Python, Ruby on Rails, PostgresSQL, Redshift, Elasticsearch
Startup experience
Flexible team player
Willingness to roll up your sleeves and get stuff done
Willingness to learn and do whatever it takes to meet deadlines in a quick, ever-changing environment
In 30 days, you will have deployed fixes to production and have an understanding of how Data is consumed/transformed/stored and the Key Infrastructure Projects
In 60 days, you will have taken on a deeper understanding of projects, working independently or with team mates to implement key features
In 90 days, owning your own project and working collaboratively with your peers","San Francisco, CA",Data Engineer,False
172,"Job Responsibilities:
Translate complex functional and technical requirements into detailed design.
Design for now and future success
Hadoop technical development and implementation.
Loading from disparate data sets, by leveraging various big data technology
Pre-processing using Hive, Impala, Spark, and/or Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies
Following and contributing best engineering practice for source control, release management, deployment, etc.
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting
Job Requirements:
5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainable code.
Ability to write MapReduce jobs
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
Preferred:
Understanding and implementation of Flume processes
Good knowledge of database structures, theories, principles, and practices.
Understand how to develop code in an environment secured using a local KDC and OpenLDAP.
Familiarity with and implementation knowledge of loading data using Sqoop.
Knowledge and ability to implement workflow/schedulers within Oozie
Experience working with AWS components [EC2, S3, SNS, SQS]
Analytical and problem solving skills, applied to Big Data domain
LI-SNNA","San Francisco, CA 94133 (Russian Hill area)",Data Engineer,False
173,"Temporary, InternshipData Engineer Intern (Summer 2019)


Position Title: Data Engineer – Intern (Summer 2019)
Location: Chicago
Department: Information Technology
We are looking for a Data Engineer Intern to join our team. You will have the opportunity to be involved in all aspects of a performance-driven database infrastructure geared towards a fast paced trading environment. On the technical front, the Database team touches every layer of the database stack from hardware to application layer, so be ready to leverage your strengths while learning a lot to improve your weaknesses. The team manages every aspect of the database environment from server hardware to arrays to SQL development and administration. On the business front, this team works directly with traders, other engineers, and business stakeholders. You will interact closely with both team members and stakeholders alike. The ability to have a keen technical understanding but communicate in layman’s terms is important. Day to day tasks can vary between database design, support, tuning, SQL report writing, scripting and anything else that could touch the database layer of the firm.
Required Qualifications
Major in CS, MIS, engineering, or an applied science3.0+ GPA0-3 years of experienceExcellent problem solving skillsRising Junior, Senior, or graduate studentDatabase related coursework
Key elements for the internship. Elaborate on these when applying:
MS SQL Server experienceCHashtag development skillsProgramming or scripting experience (T-SQL preferred)Financial or trading industry experienceDatabase design and performance optimization experienceDatabase administration (backups/restores, security, HA)Networking, DNS, AD, SAN, RAID
About Wolverine
Founded in 1994, the Wolverine companies comprise a number of diversified financial institutions specializing in proprietary trading, asset management, order execution services, and technology solutions. We are recognized as a market leader in derivatives valuation, trading, and value-added order execution across global equity, options, and futures markets. With a focus on innovation, achievement, and integrity, we take pride in serving the interests of both our clients and colleagues. The Wolverine companies are headquartered in Chicago with offices in New York and San Francisco and a proprietary trading affiliate office located in London.
Sponsorship is not available for this position.






Are you a returning applicant?


Previous Applicants:

Email:

Password:





If you do not remember your password click here.","Chicago, IL 60604 (Loop area)",Data Engineer Intern (Summer 2019),False
174,"Fox News is currently looking for a Data Engineer to join our Digital team. Responsibilities will include building out tools and new data platforms that will drive the data driven decision process within the organization.

A SNAPSHOT OF YOUR RESPONSIBILITIES
Create and maintain new Extract Transmit and Load (ETL) processes to further the capabilities of our analytics platform
Create programs that gather data from APIs, websites, and other data warehouses
Support and maintain a powerful and efficient data warehouse within the AWS environment
Run Ad-Hoc queries for stakeholders using an assortment of technology including redshift, spark, and Athena
Propose and design new methods and technologies to better leverage our data
Support the data needs of Business Intelligence and Data Science Teams
WHAT YOU WILL NEED
SQL Expertise
Fluency in an object-oriented language such as Python, Java, or C#
Experience with shell languages such as Bash or PowerShell
Understanding of relational and non-relational data modeling
Proficiency in creating ETL processes for large scale data warehouses
Excellent communication skills
NICE TO HAVES
In-depth knowledge of Hadoop and related technologies such as Hive, Presto, and Spark
Proficiency with the AWS ecosystem including technologies such as DynamoDB, RedShift, s3, Glue, Lambda, Athena and EMR
Experience with big data columnar formats such as ORC and Parquet
Knowledge of Adobe Analytics and Clickstream data feeds
Machine Learning
Knowledge of data visualization tools such as Tableau, Domo, Looker etc.
Please attach a resume to be considered, applications without resumes will be considered incomplete and will not be reviewed.



We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. We will consider for employment qualified applicants with criminal histories consistent with applicable law.","New York, NY 10036",Data Engineer,False
175,"Viant’s Engineering team is looking for a Data Engineer to join our Irvine, CA office. The engineer will be involved in the full development lifecycle, contributing to various Viant product lines.
Responsibilities:
Writing complex SQL queries.
Analyzing and tuning SQL queries.
Writing ETL and ELT jobs using various tools and programming languages.
Designing, developing, testing, debugging and deploying applications and reports using various technologies.
Importing and exporting large amounts of data in formats like CSV, JSON, XML, etc.
Need to be able to easily switch between different technologies.
Solving complex performance problems, architectural challenges and production issues.
Working with product owners, QA engineers and fellow engineers.
Following organization processes and procedures.
Ad-hoc duties as needed.
Qualifications:
Minimum B.S in computer science, mathematics or related field.
Strong foundation in computer science.
Experience with Java (Python a plus)
Exposure to cloud based technologies like Google Cloud Platform, AWS.
Experience with web services.
Experience with MySQL, Oracle and other database technologies.
Ability to troubleshoot queries, stored procedures, functions, packages and triggers.
Excellent problem solving, critical thinking, and communication skills.
Ability to learn newer technologies in a short period of time.
Good sense of humor and a team player.
Exposure or understanding of NoSQL technologies a plus
Exposure to machine learning a plus
Benefits:
Competitive Salary and Bonuses
Paid benefits for the employees: Medical, Dental, Vision, LTD, Life insurance/AD&D
Paid parental leave
401k
Summer “Work from Anywhere” Fridays
Wellness programs – info sessions and occasional chair massages
Employee discounts – e.g. gym memberships, wireless plans, entertainment tickets
Fully stocked kitchen
Casual Office Atmosphere
Commuter Benefits Program
Ongoing Education & Training
Company Sponsored Events & Team Building Experiences
About Viant:

Founded in 1999, Viant Technology LLC is a leading global people-based advertising technology company, enabling marketers to better plan, execute, and measure their digital media investments through our cloud-based platform. Built on a foundation of people instead of cookies, the Viant Advertising Cloud™ provides marketers with access to over 1 billion registered users globally, one of the largest registered user databases in the world, and infuses accuracy, transparency, reach and accountability into cross device advertising.

Viant owns and operates Adelphic and MySpace, and is a member of the Xumo joint venture.
In 2016, Viant became a subsidiary of Time Inc. (NYSE:TIME), one of the world’s leading media companies with over 100 influential brands including People, Sports Illustrated, Fortune and Time. In February 2018, Meredith Corporation (NYSE: MDP; meredith.com) acquired Time Inc. and all its subsidiary companies including Viant, creating a cross channel ecosystem of nearly 200 million unduplicated American consumers every month, including 85 percent of U.S. Millennial women.

#LI-LF1","Irvine, CA",Data Engineer,False
176,"Responsibilities:Optimize data (both underlying data structure and delivery method) for use by reporting platforms.Work with new noSQL, Document, and Graph databases.Learn and assist with our Extract Transform and Load (ETL) process for bringing data into the warehouse in order to turn it into usable information.Partner with various users company wide to build a strong data driven culture through the use of information based decision making.Help these users access and understand the data and metrics in the data warehouse.Developing appropriate new metrics to be used across the business.Maintaining DocumentationQualifications:AWS Environment Experience3 years of Python or node experience.Can construct complex SQL queriesClickstream web analytics data management experience.In-depth quality assurance expertise.Complex problem solving skills.Requires high proficiency in both written and verbal. Most communication involves technical/specific terminology, logical development of arguments or processes and requires clarity of expression.Requires high degree of independent judgment and problem solving of complex problems.Data auditing skills to verify data integrity and understand discrepanciesAbout Clearlink:Clearlink’s team of 1,700+ employees is headquartered in Salt Lake City and has been creating marketing content services for Fortune 500 companies for over 13 years. At Clearlink you will have opportunities to work with people who are as passionate as they are talented, develop yourself and your skills, and create valuable content and relationships every day. We also like to reward our employees:Up to 100% healthcare for your entire familyOver two weeks paid time offPaid ski days, wellness activities, and team outingsFully-stocked break room and gourmet coffeeAward-winning wellness program with free health coachingAll-expense paid vacations for top employeesLocation: Salt Lake CityJob Type: Full-time","Salt Lake City, UT",Data Engineer,False
177,"Job Description
Technical Support Tech II – RAD Data Engineer
The Supply Chain RAD team seeks a Data Engineer to join our RAD Program Management Organization. We are responsible for supporting Amazon’s fulfillment and logistics infrastructure in a fast-paced and dynamic work environment. To accomplish our goals, we are interested in finding top candidates that are ready to take on challenges, obsess over customers, and lead change.
In this role, you will be a part of programs of significant scope and impact. We are looking for a successful and outstanding MSSQL DBA to help manage database support and operations of our mission critical systems. The DBA will be well versed in MSSQL, data warehouse, and integration technologies and will perform administration and engineering for multiple production databases. The ideal candidate will have experience in the architecture, design, and implementation of large production systems with high transaction volumes. The candidate will also be responsible for fast-paced complex distributed database environments supporting large databases and complex integrations. Successful candidates will have the ability to rapidly troubleshoot complex technical problems under pressure.
Preferred location is Seattle, WA, but this position can be based out of any RAD US Amazon Fulfillment Center.
Basic Qualifications
Bachelor’s degree in Computer Science and 2+ years of Amazon experience5+ year of experience as MSSQL DBA in a high traffic, transactional environmentStrong knowledge of systems architecture, loosely coupled and distributed systemsDay to Day Experience supporting highly scalable, distributed, service oriented systemsAbility to work cooperatively with software engineers and system administratorsExperience with system integrations, ETL, data warehouse, queries and stored proceduresTwo Years working experience on Infor EAM software package, including; software configuration, dashboard development using Cognos, Business Objects or SSRS reporting tools.One full cycle of software implementation experience is requiredSuperior communication and analytical skills, including strong ability to identify and solve ambiguous problems.Strong knowledge in project development methodology, clear verbal and written communication skills, and the ability to handle daily activities in a dynamic environment and drive deliverables
Preferred Qualifications
Master’s degree in Computer Science with 5+ years of industry experienceExperience with high-volume OLTP and OLAP database systemsPerformance tuning of MSSQL, PLSQL, SQL processes and queriesStrong understanding of fundamental relational database design, data warehouse, and Business Intelligence best practices, methodologies, and terminologyExperience in distribution, manufacturing, logistics, or other warehouse environmentsExperience in architecture, design, and implementation of production systems with high transaction volumes","Seattle, WA",RAD-Data Engineer,False
178,"A pioneer in K– 12 education since 2000, Amplify is leading the way in next-generation curriculum and assessment. Our captivating core and supplemental programs in ELA, math, and science engage all students in rigorous learning and inspire them to think deeply, creatively, and for themselves. Our formative assessment products turn data into practical instructional support to help all students build a strong foundation in early reading and math. All of our programs provide teachers with powerful tools that help them understand and respond to the needs of every student. Today, Amplify serves more than three million students in all 50 states. For more information, visit amplify.com .


As an engineer at Amplify, you will join a talented team tackling the toughest problems in education with the best ideas in technology – including user experience, APIs and services, data analysis, and deployment pipelines. You’ll play an active role in imagining and improving product design and the classroom experience.
We hire engineers “for the slope, not the intercept” – we’re looking for intellectual ability, flexibility and ability to learn, and commitment to work together in tight-knit teams.

What You’ll Do:

Our data team builds, augments, and maintains the infrastructure that empowers teams across Amplify and our customers to make sense of and tell stories with their data. We believe strongly in teaching our teammates to serve themselves, within a safe, reliable, and agile environment. You’ll be building data systems, but also the sharing-and-learning culture so that every team uses these tools to improve their own lives, and those of our students and teachers.
Impress the toughest customers around – seventh graders – by:
helping teams create fun, compelling apps by leveraging millions of data points
Make life better for passionate, overworked teachers by:
helping teachers understand their students by building reusable data pipelines
Make life better for passionate, overworked Marketing and Sales teams by:
using REST APIs for sourcing/sending data to SAAS like Salesforce, Hubspot
Help school administrators build great schools by:
respecting privacy and ensuring security while offering useful insights by making smart choices in tech stack, database design, and encryption
helping school principals understand how teachers are teaching and how students are learning by architecting data warehouse schemas and SQL transforms with just the right CTEs, window functions, and pivots
analyzing performance and squashing tricky bugs using tools like AWS Redshift, Matillion, Python, SQL, AWS CloudWatch, AWS SNS

Example Projects You Might Work On
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines

You Must Have:
BS in Computer Science, Data Science, or equivalent
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Strong communication skills in writing, conversation, and maybe silly gifs

Extra Credit For
MS in Computer Science, Data Science, or equivalent
2+ years of professional software development or data engineering experience
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Vault)
Experience with tools we use every day:
Storage: AWS Storage Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet, Postgres
ETL/BI: Matillion, Looker
Experience with tools we don’t use, but should, and the wisdom to know when to recommend them
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Experience in education or ed-tech



Amplify is an Equal Opportunity Employer of Minorities, Females, Protected Veterans and Individuals with Disabilities.

This position may be funded, in whole or in part, through American Recovery & Reinvestment Act funds.

Amplify Education, Inc. is an E-Verify participant.","Brooklyn, NY",Associate Data Engineer,False
179,"BitTorrent offers a unique and compelling work environment. We are proponents of the open Internet and we serve one of the largest user demographics in history. We take these responsibilities seriously and hire accordingly. We work with only the brightest engineers and the most talented business people we can find. Everyone on our team is here to do meaningful work with broad reaching impact. We have a fun yet challenging work environment that fosters diversity, creativity and teamwork. Our team members receive industry leading salaries, premium benefits, and state-of-the-art offices in San Francisco’s SOMA district.

Data Engineer
Required Skills:
2+ Years data/development experience
Experience supporting and working with cross-functional teams
Proficiency in Python
Fluency in SQL
Experience with MapReduce and ETL
Nice to Have Skills:
Experience with AWS tools/services
Git","San Francisco, CA 94107 (South Of Market area)",Data Engineer,False
180,"This Data Engineer will join other extremely passionate engineers who share a common interest in distributed systems, performance, scale, and solving problems with software and data. Our technologies power the insights for financial service and corporate giants.
Ideal candidates are those who are excited by big data challenges, and enjoy using new technologies to make large datasets feel small.
Compensation includes a highly competitive salary, bonus and equity.

Responsibilities:
Work closely with the data science and research team and execute on product goals.
Build fault tolerant, secure data products to extract terabytes of disparate data through a unified scalable data extraction and reporting platform.
Assist with the troubleshooting of production support issues.
Enjoy the challenge of researching data issues to maintain quality in our large data sets
Read relevant blogs and articles and grow as an engineer.
Requirements:
3+ years working as a data engineer
Results driven with a strong desire to deliver insights to customers
Willingness to meet time sensitive deadlines
Fluent in Python and SQL
Experience with AWS data management stack (Athena, RDS, Redshift, EMR etc) is a must
Experience with Spark and Databricks is a plus
DevOps Experience is a plus
Ability to efficiently manipulate extremely large dataset
Flexible and can adapt to the changing demands of a startup environment

Company Description
7Park Data transforms data to revolutionize business decisions.
Leveraging machine learning, entity extraction and linking, and predictive models, we transform unstructured information into contextualized Leading Performance Indicators for thousands of public and private companies. Our clients are the most sophisticated investment firms and Fortune 500 corporations who depend on 7Park Data to guide benchmarking, forecasting and product development.
7Park Data offers the Avenue Suite, a collection of dashboards providing clients and partners with on- demand access to our report library, commentary from domain experts, and custom queries and visualizations
Avenue I/O, our developer site, is a suite of APIs, SDKs and tools for data scientists, quantitative analysts, and data owners who want access to cleansed, mapped, normalized data and user-level panels to build their own models. Avenue I/O provides open source tools to quickly and easily build analytical products powered by our APIs.
7Park Data was founded in 2012 and is headquartered in New York City.
For more information, please visit www.7parkdata.com.","New York, NY",Data Engineer,False
181,"$120,000 - $130,000 a yearContractLocation:  Wilmington, DEDuration:  6 months with possible contract to hireDescription: Should have good experience in working with..SPARKAPACHE NIFIKYLOPYTHON or SCALA or JAVACICD / DEVOPSAWS (EMR, EC2, S3, etc)UNIX / LINUXJob Types: Full-time, ContractSalary: $120,000.00 to $130,000.00 /yearExperience:S3: 1 year (Preferred)Scala: 1 year (Preferred)AWS: 1 year (Preferred)Apache: 1 year (Preferred)Java: 1 year (Preferred)","Wilmington, DE",Data Engineer (Wilmington DE),False
182,"Clora is looking for a data engineer to join our small, growing dev team. As a rapidly evolving company, our priorities shift quickly and there is an everchanging set of technical challenges to tackle.

Our Mission
-----------

Clora is building a platform that organizes and provides access to the world's life science expertise in order to accelerate the development of new therapies and fundamentally change how this industry works. Our technology helps innovative life science (biotech / pharmaceutical / medical device) companies find, vet, and engage with top-tier life science experts on their most critical development priorities. We make the process of finding the right expertise fast, reliable, and more cost-effective.

Clora is already drastically reducing the time it takes to find the right talent, from over two months down to just a few days, and all at a third of the cost. As we continue to scale, Clora will meaningfully reduce the time it takes for life-improving therapies to get into the hands of patients most in need.

Our investors to date include Spark Capital, Felicis Ventures, Ludlow Ventures, Notation Capital, v1.vc, the founder/CTO of Hired and 99designs, and early investors in ClassPass and ZocDoc. Collectively, our investors have backed Twitter, Ginkgo Bioworks, Wayfair, Box, Tumblr, Foursquare, Postmates, Warby Parker, and Slack, among others.

Our vision is only possible with the right team. If you are scrappy, entrepreneurial in spirit, and eager to learn and grow, read on!

What You'll Do
--------------


Improve and extend our data processing platform, including:
Build and maintain data pipelines, to clean and process platform data
Tag and store platform information in a way that can allow us to easily filter and search based on our project demand
Iterate and improve upon a recommender system, facilitating the automated matching of expertise with requirements, refining models, learners and experiment with new approaches
Take ownership of projects, laying the foundation of great code yourself and helping others around you to code to those standards.
Work on new products as well on feature enhancements of existing products

Stack & Workflow
--------------------


Rails
React + Bootstrap
PostgreSQL (RDS and Heroku)
Elasticsearch with SearchKick
Github + CircleCI + Heroku for deployment
Rollbar for error management
JIRA for project management

About You
---------


You have a positive attitude and are an excellent communicator
You want to own the success of a feature beyond the pull-request
You're used to a fast-paced environment that changes daily
You are really excited about making an impact in the life sciences
You can work seamlessly with a small but growing team, happy to be working individually, as a leader in implementation and direction, or as a pair of helping hands
You have empathy for our users and be part of the creative process to develop new features and products.

Required
--------


5+ years development experience with some exposure to data processing pipelines
You have familiarity with or a strong interest in developing data science experience
Ability to provide thought leadership on design and architecture of data platform
Experience working closely with a variety of teams including product management and front-end engineers
Experience working in a startup environment
Experience working in a continuous development pipeline that uses Jira, GitHub, CircleCI, and Heroku to manage collaborative workflow and software releases (or equivalent products)
B.S. or higher in Computer Science (or equivalent work experience)

We Accept All Humans
--------------------

We are committed to making Clora an inclusive and diverse organization. Clora is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.","Boston, MA",Data Engineer / Data Scientist,False
183,"At Globant (www.globant.com) we strive daily to deliver powerful innovative software solutions for our clients through our unique Digital Journey approach. Our passion for technology reverberates in every area of our organization and drives our creativity. Working for us means being at the convergence of engineering, design, innovation and scale. It means being part of a tightly knit and driven team whose work directly affects the products and bottom line of our clients.

Our bar is high, but so are the results we achieve.

We are seeking for a SQL and Data Engineer to join the marketing solutions and sales team of one of the biggest and most innovative data companies in the market. This position is the seed of a rapidly growing automation of many tasks using the top edge technologies before they are even in the market. The position will also involve dealing with different teams, to understand, design and implement solutions to their problems and needs.

Responsibilities:

Work with stakeholders and a cross-functional teams to understand requirements and business rules
Understand the set of technologies available and already implemented solutions by other teams to adapt them
Be aware of new internal and external technologies in order to suggest innovative solutions and analyze the possibilities they could bring
Replace existing manual processes with automated and more reliable versions
Create processes to extract and clean data into different databases
Design and create ETL pipelines to automate big data processes
Create SQL queries and other languages scripting to manipulate medium and large volumes of data
Analyze and optimize queries and pipelines for internal and external reporting
Understand the securities and compliance policies to execute proper implementations that protect the data and their users
Lead the analysis and design of quality technical solutions.
Be part of a high performance engineering culture becoming the referent in Data and a key player in the team.

Requirements


Computer Science studies or related technical field or equivalent combination of education/experience
A minimum of 3 years of experience data manipulation
Experience analyzing domain models for relational schemas, data warehouses and/or storage strategies.
Strong SQL skills and data analysis
Experience in Tableau or similar Data Visualization platforms for reporting.
Proficient in at least one scripting language for automation purposes (Python, Bash, among others)
Demonstrate ability to understand new datasets and data structures.
Proactive attitude to understand new business rules and think about the challenges they could bring, proposing solutions
Great problem solving and analytical skills
Excellent verbal and written communication skills
Ability to manage and develop multiple projects in parallel is a plus
Familiarity with finance industry is a plus

We are interested in hard-working, fast-learning talents and we have the know-how and scale to help you make your own career path. If you seek an entrepreneurial, flexible and team-oriented culture, come join us.

We are ready.","Sunnyvale, CA 94089",SQL and Data Engineer,False
184,"THE COMPANY:
JUUL's mission is to improve the lives of the world's one billion adult smokers by driving innovation to eliminate cigarettes. JUUL is the number one US-based vapor product. Headquartered in San Francisco and backed by leading technology investors including Tiger Global, Fidelity Investments and Tao Invest LLC, JUUL Labs is disrupting one of the world's largest and oldest industries.

We're an exceptional team with backgrounds in technology, healthcare, CPG and biotech, and we're growing rapidly to deliver on our mission. We're actively looking to hire the world's best scientists, engineers, designers, product managers, supply chain experts, customer service and business professionals.

ROLE AND RESPONSIBILITIES:

Partner with Data Scientists, Data Engineers and Business Analysts to build configurable, scalable, and robust data processing infrastructure
Work closely with our sales, operations, research, and finance teams on data storage, retrieval, and analysis
Develop new systems and tools to enable stakeholders to consume and understand data more intuitively
Build, manage, and support data models

PERSONAL AND PROFESSIONAL QUALIFICATIONS:

2+ years of software engineering experience with focus on data analytics
Proficiency in Python and SQL. Knowledge of google cloud, bash/shell and workflow tools (e.g. luigi, Airflow), is preferred
Experience developing ETL processes and workflows is desirable
Exposure to relational (e.g. MySQL) and non-relational databases (e.g. MongoDB)
Comfortable working with BI tools (e.g. Looker, Tableau)
Knowledge of version control (Git) and containers (Docker)
Proven critical thinking and analytical problem-solving skills
Desire to learn new ETL tools and frameworks
Passionate about data and software engineering
Able to work collaboratively in an agile environment

EDUCATION:

Undergraduate degree in Computer Science, Engineering, Math, or equivalent experience

JUUL LABS PERKS & BENEFITS:

A place to grow your career. We'll help you set big goals - and exceed them
People. Work with talented, committed and supportive teammates
Equity and performance bonuses. Every employee is a stakeholder in our success
Boundless snacks and drinks
Cell phone subsidy, commuter benefits and discounts on JUUL products
Excellent medical, dental and vision benefits
Location. Work in the heart of San Francisco, one of the world's greatest cities

JUUL Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. JUUL Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for JUUL Labs in the US.

Vapor, JUUL, Work Culture, Fast Paced, Start-up, Growth, Vape, Technology, Software, Hardware, Consumer Electronics, Manufacturing, Design, Product, Disruptive, Revolutionary, Cutting Edge, App, Android, eCommerce, B2C, San Francisco, Bay Area, IoT, San Jose, Los Angeles
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","New York, NY",Data Engineer - Analytics,False
185,"$50,000 - $100,000 a yearContractA successful Data Engineer will have the following:·B.S. degree in Computer Science, Computer Engineering or a related technical degree; or an equivalent combination of education and experience.·3 or more years of experience in ApacheSpark with development experience inScala or Pyspark·Experience working in Java & REST APIs·Development experience in Python·Strong knowledge of Hadoop or other Big Data platforms·3 or more years of experience working within an Agile software development life cycle.Preferred Qualifications:·At least 3 years of experience using version control repositories such as SVN or GIT.Job Types: Full-time, ContractSalary: $50,000.00 to $100,000.00 /year","Denver, CO",Data Engineer,False
186,"A Junior Data Engineer will leverage a dynamic set of data management tools to profile, select and target audiences for marketing efforts. The role will assist in refining targeting strategies with our client engagement teams, and delivering a data set to be ingested by one of our marketing platform or production processes.People with experience as a Marketing Data Specialist, Data Specialist would translate well.Primary Responsibilities: Fulfill requirements on data projects within a pre-defined process while maintaining client budget and schedule, and delivering the highest degree of accuracy.Keep track of all assigned action items and assigned projects to ensure they are completed in a timely manner.Partner with developers, analysts and account executives to gather required materials and instructions necessary for completing data projects (update and manage databases using client data sources, process client results reports, and prepare campaign files for production.)Document project procedures between agency team members, client contacts, and third party data partners.Incorporate data from multiple sources using data software tools like Microsoft Excel, Access, and SQL Server, as well as other proprietary Masterworks or third party software applications.Validate all data elements involved with client projects for content accuracy and procedural use, including input data received from clients, transformed data used for output, and final output files.Troubleshoot data errors and anomalies, and provide recommended solutions.Escalate anomalies and other-than-anticipated results to project stakeholders as required.Position RequirementsTechnical Competencies: Excellent organizational skills and attention to detail, plus the ability to multi-task and negotiate priorities.Ability to work at peak performance with limited direction and competing deadlines.Command of grammar and language sufficient to proofread professional documentsPrevious experience in marketing, production planning or non-profit organizationsKnowledge of database relationships and architectureExperience with A/B testing and campaign segmentationProficiency with PythonProficiency in data management tools as appropriate to functions performed at Masterworks:Relational database management (Microsoft Access and Microsoft SQL Server)Cloud-based data lake technologies (e.g. Google Big Query or or Amazon Red Shift)Extract, Transform and Load (ETL) technologies with data hygiene practices (tool-based SQL scripting, custom Python, and experience with Matillion, Mulesoft or Talend products).Data encryption and transfer technologies (SFTP or use of APIs)Behavioral Competencies: Continuous LearningInitiative and Risk TakingIntegritySelf-MasteryCollaborationProblem SolvingInnovation and ChangeCommunicationDiversityResponsive to Customer NeedsPlanning and OrganizationResource MaximizationQuality ResultsStewardshipWork Experience: 2+ years of experience in related field (marketing, technology, database management)Educational Requirements: Bachelor's DegreeJob Type: Full-time","Poulsbo, WA",Junior Data Engineer (2 yrs experience required),False
187,"Instacart is building the best way for people anywhere in the world to shop for groceries. Since Instacart started in 2012, we've launched same-day delivery in 200 US markets. We are laser focused on delivering groceries from your favorite stores right to your door. We now cover over 60% of US households and aim to have 80% coverage by the end 2018—that's 90 million households! From a technology point of view, the platform is complex, rapidly scaling and processing millions of transactions in real-time all of the time. Our technology coupled with operational expertise enables Instacart to deliver fresh groceries in as little as an hour. This is a difficult problem to master and we are making it happen. Every day, we solve incredibly hard problems to create an experience for our customers that is absolutely magical.

The Data Engineering team at Instacart is rapidly growing and you will have the opportunity to shape its direction and create large impact. The team is looking for a motivated, self-starter with a drive to tackle a variety of data challenges at Instacart.

Responsibilities


Develop robust, high performance batch and near real-time data pipelines
Design, develop and maintain data models for company wide data warehouses
Work with modern data engineering infrastructure technologies, e.g., Snowflake, Airflow, Python
Work in an agile collaborative environment

Requirements


At least 2 years of relevant work experience post college
A high level of proficiency in one or more of the following domains:
Database design, entity relationship modeling, SQL
Python programming (Pandas, SciKit, NumPy)
Big Data technologies, e.g., Hadoop, Spark
Excellent written and verbal communication skills; able to effectively collaborate with diverse teams.
BS/MS in Computer Science, Engineering, Math, other quantitative field, or equivalent experience

Benefits


Talented and collaborative coworkers who will both push and support you
Market competitive salary and equity
Medical, dental, vision benefits
Take what you need vacation (and we really mean it)
16 weeks maternity leave / 8 weeks paternity leave so you can truly bond with your child
Complimentary Instacart express membership

Resources


Tech Blog ( http://tech.instacart.com/ )
Life at Instacart ( https://twitter.com/lifeatinstacart )
Team Stories ( https://medium.com/life-at-instacart )

","San Francisco, CA",Data Engineer,False
188,"$57.52 an hourContractThis person will be responsible for migration of data from existing on prem systems into Cloud Data Warehouse. Individual will be involved in all aspects of data migration including; Data Analysis, Mapping, ETL Development, Reconciliation, Testing and Documentation , Individual is accountable to estimate the work and ensure high quality deliverable on-time while adhering to policies and procedures of the group.

Responsibilities:

Projects and Development
Responsible for migrating data into cloud datawarehouse solution.
Individual will need to meet the requirements set by the business with a focus on data quality, timeliness & group coding standards
Work with other team members to ensure that all objectives and commitments are fulfilled in line with expectations, agreements and standards
Maintain consistency in processes and follow guidelines as laid out by team standards
Monitor and report on progress on tasks assigned
Understand and apply industry practices, architectural standards and procedures relating to work assignments.
Translate business requirements and technical designs into well-developed solutions that meets business data and KPI goals.
Designing database queries, views and functions for reporting and data analytics .
Design and implement technology best practices, guidelines and repeatable processes.
Optimize and refactor SQL databases and database objects; ETL processes, reporting and analytic solutions in support of business needs.
Evaluate and assess capabilities of new technologies and Business Intelligence tools as required.
Qualifications:
Strong experience data migration/integration experience
Bachelors degree in Computer Science/ Information Technology or related field
At least 5 years* experience in data modeling, development, implementation and support of transactional databases and data warehouses preferably Teradata
Expert level experience in SQL
Experience with Business Intelligence reporting and analytical tools. (Microstrategy, Tableau etc.)
Experience working on cloud based platforms. (AWS)
Data warehouse experience (AWS redshift, Snowflake Computing)
Advanced expertise in performance monitoring and optimization.
Strong analytical, critical thinking & problem-solving skills.","Holmdel, NJ",Data Engineer Migration & ETL,False
189,"ContractWho we are

Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 244 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.

Note:
Provide full legal first Name/Family Name
DO: Capitalize first letter of First and Last Name. Example: John Smith
DON'T: Capitalize entire First and/or Last Name. Example: JOHN SMITH
NOTE: Use correct grammar for Names with multiple cases. Example: McDonald or O'Connell

Provide full address details
Resume is required
Multiple attachments can be uploaded including Resume and Cover Letter for each application


Job Description Summary:
Data Modeler Lead - PayPal Enterprise Solutions


About the Enterprise Solutions team


Keeping the engine of the global economy strong and growing, the Enterprise Solutions team, a member of the PayPal Merchant organization, is responsible for helping millions of PayPal merchants manage and run their businesses on web and mobile. We are responsible for building products that support PayPal business customers, from casual sellers to the world’s largest businesses. We are currently working on building new enterprise-grade solutions to make our products even more dependable for large/enterprise customers, putting ourselves in the best position to continue powering the world’s largest e-commerce and payments applications at a scale few companies can match.

Job Description:
We are looking for the best Data engineer in the world, who have a passion for developing enterprise scalable, distributed and performance systems that require high availability to support mission-critical business tasks. Quality is at the forefront of everything our team does, and we are looking for true passion for writing robust, re-usable, and scalable.
Responsibilities:

5 plus years experience in setting up ETL solutions for large enterprises.
Experience on OLTP and OLAP data modeling.
Experience in designing large oracle databases (500+ TB) with Sharding.
Optimizing database (Index, IOT, partitioning, Caching etc.) and query tuning for faster response time
Assist development team on ETL and DB design and actively participate in code reviews.
Determines database structural requirements by analyzing upstream sources and downstream consumer needs
Develops database solutions by designing proposed system; defining database physical structure and functional capabilities
Maintains application performance by periodically identifying, reviewing and fine tuning those applications that cause database and ETL performance issues
Provides technical guidance to engineers whenever they deploy any new/changes to production in alignment with organizational data standards
Works closely with Enterprise data architects on new product/payment flows and determines the associated data contracts
Play a key role in the Data governance council that includes representation from Upstream
Partner with the Data infrastructure team to provide leadership and guidance with enterprise data strategies
Working knowledge on Infrastructure - ETL Grid, Databases, Servers, network and Storage


Subsidiary:
PayPal

Travel Percent:
0

Primary Location:
San Jose, California, United States of America



Additional Locations:


Bachelors Degree or Equivalent




English
We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.


Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.","San Jose, CA",LE Data Engineer,False
190,"As the Data Engineer, you'll be joining a team of passionate engineers, designers and product managers. Together, we answer business questions through data.

Blue Bottle Coffee is growing, presenting us with exciting opportunities to solve interesting challenges and help create solutions to serve our production teams, our retail teams and HQ teams. You'll create impact that you can see and taste, working on all parts of the data stack.
You will:
Build, maintain, and troubleshoot ETL pipelines for several data sources
Build micro-services to augment or supplant more involved ETLs
Build automated testing, performance evaluations, monitoring tools and dashboards
Work with and build APIs from/for other microservices and outside services
Evaluate, and when necessary, rebuild existing ETLs
You are:
Constantly learning and eager to iterate; hungry to build better software and are constantly finding better ways.
A team player; comfortable working as part of a group, excited to share knowledge, and welcome support.
Adaptable and versatile; able to find compromises in situations where there is not one right answer.
Able to break-down complex problems into solvable pieces of work.
You have:
Production experience with data engineering
Experience with cloud infrastructure
Ability to model data and write SQL
Familiarity with testing a data application, integration testing, and processes that rely on CI/CD
Familiarity with machine learning
A few benefits we offer:
Medical, dental, and vision coverage for all full-time employees and their dependents starting on their first day of work
401(k) plan
Paid time off and parental leave
Annual conference budget
Free drinks at any of our cafes and a complimentary bag of beans to take home each week
Discounts on any Blue Bottle food items and merchandise
Blue Bottle is an Equal Opportunity Employer. We value an open mind, dedication to work, and a collaborative spirit. We hire based on these qualities, a job’s requirements, our business’s needs, and an applicant’s qualifications. We do not tolerate discrimination or harassment of any kind—in the hiring process or in the workplace.

We comply with the ADA and consider reasonable accommodation measures that may be necessary for eligible applicants/employees to perform essential functions. If you have a disability or special need that requires accommodation, please contact us at careers@bluebottlecoffee.com.

We may refuse to hire relatives of present employees if doing so could result in actual or potential problems in supervision, security, safety, or morale, or if doing so could create conflicts of interest.

We will consider for employment qualified applicants with arrest and conviction records.

We participate in E-Verify. We will provide the federal government with employees’ Form I-9 information to confirm authorization to work in the U.S. We will only use E-Verify once an employee has accepted a job offer and completed the Form I-9.","Oakland, CA 94607 (Acorn-Acorn Industrial area)",Data Engineer,False
191,"ContractData EngineerResponsibilities: Leverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metricsEngineer a modern data pipeline to collect, organize, and process dataProduce clean, reusable code that is unit tested, code reviewed, and adheres to code standardsRequired Qualifications: Strong SQL and data warehousing skillsStrong analytical skills and problem-solving capability3+ years of experience in implementing BI solutions with SSAS/SSIS/SSRS1+ years of experience in Power BIAbility to communicate and collaborate with peopleAbility to operate effectively and independently in a dynamic environmentPreferred Qualifications: Experience in Cosmos/Azure Data LakeExperience in Azure Analysis ServiceExperience in data security area (Row-based security, Object-based security)Job Type: ContractExperience:Power BI: 1 year (Required)Data Warehouse: 1 year (Preferred)SQL and MS BI Stack (SSAS, SSIS,SSRS): 3 years (Required)Information Security: 1 year (Preferred)Microsoft SQL Server: 5 years (Required)Education:Bachelor's (Preferred)Work authorization:United States (Preferred)","Redmond, WA 98052",Data Engineer ( Contract ),False
192,"The Data Engineer position is responsible for the integration of enterprise data by developing, testing and implementing packages and programs using Microsoft’s SSIS and/or Informatica’s Power Center. In addition, this position is responsible for building, maintaining reference tables, master data domains and data extracts that are needed to ensure the efficient and accurate processing of information within Vidant Health.
Typical tasks will include, but are not limited to: requirements gathering, analysis, software architecture and design, software development, testing, implementation and documentation.
Responsibilities
Analyzes, designs, build, test, and implement integration technology solutions that meet the specifications of a project or service request.
Effectively utilizes standard enterprise tools to develop or implement technical solutions.
Writes test cases and report results
Identifies problems following problem management expectations and performs problem resolution.
Resolve most reported issues independently or escalates as needed to senior engineers.
Provides preventative maintenance, troubleshooting and quickly resolves routine problems to ensure infrastructure and application stability.
Adheres to standards and best practices, processes, and deliverables in accordance with standards.
Participates in defining operational readiness requirements.
Follows testing best practices and standards.
Adheres to SLA expectations.
Writes technical specifications for respective applications or infrastructure platforms.
Participates in creating prototypes or proofs of concepts for new infrastructure technology.
Remains current with respective technology capabilities
Collaborates with Project Managers to resolve requirements gaps uncovered in design or within specifications or requirements.
Prepares technical documentation for platform, service or infrastructure technical components.
Adheres to change and incident management standards and expectations.
Minimum Requirements
Bachelor's degree in Information Technology or related field OR relevant work experience required
4+ years' experience in an ETL administration or ETL/application development environment Experience working with project teams to design and implement new solutions
Experience working in team-oriented, collaborative environment
Independently motivated to seek knowledge in areas pertaining to their current position Exhibits proficient in either SSIS or Informatica ETL tools
1+ years applicable experience in the Healthcare / Health domain, with good understanding of HIPAA data Privacy and Security requirements
Self-educates and seeks knowledge from mentors / management of VH Information Services, and other business leaders from VH
2+ years of working with dimensional modeled data Knowledge of data vault modeling techniques
Expert level knowledge in writing SQL code for queries, stored procedures, data cleansing and manipulation

Other Information
Questions? e-Mail: JOBS@VIDANTHEALTH.COM
General Statement
It is the goal of Vidant Health and its entities to employ the most qualified individual who best matches the requirements for the vacant position.
Offers of employment are subject to successful completion of all pre-employment screenings, which may include an occupational health screening, criminal record check, education, reference, and licensure verification.
We value diversity and are proud to be an equal opportunity employer. Decisions of employment are made based on business needs, job requirements and applicant’s qualifications without regard to race, color, religion, gender, national origin, disability status, protected veteran status, genetic information and testing, family and medical leave, sexual orientation, gender identity or expression or any other status protected by law. We prohibit retaliation against individuals who bring forth any complaint, orally or in writing, to the employer, or against any individuals who assist or participate in the investigation of any complaint.","Greenville, NC 27834",Data Engineer,False
193,"Spun out of MIT in 2014, Zylotech is a disruptive AI-powered customer data analytics company located just 5 minutes walking from the Kendall Square T station. We are ‘Zylo-techies’, big thinkers with a sense of fun who love working in an open environment full of start-up perks.

Why you'll love working here:

Zylotech is a fast-growing company, that rewards autonomy and creativity. Our team is hard working, fast and collaborative and we encourage working on personal growth. There will be many opportunities to work with other departments and you'll be able to interact with some incredibly talented people.

What You’ll Do:

1. Explore client data, analyze and Implement ETL process using open source data science platforms

2. Write python/spark scripts for Model verification and/or query database.

3. Analyze customer data, back testing, writing test cases for product layers, executing test cases, writing test plan, conduct UAT

4. Monitor model performance and advising any necessary DevOps changes.

5. Build quick POCs / prototypes around data problems

What we seek:
 A Hustler. We encourage hacking out creative ways to find and build simple and effective solutions to complex problems
 Expertise in python/ spark
 Experience in Databases (NoSQL and SQL), Cloud databases
 Experience in Datawarehouse, Data analysis
 Experience in ETL tools and techniques
 Flexible to learn and build solutions using frameworks/tools across technologies

About Zylotech:

Zylotech is a Self-Learning customer data platform that helps marketers create complete customer profiles for targeting revenue opportunities more effectively. Powered by AutoML, the platform continuously unifies and enriches internal and external data, and performs ongoing micro-segmentation, pattern discovery, and recommendations, all while integrating with a variety of marketing clouds for on-demand accessibility. Zylotech’s cross-industry clients have reported up to a 6x increase in customer lift. For more information, visit Zylotech.com.

Zylotech is an equal employment opportunity employer and does not discriminate against any applicant because of race, creed, color, age, national origin, ancestry, religion, gender, sexual orientation, disability, genetic information, veteran status, military status, application for military service or any other class protected by state or federal law.

To all recruitment agencies: Zylotech does not accept agency resumes. Please do not forward resumes to our jobs alias, Zylotech employees or any other company location. Zylotech is not responsible for any fees related to unsolicited resumes. Unsolicited resumes received will be considered our property and will be processed accordingly.

Qualified Applicants must be legally authorized for employment in the United States. Qualified Applicants will not require employer-sponsored work authorization now or in the future for employment in the United States.","Boston, MA",Data Engineer,False
194,"Reducing the Use of Jails
Please note that we will be moving our headquarters to Industry City in Brooklyn, NY next year.
The Vera Institute of Justice is working to build national momentum toward reversing mass incarceration everywhere in America. It’s an ambitious goal—so we’re also building a skilled team of researchers who like big ideas and big challenges. Does that sound like you? If so, read on.
In the last decade, the geography of mass incarceration underwent a quiet, dramatic, shift. As incarceration rates in America’s biggest cities leveled out and then declined, rates in small cities and rural America continued to rise. Today, thousands of often overlooked smaller cities, towns, and rural areas have the highest rates of incarceration and, increasingly, the most outsized jails. This untold story calls for ending mass incarceration where it begins—in all of our backyards.
To spark reform, Vera’s In Our Backyards project is advancing an ambitious research and communications agenda to inform the public dialogue and guide change to justice policy and practice. Our work is driven by the realization that if we do not respond to the shifting geography of incarceration, the apparent national gains—driven by prison and jail population declines in large metropolitan areas—will be eroded by deepening problems in the small and rural communities across the country that actually comprise most of the population.
Responsibilities
The data engineer will work in close collaboration with the team to collect, manage, and analyze data relevant to the criminal justice system, to visualize the results of these analyses, and to report findings to our partners and the public. Data analysis, data management, and software development will be the primary focus.
This position will join an interdisciplinary, cross-functional team that includes data scientists, policy experts, ethnographers, and writers that span the research, policy, and communications functions at Vera.
What we need in you:
Experience assisting with research projects in an academic or professional setting. Our projects involve working with criminal justice system data to answer cutting-edge research questions. Hands-on experience with the research process and intimate familiarity with how to work on a research team are required.
Algorithmic reasoning skills. Many of the problems you will solve require devising a step-by-step procedure from a problem statement to a solution. Experience solving such problems is a must.
Proficiency in at least one programming language. Your duties will include creating or modifying code that retrieves, maintains, analyses, and visualizes data from the criminal justice system. Experience in Python, MATLAB, or R are preferred, but no language-specific experience is required.
Strength in data. Much of the work will involve collecting, cleaning, and maintaining large datasets. Experience working with and managing data are key.
Commitment to the missions of racial justice and ending mass incarceration. You need to be fluent in the language of both, and familiar with their relationship to community/public safety.
You’re self-motivated, collaborative and look for how you can help without being told. Collaboration is a Vera value – we thrive when we learn and build on each other’s ideas, and are open to giving and receiving feedback. What’s more, it’s absolutely essential for the fast-working In Our Backyards team, which is taking on big challenges, and delving into an area of reform where so much is still unknown.
Qualifications
Candidates must possess these minimum qualifications:
Demonstrated experience in dataset preparation and analysis;
Proficiency in at least one programming language;
Strong interest or experience in criminal justice and social justice for underserved populations;
Ability to work on multiple projects effectively and efficiently, both independently and collaboratively;
Excellent oral and written communication skills;
A commitment to a collegial and collaborative workplace;
An excellent candidate will have some combination of the following:
Experience with database development;
Experience developing software;
Experience with statistical analysis using Python, MATLAB, Stata, or R.


Salary: Based on experience, and including excellent benefits, Vera believes in compensating its staff members at or above market.
To Apply
Please submit cover letter and resume, and if available, a link to your github portfolio.
No phone calls, please. Only applicants selected for interviews will be contacted.
Online submission in PDF format for cover letter and resume is strongly preferred.
However, if necessary, materials may be mailed or faxed to
ATTN: People and Culture
Vera Institute of Justice
233 Broadway, 12th Fl.
New York, NY 10279
Fax: (212) 941-9407
Please use only one method of submission (online, mail, or fax).
Vera is an equal opportunity/affirmative action employer. All qualified applicants will be considered for employment without unlawful discrimination based on race, color, creed, national origin, sex, age, disability, marital status, sexual orientation, military status, prior record of arrest or conviction, citizenship status, or current employment status.
Vera works to advance justice, particularly racial justice, in an increasingly multicultural country and globally connected world. We value diverse experiences, including with regard to educational background and justice system contact, and depend on a diverse staff to carry out our mission.
For more information about Vera, please visit www.vera.org.
Related
American Jail: The modern tragedy of mass incarceration","New York, NY 10279 (Financial District area)","Data Engineer, In Our Backyards Campaign",False
196,"Job Description

Job ID: 439
Position: Data Engineer
Location: Lombard, IL
Type: C2H

Top 3
1-Mix of building and developing Spark, R, and Python
2- Experience building data pipelines to the cloud, their cloud technology is Wherescape.
3- understanding of data flows to and from the cloud
** Must be self-directed go getter, developer mind set, connect the dots get work done, a data engineer who will take a ask and run with it.

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines.","Lombard, IL",Data Engineer - 439,False
197,"About Critigen
We develop innovative applications using the best technology out there. Our data and software engineers play a huge role in helping our clients make sense of complex data that make important things visible. We are curious by nature and serve as valued consultants to our clients. Our developers enjoy the freedom to explore new tools and methods, and apply them in a collaborative, team environment.
Position Overview
Critigen is looking for data engineers who have a passion for map data. We are building a team that will own a data pipeline to transform data from a variety of sources to high value services which provide a variety of map-based functions. Task will include monitoring data streams, evaluating issues, analyzing patterns and developing algorithms and repeatable tools to automate and improve processes. You will be a great team collaborator with excellent communication skills.
Key Qualifications
B.S., M.S. in Computer Science or equivalent
2+ years in either QA or release management role
Strong written and verbal communicator
Excellent problem solving and debugging skills
Familiarity troubleshooting software and data issues using Linux based environments
Understanding of distributed computing
Experience with Apache Airflow and Spark
Experience with NoSQL systems
Python/Bash or similar language experience
Ability to work in an Agile environment
Great work habits, organizational skills and flexible enough to roll with changing priorities and tight deadlines
A strong sense of responsibility and an obsession with quality
Preferred Qualifications
Familiar with OpenStreetMap (OSM) or commercial map data
Knowledge of directed acyclic graphs
Experience with CLIs and scripting in Linux based systems
Experienced with source control tools such as github
Critigen is an Equal Opportunity Employer, Female/Minority/Veterans/Disabled/Sexual Orientation/Gender Identity.","Seattle, WA",Data Engineer,False
198,"$600 - $700 a dayContractIT-10130330SQL Data EngineerNew YorkUp to $700 per day (dependant on experience)6 – 12-month contractOur client, a cloud transformation/technology consultancy with offices in the US, UK, EMEA are currently looking for an SQL Data Engineer to join their growing team. As a Senior SQL Data Engineer you will be playing an integral part in helping our client build an exciting sophisticated software tooling which will be placed within the marketplace.Job Objectives: -Document design and concepts that will be utilized to help solve complex data architecture problemsWorking with stakeholdersFast paced environmentExcellent understanding of solving complex problemsImplementing solutions into executable codesDeliver database solutionsDevelop and maintain databasesAnalysis of dataDesign and build strong and scalable solutions (managing structured and unstructured data using traditional databases)Conduct end to end analysis including data gathering etcPrepare and perform data analysis and transformations to align data to business rulesWork through early stages of software life cycle to profile data and to conceptual, logical and physical data model designsProvide gap analysisDeliver documentation including technical designs, data flow, ERD’s and mapping documentsResponsibilities: -Work with data owners, designers, creating technical specificationsApplication lifecycle, analysis and definition, system design, implementation, testing and deploymentLiaising with the team’s applications architect to improve upon database and application designWork with our clients Subject Matter Experts to obtain a greater understating of the business needs and goalsProvide technical expertise and capabilities in SQL query writingWork closely with web development team in ongoing development and maintenance of databases, applications or toolsAre you a Senior SQL Database Engineer who would like to be considered for this exciting adventure?Are you an expert in SQL querying writing?Able to solve complex problems and find solutions?We want to hear from you.Please send your CV or call the team at Talent AnalytixTalent Analytix is acting as an Employment Agency in relation to this role. Due to the number of applications received please note that if you do not hear from us within 2 weeks we will not be proceeding with your application.Job Type: ContractSalary: $600.00 to $700.00 /dayExperience:SQL/Data: 5 years (Required)Work authorization:United States (Required)","New York, NY",SQL Data Engineer,False
199,"At DataSync Technologies, our data engineering professionals touch every area of our company. Their insights drive our decisions and their innovations fuel projects. When you join our team of data experts, you’re helping DataSync’s customers make better, smarter and faster decisions every day. See how you can help us solve some our customer’s most challenging data problems while you grow your skills and build your own future.
Job Description
DataSync Technologies is seeking Data Engineers to support a mission critical program within the Intelligence Community.
Requirement:
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.
Responsibilities will vary by specific data engineer role – Data Architect, Data Scientist, Database Engineer, Data Governance to include the following:
Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including “big data” sources.
Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets.
Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following: Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing.
Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintain awareness of emerging analytics and big-data technologies.
Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing.
Provide data services, data administration, data management, and “Big Data” support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments.
Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management.
Hadoop, cloud, and other technologies associated with data storage, processing, management, and use.
The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.
Preferred Qualifications (All not required):
Databases/Data Stores: Oracle, MySQL, HIVE, HBASE, and HDFS
Frameworks: Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP
Indexing: SOLR and Lucine
Development/Scripting Languages: JAVA (J2EE), Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl
________________________
What makes DataSync Technologies different?
Leadership Training: We provide employees with a variety of learning opportunities, including access to exclusive classes, professional growth training and more.
Feedback & Mentoring: We believe in talking—often. So we have one-on-one feedback sessions for every employee.
Community Service: We believe in helping the community where we work. DataSync and its employees donate time and services on a regular basis to local military charities. We believe in helping, both inside and outside of the office.
Social Events: We plan social events on a regular basis to help our employees relax and socialize so we get to know one another outside of our job titles.
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537
DataSync is committed to providing veteran employment opportunities to our service men and women.
www.datasynctech.com

www.facebook.com/DatasyncTechnologies

www.twitter.com/Jobs at DataSync (@DatasyncJobs)

www.twitter.com/datasynctech

#datasynctech on Instagram

Interested in Joining Our Team? - Check out this YouTube video!

#CJ","Reston, VA 20194",Data Engineer – All Levels,False
200,"Overview
Healthcare is an opportunity to utilize data assets including electronic medical records, population health databases and clinical care models to have immediate and wide impact on people's lives. The Data Engineer is responsible for developing products to serve the analytic needs of the health system through the application of tools such as visualizations, software development and statistical analyses driven by data resources.

Responsibilities
This position is expected to carry out sophisticated operations with data to procure, transform and organize data that is required to develop insightful and actionable information. They should therefore be capable of complex and creative problem solving with the ability to complete difficult projects that fulfill our outcomes based mission. The ability to coalesce multiple, sometimes divergent, data sets to fulfill the requirements of data science and analytics is required. Creative solutions are encouraged and immersive experiences preferred.

Primary skills for this position include those requisite for developing the backend databases and data structures required for the development of analyses that use data to enable analyses, information delivery and interaction with users. Successful candidates should have a good understanding of multiple data engineering programming languages such as SQL (including SSIS), Hive, Hadoop, Spark or Storm. Our shop is primarily Python and so enough Python to be dangerous couldn't hurt. R is ok too. Domain knowledge in informatics or information modeling will be valuable.
Knowledge of information architecture (as defined in the field of design as well as the field of data architecture) is preferred to enable the candidate to construct integrated sets with complex data.
Ability to work with data science teams to assist in delivery of analytical outcomes and models.Ability to develop, maintain and test data systems and architectures especially with automation.Ability to generate a quality work product in a timely manner while maintaining a strong attention to detail.Work closely with business users and the analytics team to design data models.Utilize automation to create set processes for specific data ingestion, transformation and access procedures.Identify and communicate new data needs effectively to the leadership team.Support other members of the team with the ability to explain methodologies and assist in development.Be a strong learner with the understanding of continuous professional and technical improvement of skills and techniques.

Qualifications
Education:
Minimum: Bachelor's: Computer Science, business, statistics, information sciences healthcare, design, engineering
Preferred: Master's in Healthcare, Computer Science, Informatics, Analytics, Statistics, Engineering

Requirements:
Healthcare experience preferred but not required.
Significant background with SQL and expertise is required. Additional competency with any big data technologies and cloud platforms are a plus. [Hive, Pig, Spark, Storm, Hadoop // AWS, Azure, Cloudera)
Being conversant in data science languages (R, Python, Scala) are very useful.

Atlantic Health System aims to deliver the highest quality and care combined the best experience for our patients and their families. We are confident that you will find success within Atlantic Health System, which has been named for the 8th year in a row to Fortune's ""Top 100 Best U.S. Companies to Work For"" list. We believe you'll find that our culture of collaboration and care exemplifies the value we place on our patients, their families and our employees.

Atlantic Health System aims to deliver the highest quality and care combined the best experience for our patients and their families. We are confident that you will find success within Atlantic Health System, which has been named for the 8th year in a row to Fortune's ""Top 100 Best U.S. Companies to Work For"" list. We believe you'll find that our culture of collaboration and care exemplifies the value we place on our patients, their families and our employees.

All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.","Morristown, NJ 07961",Data Engineer,False
201,"Job Description

What do you do?
Artificial Intelligence and Machine learning are at the core of modern Money Mart. You would be part of a new, cutting-edge Artificial Intelligence and Machine Learning center of excellence, tasked with developing innovative solutions for helping the business grow and become more efficient. You get the gist – enjoy the experience of working on high impact, Silicon Valley style projects, while living in Dallas or PA (outside Philadelphia). You also get to learn about how big data is revolutionizing the world of consumer financial services.

What do we need?
You to have an amazing personality and communication style.
That you are super-organized and are a problem solver.
That you take pride in everything that you do, and it shows.
And most importantly that you have unquestionable integrity.

Why work for us?
We invest in our employees, and offer extensive training, and development programs to set you up for future success.
If we sound like a fit, and you’re ready to start an exciting career with an organization that fosters employee growth, apply today!

Job Description
This position will be responsible for:
Manage the full lifecycle of assigned data projects from requirements to technical design (platform, data, and automation routines), to project deployment of the proposed lab solution.
Identify, obtain, understand and move data sets through the company's Big Data ecosystem.
Move large data sets from multiple sources and ingest this data into company environment
Hands on data acquisition and integration using Hadoop / AWS cloud stack
Perform detailed analysis/design of functional and non-functional requirements and translate them to solutions
Perform advanced data discovery, profiling, and assessment on required data
Provide data subject matter expertise

Qualifications

Education
BS in computer science, or related scientific fiel

Experience
5+ years of professional experience in a business environment
3+ years of relevant experience in data engineering
Preferred Qualifications
Masters degree in computer science, or related technical field
7-10 yeas of Technical Business experience – ETL experience
4-7 years of solid Hadoop experience – Scoop, Pig, Hive…
Experience with SQL, Hadoop, Unix Scripting

Skills
Technical experience with big data visualization applications
Ability to clearly articulate pros and cons of various technologies
Ability to document use cases, solutions and recommendations
Strong communication and data presentation skills
The motivation to achieve results in a fast-paced environment.
Strong attention to detail
Comfortable working in a fast paced, highly collaborative, dynamic work environment
Additional Information

Benefits
Medical / Dental/ Vision benefits available after 30 days of employment
Company paid life insurance
Paid holidays
PTO/ 401K / Tuition Reimbursement
All your information will be kept confidential according to EEO guidelines.","Dallas, TX 75224 (Southwest Dallas area)",Data Engineer,False
202,"Required Experience, Skills and QualificationsTitle: Data EngineerMorrisville, NCFTE Full timeWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data.Preferred Qualifications: 5+ years of related experience is required.A BS or Masters degree in Computer Science or related technical discipline is requiredETL experience with data integration to support data marts, extracts and reportingExperience connecting to varied data sourcesExcellent SQL coding experience with performance optimization for data queries.Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.Experience in cloud-based ETL development processes.Experience in deployment and maintenance of ETL Jobs.Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.Has strong technical background and remains evergreen with technology and industry developments.Job Type: Full-timeExperience:Business Intelligence: 5 years (Required)Education:Bachelor's (Preferred)Work authorization:United States (Required)","Morrisville, NC",Data Engineer / (Business Intelligence),False
203,"Spotify is looking for a Data Engineer to join us. You will build data driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.

What you’ll do
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Leverage best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You are knowledgeable about data modeling, data access, and data storage techniques.
You care about agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of collaboration within teams.","New York, NY 10011 (Chelsea area)",Data Engineer,False
204,"At Toyota Research Institute (TRI), we’re working to build a future where everyone has the freedom to move, engage, and explore with a focus on reducing vehicle collisions, injuries, and fatalities. Join us in our mission to improve the quality of human life through advances in artificial intelligence, automated driving, robotics and materials science. We’re dedicated to building a world of “mobility for all” where everyone, regardless of age or ability, can live in harmony with technology to enjoy a better life. Through innovations in AI, we’ll…
Develop vehicles incapable of causing a crash, regardless of the actions of the driver.Develop technology for vehicles and robots to help people enjoy new levels of independence, access, and mobility.Bring advanced mobility technology to market faster.Discover new materials that will make drive batteries and hydrogen fuel cells smaller, lighter, less expensive and more powerful.

Our work is guided by a dedication to safety – in how we research, develop, and validate the performance of vehicle technology to benefit society. As a subsidiary of Toyota, TRI is fueled by a diverse and inclusive community of people who carry invaluable leadership, experience, and ideas from industry-leading companies. Over half of our technical team carries PhD degrees. We’re continually searching for the world’s best talent ‒ people who are ready to define the new world of mobility with us!

We strive to build a company that helps our people thrive, achieve work life balance, and bring their best selves to work. At TRI, you will have the opportunity to enjoy the best of both worlds ‒ a fun start up environment with brilliant people who enjoy solving tough problems and the financial backing to successfully achieve our goals. If you’re passionate about working with smart people to make cars safer, enable the elderly to age in place, or design alternative fuel sources, TRI is the place for you. ‒ Start your impossible with
About The Machine Learning & Cloud Platform Team:
The role of Senior Data Engineer for Machine Learning (ML) Infrastructure is at an exciting intersection between large scale deep learning and world scale data processing.

ML: As a member of the ML team, you work alongside top research scientists in the field. You are responsible for enabling cutting-edge Deep Learning to be applied to Petabyte-scale (and beyond) volumes of sensory data (including video, LIDAR, radar) coming from our cars, robots, and other data collection platforms. You interact closely with our cloud data team to design and deploy large-scale distributed infrastructure for rapid experimentation, training, and inference. You are passionate about applying cutting-edge machine learning to real-world problems in autonomous driving and robotics and about building the required frameworks and tools to do so.

Data: As much code runs inside an autonomous car or robot, even more code runs in the cloud. Services and pipelines that process data is what our machine learning, mapping, robotics, and simulation teams build to implement their own initiatives. The cloud data team is responsible for designing and implementing the set of services, libraries, tools, and dashboards that make this possible. We think about scale (“consume petabytes of driving data”), governance (“explain through data that our car did the right thing”), and cross-platform execution (“deploy an image-processing service in AWS or inside a robot”). We are looking for engineers that can make this possible.
Responsibilities:
Maintain and continuously improve large scale iterative labeling, experimentation, training, and deployment pipelines for modern deep learning on cameras, LIDARs, radars, and other sensors.
Collaborate with other software engineers and research scientists to develop high-performance frameworks and tools for deploying and managing services and data pipelines from cloud storage to GPUs.
Communicate, scope and design new features to meet the needs of clients inside and outside of TRI.
Develop/integrate labeling tools and work with teams to provide ground-truth in support of machine learning and simulation.
Live and breathe the software practices that produce maintainable code, including automated testing, continuous integration, code style conformity, and code review.
Qualifications:
Bachelor's degree in Computer Science or equivalent.
Strong communication skills. Team player. Good Listener.
Strong Python skills (including SciPy stack).
Experience with C++ is a plus.
Experience integrating with Cloud APIs especially AWS.
Experience with High-Performance Computing, GPUs, performance optimization.
Strong ability to write unit testable code.
Experience with data stores and related technologies for ingesting, indexing and analyzing large amounts of time series and video data: S3, Parquet, Alluxio, big data filesystems, Cloudera stack etc.
Experience with relational or NoSQL systems and integrations across different data stores.
Experience integrating with CI tools programmatically, especially Jenkins.
Experience with Docker, registries and container deployment services (e.g., AWS ECS, Kubernetes).
Experience with related tools and processes: Git, Continuous Integration, Code Reviews.
Experience with data transformation tools like OpenCV, Pandas etc.
Qualifications Bonuses:
Experience working with Machine Learning, especially Computer Vision, Deep Learning a very big plus.
Experience building and growing image and video labeling pipelines.
Experience with software development on top of Deep Learning Frameworks like PyTorch (preferred), MXNet, Tensorflow.
Experience with big data pipeline a plus, and pipeline orchestration frameworks such as Spark, Airflow, Kafka.","Los Altos, CA 94022",Data Engineer (Machine Learning Focused),False
205,"The Data Engineer will lead the development of data warehousing solutions to power business intelligence at The Hive. They will synthesize business requirements for analytics, operational and fundraising information, creating and implementing solutions to integrate data from various enterprise systems.MISSIONThe United States Association for UNHCR (USA for UNHCR) supports the UN Refugee Agency’s humanitarian work to protect and assist refugees around the world. The organization strives to meet the unmet needs of the world’s most vulnerable people, building support and awareness in the United States for UNHCR’s life‐saving programs. Established in 1989 by concerned American citizens, USA for UNHCR is a 501(c)(3) not-for-profit organization, headquartered in Washington, D.C with an office in New York, NY.ESSENTIAL DUTIES AND RESPONSIBILITIESWrite code to build ETL processes to integrate information from various enterprise IT systems.Ability to perform API integrations on platforms such as Salesforce Marketing Cloud. We use Postgres SQL for our data stores and AWS for hosting.Design new data structures, add to and optimize existing data schemas.Ability to analyze upstream and downstream effects of a change in existing data structures and planning for change management.Understand key strategies and business processes of internal clients across the organization in order to provide the best solution to achieve their outcomes.Conduct analyses of business or technical user needs, document requirements and design tailored data solutions.Monitor performance of ETL processes and build in redundancies to avoid risk of an information outage.Provide support and troubleshoot questions arising from report developers and business users of these data solutions.Track and document work done and communicate progress with business users in a timely manner.Ability to adapt and/or modify processes in response to changing circumstances.Well versed in specific industry best practices, and an ability to adapt them to U4U’s environment.Ability to work effectively across multiple complex projects.Research and recommend innovative and automated approaches.QUALIFICATIONS To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.EDUCATION and/or EXPERIENCEBachelor’s degree in computer science, information systems or related field required and/or combined equivalent of education and experience. Master’s degree preferred.5 years of experience as a business or technical professional in business intelligence, data modeling or related field.Experience in defining and documenting complex systems requirements.Experience with data integration, building database objects using SQL, optimizing queries and writing stored procedures. ( > 5 years.)Experience in data warehouse methodology and data modeling. ( >3 years)Experience in extracting data from various APIs.Strong ability to analyze business requirements and recommend solutions.Ability and desire to mentor and train others.Experience in software development methodologies.Experience in writing SQL queries, coding in Python, and creating systems in the Cloud Computing Space, preferably AWS.Experience with CRM’s, preferably Salesforce.Experience with working on different file types for both structured and unstructured data.PHYSICAL DEMANDSThe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.Tasks involve some physical effort, i.e. some standing and walking, or frequent light lifting (5-10 lb.); minimal dexterity in the use of fingers, limbs, or body in the operation of office equipment; may involve extended periods of time at a keyboard.Extended periods of sitting at a workstation or desk and manual dexterity to work efficiently on computer keyboard for data entry and composing of documentsWORK ENVIRONMENT The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The tasks will generally be performed in a typical office environment. May also involve travel to some locations within the company’s region of operations and select donor locations.DISABILITY SPECIFICATIONSUSA for UNHCR will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.Job Type: Full-time","New York, NY 10001 (Chelsea area)",Data Engineer,False
206,"The Hartford’s Data Asset Management organization is seeking entry-level talent to build careers in Data at locations that include Hartford, Connecticut. In this role, associates will grow in the Data Engineering track, providing development support on The Hartford critical data assets. Associates will participate in the entire software development lifecycle process in support of information data projects and grow with emerging technologies and processes.

Overall Knowledge

You must be a team player with a positive ‘can do’ attitude and possess the following qualities:
Interest in data analytics, technology, and problem solving
Excellent communication, analytical, interpersonal, and organization skills required
Organizational and time management skills with the ability to adjust to changing priorities in a fast-paced environment
Entrepreneurial mindset
Responsibilities
Partner with various parts of the organization to build a working knowledge of the organization, business, processes, and its customers
Learn and utilize tools such as Informatica, B2B, PL/SQL, Hadoop, etc.to develop data assets that support organizational decision making via prototyping, data discovery, profiling, etc.
Leverage data mining and Business Intelligence tools to analyze large amounts of data identifying relationships and patterns within the data
Solve a range of core business and technical questions through data analysis
Work is guided by more senior members of the team
Qualifications
Qualifications
Bachelors degree – We are seeking May 2019 graduates
A desired cumulative GPA of 3.0 or higher (out of 4.0 scale or equivalent) at the time of graduation
Desired majors include, but are not limited to: Computer Science, Engineering, IT, Management Information Systems, Data Analytics, Applied Mathematics, and Business
Desire candidates with prior Data Analysis and/or Data Engineer competencies
Understanding of current and emerging IT products, services, processes and methodologies
Prefer familiarity with Big Data technologies and concepts on a Hadoop platform (e.g. Scoop, Hive, Pig, NoSQL, etc…)
Prefer working knowledge of ETL process and experience with SQL


Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age

HIGCLG
Job Function
: Data Engineering
Primary Location
: United States-Connecticut-Hartford
Schedule
: Full-time
Job Level
: Entry Level
Education Level
: Bachelor's Degree (±16 years)
Job Type
: Standard
Shift
: Day Job
Employee Status
: Regular
Overtime Status
: Exempt
Travel
: No
Job Posting
: Oct 4, 2018, 8:09:09 PM
Remote Worker Option : No","Hartford, CT",Associate Data Engineer,False
207,"Hi, we're Nuts.com!

We're changing the landscape of snacking on nuts, dried fruit, chocolate and more! We planted our roots in Newark, New Jersey during the Great Depression, selling premium nuts on Mulberry Street's open-air market. We've come quite a long way since then, taking our multi-generational family business online in 1999. Even after 90 years, we continue to pride ourselves in expertly sourcing the highest quality foods and treating our customers like family.

What's our team like? We're driven, collaborative, and entrepreneurial. Energy and passion power our business and we look for candidates who share in that excitement to help us continue to build something special.

The role

At Nuts.com, we love our customers, and we love data. And while we're happy to say that we have a lot of both, we need help bringing it together, managing it, and making it useful -- to us and to our customers. This is where you come in: we're building our business on this foundation of data, and you'll make sure it's a stable one.

In partnership with an awesome Analyst, you'll run everything from heavy, cross-channel analysis of media interaction logs to optimize Paid Media to distilling purchase data down to understand our customer better. You will help us think of innovative ways to make it even more enjoyable and valuable to shop with us.

We have thousands of items, a data-driven warehouse, a robust order management system, an exhaustive tagging footprint, a growing number of media outlets, and happy, engaged customers. If you're already imagining all the cool stuff we'll be able to build with that, then we should talk.

What you'll do


Collaborate with analysts, marketers, and department heads to define use cases, solutions, cross-functional projects and priorities
Maintain a holistic vision of the organization, its data, and its customers enough to recognize and even anticipate their data needs
Define, build, and launch the solutions that make it all a reality
Gather, transform, clean, and secure data from many sophisticated, state-of-the-art sources and destinations
Build pipelines that move the data to, through, and from all of the sources and also identifying how they should be connected in the first place
Identify how the data will be used once it's all built
Respectfully and energetically express your thoughts and data philosophy to non-technical SMEs, backing up clear reasoning behind all tool & infrastructure decisions/proposals
Architect, create and build systematic solutions to tough problems, such as the continuous data flow from networked sensors, and the ETL pipelines for all systems, transactional and otherwise

What you'll bring


Bachelor's degree.
3+ years of experience architecting traditional and big data solutions in SQL and NoSQL databases, ideally for Ecommerce applications.
2+ years experience manipulating, processing and extracting value from large disconnected datasets.
1+ years experience in dimensional data modeling and implementing these models in Customer Data Platforms / CRM systems.
Programming capabilities in common languages (SQL, Python, Java, etc.).
Hands-on experience architecting and implementing data infrastructure from scratch using platforms such as AWS Redshift, Alooma, Databricks, etc.
Experience integrating multiple data sources and formats, e.g. weblogs, Google Analytics, Google Adwords, Zendesk, etc.
Experience working in an Agile, Scrum-based environment.
Curiosity and creativity, with sound judgment and high degree of both integrity and empathy.
Effective written and verbal communication of data-driven solutions across all levels of the organization.

Bonus points if you're..


Experienced in:
Mathematical Modeling/Statistical Modeling
Machine Learning
Data Structure and Algorithm, Optimization
Knowledgeable or have direct experience using business intelligence reporting tools. (e.g., Looker, Periscope, etc.)

What we offer


A challenging role in a rapidly evolving business
Competitive compensation, benefits, and 401K Match
Paid Maternity, Adoption and Paternity leave
An ever-evolving range of perks (they're employee feedback-driven so we're continuously improving)
A casual work environment (jeans and sneakers are A-O-K!)
All the Nuts.com snacks your heart desires + a 40% phenomenal employee discount!

EEO STATEMENT

Braver Brands is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age, protected veteran or disabled status, or genetic information.","Jersey City, NJ",Data Engineer,False
208,"Overview:
The data engineer will provide IT support and development for data services at Vistra, and will ensure the timeliness, quality and accuracy of the data and platforms that support the data management and ETL functions.


Responsibilities:
Implement and maintain IT data and ETL platforms; understand and deliver on the functional support and data needs of data management within Vistra .
Follow data policies, standards, guidelines and procedures in order to ensure data to support reporting and analysis is available, responsive, and achieving business outcomes and objectives
Implement data and ETL solutions or enhancements to improve overall Vistra data architecture; conduct research and make recommendations on new data management processes and innovations.
Work closely with vendors, service providers, the business and internal team to define and understand analytics needs in order to achieve key performance indicators and Service Level Agreements for the benefit of Vistra and its business objectives.
Translate business needs into functional requirements, update/create documentation (Business Process Designs, Functional Designs, Data Architecture)
Participate in projects and Agile teams; make recommendations and implement changes to mitigate risks and optimize data platform performance

Requirements:
Bachelor's degree in MIS, Computer Science, or a related field from an accredited college/university (or equivalent)
0-3 years of experience in data management or related practice such as data science.
Proven ability to collaborate on cross-functional teams
• Experience with open source (Hadoop) and cloud highly desirable



Job Family
Information Technology


Company
Vistra Corporate Services Company


Locations
Irving, Texas


Texas
We are a company of people committed to: Exceeding Customer Expectations, Great People, Teamwork, Competitive Spirit and Effective Communication. If this describes you, then you will have a good career here!



If you currently work for Vistra Energy, TXU or Luminant please apply via the internal career site.","Irving, TX",Data Engineer,False
209,"ContractSpark data management6 years development on Hadoop/SparkDevelopment experience using Microsoft SQL Server, PostgreSQL, MySQL or OracleLarge data sets experienceExposure to data hygiene routines and modelsDatabase design, development and data modeling experienceJob Type: ContractExperience:Spark Data management: 6 years (Preferred)","Chicago, IL",Data Engineer,False
210,"Exabeam provides security intelligence and management solutions to help organizations of any
size protect their most valuable information. The Exabeam Security Intelligence Platform
uniquely combines a data lake for unlimited data collection at a predictable price, machine
learning for advanced analytics, and automated incident response into an integrated set of
products. The result is the first modern security intelligence solution that delivers where legacy
security information and event management (SIEM) vendors have failed.

Position Overview

We are looking for a Big Data Engineer to join our Data Lake engineering team. Data Lake team is responsible for building the next-gen log management system that enables SIEM at scale. Some of the scale challenges we face are very unique in the industry. We are looking for someone who is very passionate about distributed systems and high scalability.
Responsibilities
Design & build high-performant, reliable systems which scale effectively irrespective of the size of the customer
Take pride in delivering a top quality product
Work closely with engineers, PM and customer support.
Constantly learn, innovate and mentor other members in the team.
Be responsible for end-to-end delivery of features and provide support after launch
We are a startup - wearing multiple hats is expected and encouraged.
Write clean, elegant code that solves complex problems
Participate in design & code reviews
Qualifications
B.S. in Computer Science or related.
Experience in any of these is a big plus: ElasticSearch, LogStash, Kafka, Mongo, HBase, Spark, Hadoop
Experience in Linux
Experience in scala, python desired
Exabeam is built by seasoned security and enterprise IT veterans from Imperva, ArcSight, and
Sumo Logic. We are headquartered in San Mateo, California and are funded by Norwest
Venture Partners, Aspect Ventures, Icon Ventures, Lightspeed Venture Partners, Cisco, and
investor Shlomo Kramer. Follow us on Facebook, Twitter, and LinkedIn.","San Mateo, CA 94403 (Sugerloaf area)",Big Data Engineer,False
212,"ContractJob SummaryClient is looking for a Senior Data Engineer with 8+ years of experience. Data Engineer with skills.Required Experience, Skills and QualificationsFace to Face Interview Required.8+ Years of experience.Must have Spark, Scala, Hadoop & JavaJob Type: ContractExperience:Spark, Scala, Hadoop & Java: 1 year (Preferred)Data Engineer: 8 years (Preferred)Location:New York, NY (Preferred)","New York, NY",Data Engineer,False
213,"Job Description
Would you like to support increasing customer base and the revenue for Amazon Web Services (AWS), a market-leading cloud offering? Would you like to be part of a team focused on increasing awareness and adoption of the AWS platform by analyzing customer's behavior on and outside AWS website? Do you want to empower AWS marketing team make the data-driven decisions that further establish AWS as the leader in the cloud computing world?

As a Business Intelligence Engineer at AWS in Seattle, you will be working in a large, extremely complex, and dynamic data warehousing environment. We are looking for someone with the uncanny ability to integrate multiple heterogeneous data sources like Adobe Site Catalyst, Adobe Target, Sales Force, Adobe Connect with AWS central data warehouse and build efficient, flexible, and scalable data warehouse and reporting solutions. The candidate should be enthusiastic about learning new technologies and be able to implement solutions using these technologies to empower internal customers and scale the existing platform. You should have excellent business and communication skills and be able to work with business owners to develop and define key business questions, then build the data sets that answer those questions. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing reporting applications. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive growth.

Roles and Responsibilities
Understanding business requirements and converting them to technical solutions.
Create ETLs to take data from various operational systems like Adobe Target, Adobe Connect, Sales Force and create a unified dimensional or star schema data model for analytics and reporting.
Support Technical Program Manager, Research Scientist, and a growing virtual team aimed at analyzing usage data to derive new insights and fuel customer success
Use business intelligence and visualization software (e.g., OBIEE, Amazon Quicksight, etc.) to empower non-technical, internal customers to drive their own analytics and reporting.
Develop a deep understanding of AWS’s vast data sources and know exactly how, when, and which data to use to solve particular business problems.
Monitor and maintain database security and database software.
Support the development of performance dashboards that encompass key metrics to be reviewed with senior leadership and sales management
Work with business owners to build data sets that answer their specific business questions.
Manage numerous requests concurrently and strategically, prioritizing when necessary
Basic Qualifications

Bachelors degree in CS or related technical field and 5+ years of experience in data warehousing Or Masters degree in computer science/information systems and 3+ years of experience in data warehousing.
3+ years of relevant experience with ETL and business intelligence architectures
Knowledge of SQL
Understanding of advanced Data Warehousing concepts and track record of applying these concepts on the job.
Experience building self-service reporting solutions using business intelligence software (e.g., OBIEE, Tableau, Amazon Quicksight, etc.).
Preferred Qualifications

Experience with Amazon Redshift or other distributed computing technology.
Exceptional troubleshooting and problem-solving abilities.
Excellent written and verbal communications skills.
Demonstrated ability to work effectively across various internal organizations","Seattle, WA",Sr. Data Engineer,False
214,"We are looking for a very motivated data engineer to build the framework of data reporting and analysis for our licensing partners – Labels and Publishers. This is a great opportunity for a data engineer that has worked on the back end of things to gain more business context by working closer to the partners and business.

What you’ll do:

Design, build and maintain a “licensing specific” data infrastructure, which captures all data points needed by the licensing external partners and by the internal licensing team
Design, build and maintain the standard reporting for licensing external partners (major labels and publishers). For examples: top charts, free trial conversion, market share, top search, active users etc
Partner with licensing data scientists to drive business insights on labels and publishers
Partner with licensing data scientists and the broader licensing team to drive partners business reviews
Design, build and maintain ad-hoc solutions and data pipeline to gather relevant data across the sprawling data infrastructure of Spotify
Become the center of excellence for data collection, data taxonomy and data filtering specific to licensing partners.
Drive automation of standard reporting and distribution wherever possible.
Who you are:

You are self-driven and have a good understanding of the business and communicate well with non-data colleagues.
You are interested in being the glue between engineering and analysis and feel comfortable working in a business oriented team.
You are Innovative and able to come up with new solutions to very tricky problems
You are passionate about crafting clean code and have a steady foundation in coding
You know Scala and Python language
You have experience in developing/building data pipelines (batch or streaming)
You are familiar with ETL (extract, transform and load)
Tools and infrastructure such as GCP (google cloud platform) is a plus, as well as familiarity with Amazon or Azure cloud.
At least 2-3 years of relevant experience, i.e. software engineering working with data analysis, data modeling, data processing as well as
Great role if you’re interested in making a change!

Questions? or Interested in Learning more? Contact Andrea Silvestrini @asilvestrini@Spotify.com","New York, NY 10011 (Chelsea area)",Data Engineer – Legal & Licensing,False
215,"InVision is the digital product design platform used to make the world's best customer experiences. We provide design tools and educational resources for teams to navigate every stage of the product design process, from ideation to development. Today, more 4 million people use InVision to create a repeatable and streamlined design workflow; rapidly design and prototype products before writing code, and collaborate across their entire organization. That includes more than 80 percent of the Fortune 100, and organizations like Airbnb, Amazon, HBO, Netflix, Slack, Starbucks and Uber, who are now able to design better products, faster.

Our team is in search of an awesome Data Engineer to help us change the way digital products are designed.

About the Team:
InVision's Data Science & Engineering team helps the rest of the business to measure and analyze the impact of ongoing product changes. This allows us to make data-driven decisions about what to build next: and do more of what works, less of what doesn't.

Building, scaling, and owning a high-volume data pipeline—from the frontend event code all the way back to the data warehouse—requires constant collaboration with the brightest people in the organization. Whether you're at a beach house in Hawaii or a coffee shop on the East Coast, you'll have the support of brilliant developers, analysts, and scientists at your fingertips to get you through and keep the workday challenging and fun.

What you'll do:

Supporting and scaling the pipeline that relays data from InVision's production applications back to our warehouse. This includes:
Writing, operating, and documenting analytics libraries for our production apps.
Replicating state and events from MySQL, MongoDB, and Kafka using various third party tools.
Maintaining and extending scripts and components to consume raw event data into our warehouse.
Monitoring and tuning Redshift.
Organizing our data warehouse to be a self-explanatory, self-documenting, performant resource for product managers, engineers, and business analysts.
Working with our data scientists to tune and improve shared scripts for postprocessing raw data into clean, easy-to-analyze schemas.
Restructuring our warehouse to support increasingly sophisticated and more real-time analysis.
Investigating new technologies for incorporation into our architecture.
Helping engineers implement best practices for measuring the business impact of their features.

What you'll bring:

1+ years of experience designing, implementing and operating production ETL and data pipelines.
Full stack development experience, especially writing APIs and/or libraries called by coworker teams.
A track record of maintaining databases (MySQL, Redshift, etc.) and/or querying them performantly.
A quality mentality: You write automation, do spot checks, and get creative to make sure the data you've stored tells the true, accurate story of what happened.
Excellent communication and collaboration skills with coworkers, teams, and stakeholders.

About InVision:
InVision offers an incredibly unique work environment. The company employs a diverse team all over the world. Each InVision team member is given the freedom and tools to do their best work from wherever they choose.

The benefits we offer in the United States and Canada include competitive health plans and a retirement plans. Some InVision-wide benefits offered to all employees across the globe include a flexible vacation policy, monthly coffee shop stipends, annual allowances for books related to your profession, and home office setup & wellness reimbursements. InVision is an international employer so some benefit offerings will vary from country to country.

InVision is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know.","San Francisco, CA",Data Engineer,False
216,"InternshipThis is an opportunity for
(a) a gap year internship of one year (no 6 months internships)
OR
(b) an end-of-studies internship of one year followed by a potential job offer based on performance

You would be joining AgilOne in our Engineering department starting summer 2018 in one of our offices in California (Sunnyvale/San Francisco).
Responsibilities:
Design, build, install, test and maintain a highly scalable data platform.
Ensure that said platform meets business requirements and industry practices.
Write queries/jobs/functions to feed and enhance the pipeline.
Develop efficient, testable and well-documented code.
Recommend ways to improve data reliability, efficiency and quality.
Integrate new technologies and software engineering tools into existing platform.
Qualifications:
Bachelor's Degree in Engineering, Information Technology, Computer Science, Mathematics or similar technical/analytical degree
Proficient in SQL, data analysis/exploration
Intellectual curiosity, along with excellent problem-solving and quantitative skills, including the ability to disaggregate issues, identify root causes and recommend solutions
Self-motivated and good sense of ownership - comfortable working with limited direction
Nice to have:
Experience with distributed databases or SQL engine on Hadoop - (Hive, SparkSQL, Impala, etc)
Experience with Spark/Hadoop ecosystem
Experience in query performance optimization involving large datasets
Experience in Java, Scala or similar language* Data Visualization/Reporting - e.g. Tableau, Excel PivotTables
OLAP design and implementation and knowledge of MDX
NoSQL databases (HBase, MongoDB, CouchDB, Cassandra, etc)
Experience extending Hive (user-defined functions)
Experience in data governance (access, retention etc)
Experience with data workflow management tools (Oozie, Airflow etc)
Experience with streaming data pipelines (Kafka, Spark, Kinesis etc)
AgilOne, the Customer Data Platform provides enterprise consumer marketers the power to integrate customer data across digital, physical, and mobile channels, deliver customer analytics with predictive insights and 360-degree profiles, and engage customers at every touch point in order to maximize lifetime value. Currently, the AgilOne solution supports more than 150 brands worldwide.

We leverage the latest technologies in big data, machine learning and data quality management to deliver an enterprise-grade, scalable and high performance tool for customers such as Tumi, Lululemon, Lilly Pulitzer and David’s Tea. AgilOne is funded by the best in the Valley - Sequoia Capital, Tenaya, and Mayfield.

This position does not fit your skillset or your career plans?! Please check all our internship offers :Data Scientist InternSolutions Consultant InternJava Engineer Intern","Sunnyvale, CA",Data Engineer Intern,False
217,"The Blavity Family is seeking a top performing Data Engineer to join our organization!
The ideal person has worked with a wide variety of languages, such as Python and SQL, a variety of raw data formats. In this role you will work in a highly collaborative environment to help provide data-driven insights that enable better decision-making.
What You’ll Do at Blavity...
Build real-time data capture and transformation functionality across all products
Work with other engineers to enhance data models and improve data query efficiency
Create complex data queries to facilitate ad hoc and exploratory analytics
Act as in-house data expert and make recommendations regarding standards quality and timeliness
What You Have...
3-5 years experience working with NoSQL databases like MongoDB
3-5 years experience working with relational databases like MySQL or Postgres
A solid understanding of the CAP theorem and how distributed systems work
2-3 years experience using tools for ad hoc data manipulation like R
Experience working with both large and small data sets in the Cloud
Degree in Computer Science or a related field or a minimum of 3 year’s working as a Data Engineer
Curiosity and the ability to stay organized and driven to better analyze data and identify deliverables
Eagerness to tackle new problems in a data driven environment
Thrives in a fast-paced startup environment
More About Us…
We are a venture-funded technology and new media company. We have assembled an amazing team of passionate, high-energy & focused team-players who cultivate our community and advance our strategic direction. Blavity is changing the landscape of media representation by building the world’s largest digital community for Millennials of Color. Blavity was founded in 2014 around a simple idea: Enable Millennials of Color to tell their own stories.
Today, we average 3 Million monthly unique visitors, generate 30-50 Million monthly social impressions, and distribute our message across all major social media platforms. We are based in Downtown LA with team members working across the world. Blavity Inc. includes the following brands: Blavity.com, 21ninety.com, ShadowandAct.com, TravelNiolr.com, and AfroTech.com.
What We Have to Offer...
Cool office environment and culture
Medical, Dental, Vision plans
401K Matching Program
Health Savings Accounts
Discretionary Vacation Policy
Gym Membership","Atlanta, GA 30305 (Buckhead area)",Data Engineer,False
218,"Transforming the future of healthcare isn’t something we take lightly. It takes teams of the best and the brightest, working together to make an impact.

As one of the largest healthcare technology companies in the U.S., we are a catalyst to accelerate the journey toward improved lives and healthier communities.

Here at Change Healthcare, we’re using our influence to drive positive changes across the industry, and we want motivated and passionate people like you to help us continue to bring new and innovative ideas to life.


If you’re ready to embrace your passion and do what you love with a company that’s committed to supporting your future, then you belong at Change Healthcare.

Pursue purpose. Champion innovation. Earn trust. Be agile. Include all.

Empower Your Future. Make a Difference.
The new Data Science group was formed to dramatically increase leverage of Emdeon’s data assets to create material new revenue opportunities, both within specific Business areas and also across multiple lines of business. With data and transactions from more than 1,200 payers and 340,000 providers, Emdeon is uniquely positioned to impact US Healthcare.


Do you like working with data? Do you want to use data to influence product decisions for products being used by over a third of US healthcare? If yes, we want to talk to you. Our data engineering team works very closely with Product Managers, Data Scientists and Architects to figure out ways to acquire and maintain data to support new and existing data products. In this role, you will see a direct link between your work and company growth. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems in healthcare at a scale that few companies can match.


This role has the opportunity to be based in either San Mateo, CA or Nashville, TN.



Responsibilities

? Interface with data scientists, engineers and product managers to understand data needs

? Build data expertise and own data quality for allocated areas of ownership

? Design, build and launch new data models

? Design, build and launch new data extraction, transformation and loading processes

? Building key data sets to empower operational and exploratory analysis

? Define and manage SLA for all data sets in allocated areas of ownership

? Work with data infrastructure to triage infra issues and drive to resolution

Join our team today where we are creating a better coordinated, increasingly collaborative, and more efficient healthcare system!","Emeryville, CA",DATA ENGINEER,False
219,"If you're someone who is passionate about data, seeks out complex data problems, and wants to help build a new and exciting platform, you should consider joining our team as a Data Engineer. IEX is building a big data platform to tackle specialized data problems with high-performance cloud technology to help achieve our mission of creating fairer financial markets. We're currently looking for a strong Data Engineer who can develop creative and scalable solutions to marshal unstructured data. This person will have the opportunity to play a key role on a brand new venture and work directly with the traders at the world's largest hedge funds and asset managers.

If you're up for working in a fast-paced environment and with a small and highly-collaborative FinTech team changing Wall Street for the better – join us!

About you:

Self-starter with the capability of executing on projects independently
Strong analytical and problem-solving skills
Collaborative team player
Strong communication skills to distill clear ideas to traders and non-technical audiences

What you'll do:

Work with complex financial data sets
Design flexible applications to normalize unstructured data
Validate data accuracy and map content to a proprietary database
Work closely with our tech and analytics teams to solve new data problems
Conduct self-directed research to enhance product capabilities and generate new content and ideas for traders

Your background:

Programming and database experience
Demonstrated ability to efficiently manipulate large data sets
Experience as a data engineer
Experience normalizing complex data sets at scale
Nice to haves
PhD in Computer Science, Engineering, or Mathematics
Python, numpy, and pandas programming experience
Understanding of financial products and equities

Here at IEX, we are dedicated to an inclusive workplace and culture. We are an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, alienage or national origin, ancestry, citizenship status, age, disability or handicap, sex, marital status, veteran status, sexual orientation, genetic information or any other characteristic protected by applicable federal, state or local laws. This policy not only complies with all applicable laws and protects workers' rights but is vital to IEX's overall mission and values.","New York, NY",Data Engineer,False
220,"About Copper:
Somewhere along the way, CRM got really hard to use. We’re changing that. Copper was built with three basic principles in mind: keep it simple, show what matters, and make it actionable.

Copper is the No.1 crm for G Suite that’s recommended by Google. It works instantly through a seamless integration with G Suite, has a beautiful user experience, and is designed to help teams and businesses build long-lasting relationships. Copper services more than 12,000 paid businesses in more than 110 countries.

What you'll do:
A newly formed Data Analytics team is looking for a Data Engineer - Visualization to bolster its ranks. As one of the co-founding member of this team, you will work closely with GTM, Finance, Product, and Engineering teams to build Copper’s’ next generation analytics stack.


Participate in development initiatives to Design, develop and implement visualization to turn event analytics, raw application data, and business systems into key business insights
Create, translate, develop, evolve and optimize data models
Translate business requirements into technical requirements and design solutions to adhere to standards and build visualization to drive core business intelligence
Participate in data model reviews with other team members and data validation
Monitor daily data loads and take corrective action as needed
Write complex and efficient queries to transform raw data sources into easily accessible models for supporting data visualization
Be the SQL expert - advise and train analysts and engineers inefficient schema and query design
Collaborate with PMs, data analysts, and architects to develop technical design specifications

What you'll have:

3 - 5 years of technical experience in the areas of data visualization
3+ experience creating analytics solutions that improve business performance
Experience with analytical tools for supporting data analysis, reporting, and visualization (i.e. Looker, Domo, GoodData, QlikView, Tableau, R, etc). Certification preferred
Experienced with standard and MPP databases (Postgres, Redshift, BigQuery, Snowflake)
Familiarity with AWS and GCP technologies
Familiarity with UNIX/Linux and scripted using either shell, Perl, Python, etc.
Exceptional communication (both written and verbal), interpersonal skills and experience in presenting to business and technical teams including executive management
Experience authoring technical documents and working with teams to drive reference implementation to production scale

Brownie Points:

You have experience at a high growth company, SaaS preferred
Adhere to standards and build ETLs/ELTs to drive core business intelligence
Experience inventing data solutions, not just implementing packaged software or maintaining mature systems.

Why Copper:

Great team: Founded by successful veterans of Yahoo, Zynga, and eBay
Huge market: Disrupting a massive, growing $35+ billion market for CRMs
Funding: Raised $53M for our Series C from top-tier investors like Norwest Venture Partners
Our CRM has been awarded: G2Crowd #1 in Customer Satisfaction Summer Rankings, Google Best New Tech Partner of the Year
Impact: A fun, transparent, and exciting start-up culture that empowers its people to make a huge impact.
Goodies: Awesome benefits, convenient SOMA location, beautiful office, catered breakfast/lunch/dinner, team outings, and more!

","San Francisco, CA",Visualization Data Engineer,False
221,"Job Description
Interested in using Terabytes of Data and Machine Learning?

At Amazon Advertising, we are developing state-of-the-art large-scale machine learning systems on terabytes of data. In a system where multiple machine learning components interact on high-speed, real-time data, the challenge lies in understanding model behavior to diagnose performance issues. We are looking for a talented Data Engineer who is passionate about building metrics and pipelines and has the growth goal of getting deep in quantitative algorithm analysis. In this role, you will work with scientists, engineers, and product managers on high impact initiatives in Amazon’s Display Advertising.
Our team, Measurement and Data Science, develops machine learning algorithms and high performance, petabyte-scale distributed systems. Our innovative engineers and machine learning scientists sit at the intersection of two vast data sources: e-commerce and advertising. Our systems process billions of ad impressions daily from across the internet to power all of Amazon’s advertising reporting, as well as algorithms for audience targeting, real time ad ranking and bidding, and automated campaign optimization.

Major responsibilities

It’s Day One in Amazon Ads and you will play a leading role to build out our scientific infrastructure and create our research environment.
You will be the primary owner of performance dashboards across multiple machine learning systems.

You will dive deep into complex data issues in production and simulation.

You will build tools for research, experimentation, and measurement
You will leverage petabyte scale data in strategic analysis for new monetization strategies, products and business directions.
Together, we will change the face of advertising and retail, by allowing customers to discover and research products online and allow companies to understand and interact with their customers. We'll be the dream team.

Basic Qualifications
MS with 2+ years of industry experience or Bachelors with 5+ years of experience in Quantitative field (CS, ML, Mathematics, Statistics, Physics)
3+ years of experience with data querying languages (e.g. SQL), scripting languages (e.g. Python), or statistical/mathematical software (e.g. R, SAS, Matlab)
Experience in creating data driven visualizations to describe an end-to-end system
Preferred Qualifications
Experience in processing, filtering, and presenting large quantities (Millions to Billions of rows) of data
Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment
Experience working with advertising, retail, or e-commerce data
2+ years of experience with statistical analysis techniques including linear regression/least squares, experimental design, hypothesis design, confidence intervals, t-test etc.
Amazon is an Equal Opportunity-Affirmative Action Employer - Minority/Female/Disability/Veteran/Gender Identity/Sexual Orientation

#madsjob","Seattle, WA",Data Engineer - Amazon Advertising,False
222,"Data Engineer

Who We Are

Murmuration is an organization dedicated to sustainable policy change in the U.S., and we believe that the path to real change in America’s education system is through political activism and participation. Together with our partners, we develop shared infrastructure and coordinated support for organizations on the ground to massively improve outcomes for all children. We do this by working directly with local schools and organizations to amplify the voices of parents and families as key stakeholders in setting education policy.

As part of our work, Murmuration has developed a technology product called “m{insights” that seeks to aggregate data from a combination of sources including member data from partner organizations, publicly available data, consumer data, voter file data, and more. The compiled data are then used to build a variety of analytic models and tools that enable Murmuration’s partner organizations to activate, expand, and mobilize their supporter bases effectively for sustained political change.

About the Position

We are looking for an innovative Data Engineer to maintain and architect the data systems and pipelines to support our efforts. The Data Engineer would work with our Senior Data Engineer and use a variety of leading database technologies (AWS Redshift, MongoDB) and tools (AWS EC2, AWS S3, Python) to process and store our existing data. The role calls for expertise in managing AWS resources and maintaining and expanding our Python-based data ingestion pipelines. There is also the opportunity to architect new data stores for our ever-growing data needs, so a creative, problem solving mindset is highly desirable.
The Data Team is a highly collaborative, friendly, and hard-working 6 person group and we are looking for team members who embody those values. The Data Engineer will report to the Director of Data.

The Data Engineer will:

Help maintain and enhance our robust data warehouse that houses data from partners, vendors and other data sources.
Manage and improve the data ingestion pipelines that service our data warehouse, which are currently built in Python.
Deploy and use AWS resources like Redshift and RDS clusters and EC2 instances to support our work, and manage the security around those resources through AWS Security Groups and VPCs
Collaborate with our Data Science team to help build out our data analytics automated pipeline in Python

Make recommendations and provide strategic support regarding ways to make data and database operations more efficient and effective across Murmuration

Candidate Profile

Murmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates.
The ideal candidate will have:
MSc or higher in Computer Science or equivalent degree OR 3+ years experience in a Data
Engineering role
>1 year experience managing large AWS database resources, either RDS, Redshift or DynamoDB,
and the setup of VPCs and Security Groups to manage access to these resources
Excellent SQL skills, with experience in building and interpreting complex queries
Excellent Python programming skills, with a track record of well-designed and maintainable code.
Experience in database design and structure, with an emphasis on scalability
A strong desire to develop new and innovative ways to improve our data storage and processing.

Location

This position will be based in New York, NY, and may require some travel.

Compensation

The Data Engineer position is a full-time, salaried position with a comprehensive benefits package. Compensation for this position is commensurate with experience.

An Equal-Opportunity Employer with a Commitment to Diversity

Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","New York, NY 10005 (Financial District area)",Data Engineer,False
223,"Meet CarGurus—the #1 visited online car shopping website in the US. At CarGurus, we're building the world's most trusted and transparent automotive marketplace where it's easy to find great deals from top-rated dealers.

Founded in 2006 by Langley Steinert (co-founder of TripAdvisor), CarGurus is a technology company with a passion for data and its power to simplify every aspect of the car shopping experience. Using proprietary technology, search algorithms and innovative data analytics, we provide unbiased validation on pricing, dealer reputation and vehicle history.

We're looking for a thoughtful, technical and deeply collaborative Data Science Engineer to work with our growing Analytics Engineering team! This position will provide foundational framework for the Data Science team. This includes building the infrastructure to allow the team to ""publish"" models into production, developing Python libraries to facilitate feature generation, and providing the data science team with product-based datasets to train their models.

What You'll Do:

Become a trusted advisor to the Data Science and Analytics teams.
Turn complex Data Science algorithms and predictions into reliable production-grade systems.
Create a platform to deploy said models at scale, so that we can make the Data Science team as productive as possible.
Build Python libraries to make it easy for Data Science to do feature engineering on our raw data.
Participate in creating the team's roadmap, providing feedback on priority and business value.
Maintain and tune our data warehouse to support requirements from stakeholders and dependent systems.
Become an expert in our products and data models, so that you can write performant SQL queries to enable iterative analysis.

Who You Are:

3+ years experience as a software engineer, data engineer, or related field.
Comfortable with using test infrastructure to validate code.
Team player who thrives in a collaborative environment.
Expert in SQL, with ability to optimize database and query performance.
Familiarity with OLAP databases such as Redshift, Snowflake, Vertica, or others.
A natural detective, with a keen interest in solving business problems with data driven methods.
Deeply focused on delivering value to stakeholders, with a data-as-a-product mindset.
Passionate about creating production grade systems and data quality, supporting what you build.
Experience in building statistical models a plus, but not required.

CarGurus Culture:
At the core of our company culture is a spirit of innovation, curiosity and collaboration. True to our start-up roots, we're nimble, flexible and hardworking. We have a great respect for testing and learning and a healthy aversion to scheduling meetings to discuss meetings. Lunch is catered daily. Gym membership is free. Foosball and ping pong are played often. Now a publicly-traded company, we're as committed as ever to cultivating the culture that got us here.

In addition to the US, CarGurus operates sites in Canada, the UK and Germany with other markets on the horizon. Our offices are located in Cambridge, MA, Detroit, MI and Dublin, Ireland. If you'd like to learn more, please visit our careers page ( https://careers.cargurus.com/ ).","Cambridge, MA 02138 (West Cambridge area)",Data Science Engineer,False
224,"We are looking for an outstanding engineer to join our investment team to build out our big data analytics pipeline.

If selected, the candidate will have significant responsibility in extracting signals from structured and unstructured data, and in transforming them into tradeable insights.

Required:
BS, MS, or PhD in Computer Science, Statistics, or related discipline

Proficiency in Python and analytic packages such as Numpy, Pandas, Scikit-Learn

Proficiency in SQL

Preferred:
Experience in:

processing big data with Python using Spark, Dask, Blaze
OLAP with AWS Redshift, Google BigQuery
ETL orchestration with Apache Airflow, AWS Glue, or Google Dataflow

","New York, NY",Quantitative Data Engineer,False
225,"If you have what it takes to become part of the Vistra Energy family and would like to start a promising career with a global leader, take a look at the exciting employment opportunities that are currently available and apply online.


Job Description
Overview:
The data engineer will provide IT support and development for data services at Vistra, and will ensure the timeliness, quality and accuracy of the data and platforms that support the data management and ETL functions.


Responsibilities:
Implement and maintain IT data and ETL platforms; understand and deliver on the functional support and data needs of data management within Vistra .
Follow data policies, standards, guidelines and procedures in order to ensure data to support reporting and analysis is available, responsive, and achieving business outcomes and objectives
Implement data and ETL solutions or enhancements to improve overall Vistra data architecture; conduct research and make recommendations on new data management processes and innovations.
Work closely with vendors, service providers, the business and internal team to define and understand analytics needs in order to achieve key performance indicators and Service Level Agreements for the benefit of Vistra and its business objectives.
Translate business needs into functional requirements, update/create documentation (Business Process Designs, Functional Designs, Data Architecture)
Participate in projects and Agile teams; make recommendations and implement changes to mitigate risks and optimize data platform performance

Requirements:
Bachelor's degree in MIS, Computer Science, or a related field from an accredited college/university (or equivalent)
0-3 years of experience in data management or related practice such as data science.
Proven ability to collaborate on cross-functional teams
• Experience with open source (Hadoop) and cloud highly desirable","Irving, TX",Data Engineer,False
226,"ContractPosition: Data Engineer (3 positions)

Location: San Jose, CA

Length of Assignment: 1 year contract

Description:

Excellent technical expertise in PL/SQL, Hive QL and Python
Experience with Hive, Presto, Druid, YAML configs, Apache Airflow
Experience in documenting data definitions (metrics and dimensions) in a business-friendly language and also provide logical data models
Good at conducting testing (Unit and UAT) as we change data pipelines
Should be able to build new data pipeline (ETL) with Apache Pipeline
CANDIDATES MUST HAVE A LINKED-IN PROFILE

WITHOUT THIS INFORMATION YOU WILL NOT BE CONSIDERED

DynPro’s mission is to provide customers comprehensive solutions and services that will support and drive their business to success. DynPro is committed to materializing ideas, maximizing profits, and optimizing operational efficiency for its clients. DynPro is also dedicated to providing its consultants and employees with opportunities of growth and an overall enriching work experience.","San Jose, CA",Data Engineer,False
227,"About Us

Candid™ is helping people get the smiles they always wanted at prices they can afford. By simplifying the process (direct-to-consumer) and reducing costs (by up to 65% less), we can help more people feel confident and healthy. We believe that high quality dental care should be affordable and accessible, and we're using teledentistry to facilitate the diagnosis and treatment of orthodontic patients. Our team includes startup veterans with experience across healthcare, hospitality, tech, and finance at companies such as Lyft, Squarespace, WeWork, Blue Apron, and Clover Health.

Role

We're building a best-in-class consumer / healthcare brand and are looking for a data-driven developer / scientist who has an interest in utilizing data to make an impact on our customer experience and growth in addition to the ability to execute on those ideas. We are looking for a Data Engineer to join the Candid™ ( http://candidco.com ) team to help build out our existing data pipelines, as well as interpret the analytics, and build reporting tools used by the whole business. You will be responsible for owning much of the development of our pipelines, data analysis, and integration of third-party tools for our marketing, operations, and product teams. You will partner with various teams across the business to help extract meaningful analysis from the data we have collected. You will report directly to our VP of Engineering.

What You'll Do

Write reports in SQL
Design and build ETL pipelines
Develop and maintain data products and infrastructure
Compile data to show progress and guide high-level decision making
Help to design data-driven experiments across the business
Think outside the box and maintain a flexible approach to problem solving

What You'll Need

3+ years minimum experience as a data engineer or data scientist
Fluency in two or more of: Python, SQL, R, Go, Ruby, Scala
Experience running data projects from start to finish
Cross-functional experience working with all teams throughout an organization
Proven analytical skills
Strong business instincts and fundamentals
Experience in Machine Learning algorithms and processes a plus
Experience with Redshift, Redash, Looker a plus
Experience in AWS or other cloud providers a plus

Pay & Perks

Equity in the business
Medical and dental insurance
Commuter benefits
Easy access to work, across the street from 4, 5, 6, L, N, Q, R, W trains
A collaborative, high energy work environment
You will grow a lot here. You'll be surrounded by employees with deep experience in their field, who have a strong passion for doing great work and constantly learning

","New York, NY",Software Engineer - Data,False
228,"Position Overview:
The Climate Corporation’s mission is to help the world’s farmers sustainably increase their productivity with digital tools. The Data and Analytics team is focused on creating competitive advantage for Climate and our customers through novel data infrastructure, metrics, insights and data services. We are a small but rapidly growing analysis and engineering team that builds and leverages state-of-the-art analytics systems. Our work informs decisions and direction for our business, while also impacting our products. We are looking for a Data Engineer to not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's products, data consumers and analysts. We are looking for a candidate with knowledge of data warehousing and experience with ETL tools.

What You Will Do: Help design and build a Multidimensional Data Warehouse.
 Build and maintain the core data model, data pipelines, core data metrics and data quality. Work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product) to define nees/requirements. Champion data warehousing best practices Develop and build infrastructure in an AWS cloud environment Build data expertise and own data quality for the data pipelines Design and develop new systems and tools to enable folks to consume and understand data faster Provide expert advice and education in the usage and interpretation of data systems to end consumers of the data
Basic Qualifications: B.S. or B.A. in computer science, math, economics, engineering or other technical field 2+ years of SQL experience as applied to ETL tools (Informatics, Kettle, Talend, etc.) Experience with relational databases or NoSql infrastructure
Preferred Qualifications: 2+ years of experience with dimensional data modeling & schema design in Data Warehouses. 2+ years of scripting experience (shell or python) Experience with massive scale relational databases (MPP) is a plus (Vertica/Redshift/Teradata). Experience working in a cloud deployment such as AWS is a plus Excellent communication skills including the ability to identify and communicate data driven insights
What We Offer:
Our teams are composed of industry experts, top scientists, and talented engineers. The environment is extremely engaging and fast-paced, with dozens of specialties coming together to provide the best possible products and experiences for our customers.
We provide competitive salaries and some of the best perks in the industry, including: Superb medical, dental, vision, life, disability benefits, and a 401k matching program A stocked kitchen with a large assortment of snacks & drinks to get you through the day Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hackathons to encourage participation and growth in both community involvement and career development
We also hinge our cultural DNA on these five values: Inspire one another Innovate in all we do Leave a mark on the world Find the possible in the impossible Be direct and transparent","San Francisco, CA",Data Engineer,False
229,"In order to be considered for this role, you MUST apply at the following link: https://technologyadvice.applytojob.com/apply/NFU93RFkhD/Data-Engineer?source=INDEFThis isn't your typical data engineering role. Data here isn't just reporting; it's the core of our business. Leveraging both internal and external data sources, we form the bedrock that powers our industry-leading lead generation capabilities. As TechnologyAdvice's data engineer on the Business Intelligence team, the right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.The Core Responsibilities of This Role Include: Prototyping internal data utilities to enhance and support various business units. As the Subject Matter Expert on all of the data resources we have available, your unique perspective and capabilities will be invaluable for leveraging our data resources in new and innovative ways to drive value for the businessDevelop guidelines, standards, and processes to ensure the highest data quality and integrity in the data stores residing on the data repositoryDesigning, building, and maintaining data models used by our BI platform (SeekWell and Tableau)Building and maintaining data integrations with third-party systemsIdentifying and executing on opportunities for automating relevant data processesBuilding and maintaining ETL processes and data dictionaries between our production systems and AWS RedshiftCollaborate with our software engineering team to ensure proper change management for ETL processes and data modelsMust-haves For This RoleProven, professional experience with:Managing data with AWS Redshift (that's the main playground for the data team)Designing, building, and maintaining both new and existing data systems and solutionsUnderstanding of ETL processes and how they functionNice-to-haves For This RoleAbility to learn and work with Python as needed. While we're not picky about your choice of language for any particular task (we have production code running in Python, PHP, Bash, AWK, and NodeJS), Python is the way of the data world and experience with it would be beneficialThe Perfect Personality for This Role: Be inquisitive and innovative: the desire to say ""let's do this!"" and the ability to do what's needed to follow throughA sixth sense for detecting data defects, anomalies, and risksAbility to adapt and learn quickly is a fundamental necessityEnjoy working in fast-paced, collaborative, Agile environmentsPlans and executes activities with generally-defined goals and minimal supervision. We trust you to know your stuff and don't micromanage youMinimal supervision doesn't mean minimal support - be comfortable and proactive asking questions and seeking additional help if needed to accomplish goalsWants to understand the data, not just the pipelines pushing it around. Can definitely perform some analytics and build some dashboards (because you like to, not just when you have to)Ability to work independently. While you'll collaborate with and have the full support of our business intelligence and software engineering teams, most projects will be worked on independently.Perks and Benefits: Compensation is a combination of base salary plus quarterly profit sharing potential. In addition to the opportunity to grow personally and professionally alongside a driven team, we also offer:Comprehensive health insurance (medical, dental, vision, life and disability)401k retirement plan with company matchUnlimited Paid Time OffWeekly career development meetings for your first 60 daysBucket list benefit on each work anniversaryGym Membership reimbursementMonthly team outings and volunteer opportunitiesProfessional development opportunities and incentives (book clubs, monthly learning series, Leadership Academy and more!)Catered lunches 4 days a weekCoffee, soda, snacks, ping pong, and beer on FridaysWho We Are: TechnologyAdvice is dedicated to educating, advising, and connecting buyers and sellers of business technology. As a trusted resource in a variety of technology verticals, the company helps buyers improve their businesses and vendors find their customers. Through unbiased research and crowd-sourced product reviews, TechnologyAdvice provides the insight that buyers need to find the right technology.Additionally, the company’s unique demand generation programs help vendors improve product awareness by placing matched solutions in front of qualified technology buyers. TechnologyAdvice is based in Nashville, Tenn., and was named to the Inc. 5000 list of America’s Fastest-Growing Private Companies in 2014, 2015, 2016, & 2017.Pre-employment screening required.Job Type: Full-time","Nashville, TN 37210 (Woodbine area)",Data Engineer,False
230,"Description
Team Daugherty is hiring a Data Engineer to join us in St. Louis. The ideal candidate for this position is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.

As a Data Engineer you will have the opportunity to:

Contribute to the creation and maintenance of optimal data pipeline architectures.
Collaborate and work closely with team to build data platforms.
Maintain and manage Hadoop clusters in development and production environments.
Assemble large, complex data sets that meet functional/non-functional business requirements.
Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.
Create custom software components (e.g. specialized UDFs) and analytics applications.
Employ a variety of languages and tools to marry systems together.
Recommend ways to improve data reliability, efficiency and quality.
Implement & automate high-performance algorithms, prototypes and predictive models.
We are looking for someone with:

1+ years of experience in a similar role.
Proven experience working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.
Familiarity with Python, R, sh/bash and JVM-based languages including Scala and Java.
Familiarity with Hadoop family languages including Pig and Hive.
Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.
Proven ability to pick up new languages and technologies quickly.
Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.
Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.
Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.
Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.
Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.
Some experience with programming Languages, such as Scala, Java, R and Python.
We offer members of Team Daugherty:

Excellent health, dental and vision insurance.
Revenue sharing and a 401(k) retirement savings plan.
Life, disability and long-term care insurance.
Little to no travel.
Robust career development and training.
Do you think you’re a good fit for Team Daugherty? Apply now and find out why working here satisfies the smart, the talented and the curious!","St. Louis, MO",Data Engineer - STL,False
231,"Who is Credible?

We believe life's changes create financial needs for people and that the traditional financial system often puts up unnecessary obstacles. People celebrate major milestones like going to college, getting married, and buying a home. And most of the time, these milestones come with financial implications.

At Credible, we have built a company with the mission of bringing transparency, choice, simple processes and savings to accessing credit for life's important moments. What you see is what you get. We are committed to being upfront, honest, and clear about your options. There are no mysteries, no hidden fees, and no secret clauses.

Credible is a fast-growing Australian Securities Exchange (ASX) listed Fintech company that has world class management, has raised multiple rounds of funding, is generating significant revenue and is disrupting the lending market and helping people save money and get out of debt faster.

About the role

Our Business Intelligence team is looking for a Business Intelligence Data Engineer who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You'll be an integral part of providing data-driven insights that inform significant company decisions.

You Will:

Build data pipelines and python-based ETL tools for getting, processing, and delivering data
Partner with teams across the organization to understand their analytics needs and create dashboards and reporting that allow them to execute more effectively
Work with business leaders to define key metrics and build reporting to monitor and understand performance along those metrics
Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders
Develop data models in our data warehouse that enable performant, intuitive analysis
Become an expert on all aspects of Credible's data and analytics infrastructure
Be the driving force behind the adoption and effective use of our BI tool within every team at Credible

Education and Experience:

BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Experience writing SQL queries and using a BI tool
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus

Personality and Values:

The capacity to juggle multiple priorities effectively within a fast-paced environment is critical
You're a highly motivated self-starter with the ability to work efficiently with minimal supervision.
Anticipate business needs and think with a business owner mindset – think critically about analyses, don't just complete them
Passion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise

Why you want to work at Credible

We are a fast moving, fun-loving, seriously smart group of people who really care about impacting the lives of our customers. We empower our employees to make decisions, take risks, drive our business and make changes when we don't get it right. These are our values:


Exceed Customer Expectations: We provide an exceptional experience to each and every customer that compels them to share it with others.
Take Ownership: We are trusted to make decisions that are in the best interests of our customers and our business. We think and act like owners. We care – and that makes all the difference.
Be Curious: We are curious, ask questions, seek to understand and try new things.
Do the Right Thing: We earn trust by being transparent, respectful and honest with each person with whom we interact.
Get Results: Results fuel our excitement and we know how our personal accomplishments tie to the success of the company
Be Bold: We are courageous and take risks that scare us. Our enthusiasm for experimenting is how we will find the next breakthrough.

Our benefits: We offer competitive compensation, generous benefits, free food and a flexible vacation policy.

But mainly, you want to work at Credible because you believe in our mission and want to have a major role in delivering on it! We look forward to getting to know you.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation,age,marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.","San Francisco, CA",Business Intelligence Data Engineer,False
232,"The Department of Biomedical Informatics (DBMI) at Columbia University is revolutionizing the clinical research enterprise with the help of information technology. Within the area of Precision Medicine, we are building systems that implement harnessing genetic sequences to detect health conditions and save lives. At DBMI, we are building the infrastructure of the future to support and enable better research and dissemination. We have an immediate opening for a talented and self-motivated data engineer developer who can succeed in a collaborative work environment. The ideal candidate will have experience with data pipelines and cloud environments. The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture. The ideal candidate will have professional experience in a number of programming languages, databases, and development environments. The candidate should be able to contribute in improving reliability and quality of data. Experience in clinical medicine, clinical vocabulary, and cloud development are not required, but preferred. The successful candidate will contribute to the development of open source solutions together with a community of international researchers.

Current available position is grant-funded.

Columbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact: we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population. Employees of the department are passionate, friendly and resourceful.

Minimum Qualifications for Grade
Applicant MUST meet these minimum qualifications to be considered an applicant

Bachelor's degree in computer science, biomedical informatics, information science, plus four years of related experience.

Additional Position-Specific Minimum Qualifications
Applicant MUST meet these minimum qualifications to be considered an applicant

Great communication skills; Experience with one or more compiled programming languages (e.g. Java, Scala, C#, C++, etc.) and one or more interpreted programming languages (Python, JavaScript, Perl, bash etc.). Working knowledge of SQL; Experience with big data, NoSQL databases,, and health care data a plus.

Special Instructions

Preferred Qualifications

Essential Functions

1. Software and system design, implementation, and testing (75%)
2. Application deployment and configuration (10%)
3. Communicate with technical individuals at various grant sites (10%)
4. Software requirements specification (5%)

Additional Essential Functions (Limit to 3950 characters.)

The incumbent will work under the direction of the faculty member leading the project. He/she will work closely with the project's team including the project manager, current software developers and sponsoring project personnel.

The incumbent must be organized and adhere to project deadlines.

Special Indications

This position works with:
HIPAA Compliance training required

Yes

Participation in Medical Surveillance required

No

What type of posting? Is this a waiver request?

Standard Posting

Requisition Open Date

09-10-2018

Requisition Close Date

Open Until Filled

Quick Link

jobs.columbia.edu/applicants/Central?quickFind=171852","New York, NY",Data Engineer,False
233,"ABOUT 605
At 605 we are engineers, analysts, data scientists, media experts, marketing strategists and political operatives. Our team of data scientists pioneered the field of TV data analytics. We offer unique, independent audience measurement and analytics to build better marketing and programming initiatives within the media and entertainment industries.
Data Engineers at 605 are responsible for onboarding, QA and analyses of data sets into the 605 platforms. This role will also interface with business and product owners to understand requirements and ensure that data sets are accurate, timely and available to support the business needs. Responsibilities include:
Interpret data sets and define process and procedures to ensure data is received to spec and in a timely and accurate manner
Identify and address issues with data sets
Establish clear communications both across the data provider relationships as well as within 605
Ensure appropriate procedures are in place to meet SOC 3 compliance procedures
Ensure excellent communication procedures are in place via daily/monthly/weekly reporting to advise status of data sets and that end users are informed of data status
Work with business and product owners to develop procedures, methods, code to provide efficient tools and products to meet the needs of the end users
Support all groups across 605 as needed to address questions regarding data quality, status, ETL and other platform related questions and concerns that may arise
This role will oversee junior analysts to facilitate reporting and analytics
Requirements
5-7 years in data processing and analytics industry
Strong working knowledge of SQL, Python and other database query related languages
Strong working knowledge of cable ad sales inventory, impression projections, reach and frequency analytics a strong plus
Bachelor degree or higher
Excellent written and verbal communication skills to present to end users, support teams, technical teams as well as senior management as needed
Preferred experience in application development, “big data” analytics and ability to deliver customer focused products that are simple and easy for end users to interpret
Benefits
Comprehensive health and dental insurance for employees and their families
Life insurance
401k with match, eligible for match after one year
Pre-tax flexible compensation plan for medical, transit, parking or dependent care expenses
Paid time off
Sick days—if you’re sick, you stay home
A kitchen stocked with sodas, snacks, yogurt and other goodies
A tight-knit startup community who likes to eat! We celebrate everyone’s birthdays, have frequent team lunches, and do events in and out of the office
605 is an active participant in conferences
EEO STATEMENT

At 605, we’re just as passionate about diversity as we are about pioneering the field of TV data analytics. We are committed to cultivating an environment of mutual respect and equal opportunity. All hiring and advancement decisions are made on the basis of qualification, merit, and business need.","Syosset, NY",Data Engineer,False
234,"Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices Build and maintain dimensional data warehouses in support of business intelligence tools Develop data catalogs and data validations to ensure clarity and correctness of key business metrics Drive and maintain a culture of quality, innovation and experimentation 1-3 years of experience developing in object orient Python Engineering big-data solutions using technologies like EMR, S3, Spark Loading and querying cloud-hosted databases such as Redshift and Snowflake Building data pipelines using Kafka, Spark, Flink, or Samza Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift Experience deploying data notebook and analytic environments such as Jupyter and Databricks Knowledge of the Python data ecosystem using pandas and numpy Experience building and deploying ML pipelines: training models, feature development, regression testing Experience with graph-based data workflows using Apache Airflow Bachelor’s degree in Computer Science or related field or equivalent work experience
Disney Streaming Services is a place for the creative and the bold. We’re seeking talent across disciplines to join our team. Whether New York City, San Francisco, Manchester or Amsterdam we provide opportunities to elevate your career and transform an industry. Disney Streaming Services software engineers develop premium digital media products for Major League Baseball and our partners. The products we build, such as MLB.TV, NHL.TV, HBO NOW and PlayStation Vue are paving the way for the next-generation media and sport technologies. Disney Streaming Services engineering is headquartered in the Chelsea area of New York, NY with an office in the SoMo area of San Francisco, CA and team members based around the world. If you are interested in joining Disney Streaming Services in the pursuit of not only crafting new media products but enjoying the products you build, we are interested in hearing from you. At Disney Streaming Services data is central to measuring all aspects of the business, and critical to its operations and growth. The data engineering team is responsible for collecting, analyzing and distributing data using public cloud and open source technologies and offers transparency into customer behavior and business performance. 573678","New York, NY 10036",Data Engineer,False
235,"InternshipJoin a team recognized for leadership, innovation and diversity
Your time with Honeywell could be spent in any of the following ways:
Extracting, transforming, and loading data in preparation for analysisEngineering features incorporating domain expertise and statistical aggregationsDeveloping and evaluating statistical inference models based on those features, solving a costumer’s problem.Deploying analytic solutions to production platforms, ready to be used in one of Honeywell’s products or services.
No matter which team you work on, you'll have the opportunity to participate in our intern project week - a week in which all interns come together to solve business problems. It is a fun week and you'll meet other interns who will be a great part of your network.
Our teams live by the ‘teach and learn’ mantra. We value our more seasoned data scientists because they bring additional value to our company by using their years of experience to guide the next generation. We also value you, our young talent. We'll assign you a mentor on day one, so you can take advantage of this amazing learning opportunity. We want you to finish your internship with a much better understanding of the data science field and Honeywell. We know you'll be a better data scientist by the time this internship ends!
25 Administrative support to the engineering team

25 Test data maintenance

25 Communication support

25 System support and maintenance


You must have:
Be currently pursuing a Bachelor’s or Master’s degree in Engineering, Computer Science or related discipline at an accredited college or university.Have completed at least your first year of a college or university.Be able to participate in an internship prior to completing current degree program.
We value:
Minimum 85% course average (3.7 GPA)Be familiar with one or more of the following programming languages: Python, R, MatLab, Java, or C++.Some frameworks we use are: Jupyter, RStudio, Flask, OpenCPU.Some of the tools we use include Git, Docker, Bamboo, and more.Some of the databases we use are: Hive, OpenTSDB, CosmoDB, and SQLInsatiable appetite for learning new technologies.Committed pursuit of innovative solutions.
Nonexempt How Honeywell is Connecting the World
INCLUDES
Continued Professional Development
1st Shift

ADDITIONAL INFORMATION
Job ID: HRD41023
Category: Engineering
Location: 715 Peachtree Street, N.E., Atlanta, GA 30308 USA
Honeywell is an equal opportunity employer. Qualified applicants will be considered without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.","Atlanta, GA 30308 (Old Fourth Ward area)",Data Engineer Intern,False
236,"Instagram is a global community of more than 1 billion, which means jobs here offer countless ways to make an impact in a fast growing organization. Instagram was built to connect people to the people and interests they love. Our app has played a critical part in forming meaningful communities where people can connect with each other and share what matters most to them.
Do you like working with big data? Do you want to use data to influence product decisions for products being used by hundreds of millions of people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.


This is a full time position based in our office in Menlo Park.
RESPONSIBILITIES

Inform, influence, support, and execute our product decisions and product launches

Manage data warehouse plans for a product or a group of products.

Interface with engineers, product managers and product analysts to understand data needs.

Partner with Product and Engineering teams to solve problems and identify trends and opportunities.

Build data expertise and own data quality for allocated areas of ownership.

Design, build and launch new data extraction, transformation and loading processes in production.

Support existing processes running in production.

Define and manage SLA for all data sets in allocated areas of ownership.

Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS

BS/BA in Technical Field, Computer Science or Mathematics.

4+ years experience in the data warehouse space.

4+ years experience in custom ETL design, implementation and maintenance.

4+ years experience working with either a Map Reduce or an MPP system.

4+ years experience with schema design and dimensional data modeling.

4+ years experience in writing SQL statements.

Ability to analyze data to identify deliverables, gaps and inconsistencies.

Communication skills including the ability to identify and communicate data driven insights.

Ability in managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS

4+ years experience using Python or Java","Menlo Park, CA","Data Engineer, Analytics (Instagram)",False
237,"Job Description
At Alexa Shopping, we strive to enable shopping in everyday life. We allow customers to instantly order whatever they need, by simply interacting with their Smart Devices such as Echo or Fire TV. Our Services allow you to shop, no matter where you are or what you are doing, you can go from 'I want that' to 'that's on the way' in a matter of seconds. We are seeking the industry's best to help us create new ways to interact, search and shop. You will have an impact on Amazon's new devices and the way shopping is done in the area of IoT. Join us, and you'll be taking part in changing the future of everyday life.

As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments using latest set of toolsets. We help product teams at VANS build the future of shopping by providing metrics on new features and help them perform A/B testing which will act as feedback loop for voice user experience. Our team is responsible for mission critical analytical reports and metrics that are viewed at the highest levels in the organization. We are also working on near real time analytics using Kinesis Firehose and using the latest set of tools for data visualization and investing in Big Data technologies. You should have deep expertise in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. You should be able to work with business customers in a fast paced environment understanding the business requirements and implementing reporting solutions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.
Basic Qualifications
4+ years experience with various data analysis and visualization tools

Experience in Perl, Python, or another scripting language; command line usage

Track record of diving into data to discover hidden patterns and of conducting error/deviation analysis

Experience with various machine learning techniques and key parameters that affect their performance

Strong personal interest in learning, researching, and creating new technologies with high commercial impact.

Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations

Understanding of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc.
Preferred Qualifications
A track record of communicating well with software engineers and non-technical leaders in speech technology
Background in the fields of natural language processing or computational linguistics
Excellent verbal/written communication skills, including an ability to effectively collaborate with research and technical teams, and earn the trust and influence senior stakeholders.
The motivation to achieve results in a fast-paced environment.
Experience with statistical modelling / machine learning
Strong attention to detail
Comfortable working in a fast paced, highly collaborative, dynamic work environment
Ability to think creatively and solve problems","Seattle, WA","Data Engineer, Alexa Shopping",False
238,"A Data Engineer at Omnitech would qualify for multiple of the following labels including business intelligence consultant, data warehouse consultant, data acquisition (ETL) consultant, database administrator consultant, master data management consultant, data analyst, data architect, data scientist, or many other labels currently being used. At Omnitech, we don’t believe in labels and hierarchy. We believe in helping people solve problems. We are looking for an engineering caliber person that focuses on data and has a consulting type personality.

WE LOOK FOR THE FOLLOWING:

A curious analytical mind with ability to understand business objectives, ask insightful questions, and be detailed in implementation
A track record of helping companies get value from data management and business intelligence
Diverse understanding of industry tools, software and techniques; especially Microsoft SQL Server platform and tools
A desire to mentor and be mentored
A history of working on multidisciplinary teams in a productive manner
Professional image and demeanor with the ability to present oneself in a consultative, confident, and yet humble manner

DESIRED SKILLS: (The applicable candidate must possess a number of the following.)

Strong T-SQL skills
SQL Server design and development experience
Understanding of data profiling techniques
Understanding of performance optimization, data warehousing, cube architecture
Understanding of common data extraction techniques across a diverse set of sources including structured and non-structured data
Experience with data cleansing and conforming techniques
Develop standards and best practices to ensure data standardization and consistency as required
Strong experience with dimensional modeling, star schema and Kimball Data Warehouse methodologies
Design and develop data warehousing solutions for clients across a variety of industries and business sizes utilizing Microsoft’s SQL

SERVER PLATFORM

Perform the role of subject matter expert for Microsoft Business Intelligence technologies including SQL Server and Analysis Services
Knowledge of Multidimensional Expression (MDX) and Data Analysis Expressions (DAX) languages
Familiarity with managing dimensional attribute history through Slowly Changing Dimension (SCD) concepts
Working knowledge of ETL change detection solutions such as change data capture (CDC)
Experience with Big Data technology a plus
Understanding of SQL Server FastTrack and/or Parallel Data Warehouse
Familiarity with storage technologies (e.g. SAN, NAS, etc.) a plus
Translate business requirements and technical designs into well-developed solutions that meet client business goals
Ability to explain the pros and cons of architectural decisions
Evaluate and recommend new technologies as required
Design and implement technology best practices, guidelines and repeatable processes
Provide technical assistance and cross training to other team members and clients
Participate in the business intelligence community to promote the use of the Microsoft BI platform and general data warehousing best practices
Assist in pre-sales, scoping and requirements gathering process
Ability to work closely with other project team members such as sales analysts, project managers, and software engineers

REQUIREMENTS:

3 years Data experience
A Bachelor’s Degree in engineering, computer science, physics, mathematics, or similar analytical degree
T-SQL, SQL Server, SSIS, SSRS, SSAS
Desire for continuous learning and to pursue professional certifications
Proven ability to consult and mentor others

PERSONAL CHARACTERISTICS:

Excellent communication, presentation, and interpersonal skills, confident with customers
Detail oriented, well-organized and excellent ability to multi-task
Energetic, comfortable working in a fast-paced environment
Hard-working and motivated, able to take initiative and meet deadlines
Comfortable working in a team-based environment
A hands-on attitude in a friendly work environment

CONTEXTUAL BENEFITS:

MSDN Premium and Azure licensing as Microsoft Gold Partner
Opportunities for formal and informal training on new technologies
Support for career and life goals and development
Free soda, coffee and other ingestible fuel to keep you going
On-site exercise room with equipment
Company matching on the 401k plan
Reimbursement for tuition and certification costs

Qualified candidates must be legally authorized to be employed in the United States on a full-time basis for any position. Omnitech will not sponsorship for employment visa status (e.g., H-1B or TN status) for this position.

If you are interested in being part of an exciting and growing company, please apply. If you have what it takes to support our organization, then send your resume and salary requirements via e-mail to Careers@Omnitech-inc.com.","Sioux Falls, SD",Data Engineer,False
239,"Availity delivers revenue cycle and related business solutions for health care professionals who want to build healthy, thriving organizations. Availity has the powerful tools, actionable insights and expansive network reach that medical businesses need to get an edge in an industry constantly redefined by change.


The Data Engineer aims to provide data in a ready-to-use form to enable a diverse set of business processes. Responsible for design, build, maintenance, and production support of data processing pipelines and systems. The Data Engineer will work closely with Data Analysts, Data Scientists, business and technical teams to deliver outcomes that are secure, reliable, fault-tolerant, scalable, quality and efficient. Applying analytics (e.g., machine learning) and statistical models, as needed. The Data Engineer can also provide data analysis and extracts on large and complex data sets.


KEY RESPONSIBILITIES
Design, build, quality, maintenance, and production support of data processing pipelines and systems (Real-time and batch)
Design, build, quality, maintenance, and production support of RESTful APIs
Responsible for ensuring data processing pipelines and systems are: secure, reliable, fault-tolerant, scalable, accurate and efficient
Perform data analysis and provide extracts on large and complex data sets
Data Wrangling
Deliver automated functional tests on data processing pipelines and systems
Full participation in the assigned Agile team (e.g., standups, planning, peer reviews, etc.)
Provide on-call support (24x7); may be rotational
Collaborate within and outside assigned Agile team
Continuously seek opportunities to improve skillsets
Proactively identify and communicate roadblocks. Evaluate and suggest alternatives
Support multiple projects and accommodate frequent interruptions and changing priorities
The above cited duties and responsibilities describe the general nature and level of work performed by people assigned to the job. They are not intended to be an exhaustive list of all the duties and responsibilities that an incumbent may be expected or asked to perform.
EDUCATION AND EXPERIENCE
Bachelors degree in Information Systems or Computer science (e.g. specialization: Machine learning/Artificial Intelligence /Visualization, databases, and Big Data)

SKILLS AND KNOWLEDGE
Must have experience with at least one compiled language (Java)
Must have experience building backend RESTful web services
Must have experience with SQL and relational database systems (e.g., Oracle, SQL Server)
Must have experience with Linux
Familiarity with DevOps and Microservices architecture concepts and tools
Familiarity with common data science toolkits, such as R and Python
Strong problem-solving capabilities and exhibits strong Computer Science Fundamentals
Experience working with Agile and Lean Practices
Excellent communication skills","Jacksonville, FL 32256",Data Engineer I,False
240,"With our portfolio of global Power Brands such as Oreo and belVita biscuits, Cadbury Dairy Milk and Milka chocolate and Trident gum, we're the world's #1 in biscuits and candy, and #2 in chocolate and gum. We're Mondelēz International, a snacking powerhouse with operations in more than 80 countries, with approximately 90,000 employees globally and our brands are marketed in around 165 countries.
Our purpose and vision is to create more MOMENTS OF JOY by building the BEST SNACKING COMPANY IN THE WORLD.
Customer Service & Logistics
Customer Service & Logistics (CS&L) is where you'll integrate our end-to-end demand-driven supply chain. Working ""from farm to shelf,"" you'll connect plants with customers to deliver best-in-class service in the most efficient way. Your goal will be to have the right products, at the right time, and with the right quantity and quality on the shelf.
Purpose of Role
The Analyst & Data Engineer will improve business results by supplying databases and interfaces that lead to models and analysis to solve various supply chain challenges. The Analyst & Data Engineer will be responsible for providing seamless ""digital highway solutions"" that are automated and repeatable to Drive SC Guru models, ensuring integrity of data. The role also supports the SC Guru Modeler and Supply network design manager in running and analyzing models supporting $100MM to $9 Billion in revenue .
Main Responsibilities

Work with internal and external customers to obtain data needed for analysis and validate data for accuracy.
Design, setup and Maintain (SQL) databases supporting Supply network design studies.
Design, setup and Maintain data visualized reports & interface (Tableau / SupplyChainGuru.com/App) supporting Supply network design studies.
Setup data process and governance ensuring predictability and accuracy.
Primary point of contact with IS/IT and data owner to design, setup and maintain the digital highway interface.
Communicate with internal and external customer with automated/custom reports to understand the business model and obtain current state costs.
Coordinate with key stake holders and help in developing strategies, assumptions and supply chain analysis deliverables.
Analysis of various manufacturing and distribution network alternatives.
Analyze alternatives to determine the cash, cost and service impacts.
KPIs / Dimensions
Project & Analysis Excellence
Capability Building & Knowledge
Transfer
Targets / Dimensions
Stakeholder confidence, Timely Completion, Accurate Analysis, and Detailed Documentation
1% of Revenue
Building NA SND knowledge & knowledge transfer to the broader ISC team
Key Interfaces / Stakeholders
External: Consultants
Internal: ISC - Finance - IT/IS
Key Leadership / Functional Competencies
Leadership Competency

Intellectual horsepower
Problem solving
Learning on the fly
Problem solving
Perseverance
Functional Competency
Sequel/Alteryx
Tableau
Supply Chain Guru
SAP, BI, BO
Excel, Access
Qualifications
Career Experiences Required & Role Implications

Minimum 1 year experience modeling in SQL
SQL server environment experience
Bachelor's degree
Tableau, Alteryx, Excel, Access experience desirable
Demonstrated examples in experience of applying analytical skills and problem solving skills.
Ability to deal with amibiguity and work under pressure.
Strong collaborative skills for cross-functional and cross-country studies.
Fluent in English with strong communication skills.
Mondelēz Global LLC is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected Veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Applicants who require accommodation to participate in the job application process may contact 847-943-5460 for assistance.
Applicants must complete all required steps in the application process, including providing a Resume/CV, in order to be considered for this position.","East Hanover, NJ 07936",Data Engineer & SND Analyst,False
241,"Description:
Viral Launch is searching for a highly experienced, business minded Data Engineer. We are looking for someone who is very passionate about building high performing and scalable data infrastructure, enabling data science research and analysis, and helping data scientists to take machine learning models to production. Our Data Science team is based out of our Indianapolis HQ.

Why is this job important to Viral Launch?
Viral Launch has established themselves as a market leader in a short amount of time by providing unrivaled value for our clients by harnessing powerful market data to drive new innovative products and services within the nation’s largest e-commerce marketplace.

Critical to Viral Launch and at the heart of its operations is data. We consume, create and store lots and lots of data. This role is vital in bridging the gap between multiple sources of data and the internal consumers of data (analysts, systems, data scientists, etc). You will work closely with engineering and data scientists to tackle problems and develop innovative tools to make our customers more profitable. .

Requirements:

BSc/MSc in Computer Science or related field
4+ years of a proven track record in a data engineering or related role
Ability to take ownership of the design, implementation, and maintenance of scalable end to end data pipelines.
Ability to quickly evaluate and make trade-off decisions on adopting emerging technologies.
Experience in architecting data management solutions and deep understanding of data management discipline
Knowledge in big data engineering: Spark, Hadoop, Hive, Spark Streaming, Kafka, etc
Proven ability to build and deploy scalable real-time data pipelines on leading cloud platforms.
Experience in developing and deploying tools and supporting rapid iteration of machine learning algorithms
Experience architecting data platforms to support data science and analytics
Extensive experience collaborating with data scientists in wrangling, cleansing, extracting and transforming data
Excellent problem-solving skills
Ability to interpret and analyze data is a must. Consequently, mathematical inclination is a major plus
Proficiency in one or more of the following languages: Python, C#, Java, Scala, JavaScript
Extensive knowledge of SQL, deep understanding of database full life-cycle designing, and application development

No Visa Sponsorship is available for this position.

Why work with us?

Medical/Dental/Vision/Life Benefits
SIMPLE IRA with company match
Potential to earn equity (not guaranteed)
Flexible schedules and Unlimited PTO

Please submit your resume, portfolio, and any additional materials. If you have any relevant web links, please be sure to include them.

Ready for launch? 3…2…1…Apply!","Indianapolis, IN 46202",Data Engineer,False
242,"About Comfy

Comfy is on a mission to create amazing workplace experiences. We are a leading workplace app provider that connects people, places and systems. We started by solving the number one complaint in the office (temperature!), and continue to expand to our product suite to give employees greater control over their workplace, including room booking, lighting and feedback. We create amazing workplace experiences through a consumer-facing app for employees and solve real business problems for corporate real estate teams. Headquartered in Oakland, California, with expertise in machine learning, UX design and enterprise service, Comfy develops software solutions for everyone — from the people who operate the building to the people who fill it.

About the Role

Comfy is looking for a Data Engineer to help build our next-generation analytics products. Having successfully deployed the first generation of our app--which provided end users with an intuitive way to control the temperature within their building, learning their preferences in the process--we are now well into the next generation, which extends into a range of IT, OT, and IoT use cases (including room booking, access control, wayfinding, and visitor management). Simultaneously, we are aggressively growing our user base, with some of the largest-ever Comfy deployments currently in flight.

As the front-end app and our user base evolves, we are seeing an increasing number of opportunities to leverage data within the Comfy platform for both enhancing our existing customer-facing analytics product (Comfy Insights) as well as our internal-facing product analytics (which informs how we manage the product). In addition, we are also looking at more aggressively embedding learning across our product, so that Comfy can better anticipate our end users' needs across their entire day. Towards that end, we are looking for a rising technical star who wants to shape and contribute to the foundation and platform upon which our analytics roadmap will be built.

About You


You're willing to roll up your sleeves and do whatever it takes to succeed. You love learning new things, and no task is too boring or too challenging to undertake.
You enjoy the fast pace and chaos of a start-up environment, and aren't intimidated by the vast unknowns and lack of resources that start-ups deal with every day. (Yes, we're still a start-up culture!)
You love creative problem-solving, and are comfortable working independently on your own or with a team. You thrive on pair-programming (esp with data scientists) and prefer rapidly iterating on prototypes to get something ""quick and dirty"" into the hands of our customers.
You've worked before in software development teams, and are comfortable with agile frameworks--including sprint planning, backlog grooming, or managing a kanban board. You have experience with jira, git, confluence and all that jazz.
You are proactive about influencing future directions, and aren't shy about putting together documentation or proposals that may help evangelize your point-of-view. You realize that getting buy-in from your peers is key to longer-term success.

Your Skills and Experience


You have a bachelor's degree in Computer Science (or equivalent) and at least 2 years of work experience. Note that the primary focus of this role is around data infrastructure & engineering, not algorithms or data science; although you will have the opportunity to dabble in both.
You have deep expertise in data platforms, and are comfortable working with structured, unstructured, and streaming data. Whenever you see a new data stream, you're eager to stand up a data lake and/or a distributed systems environment, so you can dive into all sorts of interesting analysis yourself. Our current infrastructure uses Redis, Cassandra, and Postgres; you've worked with at least a subset of those before.
Along those lines, you know what it takes to stand up a Big Data environment from scratch--be it on-prem or in the cloud. Whether it's spinning up new VMs and containers or diving into deployment across multiple clusters, you're up to the task. AWS experience preferred.
SQL is second nature; if you could, you'd have a full conversation in SQL. But you also love data visualization, since a picture is worth a thousand words. Ideally you have prior experience working with d3.js or similar.
Must be well-versed in Python.
You're well-versed in evaluating the trade-offs associated with any analytics deployments--performance, accuracy, cost, usability, complexity--and have the knowledge base to reason through these trade-offs.
You have some experience with machine learning and ML platforms, and are keen to apply what you know in new environments.

Our Benefits Include


Market-leading software application centered on improving the workplace experience, Comfy : )
Diverse, quirky, passionate and supportive team of coworkers
Brand new, bright, airy office in sunny downtown Oakland close to BART
Take-what-you-need vacation policy
Competitive compensation
Full medical, dental and vision insurance
Monthly wellness subsidy
Independent start-up culture with the backing of a global corporate powerhouse

Our Promise to You

We believe your work is an extension of yourself. At Comfy, we hire many sorts of selves and that's what makes us exceptional. We value diversity of thought, always asking tough questions, committing to solutions—and we do that best when we have and nurture every point of view. We value you, and we want to hear you, learn from you, and move forward together.","Oakland, CA",Data Engineer,False
243,"ContractHaving spearheaded best practices throughout the evolution of data from structured data warehouse methods to big data analytics, Caserta provides enterprise-level innovative solutions for our clients to keep ahead of the technology curve and leverage their data to the fullest extent.

Data Engineers will be responsible to help build the solutions along with team members for full data transformations, as to the clients specifications. The right candidate will have the ability to look beyond the requirements and has a true passion for how data works.
Principal Duties and Essential Responsibilities:
Solid work history using Big Data Applications with Python
Optimizing the performance of business-critical queries and dealing with ETL job related issues
Building and migrating the complex ETL pipelines from system to Redshift
Extracting and combining data from various heterogeneous data sources
Experience using Apache Spark
AWS experience
Strong communication – ability to explain complex technical issues in non-technical terms
Knowledge of database structures, theories, principles, and practices
Experience with S3 datalakes ideal
Caserta is an equal opportunity employer. Applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Caserta fosters a collaborative environment for true technologists with a passion for creating innovative data solutions to solve the most complex problems businesses face today.","New York, NY",Data Engineer - AWS,False
244,"Employment PermanentApply

Start Date Immediately

Location New York, NY

Openings 1


The Market Data Engineering role consists of designing and supporting market data ticker plants globally. Competency in Reuters TREP, Bloomberg, and direct feed infrastructures are required. Software development experience in C, C++ or Java is a BIG plus.
This role has design, engineering, and support responsibilities. The primary focus will be for our US based locations but will, on occasion, be asked to support international locations. The candidate must be comfortable speaking directly with Portfolio managers, traders and other key personnel within the firm. This Engineer is responsible for ensuring that all of the firm's market data ticker plants are functioning, answering users' technology questions, and addressing problems as quickly as possible. The function of the role will increase over time so some experience and the desire and ability to work in this space is a prerequisite.

Responsibilities
Ensure the market direct ticker plants (Reuters, Bloomberg, direct feeds) are operational reliable, and are maintained to specifications as designed.
Create innovative internal alternatives to commercial solutions if business needs require competitive distinction
Provide basic API assistance to traders and internal technologists.
Assist in testing and evaluating of different market data solutions as directed by management.
Monitor the system for market data utilization and participate in capacity planning exercises.

Qualifications
Experience with maintaining Reuters, Bloomberg, and direct feed market data ticker plants.
Creativity in recommending innovative approaches to capture, distribution and usage of data Software development experience in C, C++ or Java is a BIG plus.
Familiarity with basic Reuters, Bloomberg, Exchange symbologies and content.
Working knowledge of network protocols.
Ability to interface and maintain excellent working relationships with traders, other IT staff and vendors.
Ability to maintain composure under pressure and to communicate effectively with business and IT management regarding outages and problems","New York, NY 10005 (Financial District area)",Market Data Engineer,False
245,"A Better company

We believe homeownership is valuable. Here's why: Not only has homeownership historically been one of the most reliable ways to build wealth, it also strengthens ties to community and contributes positively to overall well-being. The way the mortgage industry operates today makes it harder and harder for people to consider homeownership -- so we set out to change that. The traditional mortgage process is designed to confuse, riddled with unnecessary fees, takes too much time, and is built on a foundation of misaligned incentive structures. It's time Americans had a Better option.

A Better option is one that puts customers in control of the largest financial transaction of their lives. It's built with best-in-class technology, supported by non-commissioned staff, and offers affordable financing options that meet customers where they are.

Since 2016, we've already funded over $1B in loans, raised $75M in capital, and won the NerdWallet Best Online Mortgage Lender for Customer Service and we're just getting started.

Trying to modernize a decades old industry isn't easy, but it is supremely rewarding. Become part of a Better team.

A Better opportunity

Help us hack a thirteen trillion dollar industry by building a product that will allow more people than the status quo to own a home and build wealth rather than rent for life. Our tech team is small, and you will be a big part of defining the technical direction and culture. We encourage proposals for projects off the beaten path, experimentation with different frameworks and libraries, and doing as you see fit to solve problems. We also offer above-market compensation and equity, as well as full benefits.

Some projects you could be working on


Work closely with our product team to understand funnel drop off and come up with product ideas
Work closely with the marketing team to optimize our acquisition funnel
Present conclusions to the executive team that can impact the strategic direction of the company
Build a lead scoring model to help our customer support team prioritize.
Model the time-lag of conversions ( https://github.com/better/convoys ) using Tensorflow and fun math like Gamma distributions
Design an experiment to understand the causal impact of an outbound phone call on conversion rates
Build web scrapers to track price data for other mortgage lenders
Migrate our data warehouse to Redshift
Work on our underwriting engine, which turns out to be NP-complete and can be posed as a mixed integer programming problem
Transcribe all our phone calls using speech-to-text and figure out ways to optimize customer support.

Better Technology


We do continuous deployment and we ship code 50-100 times every day
The data stack is all in Python 3.6
We use Node.js, Python and Scala for services
Postgres for the database
Kubernetes, for deployment and devops
AWS for infrastructure, leveraging EC2, S3, SWF, CloudFront, Route53, and much more

The team


The tech team is currently 19 engineers but growing quickly
Erik Bernhardsson ( https://twitter.com/fulhack ) (CTO) used to run the data team and the music recommendation team at Spotify. He is the open source author of a few popular projects like Annoy ( https://github.com/spotify/annoy ) and Luigi ( https://github.com/spotify/luigi ) and writes a blog ( http://erikbern.com/ ) about (mostly) data

","New York, NY",Data Engineer,False
246,"Join Tubi and reinvent the way consumers consume movies and TV! With over 8,500 active movies & TV shows, our users are watching millions of hours of the world's largest free catalog of premium content per month. With backing from leading VC firms, and content partners like MGM, Paramount and Lionsgate, we are disrupting the streaming media space.

Tubi is a data-centric company, from product roadmap decisions to algorithms that power our services to what shows we license, data and experimentation are at the center. We are looking for engineers passionate about building scalable, high throughput data infrastructure as well as tools to enable rapid iteration of machine learning algorithms.

About the role:
In this Data Engineering role, you will work closely with product engineering teams and data scientists to tackle problems in personalization, content discovery, search, advertising and content production. Some of the challenges you will take on will include streamlining feature engineering, helping data scientists take machine learning models in production and enabling data science research and analysis even amongst non-scientists in the company.

Your background:

BSc/MSc in Computer Science or related field
2+ years of a proven track record in a data engineering or related role.
Ability to take ownership of the design, implementation, and maintenance of scalable end to end data pipelines.
Ability to quickly evaluate and make trade-off decisions on adopting emerging technologies.
Strong knowledge of Scala and Apache Spark 2.0+.
Building real-time data pipelines using Redshift, S3, Kinesis, Spark structured streaming, Akka streams, and similar stacks on leading cloud platforms.
Experience with DAG workflow schedulers like Airflow.
A passion for shipping production quality code with good test coverage using testing frameworks for testing Spark code.
Excellent problem solving skills.
Ability to interpret and analyze data is a must. Consequently, mathematical inclination is a major plus.

Benefits:

A tight-knit team of passionate people and a tech-first business
Autonomy and end-to-end ownership
In addition to VC funding, Tubi TV generates healthy revenue
We offer very competitive pay, equity, full medical, dental & vision benefits, catered lunch and dinner, and gym subsidies
Your choice of hardware
Work with other fellow AVOD enthusiasts
Opportunity for internal growth
Open PTO
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.

","San Francisco, CA",Data Engineer,False
247,"Description:
Join the EDABI team, where your goal is to unlock the value of the vast treasure of data available – make it accessible through natural language interfaces and build novel Big Data solutions to create insights – your customer is business analysts/users.
In this role, you will be at the forefront of creating high performance, real time, streaming as well as Data at Rest analytics platforms. You will derive satisfaction from deploying non-trivial scale solutions to solve business problems and create insights that were either downright impossible until now or took several days of effort.
Use your skills, experience and talents to be a part of groundbreaking thinking and visionary goals. You will be required to:
Understand how to build scalable, real time, streaming based, Big Data systemsHave developed and been a key influential member in a fully delivered data product
Lead the architecture and design of several modules related to the backend of a search system, a real time relevance engine, a system that computes several complex functions on the data on the fly, etc.Be a hands-on developer and lead by example as a programmerProvide guidance and contribute to coding standardsProvide leadership in sprints, CI/CD and the DevOps efforts

RequirementsB.S. in computer science or related areas3+ years of experience developing production grade softwareProficient in Linux or related unix systemsExpert in one or more of C, C++, Java and Python. Exposure to modern programming languages such as Rust and Go a huge plus.
Experience building and deploying large scale distributed systems
Excellent written and verbal communication skills
Qualifications:","Sunnyvale, CA",Sr Data Engineer - Next Gen Analytics,False
248,"First Republic is an ultra-high-touch bank that provides extraordinary client service. We believe that one-on-one interactions build lasting relationships. We move quickly to serve our clients’ needs so that their financial transactions are handled with ease and efficiency. Client trust and security are paramount in our line of business. Ultimately, our goal is unsurpassed client satisfaction which will lead to personal referrals – our number one source of new business. We recognize that our competitive advantage starts with our people and our culture. At First Republic, we work hard and move quickly as a very coordinated team. If you are looking for an opportunity to grow and contribute in a fun, fast-paced environment, First Republic is the place for you. We have exceptional people focused on providing extraordinary service.
The Data Engineer will play a crucial role in our Eagle Lending Group, building the data infrastructure that will allow the team to conduct, analyze, and consult on business decisions. Eagle Lending is leading the bank on how to leverage data and technology to acquire and engage the next generation of First Republic clients. The Data Engineer must be comfortable with a fast paced environment where he/she may need to shift responsibilities depending on business needs. We’re looking for someone who is willing to roll up their sleeves to help build the foundation so that we can continue to create the reports and run analysis.
Responsibilities
Build data pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Manage and extend a reliable, effective, and scalable data infrastructure
Work closely with analysts, data scientists, and product engineers to understand business needs and design/maintain scalable data models
Partner with leadership and stakeholders to develop and execute various reporting packages and ad hoc requests
Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes
Qualifications
BS or MS in Engineering, Computer Science, Math, Physics, Statistics or related quantitative field
Programming experience in one or more of the following: Java, Python, R, and/or C, C++
Strong SQL skills, ability to perform effective querying involving multiple tables and subqueries
Advanced Tableau coding skills with a track record of developing visualizations that drive and optimize business outcomes
Strong R and/or Python (preferred) coding skills
Experience and knowledge of statistical modeling techniques: GLM multiple regression, logistic regression, log-linear regression, variable selection, etc.
Understanding of and experience using analytical concepts and statistical techniques: hypothesis development, designing tests/experiments, analyzing data, drawing conclusions, and developing actionable recommendations for business units
Experience creating and using machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
Excellent written and verbal communication skills for coordinating across technical and non-technical teams","San Francisco, CA",Data Engineer,False
249,"Job Description
At MarketDial we're building software that enables our customers to make sophisticated multi-million dollar marketing, pricing, staffing, and operational decisions through offline A/B testing. Our customers are leaders in the retail, grocery, and c-store spaces. We are an early stage startup with lots of room to grow.

Part of building this software is transferring, and reshaping our clients’ data in the MarketDial data ecosystem. We’re looking for a developer that is looking to grow their big data skills and work closely with industry-leading retail companies to develop creative analytical solutions. This job includes managing the end-to-end lifecycle of integrating client data into the MarketDial app, ensuring data accuracy, a robust data architecture, and automated ETL process. Perfect data and a robust data pipeline is the lifeblood of our business.

Daily duties will include: Large file processing (map, reduce), ad-hoc report generation, development and maintenance of our automated ETL code base, and eating free food. Ultimately, we’re looking for creative problem solvers to work in a caring, fun and profitable environment.

Skills and Qualifications
Successful applicants will have some flavor of these skills (1+ years experience):

Python (must have at least 1 yr experience)
SQL
BigQuery
Data Engineering and Database layout Skills
GitHub or other version control tools
Unix / shell
Docker
Luigi
Apache Spark
Communication with people in both technical and non-technical roles
Dash of Statistics. The word Standard Deviation should treat you like a warm hug.
Business sense. The ability to look at numbers and sense check if they look right or wrong is key to success in this role.

Here at MarketDial, we embrace and celebrate difference. We believe that a diverse culture is essential to the benefit of our culture, company, and customers. MarketDial is proud to be an equal opportunity workplace and is an affirmative action employer. We believe that in hiring the best talent, a diversity of cultures, ideas, and perspectives will reflect the global diversity that our world is today. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or need that requires accommodation, please let us know at
erika@marketdial.com [erika@marketdial.com]
If you are interested and want to learn more about what MarketDial does and what role you might be able to fit here please fill out an application below. We look forward to getting to know you.","Salt Lake City, UT",Data Engineer,False
250,"Temporary, ContractLooking for suitable candidates for Data engineering positions for our client in Mclean VA.Technology: Spark/Python/AWSAvailablity: ImmediateExperience: 3+Please apply with your latest profile and mention your employment status.Thanks.Job Types: Full-time, Temporary, ContractExperience:Spark: 1 year (Preferred)AWS: 1 year (Preferred)Python: 1 year (Preferred)Work authorization:United States (Required)","McLean, VA",Data Engineer,False
251,"The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health.
We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists and engineers to develop and support effective data analysis and machine learning workflows.

Key Qualifications
Experience with software engineering frameworks:
Excellent coding skills in Python (e.g., Scipy, Pandas, Jupyter, Plot.ly), C++, scripting, and object oriented programming languages.
Experience with Web development (e.g., Javascript, HTML, React, D3, AngularJS)
Experience with Web Service APIs (e.g., AWS, REDCap, XNAT)
Experience designing and maintaining (non-)relational databases (e.g. MySQL, MongoDB)
Experience with Linux, MacOS based development frameworks
Experience with version control frameworks (Git, virtualenv)
Experience contributing to open source projects (e.g., Using Github)
Sense of design and appreciation for user experience are plusses
Description
Script and automate data ingestion and transformation pipelines, with hooks for auditing, QA and redaction
Collaborate with team members to develop novel visualization and interactive front ends for navigating large volumes of data
Process, troubleshoot, and clean incoming data from human studies
Apply best practices for information security, including safe harbor privacy principles for sensitive data
Incorporate and comply with FDA regulations as they pertain to electronic and clinical data and databases
Work closely with team members and study staff to architect data management plans
Implement and automate compliance and edit checks per data management specifications
Collaborate with team members to architect data models and create tools to harmonize disparate data sources
Create and populate databases with existing and incoming clinical data
Translate data management requirements into computer hardware requirements
Perform IT administration of secure computing, data stores and databases for structured and unstructured data
Interest in data analysis, data science and machine learning
Experience with biomedical sensors/platforms for measuring physiological signals in the health, wellness and/or fitness realms

Education
BS/MS/PhD in Computer Science, Biomedical Engineering, Informatics, Statistics, or equivalent with relevant 0-2 years industry experience in medical, physiological, health, wellness and/or fitness fields.
Bootcamp or Self-taught Data Engineering + Science skills are welcome

Additional Requirements
You will thrive in our fast-paced environment if you are highly organized and able to multitask.
Flexible thinking, adaptability to change and comfort with ambiguity are hallmarks of successful people on our team.
We look forward to witnessing your excellent communication and interpersonal skills during the interview process.
Apple is known for heterogeneous and cohesive teams. A proven ability to work seamlessly with others is required to acclimate quickly to our culture.
We highly value your analytical mind and problem-solving skills, and expect a stellar attention to detail.
You will let the customer experience guide your decision making. You will design with Apple’s culture and values, inclusion for all and privacy, as fundamental requirements","Santa Clara Valley, CA",Biomedical Data Engineer - Health Technologies,False
252,"We are looking for an exceptional Data Engineer to help build a state-of-the-art data platform that scales with the rapidly growing ecosystem at Jet.

About the Role

 We are looking for a creative, resourceful problem solver, who is comfortable working both independently and collaboratively. This person would take on the following responsibilities:

Design, implement, and manage a data platform that supports real-time ingestion, low-latency reads, and ML workflows
Implement reusable APIs for stream processing and data transformation
Gather and process data at scale across all business domains
Evangelize an extremely high standard of code quality, system reliability, and performance.
Influence cross-functional architecture across the organization
About You
You have at least 2 years of working experience in a product-driven engineering environment
You have worked with high-volume, high-velocity data, ideally using technologies like: Kafka, Spark, and Cassandra
You know how to write microservices in Java or Scala, or are willing to learn
You are interested in learning more about functional programming
You have experience building and deploying services in a cloud environment
You prioritize building maintainable, scalable, operationally sound services","Hoboken, NJ",Software Engineer - Big Data,False
253,"Company Description:
Quartet is a pioneering technology company connecting physical and mental care to improve people’s health and quality of life. We are building a collaborative technology platform that brings together physicians, mental health providers, and insurance companies to effectively improve patient outcomes and drive down healthcare costs. Our data-driven platform identifies high-need patients and facilitates access to personalized care. Backed by $92MM in venture funding from top investors like GV (formerly Google Ventures) and Oak HC/FT, Quartet is headquartered in NYC and is currently operating in several markets across the US - Pennsylvania, Massachusetts, Louisiana, Washington, Northern California, and New Jersey.

Mission
We're creating a platform to improve outcomes for patients with mental health and chronic medical conditions. We do this in many ways, from identifying patients at risk and routing them to the right specialists to creating a feedback loop between the primary care physicians and behavioral health specialists so they can collaborate on each patient’s treatment. These patients are often lost in our healthcare system and don't get the right care at the right time. We want to fix that.

Outcomes
We are looking for a Data Engineer that will work on ingesting, transforming, and organizing disparate sources of healthcare data from our partners and internal teams into canonical data models for application and data science insights. Our data platform team works to not only clean, standardize, and load external data sources into our platform but also generates key base features from that data to be utilized by downstream systems and internal customers — from the backend API to data science teams. This role will also work to vend tooling and automation to support our growing data needs and allow data integration team members working with customers to self service data prior to it being ingested and distributed to the rest of Quartet. You are passionate about data and will collaborate with and learn from other engineers, data scientists, and clinicians.
Responsibilities
Develop and maintain tools and systems to ingest, transform, and distribute numerous application streams as canonical data models and features.
Implement and vend data ingestion, transformation, and feature generation tools to data science, ML, and partner services teams.
Work with subject matter experts and product managers in architecting models to improve reliability and interpretability of data for analytical and business needs.
Understanding Quartet business objectives and design services that couple business logic with reusable components for future expansion.
Monitoring performance and advising any necessary infrastructure changes.
Leverage best practices in testing, continuous integration, and delivery.
Build a world class, scalable health data ingestion pipeline.
Drive down the time to insight by improving the accessibility and organization of data.
Competencies
Must Haves:
Experience with integration of data from multiple heterogeneous data sources
Experience developing in Python and Spark (Scala and Java a plus)Experience working in distributed systems
Experience with SQL and data modeling
Experience standing up and maintaining infrastructure at all levels of the stackInterest in solving real-world healthcare problems
A great sense of humor and an ability to add to our dynamic culture
A desire to work on a highly-collaborative, mission-driven team

Nice to Haves:
Ideal candidate has 5+ years of engineering experience
Experience with HL7 and other healthcare data exchange standards a big plus
Experience with AWS Experience with Apache Airflow
Benefits and perks of working at Quartet include:
Medical, dental, vision and life insurance
Enhanced mental health benefits
Paid membership to One Medical
Pre-tax health, transit and dependent care flexible spending accounts
Fee-free 401(k) program
Unlimited vacation and sick leave, and competitive family leave policy
Amazing office with stocked kitchen and regular company gatherings
Working with some of the most talented and mission-driven minds in the industry!


Employee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, full medical, dental + vision coverage, generous parental leave, commuter benefits, 15 free therapy sessions + unlimited copay reimbursements for mental healthcare, 401K, ESPP, gym benefits.
Want to know what Quartet life is like? Click here to meet our team.
Quartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet.
Please note:","New York, NY 10018 (Clinton area)",Data Engineer,False
254,"SUMMARY

The Data Engineer will lead the development of data warehousing solutions to power business intelligence at The Hive. They will synthesize business requirements for analytics, operational and fundraising information, creating and implementing solutions to integrate data from various enterprise systems.

ESSENTIAL DUTIES AND RESPONSIBILITIES
 Write code to build ETL processes to integrate information from various enterprise IT systems. Ability to perform API integrations on platforms such as Salesforce Marketing Cloud. We use Postgres SQL for our data stores and AWS for hosting. Design new data structures, add to and optimize existing data schemas. Ability to analyze upstream and downstream effects of a change in existing data structures and planning for change management. Understand key strategies and business processes of internal clients across the organization in order to provide the best solution to achieve their outcomes. Conduct analyses of business or technical user needs, document requirements and design tailored data solutions. Monitor performance of ETL processes and build in redundancies to avoid risk of an information outage. Provide support and troubleshoot questions arising from report developers and business users of these data solutions. Track and document work done and communicate progress with business users in a timely manner. Ability to adapt and/or modify processes in response to changing circumstances. Well versed in specific industry best practices, and an ability to adapt them to U4U’s environment. Ability to work effectively across multiple complex projects. Research and recommend innovative and automated approaches.

QUALIFICATIONS
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

EDUCATION and/or EXPERIENCE Bachelor’s degree in computer science, information systems or related field required and/or combined equivalent of education and experience. Master’s degree preferred. 5 years of experience as a business or technical professional in business intelligence, data modeling or related field. Experience in defining and documenting complex systems requirements. Experience with data integration, building database objects using SQL, optimizing queries and writing stored procedures. (> 5 years.) Experience in data warehouse methodology and data modeling. (>3 years) Experience in extracting data from various APIs. Strong ability to analyze business requirements and recommend solutions. Ability and desire to mentor and train others. Experience in software development methodologies. Experience in writing SQL queries, coding in Python, and creating systems in the Cloud Computing Space, preferably AWS. Experience with CRM’s, preferably Salesforce. Experience with working on different file types for both structured and unstructured data.

PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
 Tasks involve some physical effort, i.e. some standing and walking, or frequent light lifting (5-10 lb.); minimal dexterity in the use of fingers, limbs, or body in the operation of office equipment; may involve extended periods of time at a keyboard. Extended periods of sitting at a workstation or desk and manual dexterity to work efficiently on computer keyboard for data entry and composing of documents

WORK ENVIRONMENT

The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The tasks will generally be performed in a typical office environment. May also involve travel to some locations within the company’s region of operations and select donor locations.

DISABILITY SPECIFICATIONS

USA for UNHCR will make reasonable accommodations in compliance with the Americans with Disabilities Act of 1990.","New York, NY",Data Engineer,False
255,"If you love clean data, we want to hear from you. We are looking for a Data Engineer who will help take the company’s data reporting and infrastructure to the next level. You will work with the Executive, Product, Marketing, Sales, Sales Engineering, Finance and Customer Success teams every day—in short, your work will have an impact on the whole company. You will be our second Data Engineer and will be critical in shaping our data foundation.

You will contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing and operating stable, scalable and efficient solutions to flow data from production systems into the data warehouse.

We know new analytics technologies are emerging every single day and we are excited about the impact they will have – we hope you share our enthusiasm!

What You'll Do
Collaborate with engineers to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources
Ensure we have data consistency on both production and analytical databases. You will own the integrity of our data from end-to-end and the company will make high impact decisions based on this data.
Build a data warehouse to provide timely data to multiple third party applications (Salesforce, Marketo, etc)
Design and build tools that make our data pipelines and surfacing more reliable and easier to use
Work closely with backend engineers to roll out new tools and features
Triage, identify, and fix scaling challenges
Collaborate with internal data customers to gather requirements
Help develop our data engineering function in areas of data architecture, business intuition and insight.
Who You Are
You have at least 2 years of experience with at least one relational database—MySQL, Postgres, Oracle or other.
You have experience with SQL and Data Warehousing using a relational database.
You are experienced with large-scale data pipelines and ETL tooling.
You have previous coding experience. Our ETL process is in Ruby and we use Python for some data analysis.
You can maintain confidentiality of sensitive customer data.
Bonus Points
You are experienced with EDA (Exploratory Data Analysis) and Data Visualization (we use Tableau).
You have used Amazon Redshift.
You can manipulate data using Python (pandas, numpy, scikit-learn, etc).
You have previously worked in Business Intelligence, Analytics or Finance.
About HelloSign:
We believe that the way business gets done today is broken. That’s why we’re dedicated to simplifying work for everyone - from small startups to large enterprise companies. Millions of individuals and over 75,000 companies world-wide trust the HelloSign platform – which includes eSignature, digital workflow and eFax solutions – to automate and manage their most important business transactions.
With a sharp focus on user experience and a lust for innovation, HelloSign is on a mission to Simplify Work.

Life at HelloSign:

We are centrally located in downtown San Francisco near BART, the Transbay Terminal, and the Ferry Building. Just over 100 employees, we are growing the company deliberately, with a keen eye towards maintaining a culture that values lifestyle, fun and continuous improvement. We were awarded the Hirepalooza Culture Award for Lifestyle in 2015 and the Healthy Mothers Workplace Bronze Award in 2016 and 2017. This year, we won SF Business Times' Best Places to Work Award for Small Employers. We continue to maintain an overwhelmingly positive presence on Glassdoor and The Muse.

We have raving fans who love what we make • We're user-focused and product-driven • We're always evolving with an eye towards improvement • We're committed to building a product people want • We thrive on collaboration and learning from each other • We have a supportive, familial atmosphere • We work in an open, airy, creative space • We laugh a lot • We love dogs • And we'll never forget your birthday!

HelloSign is an equal opportunity employer committed to hiring a diverse team of qualified individuals • HelloSign conducts background checks; pursuant to the San Francisco Fair Chance Ordinance, HelloSign will consider for employment qualified applicants with arrest and conviction records • HelloSign participates in E-Verify.","San Francisco, CA",Data Engineer,False
256,"Univision is seeking a solutions-oriented Data Engineer to join the Media Intelligence team. The ideal candidate will not only possess great communication skills, but will also be equally at home writing code and solving complex problems in a fast paced creative environment. This role focuses on transforming large datasets into meaningful insights that will ultimately inform business decisions across an array of marketing channels including digital, social media, email, TV, radio, and out-of-home. This individual must be a strategic and creative thinker, a self-starter, and a collaborator who’s willing to share ideas and proactively discover opportunities for marketing efficiencies and increased ROI.

Job Responsibilities
The Data Engineer will be responsible for developing and implementing databases, data collection systems, data analytics and other strategies to identify opportunities in the performance of media campaigns. In addition, the role will inventory all sources of data to maximize data analytics opportunities. Further responsibilities include:
Conduct analysis on key initiatives using internal and external data and understand key drivers of the business
Analyze effectiveness of marketing spend per campaign
Interpret data, analyze results using statistical techniques and provide ongoing reports
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
Acquire data from primary or secondary data sources and maintain databases/data systems
Manage relationship with data and analytics vendors as well as internal counterparts on other teams
Manage ingestion of daily reporting from various media channels and platforms
Audit and “clean” current data sets, and redefine reporting processes and workflow for sharing information
Identify, analyze, and interpret trends or patterns in performance data and recommend optimization opportunities in real-time for improved marketing performance and/or cost efficiencies
Project manage quarterly and end-of-campaign reporting to present data and learnings to key stakeholders
Advance the data and analytics capabilities of Univision by researching, exploring, and onboarding future tools
MINIMUM QUALIFICATIONS
Desired Experience
Candidates must be comfortable with manipulating, transforming, and analyzing complex data from varying sources. We are actively looking for professionals with experience in the following Enterprise Information Management disciplines:

Data Warehousing
Data Warehouse Design and Architecture
Dimensional Modeling
Data Integration/Data Services

Analytics and Reporting
Dashboards
Ad hoc and Guided Analytics / OLAP
Predictive Analytics
Data Visualization

Data Governance
Design and Initiation of Data Governance Process
Metadata Management
Master Data Management
Data Auditing and Security
Data Architecture Organization and Practices

Required Education & Skills
BS in Mathematics, Computer Science, Information Management or Data Science preferred
2 – 5 years of proven experience as a Data Engineer or similar role working with complex data sets
Technical expertise regarding data models, database design, data mining and segmentation techniques
Strong knowledge of and experience with reporting suites (DOMO and Tableau preferred), databases (SQL, etc.), programming (XML, Javascript, and ETL frameworks)
Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc.)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Ability to manage multiple priorities under tight deadlines
Strong analytical, strategic thinking and problem-solving skills
Strong skills in developing PowerPoint presentations and communicating performance data to senior leadership
Excellent verbal and written communications skills","Miami, FL",Data Engineer,False
258,"We are seeking a detail-oriented Imagery Data Engineer to join our team, helping to maintain an imagery data platform that support model training. In this particular role, you will work with large amount of imagery data coming from different perception sensors.
Responsibilities
Responsible for the maintenance of DL training data pipelines.
Keep track of the progress of image data in-house labeling
Responsible for data customization to meet various demands from deep learning team.
Manage the tasks and progress of labeling to support model training
Requirements
Strong familiarity of Python
2 year + experience in programming in Python, MySQL
Good knowledge of Web service, WebGL and react.
Experience with data pipelines is a plus
Experience with C++ is a strong plus
Experience of ML/DL project is a plus
Excellent communication skills and team player
BS/MS in Computer Science or related technical discipline","Fremont, CA",Data Engineer,False
259,"CommissionWorking in a team to build a new enterprise platform that will consolidate enterprise datasets and handle data integration and transfer between enterprise systems.
Developing components that comply with standards, and where required, developing key integration touchpoints for interoperability.
Working closely with the Lead Data Engineer
Provide recommendations and evaluations of new designs and solutions
Work in technologies in which you are unfamiliar and learning it as you go
Solve advanced programming problems with minimal supervision
Provide technical expertise to peers on the team
Requirements
Key required areas of experience
4+ years working with large-scale enterprise database warehouses
Experience working with Azure Blobs, Events, AzureSQL and other Azure technologies
Experience with data analytics and data modeling
Experience with Aspera, Tibco Data Virtualization (formerly Cisco Composite)
Experience with integration platforms, like Snaplogic, IIB, BizTalk
Must have strong ETL experience with tools like Informatica, SSIS
Not required, but a plus if you know…
Experience with No-SQL, Azure’s implementation of it is a double plus.
Experience with Elastic Search
A Computer Science degree or background
Benefits
ABOUT THE COMPANY
ClearlyAgile is one of the fastest growing Agile companies in the Tampa Bay area. We foster career growth and are focused on having fun and delivering quality products and services to our clients. With 15 paid holidays off per year (including the entire week of Thanksgiving and Christmas), plus 15 days of paid time off, medical, vision and dental benefits that start on your first day (and we contribute $250 towards your medical), 401K, paid training certifications and very competitive pay & commission plans, ClearlyAgile strives to listen to and invest in its most important asset…its people.
Our Mission: Transform our customer’s businesses using Agile methodologies and principles to help them succeed in a flexible, collaborative, self-organizing and fast-paced environment.

Our Values: We hire, fire and reward based on our core values.
Produce the Best Quality of Work Possible
Be a Team Player
Invest in Yourself
Be a Professional
Be a Leader
We are an equal opportunity employer and committed to a diverse workforce.
To learn more about us visit our website.","Tampa, FL 33605 (East Ybor area)",Data Engineer,False
260,"$90,000 - $140,000 a yearHi,Please have a look at the job description below and let me know if you would be interested. If so, please revert with your updated Resume and Expected Salary.Job Description: Title: Big Data EngineerLocation: Irving, TX and Tampa, FLDuration: Full-Time/PermanentHands-on experience as Big Data Engineer & data analyst with experience on Big Data Hadoop echo systems, HDFS, Hive, Map Reduce, Sqoop, SPARK.Extensively worked on SPARK and HIVE for performing data analysis.Implemented business functionality using spark core and spark SQLTransforming the Hive jobs into spark SQLDeveloping new jobs using spark and evaluating their performanceHaving good experience in writing Hive queries.Worked on Spark Core, Numeric RDD's, Pair RDD's, Data Frames, and Caching for developing Spark applications.Excellent programming skills in Object Oriented Programming, Core Java, Java EE 8 ( JSP, JDBC).Working knowledge in Web Services (SOAP and REST).Good knowledge & skills in Java Multi-threading, Collections etc.forward details to adam(at)klaxontech(dot)comJob Type: Full-timeSalary: $90,000.00 to $140,000.00 /yearExperience:Hadoop: 1 year (Preferred)Spark: 1 year (Preferred)SQL: 1 year (Preferred)Hive: 1 year (Preferred)Java: 1 year (Preferred)","Tampa, FL",Big Data Engineer,False
261,"Job Description

Our leading retail sportswear client is looking for a Data Engineer/SQL Developer! Please note this is strictly a W2 position only and we cannot do C2C.
Job Duties:
Develop and support data solutions in support of Supply Chain Planning reporting and analytics requirements
Engage with product owner, technology lead, report developers, product analysts, and business partners to understand capability requirements and develop data solutions based on product backlog priorities

Qualifications

3+ years of experience with data engineering with emphasis on data analytics and reportingStrong experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata)— expert-level SQL abilitiesExperience developing with scripting languages such as Shell and PythonExperience developing with the AWS EMR managed service, leveraging tools such as SparkExperience with agile delivery methodologies– Scrum, SAFe, Extreme ProgrammingExperience working with source-code management tools such as GitHub and JenkinsAbility to partner with business and technology team members, to understand business requirements and translate those into value-add technology solutions
Additional preferences are:
Experience developing solutions in SnowflakeExperience with workload automation tools such as Airflow, Autosys.Experience building solutions with data visualization and reporting tools (Tableau, Cognos)Knowledge of Supply Chain Planning business processes and objectives
Additional Information

To Apply:
Aroghia Group provides top market compensation, H1B transfers, Green Card processing, and a great company culture. Please provide your resume, LinkedIn profile address, and phone number when applying.
Aroghia Group has established a solid reputation in the marketplace by providing our employees with outstanding opportunities for personal and professional growth. Some additional benefits include (but are not limited to):
We are a preferred IT vendor for top-notch companies in a wide range of industries across the U.S.
Aroghia offers various compensation structures (hourly, salary) based on qualifications and market demand.
We provide continuous training and development to ensure our team remains at the forefront of technological advancements.
Open Positions: http://aroghia.com/careers","Beaverton, OR 97005 (Central Beaverton area)",Data Engineer/SQL Developer,False
262,"Under moderate supervision, the Data Engineer II accepts and validates all student level data for assigned projects in order to ensure data integrity. Combines several sources of data used for reporting test results. The Data Engineer II also provides input and support into internal products and systems that affect data in the company. This position follows established protocols and standards when performing tasks and work outside of standard processes and procedures must be approved by department manager. Additional responsibilities include the following:
Validate and accept student level data including student demographic and test data from various sources and provided in various formats.
Perform cleanup activities to data per specifications created.
Perform documentation of data processing specifications to ensure common understanding of processes and procedures used on a project.
Participate in project meetings in order to ensure data integrity issues are discussed and resolved.
Develop code and process to support validation, cleanup, transformation and delivery of student data.
Provide data support to internal products and systems.
Provide input into the development of internal products and systems.
Develop appropriate quality assurance steps to be implemented on projects to ensure data accuracy.
Provide support to team members on projects as needed.


Qualifications
Bachelor’s degree and three (3) to five (5) years of experience in SQL development, or an equivalent combination of education and experience.
Comprehensive knowledge of Microsoft SQL Server
SQL programming of complex views, stored procedures, functions, and scripts
Strong understanding of the fundamentals of developing user-facing reports and forms
Experience with Visual Studio and C# preferred
Knowledge of Microsoft Office","Dover, NH 03820",Data Engineer II,False
263,"$50 - $95 an hourTemporary, ContractBig Data EngineerExtensive experience of working in Scala (a must)Job OverviewWe are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.QualificationsAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Experience with big data tools: Hadoop, Spark, Kafka, etc.Extensive experience of working in Scala (a must)Experience with relational SQLExperience with AWS cloud services: EC2Experience with object-oriented/object function scripting languages: Java, ScalaUnderstanding Data Science Algos (Regression, Deep Learnig methods) will be a BIG plus.Job Types: Temporary, ContractSalary: $50.00 to $95.00 /hourExperience:Hadoop: 2 years (Required)Java: 1 year (Required)Big Data: 3 years (Required)Spark: 1 year (Required)Scala: 3 years (Required)SQL: 5 years (Required)Location:Washington, DC (Required)","Columbia, MD",Big Data Engineer,False
264,"Blizzard Entertainment games don’t just begin with game ideas nor do they end once those games are released. A lot more goes into the creation of an epic game than the work of developers and more than you can ever imagine goes into continuing to build and hone and perfect the most epic gaming experience in the years after our games are in the hands of gamers world-wide.
Blizzard Entertainment is looking for a Data Engineer to join our Global Insights team to partner with our respective analytics teams with technical leadership and vision, innovation and agility with big data at scale, and to develop a deep and rich subject matter expertise with the data most important to the franchise.
In this role, you will be responsible for providing accurate, timely, reliable, and ultimately useful data to the analytics group. This mandate involves everything from the discovery, access, and integration of source data into the ecosystem to guaranteeing its health and robustness within that ecosystem to modeling and presenting that data in a manner that allows stakeholders within the ecosystem to perform their duties effectively. You will have a delicate balance in this role: you work most closely with the other members of your group and translate their needs into agile, highly-customized implementations for franchise-specific data; at the same time, though, you work just as closely with the Core Data team both to ensure that your group does not introduce problematic deviations from core data and that the core data infrastructure itself is supporting your group’s evolving needs; you are just as closely tied to the Tools and Technology team to ensure that your franchise-specific system usage is “playing nice, playing fair” within the greater analytical data ecosystem. You are one step closer to the business needs of the stakeholders and act as the primary technical resource for the franchise analytics team. Your priority is to ensure that the Analytics groups can provide best-in-class analyses, insights, and recommendations in the most timely and effective manner possible.
Our ideal candidate is the druid of the analytics space: a powerful technologist whose dedication to the craft allows him or her to morph into whatever form necessary to get the job done while retaining the flexibility to see the bigger picture and respond accordingly. The ideal candidate uses the speed, agility, and targeted on-point damage of Cat form to meet the quick-shifting, high-impact immediate needs of his or her franchise group before morphing seamlessly into a Bear form, responsible for tanking the ongoing operational needs of the group as a whole. A shift to Moonkin allows for the push-and-pull flow of ideas between Blizzard Analytics and Core; and the final shift to Tree provides the stability and integration between the franchise group and the other users, approaches, policies, and procedures of the overall analytical data ecosystem.
Responsibilities
Design, develop, implement, and evolve franchise-specific data pipelines
Troubleshoot any performance, system or data related issues, and work to ensure data consistency and integrity
Work with the franchise team as technical lead and primary technical resource
Plan and coordinate franchise-specific technical projects
Innovate in a more agile environment and summarize ideas/approaches to the Core Data team for validation and potential integration into core data infrastructure
Ensure no deviation from core data
Act as a primary technical liaison between the Blizzard Analytics teams and the Core Data / Tools and Technology teams

Requirements
2+ years working in a large analytical data ecosystem
Strong technical understanding of data modeling, design, architecture principles, and techniques to take business requirements from concept to implementation
Very strong knowledge of relational databases, MPPs, SQL, with an emphasis on Teradata fundamentals
Very strong experience with the Hadoop ecosystem including HDFS, MapReduce, Spark, Hive/Pig, et. al
Strong verbal and written communication skills across both technical and non-technical audiences
Knowledge of Python, Java, Linux architecture and scripting
Extensive background extracting and transforming complex data sets (ETL process design and administration)
Experience with database design and star schema data warehouse theory
Passionate video gamer and in-depth knowledge of Blizzard Entertainment games, products, and services

Blizzard Entertainment is a global company committed to growing our employees along with the business. We offer generous benefits and perks with an eye on providing true work / life balance. We’ve worked hard to foster an intensely collaborative and creative environment, a diverse and inclusive employee culture, and training and opportunity for professional growth. Our people are everything. Our core values are real, and our mission has never changed. We are dedicated to creating the most epic entertainment experiences…ever. Join us!","Irvine, CA",Data Engineer,False
265,"About Skillz:
Today, people spend more time playing video games than they do playing all physical sports, and eSports are poised to become bigger than the NFL, NBA, MLB and NHL combined. Skillz provides the technology that powers an eSports industry forecast to exceed as much as $40 billion in revenue by 2020.

As the eSports provider for over 8,000 game developers, Skillz enables any mobile game to be turned into a competitive eSport that can also be broadcast on major streaming sites like Twitch and YouTube. We do this by integrating our unique layer of code directly into a mobile app, which activates our end-to-end tournament administration and other important features like player matching, anti-cheating mechanisms, customer support and a built-in loyalty program. Using our patented technology stack, we've already hosted more than 500 million tournaments for 15 million players around the world.

Skillz has raised over $53 million in funding from sources including Telstra, Liberty Global, and the owners of the New England Patriots, Milwaukee Bucks, New York Mets and Sacramento Kings. In our quest to make gaming better for players and developers, we're looking for savvy, driven and enthusiastic teammates to help us build the future of sports. If you're excited about defining a multibillion dollar industry, building an awesome product, or working with cutting-edge technology, Skillz just might be for you!

Our Culture:
We are true believers in electronic sports, so don't be surprised if you see us playing mobile games at our desks or in the kitchen. At Skillz, this type of behavior is encouraged! You can also catch us at our weekly game nights playing anything from Super Mario Kart to Codenames over dinner, while discussing new technologies or brainstorming ideas to improve our business.

Not a ""gamer""? Don't worry about it. Every Skillzian brings a unique perspective to the team, and we bond over plenty of activities that don't involve games. We're working to build a truly groundbreaking company, and we want top-notch people to join us in that mission. As the creators and leaders of a new marketplace, we work with a ""do whatever it takes"" mentality, and frankly, we get the job done!

Our team is comprised of passionate, intelligent and creative individuals who consistently seek out new challenges and knowledge. We embrace out-of-the-box strategies to propel our business forward, and foster a culture where every person's voice is heard. Our team comes from diverse backgrounds, and our leaders have a strong history of funding and building successful enterprises. If you're up for the challenge, we'd love to meet you.

Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.

What You'll Do:

Maintain current data infrastructure ensuring that the wider organization can analyze millions of tournaments a day
Prototype emerging technologies so that Skillz can get the benefit of cutting edge technology
Provide infrastructure expertise to data consumers, evangelize Data Engineering principles and enable others to self-help
Develop instrumentation for analytics and alerting. Collect and report on meaningful data to improve costs, reliability, and performance
Detect, analyze, and fix faults with data infrastructure components; propose design changes that eliminate weaknesses and obstacles to scaling reliably

Your Skillz:

2+ years of experience with Python
1+ years of experience with SQL
Independent and resourceful; able to assess, plan, and execute in unfamiliar conditions.
Able to re-prioritize on the fly and respond to ad-hoc events.
Excellent analytical and problem solving skills
Experience working and improving the ETL process
Understanding of optimization methodologies

Bonus:

2+ years industry experience
Familiarity with AWS data products (Data pipelines, DMS, S3, etc)

","San Francisco, CA",Data Engineer,False
266,"Data Engineer (Data Science) - Palo Alto

Are you a coding ninja with an appetite for healthcare innovation Would you like to help us reinvent the way parents use technology to monitor and improve the health of their young children

We are looking for a software developer with solid foundational knowledge in machine learning, proven record of building clean scalable software in the real world, and an overflowing passion to make a difference.

You should have:

Team spirit and enthusiasm
Flexibility, versatility, and resourcefulness needed to thrive in at a fast-paced startup
Strong foundation in software development
3+ years of market experience applying software development specifically to data science and machine learning projects
Degree in computer science, software engineering, or a related field

Your Responsibilities:

Own and manage the deployment of ML models into our production back-end in a reliable, seamless, and quality-controlled way
Identify and abstract common data-science experimentation use cases and implement into modular frameworks accessible via clean well defined APIs
Own and manage site traffic and user analytics including data pipelining, integrating multiple third party analytics platforms, dashboard design and implementation

","Palo Alto, CA",Data Engineer,False
267,"Cross River is actively looking to expand our business intelligence and data analysis team.
A suitable candidate for this position will have the experience in data warehouse development, architecture and hands-on physical and logical database designing. The candidate will have experience working on projects in a collaborative setting composed of cross-functional teams.
A candidate for this position must have a deep passion for data analytics technologies as well as analytical and dimensional modeling. The candidate must be extensively familiar with data warehousing and have the skill to draft, analyze, and debug SQL queries.
Job responsibilities include
Design and develop systems for maintenance of business data warehouse, ETL processes, and business intelligence.
Work closely with other departments and teams to create and implement solutions that balance data needs across the company.
Establishes the documentation of reports, develops, and maintains technical specification documentation for all reports and processes.
Any other duties delegated by Director of Data, Analytics, and Data Science
Qualifications
3+ years of data analysis experience
Experience with creating SPs, functions, views, SQL jobs
Experience with SQL Server Management Studio
Experience with Tableau a plus
Coding experience would be a plus, preferably Python
Extremely detail oriented with excellent communication skills
Excellent problem solving and troubleshooting skills
BS or MS in Computer Science or related technical field","Fort Lee, NJ",Data Engineer,False
268,"$110,000 - $120,000 a yearThe Enterprise Data Architect is a thought leader sitting in the Enterprise Architecture organization partnering with both business and IT stakeholders to define and execute an enterprise data strategy and architecture that supports our Federal customers mission needs. Our customer is transforming its applications and data layers to better control the acquisition, validation, quality and distribution of data across multiple mission critical applications, data warehouses, and external partners. The Enterprise Data Architect will set the overall vision, strategy and data architecture as well as direct future initiatives that build components of architecture ensuring convergence towards the defined future state. This leadership role will also set the direction for technology including tools for data acquisition, reporting, analytics, data discovery, and visualization.ResponsibilitiesDevelop, maintain and evangelize data architecture guiding principles, policies, best practices, reference architecture and standardsDefine and maintain target data architectures and master data management strategies in alignment with business and technology objectivesLeader of data transformation efforts from legacy to new and alternative technologiesProvide thought leadership for IT’s data security, data integration, data services and API effortsCollaborate with various Business, Operations, Applications and Data and Analytics groups to define, establish and run a process that ensures adherence to enterprise data standards and architecture principlesAdvocate for, and champion, efforts to close gaps between as-is and to-be architecturesMonitor compliance of development work against established standards and principles and work to resolve issues through appropriate formal and informal channelsWork closely with other architects and IT managers and developers so that they understand and support data architecture prioritiesReview and analyze existing systems and make recommendations for improvementsCreate governance policy for the Enterprise Data ModelPlay a lead role in selecting and implementing technologies supporting MDM, data discovery, data quality, analytics and BI tools, and champion related process changesProvide general guidance to teams on data architecture topicsRepresent data architecture concerns in design, implementation and deployment reviewsAnalyze proposed business solutions to understand and document data requirements and partner with solution architects to define required data architecture elementsRequirementsBachelor’s degree in Information Technology, Computer Science, Data Management or related filed with an emphasis on data and databases required but a Master’s preferred. Equivalent extensive relevant work experience will be considered8+ years of experience requiredAdvanced knowledge of applying TOGAF methodology (or similar EA framework) to the document data architectureStrong working knowledge of application development processesAdvanced knowledge of data management, data integration and database development technologies and processesAdvanced knowledge of cloud-based data strategies, NoSQL, SQL, Data Warehouse, ETL, Business Intelligence and Analytics toolsAbility to translate complex technical terminology, concepts and issues in terms understandable to technical and non-technical management and staffStrong interpersonal skills to resolve problems in a professional manner, lead working groups, negotiate and create consensusAdvanced database development skills and experience requiredIntermediate knowledge of UML and object-oriented design requiredExcellent oral and written communication skills requiredJob Type: Full-timeSalary: $110,000.00 to $120,000.00 /year","McLean, VA",Data Engineer,False
269,"ContractJob Title: Data EngineerLocation: Whitehouse Station, NJDuration: 12 MonthsVisa: H1B, GC, or USCInterested people, please keep in touch with me on 732-412-1384.Ideal candidate for this role is someone with a strong background in computer programming, statistics, and data science who is eager to tackle problems with large, complex datasets using the latest Python, R, and/or PySpark. You are a self-starter who will take ownership of your projects and deliver high-quality data-driven analytics solutions. You are adept at solving diverse business problems by utilizing a variety of different tools, strategies, algorithms and programming languages. Specific responsibilities are as follows: Utilize the data engineering skills within and outside of the developing Client information ecosystem for discovery, analytics and data managementWork with data science team to deploy Machine Learning ModelsYou will be using Data wrangling techniques converting one ""raw"" form into another including data visualization, data aggregation, training a statistical model etc.Work with various relational and non-relational data sources with the target being Azure based SQL Data Warehouse & Cosmos DB repositoriesClean, unify and organize messy and complex data sets for easy access and analysisCreate different levels of abstractions of data depending on analytics needsHands on data preparation activities using the Azure technology stack especially Azure Databricks is highly desiredImplement discovery solutions for high speed data ingestionWork closely with the Data Science team to perform complex analytics and data preparation tasksWork with the Sr. Data Engineers on the team to develop APIsSourcing data from multiple applications, profiling, cleansing and conforming to create master data sets for analytics useUtilize state of the art methods for data manning especially unstructured dataExperience with Complex Data Parsing (Big Data Parser) and Natural Language Processing (NLP) Transforms on Azure a plusDesign solutions for managing highly complex business rules within the Azure ecosystemPerformance tune data loadsSkills Required  · Mid to advanced level knowledge of Python and Pyspark is an absolute must. · Knowledge of Azure, Hadooop 2.0 ecosystems, HDFS, MapReduce, Hive, Pig, sqoop, Mahout, Spark etc. a mustExperience with Web Scraping frameworks (Scrapy or Beautiful Soup or similar)Extensive experience working with Data APIs (Working with RESTful endpoints and/or SOAP)Significant programming experience (with above technologies as well as Java, R and Python on Linux) a mustKnowledge of any commercial distribution like HortonWorks, Cloudera, MapR etc. a mustExcellent working knowledge of relational databases, MySQL, Oracle etc.Experience with Complex Data Parsing (Big Data Parser) a must. Should have worked on XML, JSON and other custom Complex Data Parsing formatsNatural Language Processing (NLP) skills with experience in Apache Solr, Python a plusKnowledge of High-Speed Data Ingestion, Real-Time Data Collection and Streaming is a plusQualifications/ExperienceBachelors in Computer Science or related educational background3-5 years of solid experience in Big Data technologies a must Microsoft Azure certifications a huge plusData visualization tool experience a plusJob Type: ContractExperience:Bigdata: 1 year (Required)Python: 1 year (Required)Pig: 1 year (Preferred)pyspark: 1 year (Required)Hive: 1 year (Preferred)","Township of Warren, NJ",Data Engineer,False
270,"How should you decide when to leave for your next appointment? How will you find a parking structure closest to your destination?
Our team builds solutions which seek to answer these and many other important questions. We are the Maps Predictions and Extensions Team, and we are seeking a team-member to help us innovate and craft the next generation of the Maps experience. We support Apple's iOS, macOS, tvOS and watchOS solutions. Join us to build a new experience that will surprise and delight millions of travelers for every single trip they take.

Key Qualifications
Object-oriented programming and design experience
Strong skills in Objective-C and/or C++
Deep understanding of multi-threaded programming
Experience with Decision Support and Analysis Techniques
Real passion for product quality and attention to detail
Experience translating complex functional and technical requirements into deliverable solutions
Ability to blend multiple concepts together into a cohesive and scalable design
Strong overall systems knowledge is desired
Description
On our team, you will be responsible for building frameworks which make Apple products more personalized and intelligent. You will work closely with designers and engineers across our company to ideate and craft new features which will positively improve our customer's navigation experience.
In this role, you will collaborate with many other Apple team members working on internal and public APIs used by 3rd parties. To meet these demands, you will use your iOS or macOS development skills to contribute to the best-possible application performance solutions possible. Using a variety of tools and techniques along with your aptitude for building user-friendly interfaces, you will develop and support various Maps frameworks and solutions on multiple platforms (iOS, macOS, watchOS, and tvOS). Understanding and developing shared codebases between different operating systems will be a comfortable concept for you.

Education
BS or MS in any Engineering or Science Field or equivalent experience will be considered.

Additional Requirements
While not required, exposure or knowledge of one or more of the following skill-sets would be beneficial:
- Interest in maps or location related technologies
- Experience with Machine Learning technologies or advanced Data Analysis
- Systems programming experience (frameworks/libraries/daemons)
- Knowledge of Applied Research Techniques
- Published an app for consumers or have written production code for a device
Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.","Santa Clara Valley, CA",Predictions and Extensions Data Engineer - Apple Maps,False
271,"The Data Engineer will collect and manage the data we receive from the field including casinos.

Job Responsibilities:
---------------------


Builds and maintains ETL systems
Collects and reviews the data from Native American tribes and makes changes to the data as needed
Gathers, cleans, imports, and manages our performance data
Enhances data collection procedures to include information that is relevant for building analytic systems
Munges the data from a lot of data sources including processing, cleansing, and verifying the integrity of data used for analysis
Builds a roadmap for the tools and processes we will use to manage our data as it grows exponentially
Integrates new sources of data, such as competitive, geographic, weather, and calendar data
20% travel is expected in this role

Qualifications:
---------------


BS in Computer Science or Equivalent degree
Solid knowledge of statistics including familiarity with statistical tests, distributions, maximum likelihood estimators, etc.
Proficiency in using SQL
Previous ETL and data integration experience
Experience with Python or other scripting language
Comfortable in a windows environment
Excellent organizational and communication skills; must be detail oriented
Occasional travel to various locations as needed
Excellent skills in articulating ideas and information to clients in a clear manner
High ethical standards and integrity
Team player with high-performance standards

Preferred Qualifications:
-------------------------


1-3 years of experience as a Data Engineer
Strong customer service/support experience via phone, email, and in-person

","Charlottesville, VA",Data Engineer,False
272,"Dialpad is the cloud based phone system that powers voice, video, and messages all from a single platform. With a beautifully intuitive interface that works on your existing devices, your phone system is finally as adaptable as your team.

Who we are:
At Dialpad, we're a team of do-ers. A team that thinks outside the box and when that doesn't work, we reinvent it. We don't settle for the status quo and neither do the things we build. Led by the same minds behind Google Voice, we build products that get businesses talking—whether it's across the hall, street, or country.

With $120 million in funding from ICONIQ Capital, Google Ventures, Andreessen Horowitz, Scale Ventures and other top VC’s Dialpad attracts top engineers from companies like Microsoft and Google, and every member of our team plays an essential role in creating dynamic products that doesn’t just combine design and mobility but works with you wherever productivity may strike.

About the role:
We're looking for data engineers who are eager to embrace broad aspects of data engineering, from bare metal to data model. You’ll work directly with our Data Science and Backend Engineering teams to build infrastructure and manage the overall performance and reliability of our systems. You’ll design software to process, search, analyze, and store data for applications that are continually growing in scale, while continually optimizing for security and speed. Interested in the science of conversations, data, APIs and scaling a system that supports real-time voice transcription & analysis? Let’s meet!

Environment


Languages: Python
Batch and Stream Processing with Beam, Data Bricks, Dataflow, Kinesis, BigTable
Data Warehousing: BigQuery, Google Storage
Experience building and managing efforts in both batch and streaming data processing pipelines with technology like Beam, Spark, etc.
RDBMS, NoSQL, shared storage, ETLs, queuing systems, Pub/Sub, Message Buses
Operations: cloud providers (i.e. GCloud, AWS, etc.), Docker, orchestration systems (Salt, Puppet, Ansible, etc.), CI/CD Systems (CircleCI, Jenkins, etc.)
Also: Linux, Gi

Required


Strong fundamentals in computer science and software engineering.
Your experience with the tools we use is nice. Your skill with lots of tools is better.
You aren't afraid to switch between building long-term stable systems and scrappy demos when needed.
You measure & monitor everything you build to ensure that it remains stable.
You work well on on siloed as well as collaborative projects.
You want to work at a startup! This means things are going to change quickly, and you’re comfortable with that
You’re committed to personal and professional growth both inside and outside of the workplace, through code reviews, conferences, and exploration

About Us:
Joining our team means collaborating with people that aren’t just passionate about their work but about Argentine tango, musicals, sushi burritos, comic books - you name it. Because if you’re going to redefine the status quo, you need a group of people hungry to do more, to see more, and be more than where they started.

There is no idea too crazy and no task too small — we work together to make things we’re proud of

Compensation & Equity

Teamwork makes the dream work. We recognize that our dedicated team members are what make our success. That’s why we offer competitive salaries in addition to stock options.

Healthcare

An apple a day keeps the doctor away - and it doesn’t hurt that we offer 100% paid medical, dental, and vision plans for you and your dependents.

Reimbursements

We offer a monthly stipend to help cover your cell phone, home internet, and even gym membership costs.

Education

We believe in your future as much as you do! That's why we offer a yearly stipend for continued learning and education expenses.

Office Meals

Bon Appetit! Enjoy catered lunches, free snacks & drinks (both health and unhealthy - no judgment!)

Location, Location, Location

San Francisco San Ramon Austin Raleigh Vancouver Kitchener Tokyo New York. From coast to coast, our offices are nestled in active and growing downtown areas

Dialpad is an equal opportunity employer; we believe in creating a community of inclusion and an environment free from discrimination or harassment","San Francisco, CA 94111 (Financial District area)",Data Engineer,False
273,"Meet CarGurus—the #1 visited online car shopping website in the US. At CarGurus, we're building the world's most trusted and transparent automotive marketplace where it's easy to find great deals from top-rated dealers.

Founded in 2006 by Langley Steinert (co-founder of TripAdvisor), CarGurus is a technology company with a passion for data and its power to simplify every aspect of the car shopping experience. Using proprietary technology, search algorithms and innovative data analytics, we provide unbiased validation on pricing, dealer reputation and vehicle history.

CarGurus is looking to hire a highly motivated Data Engineer to help shape our growing data infrastructure. You will work closely with analysts in all areas of the organization, becoming the expert in crafting data assets, pipelines, and reporting tools. You'll be a founding member of this team, and will be a key participant in its growth. This role is key to helping CarGurus become an insight-driven organization by democratizing our data assets.

What You'll Do:

Create and promote the CarGurus Data Dictionary, a key component of our data analytics strategy. The data dictionary contains the definitions of our core entities, measures, and metrics.
Become a trusted advisor to Product, Marketing, Finance and other areas of the business to define and provide data critical to strategy.
Maintain and expand ETL and reporting tools.
Participate in the design of our overall data collection strategy, including technology, data pipelines, and visualizations.
Build a scalable data insight platform that makes every decision data driven.

Who You Are:

2-4 years experience as a software engineer, data engineer, or related field.
Significant software engineering skills, preferably in Python, Ruby, or similar.
Familiarity with Linux/Unix shell scripting.
Comfortable with using test infrastructure to validate code.
Ambitious team player who thrives in a collaborative environment.
Expert in SQL, with ability to optimize database and query performance.
Familiarity with Redshift, Snowflake, BigQuery, or other large-scale databases a plus.
Familiarity with NoSQL environments a plus.
Detective, with keen interest in solving business problems with data.

CarGurus Culture:
At the core of our company culture is a spirit of innovation, curiosity and collaboration. True to our start-up roots, we're nimble, flexible and hardworking. We have a great respect for testing and learning and a healthy aversion to scheduling meetings to discuss meetings. Lunch is catered daily. Gym membership is free. Foosball and ping pong are played often. Now a publicly-traded company, we're as committed as ever to cultivating the culture that got us here.

In addition to the US, CarGurus operates sites in Canada, the UK and Germany with other markets on the horizon. Our offices are located in Cambridge, MA, Detroit, MI and Dublin, Ireland. If you'd like to learn more, please visit our careers page ( https://careers.cargurus.com/ ).","Cambridge, MA 02138 (West Cambridge area)",Data Engineer,False
274,"Analytics Division- (Philadelphia, PA or Remote) - Full Time

Our Data Engineer will be a data-driven expert but also strategic that can deliver insightful and actionable insights from complex datasets. The Data Engineer will provide data & analytical support to all Elite SEM divisions. Our analytics team is helping big brand ecommerce optimize their marketing stack.

Essential Functions:

Collaborate with division leads to design, implement and deliver insightful analytic solutions for clients.
Build out API integrations for ingesting vendor data
Build out ETL pipelines to push data to reporting and other systems
Prepare customer facing reporting, including working with account managers to determine correct metrics/dimensions

Core Competencies:

Strategic Agility - Sees ahead, can anticipate future consequences and trends, broad knowledge and perspective
Problem Solving - Uses rigorous logic and methods to solve difficult problems with effective solutions
Innovation Management - Brings creative ideas to market, facilitates effective brainstorming, helps the creative process
Customer Focus - Dedicated to meeting and exceeding expectations to customers, establishes and maintains effective relationships with customers
Business Acumen - Knows how businesses work, knowledgeable in current/future policies, practices, trends, technology, etc.
Process Management - Excellent at figuring out the processes to get things done, understands efficient work flows
Learning on the Fly - Learns quickly when facing new problems, relentless learner, open to change, improves, enjoys challenges and finding solutions

Requirements:

1-3 years of professional software development experience
1+ years working with big data sets
Experience with a host of tools including: Google Analytics, Adobe Analytics, Google Tag Manager and tag auditing tools such as ObservePoint.
Strong experience using Postgres/SQL and Version control with GIT
Basic experience using Python, Javascript, and Linux.
Advanced MS Excel skills required
Strong statistical and modelling ability

Preferred Requirements:

ETL experience, API integration and marketing/adtech domain knowledge.

About Elite SEM: Elite SEM is an award-winning digital marketing agency. Founded on search, focused on holistic performance-driven digital marketing, Elite expertise spans Paid Search, SEO, Shopping & Feed, Display Advertising, Paid Social, Conversion Rate Optimization (CRO), and Insight & Analytics. Elite brings a unique business model to the market that ties account performance to account managers, and rewards our teams for continuous performance and satisfied clients with a share of company profits. Elite SEM survives and thrives by the mantra ""Great Lives for Great People"" – this includes employees, clients, partners, vendors, fans and everyone that crosses our paths. Our core values – Love What You Do, Circle of Education, Attitude of Gratitude, and Strive for Greatness – help foster this goal each and every day.

Disclaimer: This description has been designed to indicate the general nature and level of work performed by employees within this position. The actual duties, responsibilities, and qualifications may vary based on assignment or group. Elite SEM is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, veteran status, genetic information, or any other protected status.

FLSA Classification: Exempt

Working Conditions: Working indoors, sitting at a computer for extended periods of time, lifting no more than 10 pounds.","Philadelphia, PA 19107 (City Center East area)",Data Engineer,False
275,"$60 - $65 an hourContractJob SummaryThis person will be responsible for migration of data from existing on perm systems into Cloud Data Warehouse. Individual will be involved in all aspects of data migration including;Data Analysis, Mapping, ETL Development, Reconciliation, Testing and Documentation ,Responsibilities and DutiesResponsible for migrating data into cloud data warehouse solution.Individual will need to meet the requirements set by the business with a focus on data quality, timeliness & group coding standardsWork with other team members to ensure that all objectives and commitments are fulfilled in line with expectations, agreements and standardsMaintain consistency in processes and follow guidelines as laid out by team standardsMonitor and report on progress on tasks assignedUnderstand and apply industry practices, architectural standards and procedures relating to work assignments.Translate business requirements and technical designs into well-developed solutions that meets business data and KPI goals.Designing database queries, views and functions for reporting and data analytics .Design and implement technology best practices, guidelines and repeatable processes.Optimize and refactor SQL databases and database objects; ETL processes, reporting and analytic solutions in support of business needs.Evaluate and assess capabilities of new technologies and Business Intelligence tools as required.Required Experience, Skills and QualificationsStrong experience data migration/integration experienceBachelor’s degree in Computer Science/ Information Technology or related fieldAt least 5 years* experience in data modeling, development, implementation and support of transactional databases and data warehouses preferably TeradataExpert level experience in SQLExperience with Business Intelligence reporting and analytical tools. (Microstrategy, Tableau etc.)Experience working on cloud based platforms. (AWS)Data warehouse experience (AWS redshift, Snowflake Computing)Advanced expertise in performance monitoring and optimization.Strong analytical, critical thinking & problem-solving skillsJob Type: ContractSalary: $60.00 to $65.00 /hourExperience:data modeling, development, implementation: 5 years (Preferred)Education:Bachelor's (Preferred)","Holmdel, NJ",Data Engineer,False
276,"ContractThis is one of the super urgent position. Prefer local candidatesRole: Senior Big Data EngineerDuration: Long Term ContractLocation: NYC, NYSkill Set : Spark, Impala, Scala/Java, HadoopJob Type: Contract","New York, NY",Senior Big Data Engineer,False
277,"Required Experience, Skills and QualificationsTitle:  Data EngineerLocation: Morrisville, NCType: FulltimePreferred Qualifications: - 5+ years of related experience is required.- A BS or Masters degree in Computer Science or related technical discipline is required- ETL experience with data integration to support data marts, extracts and reporting- Experience connecting to varied data sources- Excellent SQL coding experience with performance optimization for data queries.- Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.- Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions.- Experience in cloud-based ETL development processes.- Experience in deployment and maintenance of ETL Jobs.- Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.- Has strong technical background and remains evergreen with technology and industry developments.Job Type: Full-timeEducation:Bachelor's (Preferred)Work authorization:United States (Required)","Morrisville, NC",Data Engineer,False
278,"DUTIES AND RESPONSIBILITIES:
As a member of cross-functional project teams, work with partners to build data services suitable for data analysis, machine learning, and visualization.
Develop analysis programs and visualization code based on specifications from data scientists.
Assist in data management and metadata capture.
Train users on systems and provide basic support.
Prepare technical reports and make presentations to project teams, leadership, and other stakeholders.


EDUCATION AND EXPERIENCE:
MS (2+ years’ experience) or BS (3+ years’ experience) in physics, mathematics, statistics, biology, engineering, bioinformatics, computer science or a related field.
Strong computer science, database, and programming background.


TECHNICAL SKILLS:
Hands-on experience in developing and implementing data services and data analysis pipelines.
Programming in Python & R.
Database design, querying, and performance optimization.
API development, workflow management.
Experience with visualization tools is desired.
Experience with cluster and queue management systems is a plus.
Web site development is a plus.
Software development skills (functional vs object oriented design, version control, modular design).


DESIRED KEY COMPETENCIES:
Ability to understand and execute on the company’s mission and values.
Maintain a high degree of ethical standard and trustworthiness.
Capable of fostering change in an organization.
Deals with conflict in a direct, positive manner.
Ability to think and adapt to a rapidly changing environment.
Able to reach rational conclusions through complex processing of information.
Fosters innovation through creative solutions.
Successful at communicating in both oral and written forms.
Fosters constructive dialogue and feelings toward the company, coworkers, and tasks being managed.
Well organized and capable of clear communication through technology (i.e. Outlook, PowerPoint, and other programs used to create and distribute reports and key information).
Effective organization and implementation of group projects.
Up-to-date knowledge on industry current events.
Maintain a high degree of accuracy and attention to detail.
Energized by accomplishments and excellence in the workplace.
Capable of high performance in independent work as well as in team setting.
Understanding eukaryotic and prokaryotic biological systems including molecular and cellular biology.
Ability to effectively communicate complex data and analysis to audiences with diverse technical backgrounds.
Effectively portrays analysis conclusions in a graphical and/or interactive format.
EOE MFDV","South San Francisco, CA",Data Engineer,False
279,"The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company.

About Us
Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “ The Rise of Data Engineering ”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large.

A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world.

We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog , which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to.

About the Job
We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others.

We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers.

As a data engineer, you will:
Run and support a production enterprise data platform
Design and develop data models
Work with languages like Java, Python, Go, Bash, and SQL
Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub
Develop processes for automating, testing, and deploying your work

About You
To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be.

Benefits and Perks:
Make an impact by supporting our original, independent and deeply reported journalism.
We provide competitive health, dental, vision and life insurance for employees and their families
We support responsible retirement planning with a generous 401(k) company match.
We offer a competitive parental-leave policy.
We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.
We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.
Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.

#LI-HK1

The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.

The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics.","New York, NY","Data Engineer, Various Data Engineering Teams",False
280,"Job Description

Minimum Qualifications:

Bachelor's degree in Computer Science, Software Engineering or other technical degree
3+ years experience developing business critical big data solutions, including data-modeling, data architecture, data platform development and optimization for same.
3+ years architecting & building Business Intelligence/ OLAP solutions using SQL or similar
Familiarity in one or more scripting languages (eg. Python) and UNIX.
Self starter who can meet critical deadlines in a fast paced environment with little direction and guidance

Preferred Qualifications:

Advanced degree in a technical field.
Experience working with web-scale databases and building scalable data pipelines capable of aggregating and processing millions of events per day on Cloud Platforms.
Experience with implementing CRM domain usecases (sales, marketing, customers)

Keyskills - Must Have

Data Analytics
SQL
Unix

Keyskills - Nice to Have

Python
CRM

","Sunnyvale, CA",Data Engineer,False
281,"ServiceTitan is looking for a Data Engineer to join our data team. Strong candidates will have solid command of the SQL, ETL operations, and SSIS packages in addition to very strong analytical and critical thinking skills. The most critical portion of the role is deconstructing and re-interpreting databases from other software to translate them into our database. It also includes support of existing, and development of new automated import methods. The candidate will work closely with cross departmental resources to ensure on time delivery of transferred data for customers adoption of ServiceTitan.

Our Data Engineers receive projects and deadlines from several departments. The ability to own tasks, clearly communicate requirements, delays, completion and prioritize a constantly shifting schedule of deadlines is paramount.

This work is highly critical to successfully building our company. It is the lynchpin of the sales and onboarding process. Data concerns are a blocker for most of our customers, so delivering high quality, highly accurate transfers is absolutely crucial and greatly appreciated by both our customers and our company.

What you'll do


Map data from various legacy databases into the ServiceTitan platform, subsequently developing SQL scripts that will extract the information efficiently and accurately
Develop automated scripts to validate legacy database values and identify previously unmapped fields prior to loading them into the ServiceTitan platform
Receive and incorporate feedback from customers and internal stakeholders on data import quality into previously developed extraction scripts
Identify opportunities to leverage information from legacy databases into the implementation process to avoid inquiring for additional information from customers
Given the experimental nature of this job, we will require very tight compliance when it comes to data - we need to focus on learning
Establish quality working relationships with internal stakeholders
Contribute material input to go/no-go/continue decisions upon test completion

What you'll need


2-5 years of experience with SQL Server 2008/2012/2014/2016
Advanced knowledge and experience in T-SQL, complex ETL tools and operations, and SSIS
Expert level understanding of database and data model concepts
Vertical SaaS experience is highly desirable
Results and solution oriented - we want to know how we can win, not why we can't
Ability to work independently and cross functionally

About ServiceTitan

ServiceTitan is a mobile, cloud-based software platform that helps home services companies streamline operations, improve customer service, and grow their business. ServiceTitan's end-to-end solution for the multi-billion dollar residential home services industry includes CRM, intelligent dispatch, custom reporting, marketing automation, mobile solution for field techs, and accounting integration with Intacct and QuickBooks. ServiceTitan brings a fully operational modern SaaS infrastructure to an industry traditionally underserved by software. ServiceTitan is the preferred software for hundreds of the world's most successful plumbing, HVAC, and electrical companies. For more information about ServiceTitan, visit www.ServiceTitan.com ( http://www.servicetitan.com ).

Los Angeles Business Journal Best Places to Work 2018
Inc 5000 Best Workplaces 2018
Inc 5000 America's Fastest Growing Companies 2018
Mogul Top 1000 Companies Worldwide for Millennial Women 2018
Glassdoor/Battery Ventures Cloud Computing Companies 2018
Forbes Next Billion Dollar Startup 2017","Los Angeles, CA",Data Engineer,False
282,"The iTunes Store is looking for a top-notch Big Data engineer to develop an analytics infrastructure that will generate insights into customer experiences on products such as the iTunes Store, App Store, and iBookstore. Our products reach hundreds of millions of customers around the world, and have revolutionized how people interact with their music, movies, TV shows, apps, books, and podcasts.

Key Qualifications
Language: Java or Scala
Working knowledge on the following distributed data processing platforms:
Required: Spark, Hadoop
Great if you also know: HBase, Kafka, Java Map Reduce
Algorithms: You will be working on developing new algorithms to process large scale data efficiently. We expect you to know:
Basic Computer Science algorithms and Data Structures
Distributed Algorithms to process and mine data, e.g. Map Reduce Algorithm
Great but not required if you also know about how to develop: Graph, Data classification and clustering algorithms in distributed environment
Good debugging, critical thinking, and communication skills
Knowledge in engineering machine learning, feature engineering systems is a plus.
Able to gather cross-functional requirements and translate them into practical engineering tasks
5+ years of programming experience
Description
The iTunes Store Analytics team is responsible for collecting, analyzing, and reporting on customer experience data. From this data we generate insights into how customers interact with our products, and use these insights to drive improvements to user-facing features.
You will be working on a small team and will be responsible for processing large amounts of data and developing platforms to process, analyze and mine that data to extract intelligence. Prepare data for visualization, ad-hoc exploration, reporting, and further analysis. We are looking for a well-rounded data engineer who has good design sense.
The ideal candidate pays close attention to details - caring about the quality of the input data as well as how the processed data is ultimately interpreted and used. You are also a team player - ready to contribute during design sessions, and able to give and receive constructive code reviews. Your curiosity drives you to explore new technologies and apply creative solutions to problems.

EducationBS degree in Computer Science or a related field

Additional Requirements
• Build large scale data processing, mining and analysis projects and features, ensuring robust & maintainable solutions are implemented with special attention to data quality, performance and usability details.
• Effectively demonstrate feature prototypes to executives
• Develop, advocate for, and build consensus on, coding best practices.
• Ability to effectively work with cross functional teams to understand requirements and identify design and engineering impacts
• Experience with architecting big data and analytical applications that scale to petabytes highly preferred.","Santa Clara Valley, CA","Big Data Engineer, Apple Media Products Analytics",False
283,"----------------
Role Description
----------------

In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a ""maintain existing platform"" or ""make minor tweaks to current code base"" kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!

This position is open in the following offices: San Francisco, CA; Mountain View, CA

----------------
Responsibilities
----------------


You will help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models
You will help define/design data integrations, data quality frameworks and design/evaluate open source/vendor tools for data lineage
You will work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture

------------
Requirements
------------


BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale

------------------
Benefits and Perks
------------------


100% company paid individual medical, dental, & vision insurance coverage
401k + company match
Market competitive total compensation package
Free Dropbox space for your friends and family
Wellness Reimbursement
Generous vacation policy
10 company paid holidays
Volunteer time off
Company sponsored tech talks (technology and other relevant professional topics)

","San Francisco, CA",Data Engineer,False
284,"Job Description
The Marketplace Tech Platform team is looking for a talented Analytics Engineer to help build/enhance the platforms that manage Amazon's Global Marketplace business. You will play a key role in driving business insights about our businesses and how they interact. You will transform data into actionable information and make it readily accessible to worldwide stakeholders.

As an Amazon Data Engineer you will be working with one of the world's largest and most complex data processing environments. You will need expertise in the design, creation, management, and business use of extremely large (100TB+) datasets. You must have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You must be able to work with business customers in understanding the business requirements and implementing reporting solutions. Above all you should be passionate about working with huge data sets to answer challenging, business changing questions as a global scale.
Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
3+ years of industry experience in data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
3+ years in using OLAP technologies and BI Analytics.
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficient in one scripting languages (e.g., Python)
Preferred Qualifications
Experience writing machine learning algorithms
Experience using big data technologies (Hadoop, Hive, Hbase, Spark etc.)
Experience working with AWS big data technologies (Redshift, S3, EMR)
Familiarity with statistical models and data mining algorithms
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy","Detroit, MI",Analytics Data Engineer,False
285,"Short Description

About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in over 40 countries. The Group reported 2017 global revenues of EUR 12.8 billion.
Visit us at www.capgemini.com. People matter, results count.
Job Title: Big Data Engineer
Job Type: Full Time
We are looking for a Data Engineer to design and develop consumer-centric low latency analytic applications leveraging Big Data technologies for our Enterprise Data Lake initiative.
Essential Responsibilities:
Design and develop cutting edge Analytic applications leveraging Big Data technologies: Hadoop, NoSQL, and In-memory Data Grids.
Build automation of deployment and configuration using open source frameworks.
Act as the subject matter expert for Big Data platforms and technologies.
Work across IT teams to ensure code quality, performance and scalability of deployed data products.
Perform other duties and/or special projects as assigned.
Qualifications/Requirements:
Bachelor's degree with minimum 2 years of IT experience in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics) Or in lieu of Degree, a High School Diploma/GED and 5 years of experience in quantitative field with programming (Java/J2EE) and data services.
Disclaimer: Capgemini America Inc and its U.S. affiliates are EEO/AA employers. Capgemini conducts all employment-related activities without regard to race, religion, color, national origin, age, sex, marital status, sexual orientation, gender identity/expression, disability, citizenship status, genetics, or status as a Vietnam-era, special disabled and other covered veteran status. Click the following link for more information on your rights as an Applicant: http://www.capgemini.com/resources/equal",Illinois,Big Data Engineer,False
286,"Our vision is to bring more innovation, efficiency, and equality of opportunity to the world by creating an open financial system. Our first step on that journey is making digital currency accessible and approachable for everyone. To achieve that, it is critical to have timely and reliable access to all of our data, from user clicks on our website down to blockchain transactions.
As a Data Platform Engineer, you will build our next generation data platform and accompanying services. Our data pipelines are growing rapidly, currently processing several terabytes of data from production databases and external providers to our data warehouse. We build foundational self-service systems that allow end users to create ETL flows and consume data in batch and streaming fashion for machine learning, fraud prevention, A/B testing and analytics purposes.
Responsibilities:
Data ingestion pipeline: Build our next generation streaming ingestion pipeline for scale (10x data), speed (<1 minute of lag), and ease of use (<1 hour to add a new source). Read from a variety of upstream systems (MongoDB, Postgres, DynamoDB, MySQL, API) in both batch and streaming fashion (tail MongoDB’s oplog and Postgres’ WAL). Today we do this with Apache Airflow, Hadoop, Spark and a pure Kotlin service.
Self-service transformation engine: Build and maintain our self-service tooling that allows anybody at Coinbase to transform complex JSON and create dimensional models. Specific challenges are supporting type 2 slowly changing dimensions, end-to-end testability, validation/monitoring/alerting and efficient execution. Today we do this with Apache Airflow.
Anomaly detection: Build a comprehensive anomaly detection service that allows anybody at Coinbase to quickly set up notifications in order to detect process breakage.
Security: build a security layer that authorizes data access at the row/column level. Build a logging and auditing system in order to surface suspicious data access patterns.
REQUIREMENTS:
Exhibit our core cultural values: positive energy, clear communication, efficient execution, continuous learning
Experience building (data) backend systems at scale with parallel/distributed compute
Experience building microservices
Experience with Python and/or Java/Scala
Knowledge of SQL
A data-oriented mindset
PREFERRED (NOT REQUIRED):
Computer Science or related engineering degree
Deep knowledge of Apache Airflow, Spark, Hadoop, Hive, Kafka/Kinesis
WHAT TO SEND:
A resume that describes scalable systems you’ve built","San Francisco, CA",Data Engineer,False
287,"Expedia

We are looking for talented software engineers to join our data services development team. Your experience matters, but more meaningful to us is what you can do going forward. If you are technically talented and have the tenacity to build upon your current skill set, then we want to talk to you. If you have both a willingness and dream to work in a dynamic environment and is able to apply Agile methodologies in day-to-day activities and is a self-motivated developer who mentors and shares knowledge.

Experience in:
Hands on experience working on Hadoop for 3 years or more
Deep understanding of the internals of core Hadoop components - YARN, HDFS, Sparx and MapReduce
Good knowledge of SQL, noSQL, and ETL concepts
Good hands-on knowledge of Python, Shell or any similar scripting language
Hands on Experience on HIVE, OOZIE, PIG and Data Modeling

Duties and Responsibilities:
Translates technical specifications into code for new or improvement projects for internal clients.
Elevates code into the development, test, and Production environments on schedule. Provides follow up Production support. Submits change control requests and documents.
Participates in design, code, and test Inspections throughout life cycle to identify issues.
Participates in systems analysis activities, including system requirements analysis and definition (e.g., prototyping), and logical and physical design.
Writes the system/technical portion of assigned work, including the Version Description Document. Assists technical team members with the system/technical portion of their work (e.g.,systems testers, test plans).
Aligns with IT policies and procedures, especially those for quality and productivity standards that enable the team to meet established achievements.
Participates in special projects and performs other duties as assigned.

Qualifications:
Undergraduate degree in a related field or the equivalent combination of training and experience
Strong analysis, analytical skills and social skills


LI-MS1

Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.","Bellevue, WA 98004 (Downtown area)",Data Engineer,False
288,"Data Engineer
The Data Services team is seeking a Data Engineer with a passion for creating data products to help create more engaging, personalized experiences for users across Conde Nast’s properties. You will have the opportunity to build both product features and internal data tools, all while working with a diverse group of datasets – web events, ad streams, content and context models, etc. You will also get to work with the newest data technologies available. Above all, you will influence how users interact with Conde Nast’s industry-leading journalism.
Primary Responsibilities

Develop and maintain scalable data pipelines, with a focus on writing clean, fault-tolerant code
Maintain various data stores and distributed systems, such as Hive and Presto
Optimize data structures for efficient querying of those systems
Collaborate with internal and external data sources to ensure integrations are accurate, scalable and maintainable
Collaborate with data science team on implementing machine learning algorithms to facilitate audience intelligence and cross-brand personalization initiatives
Collaborate with business intelligence/analytics teams on data mart optimizations, query tuning and database designs
Execute proof of concepts to assess strategic opportunities and future data extraction and integration capabilities
Define data models, publish metadata, and best practice querying standards
Required Skills

2+ years data engineering and/or software development experience, preferably with experience using a scripting language such as Python or Java
Fluency in SQL (any variant)
Experience with Hadoop and related technologies (Hive, Presto, Spark)
Exceptional analytical, quantitative, problem-solving, and critical thinking skills
Have a collaborative work style with strong desire to work in dynamic, fast paced environment that requires flexibility and ability to manage multiple priorities
Desirable Skills

Experience with workflow / ETL tools and schedulers; e.g. Luigi, Airfow
Experience with AWS tools; especially EMR, S3, Lambda
Experience with GCP tools; e.g. BigQuery, DataFlow, PubSub
Experience with Apache Beam","New York, NY 10007 (Financial District area)",Data Engineer II,False
289,"Temporary, InternshipOVERVIEW
At Sentry, we realize that getting valuable, hands-on experience is important for you and your career. We don’t offer just an internship; we offer a whole experience built to give talented students like you a head start for your career. From rewarding work in your business area to professional development trainings all the way to paid volunteer time and diverse social events, we make sure our internship program fits all of your needs.
WHAT YOU'LL DO
Our Data Engineering Internship offers opportunities to work hands-on with some of Sentry's most significant data assets. You will gain valuable, real-life experience working with Sentry’s developers and architects to create systems to transform data into structures that facilitate critical business processes, present information for in-depth analysis, and drive advanced, predictive analytics. You will get first-hand exposure to the entire data development lifecycle and find out what it takes to build successful, large-scale data ecosystems.
In addition, you’ll:
Design, code, test, and debug while maintaining programsParticipate in prototyping solutions, preparing test scripts, and conducting tests and for data replication, extraction, loading, cleansing, and data modeling for data repositoriesUtilize and adhere to Sentry’s Solution Development Life Cycle (SDLC) project management methodologyAdhere to Sentry Information Technology standards and proceduresBecome technically proficient in programming languages, software, and other technological areas
WHAT IT TAKES
In order to be eligible for this elite opportunity you must meet the following criteria:
Pursuing an undergraduate degree in Computer Science, Computer Engineering, MIS, or a related fieldObtain a minimum in-major GPA of 3.0Graduating in Fall of 2019 or Spring of 2020Ability to work in a team atmosphereSentry does not offer employment in this position to holders of F-1, J-1, and H-1 Visas for the purpose of obtaining practical experience
WHAT YOU'LL RECEIVE
Sentry's excellent benefits package is designed to meet today's most important needs. Benefits for full-time Sentry Insurance employees include:
Competitive Compensation
Group Medical, Dental, Vision and Life
401 (K) plan with a dollar-for-dollar match on your first eight percent
Comprehensive paid training
Generous Paid-Time Off Plan
Pretax Dependent Care and Health Expense Reimbursement Accounts
HOW YOU’LL APPLY
Sentry Insurance has an online employment application. In order to complete it, you need to apply for a specific position. We ask that you apply for one position at a time with us; so if you are interested in several positions, please determine the position in which you are most interested and apply for that position first. If you are not selected for your first choice, we invite you to apply for the next job in which you are interested.
If this is the first time you have applied for a position at Sentry, you will be asked to register. Returning applicants will only need to provide their email address and password.

WHO YOU’LL WANT TO CONTACT
Hannah Krueger at 715/346-6281
HANNAH.KRUEGER@SENTRY.COM
ABOUT SENTRY
Sentry Insurance is one of the largest and strongest mutual insurance companies in the United States, holding an A+ (superior) rating from A.M. Best. The company and its subsidiaries sell property and casualty insurance, life insurance, annuities and retirement programs for business and individuals throughout the country. Headquartered in Stevens Point, Wisconsin, Sentry employs more than 4,000 associates in 41 states. A complete list of underwriting companies can be found at www.sentry.com.
EQUAL EMPLOYMENT OPPORTUNITY
It is our policy that there be no discrimination in employment based on race, color, national origin, religion, sex, disability, age, marital status, or sexual orientation.","Stevens Point, WI 54482",Data Engineer Internship - Summer 2019,False
290,"About RevolutionParts:
RevolutionParts is a rapidly-growing, 50+ employee SaaS company based in sunny Arizona. We’re a tech company dedicated to modernizing the auto industry with a revolutionary eCommerce platform that’s already helped hundreds of dealerships nationwide sell auto parts online.

We’re seeking talented individuals who can adapt to a fast-paced environment. Join our team to work with a down-to-earth group of people who stick to our core values and genuinely care about providing a quality experience to customers and employees alike!

About the Role:
RevolutionParts is looking for a talented Data Engineer with passion and drive for customer success and to help evolve our technology to support our next phase of growth. Your primary focus will be on improving customer business outcomes through optimization and automation of data operations. Strong verbal and written communications as well as troubleshooting skills are critical for success in this role. You'll be joining a team of professionals that are dedicated to providing cutting edge e-commerce solutions to the auto parts industry.

If you want a clear-cut role with the same tasks for years, this is not for you. If you like to think big and want to help drive how we leverage data at the company, we need you!

What you’ll do:

Creates processes to streamline data loading process, increasing performance, reducing effort, and increasing data quality
Communicates with customers and internal team to discuss any issues with received data and helps them identify and fix data issues
Understand existing data environment, variations of implementation and develop effective triage mechanisms and tools
Establish baselines for key metrics and drives improvement of those metrics
Analyze data trends to identify issues
Work with team to ensure data for all brands is accurate and up-to-date
Oversee data processing
Create and maintain supporting documentation related to the management of the parts data and catalog
Track and manage projects using Scrum framework

What you need:

Bachelors in Computer Science or Computer Engineering or related field
5+ years experience building queries, creating reports, running data infrastructure.
Understand what is involved in processing structured data from ingestion to production
Understanding of databases, relational or not. SQL.
Have an in depth knowledge of ACES and PIES data standards
Understand how structured data sells more parts
Understand how to improve eCommerce conversion through data
Architecture decision making - real-time, batch, queue-based processing
Must like the dynamic and fast-paced nature of a smaller team working in a company doubling in size each year.
GSD - Get Stuff Done

Why you’ll like working here:

Top salary and stock options
Medical, dental, and 401k retirement plan
Rewards for high-performers (opportunity for bonuses!)
Opportunity for career advancement
Collaborative team environment that values multiple perspectives and fresh thinking
Flexible hours and PTO
Casual dress code
Free food! Catered lunch every Friday + fully stocked fridge and snacks
Drinks on us with team happy hours and beer fridge
Gym reimbursement

","Phoenix, AZ 85034 (Central City area)",Data Engineer,False
291,"Under Armour is the chosen brand of this generation of athletes... and the athletes of tomorrow. We're about performance - in training and on game day, in blistering heat and bitter cold. Whatever the conditions, whatever the sport, Under Armour delivers the advantage athletes have come to demand.
That demand has created an environment of growth. An environment where building a great team is vital. An environment where doing whatever it takes is the baseline and going above and beyond to protect the Brand is commonplace.
The world's hungriest athletes live by a code, a pledge to themselves and everyone else: Protect This House... I Will. Our goal is to Build A Great Team! Will YOU…Protect This House?!

SUMMARY:
Under Armour is searching for a committed, talented, and high-energy Data Engineer III and technical lead to join a results-oriented team. This position provides the opportunity to grow into a technology expert, leading the development of new capabilities in ETL (extract, transform, load), data applications, and data cleansing. The role will include extracting data, developing integration and load programs, enhancing existing development, and unit testing.

ESSENTIAL DUTIES AND RESPONSIBILITIES:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Collaborate with appropriate data owners to identify and map data from the source environment to target data environment. This data may include SAP and non-SAP data sources. Technical development activities including:
Develop data extraction programs
Develop interfaces between UA systems and third-party systems
Write code or use specialized development tools to create, enhance or customize software components
Test developed programs and integration of data from various sources
Ensure that development adheres to organizational architecture guidelines
Identify data quality gaps and work with data owners to develop solutions and close gaps

On-going service delivery including:
Documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification)
Coordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team
Engage system owners to filter, size and prioritize business requests and drive towards appropriate decision points
Contribute to development policies, standards, and conventions
Collaborate with peers to enable quality service for business community, project schedules, and support IT performance metrics
Maintain expert knowledge of development tools, technologies and related delivery methods
Establish consistent technical architecture

EDUCATION AND/OR EXPERIENCE:
Bachelor's degree STEM (Science, Technology, Engineering, Math)
At least 5 years of experience with assorted data management tools (ETL, Data Quality, etc.)
At least 5 years of experience with industry standard relational database systems (MS SQL Server, Oracle, etc.)
Experience using SAP Data Services
Experience with SAP software systems, both master and transactional data
Experience with job scheduling tools such as Control-M
At Under Armour, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. Under Armour believes that diversity and inclusion among our teammates is critical to our success as a global company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool.","Baltimore, MD",Data Engineer,False
292,"Overview
Software and Data Engineer
At Hospital for Special Surgery our clinicians and scientists collaborate to deliver the most innovative care. Our specialized focus on orthopedics and rheumatology enables us to help patients get back to what they need and love to do reliably and efficiently. Our patients are overwhelmingly satisfied with the care they receive at our facilities. When you join us, you will become part of this legacy of commitment to the most cutting-edge research and coordinated care.
The Software and Data Engineer will be responsible for the implementation, security and maintenance of the Data Analytics Platform. The ideal candidate must be passionate about the field of analytics and well versed in all aspects of an enterprise-wide analytics architecture. The software and data engineer will work closely with the Interface Team and Infrastructure and Server Team. The engineer support and collaborate with our data engineers, researchers, report writers and data analysts.

Responsibilities
Implement, secure and maintain the advanced data analytics platform
Implement, secure and maintain the front-end interface to platform
Design, implement and automate data flows to and from the platform
Work with partners and vendors on data integration projects
Create data models for analytics applications
Assist in the Data Warehouse ETL design and implementation
Assist in resolution of production issues and root cause analyses
Technical Qualifications and Experience
3+ years with Python analytics platform, Python notebooks and Data Science libraries
3+ years’ experience with ETL tools: SSIS (preferred), Informatica, Talend
3+ years’ experience with RESTFul APIs and Web Services: JSON and XML
3+ years’ experience with RDMS databases: SQL Server, Postgres, Oracle
Strong expertise in SQL scripting required
Strong expertise in Python scripting required
Experience integrating large datasets, many 100Gbs to several Terabytes required
Experience with Linux and Windows Server Operating Systems required
Strong expertise in a programming language such as Java/Scala or C# a big plus
Experience with web front scripting such Javascript, HTML and CSS a big plus
Experience supporting researchers in bioinformatics a big plus
Experience with statistics, machine learning and deploying predictive models a big plus (R, Python, Matlab, Spark)
Experience with SAS, administration and SAS language a plus
Experience with public cloud a plus: AWS, Google Cloud, Azure
Experience with Big Data technology a plus: Hadoop, Spark, AWS EMR, Hive, etc.
Experience in Healthcare a plus
Elements of success
Passionate about data
Enjoy learning and exploring new technologies
Problem solver and critical thinker
Team player, committed to improving quality and service and maintaining a spirit of cooperation and respect
Education and Certifications
Master’s degree or bachelor’s in computer science
Other Requirements
#LI-JL1","New York, NY",Data Engineer,False
293,"*******************************
Global problems. Global impact.
*******************************

Flexport's mission is to make global trade easy for everyone. We are re-making a trillion dollar industry that touches every person on Earth, which means solving some of the global community's most complex challenges. We are looking for makers who love learning, are passionate about collaborating, and desire to build solutions with a global impact.

Reliable and scaleable data pipelines and data warehouse are integral in allowing us to fulfill our mission of fixing the user experience in global trade. At Flexport you'll provide data for products that are at the forefront of reshaping the entire logistics & supply chain industries. You'll work alongside self-starters interested in solving real-world problems and streamlining the inefficiencies in the complex global trade industry.

You'll have the opportunity to reshape an industry by creating the new operating system for global trade. Started in 2013, with a total addressable market of $1 trillion, we've raised $304M from investors that include Google Ventures, First Round Capital, Bloomberg Beta, Y Combinator and more.

********************
What you'll do
********************


Develop resilient pipelines from a variety of data sources, both internal and external
Implement Comprehensive Testing and Continuous Integration frameworks for schema, data, and functional processes/pipelines
Design the scheduling framework (Airflow) to automatically recognize and set-up object dependencies for robust and timely data delivery
Be part of a close-knit data engineering team that ships new code every day

**********************
What you'll need
**********************


5+ years experience developing data frameworks using Python
5+ years of using SQL for data manipulation in a fast-paced work environment
Experience with Airflow is strongly preferred
Data Warehousing development and fundamentals preferred
Passion for business-oriented data development

*********************
What you'll get
*********************


The opportunity to shape data engineering at agile startup
Meaningful equity in a startup growing their business 20% every month
Feature ownership - design, implement and own significant portions of data infrastructure

*********
Our Stack
*********

Our pipelines are built using Python and SQL, glued with bash. Data Warehouse is a combination of Postgres and Redshift. All of our infrastructure is based on AWS. Our BI data is only 15 minutes behind production servers.

********************
Culture & Values
********************


Learn more at www.keyvalues.com/flexport ( https://www.keyvalues.com/flexport )

","San Francisco, CA",Data Engineer,False
294,"What if you could have it all? A smart and passionate team to work with, innovative games to work on, and an environment that's collaborative and transparent. Join us at Tilting Point as a Data Engineer in New York City.

As part of this team, you will work on the collecting, storing, processing, and analyzing huge sets of data. The primary focus will be to oversee the construction and maintenance of our data pipeline, ETL processes and data warehouse. Our Senior Data Engineer will also be responsible for data quality and understanding the data needs our various internal departments in order to anticipate and scale our systems.

Your Future at TPM


Develop an AWS Cloud platform that will support Big Data processing and analytics
Work with data analysts, product managers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value
Liaise with third parties for data integration and validation
Implementing, maintaining and improving ETL/DDL processes
Monitoring performance and costs; and advising any necessary infrastructure changes
Working with the various internal teams to deliver various analytical needs by building both large systematic reports and small custom pieces
Become a Tilting Point key holder of data quality

Your XP & Skills

Need to Have:

2+ years of experience in architecting, building and maintaining software platforms and large-scale data infrastructures in a commercial or open source environment
Prior experience using and building on Apache Spark / Databricks
Must have expertise in at least 2 of the following: Go, Python/R, SQL, Java/C++
Experience with integration of data from multiple data sources, knowledge of various ETL techniques and frameworks
Comfortable with AWS cloud (S3, EC2, EMR, Redshift, etc.)

Nice to Have:

Knowledge of existing third party video game analytics services is a plus
Ability to collaborate with colleagues across different disciplines/locations
Strong written and oral communication skills and ability to work with creative partners
Love for video games
Experience in a free-to-play mobile game company a plus

Choose Tilting Point For


Prominent role in making tech decisions that will shape the company and industry
Involvement in a large range of mobile game titles
Work with great people on great games that reach millions of people each month
Excellent location with a rooftop patio in midtown NYC
Free snack and beverages
Robust perks & benefits

( https://www.glassdoor.com/Reviews/Tilting-Point-Reviews-E1370330.htm )


Direct applicants only - No agencies/headhunters please.*

Tilting Point provides equal employment opportunity to all individuals regardless of their race, color, creed, religion, gender, age, sexual orientation, national origin, disability, veteran status, or any other characteristic protected by state, federal, or local law. Discrimination of any type will not be tolerated. This policy applies to all terms and conditions of employment, including recruiting, hiring, promotion, termination, time off, and compensation.","New York, NY",Data Engineer - Spark,False
295,"Job Description

As a search engine data engineer on our team you will be working on developing, maintaining, testing and optimizing the search engine that powers the Merchant Cloud platform. You will have to participate in various automated search related activities like:Maintenance of an ElasticSearch cluster in the cloud (update versions, monitor performance, etc.)Optimize cluster configuration and usagePlanning, allocating, recovering and migrating shardsCluster recovery and geo-replicationSchema creation, optimization and maintenanceCreating, monitoring and constantly improving metrics to track quality of search resultsDevelop and maintain the ETL procedure that ingests data in ElasticSearchDeveloping identical search capabilities in multiple languages (English, Japanese and others to be added over time)Supporting our support engineers with ElasticSearch, Logstash, and Kibana
Requirements
2+ years of experience with ElasticSearch (or related technologies like Solr or Lucene)Experience in developing and maintaining a search engine serviceExperience in developing and maintaining search related data modelsExperience in data manipulation for ElasticSearch ingestionExperience in working with different deployment environmentsAn open mind; desire to learn the best language/technology to solve a given problemAutonomous and responsible; organized and structured in initiatives and work;Detail-oriented and able to keep a global vision of the issues and their solutionsProficient with Linux

Qualifications

BS in Computer Science or related technical field or equivalent practical experience5+ years of industry experience
Preferred Qualifications
Knowledge and experience in performing search query expansionKnowledge and experience in search ranking optimizationKnowledge and experience in using ElasticSearch for auto-completion tasksKnowledge of and experience with the Elastic stack
Additional Information

All your information will be kept confidential according to EEO guidelines.","San Mateo, CA",Search Engine Data Engineer,False
296,"Description:
JOIN US AS A PRINCIPAL DATA ENGINEER - PERSONALIZATION
As a Principal Data Engineer, you’ll have the opportunity to design and architect high quality, flexible, manageable and performant systems and services. You’ll architect solutions that will capture, manage, process and serve small as well as some of the largest data sets in retail technology. The Principal Data Engineer works with business partners to provide technical solutions for business problems in a fast paced environment. Responsibilities will include analyzing, designing, programing, debugging, modifying software for existing and new products used in distributed, large-scale analytics solutions for Target.
We’re looking for a highly motivated engineering professional who will partner with Technical Product Owners and management at one end as well as lead and motivate highly skilled Scrum dev teams on the other – while having fun along the way!
Key Responsibilities:
Lead decision-making process for selection of software products and architecture solutions
Develop software systems using test driven development employing continuous integration practices
Mentor and partner with engineers to develop software that meets business needs
Follow agile methodology for software development and technical documentation
Innovate constantly and keep systems up to date with current technologies
Requirements
MS or PhD degree in Computer Science or area of study related to data sciences and data mining
10+ years of experience in developing software applications at scale
3+ years of experience with Big Data technologies - Spark, Druid, Hive and Apex
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Advanced understand application/software development and design
Collaborative personality, able to engage in interactive discussions with the rest of the team
Inquisitive on Big Data technology as well as stay current on new ideas and tools – constant learner
Qualifications:","Sunnyvale, CA",Principal Data Engineer - Personalization,False
297,"RoadBotics is a VC backed company based in Pittsburgh, PA that monitors and manages our world’s roadways by identifying and rating a wide array of important roadway features and conditions, including cracks, potholes, signage and other characteristics. Our AI technology was spun out of the Carnegie Mellon University Robotics Institute with the explicit goal of providing efficient and cost-effective roadway transparency to those responsible for roadway infrastructure. Our technology turns any smartphone and car into a sophisticated, mobile sensor.
With our product, we attach a smartphone to a dash mount in a car windshield, giving the phone’s camera a full view of the road ahead. The data is uploaded to our AI-driven cloud platform to be analyzed using our proprietary deep learning technology. The video data, along with the smartphone’s other sensor capabilities, allow us to precisely calculate a wide variety of vital and established roadway metrics and conditions. The RoadBotics RoadWay web platform provides a map-based visualization giving roadway managers a comprehensive status of their roads.
Go to: goo.gl/FpZU3S and goo.gl/itqPGJ for some news on our company. Learn more at roadbotics.com and visit demo.roadbotics.com to see the live demo.
The Data Engineering role will be responsible for the design, implementation, and improvements to RoadBotics data systems and pipelines. This involves data ingestion pipelines from smartphones in the field deployed all over the world, data storage and manipulation for future access, as well as APIs and analytics-ready database developments.
The RoadBotics video and geospatial data must be analyzed, queried, and delivered to a the internal (Data Science, Operations, Engineering Divisions) as well as external (Customers) in a variety of formats and with varying reliability requirements. This role involves managing these pipelines and data warehouses to ensure successful access and control of that data.
Positions responsibilities:
Design, development and management of data pipelines and data storage systems.
Clear communication with company divisions to understand data requirements and deploying APIs and tools for efficient retrieval and utilization of that information
Developing and executing a clear data infrastructure plan for the RoadBotics data systems
Working closely with other engineering teams to develop robust and reliable tools
Supporting the RoadBotics data science effort by efficiently developing data pipelines by delivering high quality and analytics-ready data sets
Requirements include:
3-5+ years of job experience in a software or database development role
Must be fluent in Python
Must be proficient and have working experience with algorithms and distributed design challenges and data structures
Must be proficient and have working experience with API design
Must be proficient and have working experience with modern scalable databases, their design challenges, and implementations
Must be proficient and have working experience with development and deployment related technologies: Git, Docker
Must be proficient and have working experience with a modern cloud environment (GCP, AWS or Azure - Preferrably GCP)
Must be knowledge of modern batch and streaming pipeline systems (Apache Beam, Dataflow etc.)
Must be proficient and knowledge of database schemas in both SQL and schema-less based database architectures
Must be proficient with modern object stores (Google Storage, S3, etc.)
Successful Candidates will also:
Have experience with Data Science and Data Science related practices
Have knowledge and experience working with geospatial data and techniques
Be Very comfortable learning and using new techniques
Have an ambition towards strong personal growth. As our company continues to rapidly grow, we are looking for future leaders who can take ownership and get things done
Have superb communication skills: we are a small start-up company working in a novel market. We need people who understand complex business needs and can translate them into working solutions
Be able to clearly articulate problems and their solutions before they become critical
Be Biased toward action, and have strong initiative and personal drive
This is a full-time position in our Pittsburgh, PA offices.","Pittsburgh, PA 15206 (Larimer area)",Data Engineer,False
298,"Who we are
Cityblock Health is a new type of healthcare company ( https://www.cityblock.com/ ), operating out of Brooklyn and backed by Alphabet's Sidewalk Labs, along with some of the top healthcare investors in the country.

Our mission is to radically improve the health of urban communities, one block at a time. Importantly, our solutions are designed specifically for Medicaid and lower-income Medicare beneficiaries, and we bring the capability to deliver care in the home and neighborhood with our field-based teams.

In close collaboration with community-based organizations and leading commercial partners, we are reorganizing the health system to focus on what matters to our members. We deliver personalized primary care, behavioral health, and social services through a network of neighborhood hubs with deep community-based partnerships and world-class technology.

We partner with payers and at-risk providers, accepting capitated financial risk to care for their most vulnerable, high and rising risk members. Additionally, we invest in strategic partnerships with community-based and social services organizations, in some instances bringing them into the value equation through sub-capitation and performance-based compensation.

Over the next year we'll grow quickly, including entering new markets, each with their own commercial relationships and field-based teams. This role will be a key contributor to the success of our business.

The role
We are looking for a data engineer to help build and maintain the data infrastructure that backs Commons—our digital care management platform. Commons enables our clinical teams to engage with patients, collect structured data about medical, behavioral and social needs, and develop personalized care plans that drive good health. Commons will allow our care operations to scale to eventually support hundreds of thousands of patients in cities across the country.

As one of our first data engineers, you will help build Mixer—our data pipeline. You will work closely and cross-functionally with your peers from the product, data, and clinical teams to collect and structure data—both from primary and 3rd party data sources— analyze that data, and make it useful in promoting the medical and social well-being of our patients through automation and decision-support capabilities.

Mixer involves a collection of services that run on Google Cloud Platform, including PubSub, Dataflow (via Scio ( https://github.com/spotify/scio )), Datastore, and BigQuery. These services are provided by Scala, Ruby, and Elixir.

If you're inspired by such a challenge and are an amazing teammate, we'd love to hear from you!

You will:

Be responsible for the integrity and accuracy of data in our systems.
Focus on building our data pipeline, i.e. the backend features and related data infrastructure that will store, clean and transform data—making it useful to our application and reporting layers.
Develop secure integrations into the data systems of various community and health system partners (e.g. EHRs, HIEs, ADT feeds, claims/pre-auth feeds, predictive analytics as well as data sets from government services and community-based organizations).
Design our data pipeline to scale up to handle integrations across hundreds/thousands of partners
Write clean, well-tested, code that will stand the test of time.
Participate in creating and maintaining strict compliance, data privacy and security measures.
Help recruit highly capable engineers to the team from diverse backgrounds.
Learn continuously via mentorship (and on your own!) to upgrade your skills and thinking as an engineer.
Collaborate with clinicians and social workers to understand the challenges that they face and develop solutions to drive better care for patients.

You'd be a good fit if:

You are excited at the prospect making messy data useful.
You have done data engineering work like this before in healthcare.
You enjoy doing whatever it takes to execute on complex projects with little guidance.
You have 3+ years experience writing production code.
You have worked in fast-moving startup environments before.
You enjoy taking initiative.
You have a process-oriented mindset and ability to execute.
You have a passion for doing mission-oriented work.

Nice to have:

Experience with the technologies in our stack.
Experience deploying models created by data scientists.
Familiarity with BI tools (e.g. Looker, Tableau, etc).
Experience building user-facing features.
Previous exposure to clinical operations and/or working with physicians.

""Nice to have"" really means ""nice to have"". It's completely possible that you don't have any of these and are still a great fit for the team.

You should include these in your application:

A resume and/or LinkedIn profile.
A cover letter including a one paragraph summary of a technical project you're most proud to have built and what was hard about it. Optionally include links to public-facing documentation about the project such as a Github repo or blog post, if available.

Cityblock values diversity as a core tenet of the work we do and populations we serve. We are an equal opportunity employer, indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.","New York, NY",Data Engineer,False
299,"InternshipIntern - Data Engineer / Data Sciences Developer (
Job Number:
 043033 )
hours per week : 40
Description
 Summer 2019 Paid Internship

Zions Bancorporation is currently accepting resumes for our Data Engineer Internship position. The intern will have the opportunity to:
Work with other developers and data scientists to code proof-of-concept projects on large scale data sets
Assist in developing data processing and system integration applications
Assist in constructing web based user interfaces and visualizations
Document design decisions, code and work flows
Assist in designing, developing, and testing ETL applications consistent with application architecture guidelines.
Qualifications
Preferred candidate will be pursuing a degree in Computer Science, Software Engineering or Computer Engineering.
Strong analytical, organizational and problem solving skills
Ability to elicit, gather and analyze user requirements
Ability to work independently and provide updates to management
Requires strong interpersonal skills
Must be able to meet deadlines
Technical Knowledge in the following is preferred:
Programming languages, including R, TCL, JAVA, Ruby, and Python
SQL and NoSQL data stores and solutions
Knowledge of Big Data Technologies (e.g Apache Hadoop, Spark)
Work Locations
 Utah-Salt Lake City-Zions Bancorporation - HDQTRS
Business Operations
Sep 12, 2018","Salt Lake City, UT","Intern, Data Engineer / Data Sciences Developer",False
300,"About Aaptiv

Aaptiv is the fastest growing mobile fitness product on the market with a community of nearly 200,000 members and is backed by leading venture capital firms and top companies, including the Amazon Alexa Fund and Disney.

With a mission to empower everyone to live a healthier life, Aaptiv has transformed the way people exercise and train through its innovative audio fitness classes. Every Aaptiv class combines the guiding voice of an expert Aaptiv trainer with motivating music by top artists in every genre. Aaptiv members have unlimited, on-demand access to over 2,500 classes and structured programs across every type of exercise and a wide variety of activities, including running, strength-training, yoga, indoor cycling, meditation, and more.

Want to join Team Aaptiv? We're looking for team members who are passionate about building a world-class fitness experience. There are over 80 million Americans who value fitness - and we believe every one of them should be an Aaptiv user.

About the role

We’re looking for a Senior Data Engineer to help us design data access patterns, optimize integrations, and solve data problems. At Aaptiv, we thrive on building data driven solutions to help our members find workouts that best fit their lifestyles to pursue their wellness goals.

In this role, you’ll be working with a team of Data Engineers tasked with data integrations, explorations and enabling stakeholders to find actionable data in a timely manner. You will spend majority of your time working in the code whether that’s building new data pipeline yourself, pairing with team members or reviewing pull requests. The remaining time will be spent mentoring other Software Engineers to apply best practices in schema design, optimize data access patterns, and enforce data standards. You will coordinate and execute online schema changes, debug production issues and perform root cause analysis should things not work as expected.

In addition, you will work collaboratively with other data engineers, cross functional teams, analysts and stakeholders, and your input will have a large impact on how our product is built. You will thrive to explore new and better ways to do things. Last but not least, you will ensure the team is delivering high quality codes with test coverage, streamline deployment process and respond to incidents as appropriate. Candidates at this level should be comfortable in all of those facets.

What You’ll Do


Provide technical guidance in designing data warehouse architecture, schemas and pipelines that accurately represent our business model, and store data efficiently to enable fast application access and easy reporting.
Enforce data precision, implement guidelines and support software engineers in creating software that access and store data in the most optimal way while generating reliable data.
Coordinate with software engineers to deploy online schema changes with minimal customer impact.
Work closely with product managers, other data engineers and analysts to identify data gaps, brainstorm ideas, break down projects and estimate how long they will take.
Part of a team that’s responsible for developing of all data products, ensuring we are shipping only high quality codes that are tested thoroughly and monitored appropriately.
Be a champion of engineering best practices within your team. We want to build high-quality software that’s easy to understand, easy to change and works the way it’s supposed to.

Who You Are


6+ years of experience building data solutions or software as an engineer
You’ve administered data warehouse in production environment
You’ve previously held a more senior engineering position for longer than a year
Expert in SQL, data modeling
Expert in data warehouse performance analysis and optimizations
Strong Python programming skills
Experience in creating custom data integration tools to extract/load/transform data
Experience in building data products with unit and integration tests
Experience with both SQL and noSQL datastores (MongoDB)
Experience using AWS services (RDS, EC2, Data Pipeline, Lambda) is a plus
You keep up to date with advances in data solutions and selectively choose new tools and approaches when appropriate to be more effective.

","New York, NY",Senior Data Engineer,False
301,"About Vydia:Vydia (vydia.com) is a fast growing, Inc. 500 technology company that has built a premiere platform for the exploding video content industry. Our solutions empower music creators to easily distribute, manage, and optimize their video and audio content through one centralized platform. The company is viewed as one of the leaders in the space as evidenced by strategic partnerships and integrations with Vevo, YouTube, Facebook, Spotify, Apple, Dailymotion and several more. The vision of the company is to disrupt the entertainment industry through automation, data, and artificial intelligence that helps both creators and the next generation of music distribution. Vydia is constantly evolving and has attracted more than 200,000 musicians, influencers, and brands worldwide. The company has recently completed Series A financing, poising it to continue the exponential growth it has achieved since its inception.Summary: Vydia’s Data Engineering & Data Science efforts are an integral part of our success. Through our data, we nurture our artists, creators, partners, and internal users with insights to help them engage their audience.Our Extract-Load-Transform workflows promote quick analysis and richer, complex investigations all at once, and our data warehouse supports both data science and analytics. Of all the terabytes of data we gather, one-third is well-structured, with the remaining being mostly semi-structured and some wholly unstructured. And, it currently doubles every 5.3 months!The Role: As a Sr. Data Engineer, you will own Vydia’s multitude of data pipelines. You will design and implement our ELT workflows, which originate at partner APIs and conclude in the Warehouse. You will work closely with our data science, BI, and product teams in figuring out current and future needs.As a leader on the Data team, your responsibilities will include: Ensuring the availability and timely delivery of data, company-wideModeling new data sets and crafting all new ELT workflows and pipelinesLead the orchestration of the workflows and contribute strongly to infrastructure decisionsImproving on and monitoring of existing pipelines and oversight of our ELT workflowMaintaining a single version of truth for our data and working with others to implement continuous integration (CI) data quality testsMentoring and guiding your junior colleagues and leading with vision, and with respect to the company’s data strategyTechnologies: While we are not married to any tool or technology we also look for those intimately familiar with Python and have previous experience using Airflow, Docker and Kubernetes.We use AWS S3 & EC2 extensively, our current DW is on Redshift and our app relies mostly on Postgres. We use Looker in-house for BI and Product Engineers work mostly in Ruby and Python, while our data scientists work in R.About You: We want to learn more about you. If you feel like you are the right fit for this role, let us know.In our mind, being a perfect fit means that you have the necessary hard skills and expertise, and the complimentary soft skills.You are a Python pro and have regularly used AWS or Google Cloud Platform to manage data and move it between applicationsDo you love APIs? When you encounter a new one, do you study it inside and out and learn every corner of it, as though you designed it yourself?Working with deeply-nested, complex JSON is a fun day at the office for you.You can articulate the merits and pitfalls of the different approaches in designing a pipeline.You are passionate about data quality control and know how and where to anticipate potential errors.Working “in the cloud” is not a point of distinction for you, it is a given.You understand what it means to work at a tech startup. Hopefully, this is what excites you more than anything else about working here.You intuitively know how to extract value and insights from data.You love the idea of building the data scene in NJ and being a leader in this community.You have orchestrated workflows using Airflow and are familiar with the challenges and how to overcome them.Critically, you're a person who thinks in data; you relate the real world to data, and vice-versa. You understand that data is not the end goal but a vehicle to help get us where we are going, and you see your role as the person most critical in making that happen.Reasons to work with us: As an Inc 500 Fastest Growing Company in America Vydia offers huge opportunities to grow with the company.Vydia was named a Best Place to Work in NJ by NJBIZ in 2017 due to its collaborative, fast-paced, and fun, thriving environmentFull medical/dental/vision packageGenerous vacation policy; work hard and take time when you need itTeam breakfast every Monday and a well-stocked kitchen full of snacksAn on-site gym facility and team yoga on ThursdaysOpen, creative workspace located inside the historic site of NJ innovation, Bell WorksLeadership identified as the 2017 Tech Innovator of the year and highlighted by Entrepreneur Magazine as builder of one of the most Entrepreneurial companies in America.We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.Job Type: Full-timeExperience:Kubernetes: 1 year (Required)Docker: 1 year (Required)Python: 5 years (Required)AWS or GCP: 5 years (Required)Location:Holmdel, NJ (Required)Work authorization:United States (Required)","Holmdel, NJ",Senior Data Engineer,False
302,"Gathering the business requirements. Analyzing business requirements and defining functional specifications, Analyzing existing applications consisting of multiple batch programs in SAS and developing the code in Python and Spark, Designing rules and workflow processing using python and Spark frame work, Identifying and developing automated batch packages, Participating in the deployment of the applications into existing systems and databases, Documenting modifications and enhancements made to the applications, systems and databases as required by the project.

Educational And Experience Requirements :
Must require at least a bachelor’s degree in Electrical Engineering or related field of study.
Location: Edison, NJ

CV to I5Tech Address: 3 Ethel Rd, Suite#306, Edison, NJ -08817 or
E-Mail: careers@i5techinc.com","Edison, NJ",Data Engineer,False
303,"ContractData Warehouse EngineerLocation: Seattle, WAJob Type: ContractVisas: US Citizen, Green Card Holder, H1BNO OPT/CPT candidatesCurrent state of Migration project: o Has a T-SQL database which houses several SQL views that contain business logic, creating calculated metrics/dimensionso QlikView Data Mover files source data from these views, and store data in QVDs for consumption by our Qlik Dashboardso Migrating from T-SQL to the Snowflake engine, necessitating rewrite of all SQL views into Snowflake's syntax and revision of QlikView Data Mover script files.o Scope of transition work is approximately 110 SQL views and 50 QVDMsRequired Skills: o Strong capabilities in SQL scripting language + best practiceso Familiar with SQL database navigation with tools like SSMSo Familiar with Qlik scripting languagePreferred Skills: o Prior experience with T-SQL scriptingo Prior experience with Snowflake's scripting languageJob Responsibilities: o Review SQL Views and develop candidate scripts in Snowflake scripting languageo Identify/troubleshoot View dependencies if encounteredo Update QlikView Data Mover load scripts to reference new field names and sources in Snowflakeo Query-test data from Snowflake to ensure output of resultsJob Type: ContractExperience:T-SQL: 3 years (Preferred)Snowflake: 1 year (Preferred)Data Warehouse: 5 years (Preferred)ETL: 3 years (Preferred)QlikView: 2 years (Preferred)SQL: 3 years (Preferred)","Seattle, WA",Data Engineer,False
304,"Job Summary

At Revinate, we are revolutionizing the hotel experience for both operators and guests by unlocking the power of data intelligently and intuitively to drive revenue. The Data Engineering & Analytics team is the nucleus that drives the innovation and advancement of the company's core mission and vision for the hospitality industry. We are looking for an experienced Data Engineer to join our team in San Francisco. The data engineer will be integral in building, maintaining, monitoring and improving our data pipelines and analytics platform at scale. We are committed to delivering highly available, scalable, fault-tolerant and performance-efficient data solutions. The ideal candidate will be a hands-on expert in architecting a data ecosystem that can fulfill the data needs of our internal and external customers in real time. You will be required to design and develop code, scripts and data tools that process large volumes of structured and unstructured data from multiple disparate data sources. Our engineers work in a collaborative environment and learn from the best among each other.

Job Responsibilities


Architect, Develop and Own key components of the data platform including the deployment and maintenance of complex ETLs, scripts and custom code.
Adhere to disciplined best practices to produce reusable, maintainable, efficient and well-documented code on time every time.
Analyze/Troubleshoot existing issues and bottlenecks in the system and propose technically creative solutions and implement to completion.
Evangelize data products across the organization to enhance visibility and transparency.
Communicate timelines and status across teams effectively using productivity metrics.

Qualifications


Masters or higher degree in Engineering or Computer Science disciplines. Background in Economics, Mathematics or Statistics will be a plus.
7+ years of hands-on industry experience using big data tools like Spark, Kafka and related.
Working experience with NoSQL and distributed databases (Cassandra, Hive, Google BigQuery or similar)
Hands on experience with REST APIs and functional programming tools (Scala or similar).
AWS Cloud experience with EC2, S3, VPC, RDS, etc in a big data environment.
Deep understanding of MapReduce methodologies and tools.
Data Warehouse architecture and modeling experience.
Experience with administering and optimizing Business Intelligence/Data Visualization tools (Looker, Tableau, R Studio or similar).
Self starter and motivated individual who is able to think objectively about abstract concepts and willing to question the status quo to make the necessary improvements.

Nice to Have


Scripting languages like Python, Node.js or similar.
Knowledge of source control and automated deployment tools (github, Jenkins, etc)
Implementation of monitoring and auditing processes.
Experience working on agile teams in a startup setting.
Experience developing Machine Learning algorithms is a plus.

About Revinate

Revinate enables hoteliers to transform their guest data into revenue. With Revinate Marketing and Revinate Guest Feedback, hoteliers are empowered to make smarter decisions, resulting in increased direct revenue and guest engagement. The company is backed by leading Silicon Valley investors, including Benchmark Capital, Tenaya Capital, and Sozo Ventures. Headquartered in San Francisco with regional offices in Amsterdam and Singapore, Revinate counts tens of thousands of the world's leading hotels as customers.

To learn more, please visit www.revinate.com ( http://www.revinate.com/ )

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","San Francisco, CA 94129",Data Engineer,False
305,"Company Overview: We provide services to help healthcare institutions in Southeastern portion of the United States. Mitivate successfully manages the complexities of finance, care evaluation, and patient conditions. Mitivate's data analysis tools evaluate patient data, claims data, & clinically-generated data to identify clinical variations and financial inconsistencies. In short, our goal is to provide revolutionary analytical services, to help the healthcare ecosystem become more efficient. Mitivate provides automated technology services to the many manual tasks currently produced today, to provide our clients a faster service in a streamlined manner.Job Overview: As a Data Engineer, you have a passion for the value and capability of data and information. At Mitivate, we are looking for a savvy data engineer to join team of analytics experts. The hire will be responsible forproviding technical expertise on data storage, data mining, and data cleansing. The ideal candidate is an experienced data engineer that is self-directed and comfortable supporting the data needs of multiple teams, systems and products. Because we work on the cutting edge of a lot of technologies, we need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively. This person would also take on the following responsibilities:Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, etc.).Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Work closely with our engineering team to integrate your amazing innovations and algorithms into our production systems.Process unstructured data into a form suitable for analysis – and then do the analysis. Support business decisions with ad hoc analysis as needed.Strong knowledge of and experience with statistics; potentially other advanced math as well. Programming experience, ideally in Python or JavaDevelop tools that provide customers with unprecedented transparency into their operationsWrite SQL queries to support product implementationsRequired Skills: Providing technical expertise on data storage, data mining, and data cleansing.Supporting initiatives for data integrity and normalization.Loading, cleaning, and manipulating data from multiple sources including internal, external, and third parties.Accessing and analyzing rich healthcare data to generate insights and make proactive recommendations.Formulating success metrics for optimizing healthcare resources and patient experience, creating visualization to monitor themJob Type: Full-timeExperience:Data Mining: 3 years (Preferred)Python: 3 years (Required)SQL: 3 years (Preferred)Java: 3 years (Preferred)Library Outreach: 2 years (Preferred)Education:Bachelor's (Required)Location:Atlanta, GA 30309 (Preferred)","Atlanta, GA 30309 (Midtown area)",Data Engineer,False
306,"We're reimagining sports and technology.
DraftKings is bringing sports fans closer to the games they love and becoming an essential part of their experience in the process. An industry pioneer since our founding in 2012, we believe we can continue to define what it means to be a tech company in sports entertainment. We love what we do and we think you will too.

Love data? We do too.
As a Data Engineer, you'll be a creative contributor to our data analysis and scalability processes, and you will use your experience to provide key insights that help us make smarter decisions. Analytical thinking drives our business and when you join our team, you'll not only solve new problems every day, you'll see your data solutions immediately improve our users' experience.

What you'll do as a Data Engineer:

You'll build tools to provide actionable insights into key business metrics.
Care about agility like you care about scalability. We roll out products very quickly and priority management is key.
Design processes that support data transformation and structures, metadata, dependency and workload management.
You will also focus on performance analysis, optimization, and tuning.
Have the opportunity to see your personal work make an immediate impact on influential products.

What skills you will use:

Ideally, you have experience in aspects of business intelligence and data engineering, including data warehousing, delivery, and operations.
The ability to build and optimize data pipelines, transformations, architectures, and data sets.
A deep knowledge of a variety of data engines like SQL Server, MySQL, and Redshift.
You'll also have a solid understanding of dimensional modeling.
Experience working in AWS, EMR, and Spark is a big plus.

Who are we a good fit for?
We love working with talented people but more than that, we seek out compassionate co-workers with a collaborative spirit. Our work moves quickly and we're great at coming together to find creative solutions to some of tech's most interesting problems. If that sounds good to you, join us.

Apply now
We're proud to believe that your gender, race, nationality, religion, sexual orientation, status as a protected veteran, or status as an individual with a disability should have nothing to do with our hiring practices. We'll never discriminate against anyone's background or creed. If you're good at what you do, we want you to do it at DraftKings.","Boston, MA 02110 (Central area)",Data Engineer,False
307,"We are UMG, the Universal Music Group. We are the world’s leading music company. In everything we do, we are committed to artistry, innovation and entrepreneurship. We own and operate a broad array of businesses engaged in recorded music, music publishing, merchandising, and audiovisual content in more than 60 countries. We identify and develop recording artists and songwriters, and we produce, distribute and promote the most critically acclaimed and commercially successful music to delight and entertain fans around the world.
We have opportunities for data engineers to join our growing tech team at Universal’s offices and contribute to the development of our innovative approach to finding new and impactful insights to grow UMG’s business.
We have moved all of our Data and Analytics to Google Cloud Platform (GCP). As a data engineer, you will be responsible for designing and implementing new GCP-based data solutions – new data processing, data sets and systems to support various advanced analytics needs. This involves working with the existing engineering team, data scientists, analysts and the business to understand requirements and data needs and definitions, all the while thinking creatively about what data can be best exploited to solve a wide array of business problems. You will create data flows to integrate with multiple external sources using APIs, database connections and flat files. You will liaise with members of the wider Universal Data & Analytics teams to ensure alignment with existing systems and consistency with internal standards and best practice.
Job Functions:
Build understanding of data sources and downstream systems
Liaise with key stakeholders to understand requirements, business definitions and the potential value of different data
Design and document and implement suitable solutions for loading, piping and exposing data from multiple sources
Design and build well-engineered data systems to support analytical needs using Google Cloud Platform (Cloud DataFlow, BigQuery, BigTable are musts)
Assure accuracy of data processing and outputs through consistently high software development skills, adherence to best practice, thorough testing and peer reviews
Habitually approach problem solving with creativity and resourcefulness; carefully evaluate risks and determine correct courses of action when completing tasks
Job Requirements:
Skills/Abilities:
Expertise using cloud-based systems and services to acquire and deliver data via APIs and flat files
Demonstrable, hands-on professional software development skills using Java. Python is a plus.
Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions
Demonstrated willingness and ability to effectively work with various team members when gathering requirements, delivering solutions, and eliciting suggestions and feedback
Extremely quick learner both in terms of new technical skills and acquiring domain knowledge
Experience:
Demonstrable professional experience designing, building, and maintaining data systems and processes using cloud-based platforms (Google BigQuery and Cloud DataFlow extremely desirable; AirFlow is a big plus), including experience working in Unix/Linux operating systems and tools.
Extensive hand-on experience working with data using SQL
Education:
Bachelor’s Degree in Computer Science or closely related discipline
Universal Music Group is an Equal Opportunity Employer.
Disclaimer
This job description only provides an overview of job responsibilities that are subject to change.","Woodland Hills, CA 91367",Data Engineer,False
308,"Upstart is a fast-growing and profitable company revolutionizing how people access credit. We use data science and automation to make smarter lending decisions and help borrowers access credit easier and faster. Join our engineering community as a Data Engineer, and help Upstart build infrastructure to move and analyze large volumes of data. Our success depends on our ability to efficiently make sense of data, and as a Data Engineer at Upstart, you'll be responsible for architecting systems to move, store, transform, and analyze large amounts of data, including financial and business data. You'll have an outsized impact on the productivity of the entire engineering and data science team and directly contribute to Upstart's core competitive advantage.

Here is more about what you'll be doing:

Build data pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Manage and extend a reliable, effective, and scalable data infrastructure
Partner with Data Scientists and other business stakeholders to meet their data requirements
Implement systems for monitoring data quality and consistency
Productionize machine learning models that power our data products and integrate with our loan origination platform
Help the Data Science team apply and generalize statistical and econometric models on large datasets
Identify inefficiencies, optimize processes and data flows and make recommendations for improvements

Requirements:

3+ years of experience as Data Engineer or Software Engineer
Proficiency in Python or Java
Experience with streaming Kafka, ETL for service oriented architectures
Ability to collaborate cross-functionally and communicate effectively
Ability to bring new ideas and promote process improvement

","San Carlos, CA",Data Engineer,False
310,"Data Engineer

Centerfield develops intelligent Big Data driven marketing and sales technology utilizing real time biddable media (RTB), automated call routing and customized scripting. Our proprietary platform, Dugout, combined with our 1500 person sales and retention center delivers new customers at scale to many of the leading brands worldwide.
We’re looking for highly motivated, rock star, Data Engineers with 1-2 years of professional experience. You will help design, maintain, support, and improve company ETL processes. You must have practical experience working with large data sets in the website lead generation & search engine marketing, SaaS, or cloud computing domains.
Skills/Attributes Required
1+ years working in a Data Engineer, or BI Engineer, or Data Warehousing Engineer role
Hands-on experience with ETL tool like Talend, or SSIS, or Informatica
Strong experience with SQL
Some experience in performance tuning techniques
Strong sense of ownership
Proactive team player with good communication skills
Analytical mindset
Skills/Attributes Desired
Experience building reports and with data visualization
Hands-on experience with Tableau
Hands-on experience with Talend
Experience with data services in AWS is a plus
Why Centerfield?
Competitive salary, and profit sharing bonus
401K match
Take a break when you need it – unlimited PTO
Award winning culture & unprecedented team spirit
Fully stocked break rooms with drinks, snacks
Toro the Bull Dog Mascot and Everyone’s Friend
Paid volunteer days","Los Angeles, CA 90094",Data Engineer,False
311,"InternshipWant to work for a company where your everyday projects can meaningfully change or save people’s lives? Where you have the support and cutting-edge resources you need to be creative and challenge the norm? Are you looking for an enriching internship with an adaptive and innovative technology company located in the heart of San Diego’s booming tech hub?
We are looking for passionate Data Science/ Data Engineering Interns with fresh ideas and strong technical skills to join us as we solve the emerging big data challenges in the healthcare industry.
This 10-12 week paid internship will offer you the opportunity to work alongside industry pioneers and healthcare experts to develop a sophisticated, distributed, and scalable healthcare data and machine learning infrastructure.
Your experience at CliniComp will expose you to the foundational concepts of healthcare data science and give you a bird’s-eye view of the latest developments in the industry. On a day-to-day basis, you will be leveraging open source technologies, such as PyTorch/ TensorFlow/ Keras, Kafka, ElasticSearch, Spark, etc. to analyze a large volume of real world clinical data and discover new actionable intelligence used to save lives around the world.
About CliniComp:
CliniComp, Intl. is a global provider of electronic hardware, software and support solutions that has maintained an unrivaled track record of performance and reliability in complex high acuity hospital environments for decades. This integrated offering guarantees a seamless transparent patient record viewing/charting across all patient environments, distinct sites, and time. CliniComp has managed over 600 million real world patient records and provided virtually zero down time since 1983.
We are a San Diego based company made up of inspired people that form a team as strong as each of its individuals. We design, develop, and implement cutting-edge technology that contributes to fundamental advancements in computer science. As technology advances, we adapt and evolve so that we remain at the forefront of our industry and continue to provide unrivaled performance and reliability.
We strive to find self-motivated team players; problem-solvers who thrive in a fast-paced, agile environment, who love innovative technology as much as creating it. If you’re smart, creative, ambitious, and always looking for ways to improve, we’d like to talk with you.
Requirements
Minimum Qualifications:
Currently enrolled in a Master’s or PhD degree program in Computer Science or related field with an expected graduation date between December 2019 and June 2021.
Strong background in one of the following: Computer Science/Engineering, AI/Machine Learning or Bioinformatics.
Extensive knowledge and experience with CNN and RNN (GRU/LTSM), C/C++ and Python.
Ability to think outside the box and present new ideas to enhance and improve current processes.
Must be available to work 40 hours a week for 3 months during Summer 2019 (May-September).
Preferred Qualifications:
Industry experience in designing and developing professional software.
Demonstrated progressive interest and development in areas such as Neural Information Processing Systems, Computer Vision and Pattern Recognition, Knowledge Discovery and Data Mining or a related field through publications and/or research projects.
Experience with CUDA/MIOpen
Excellent communication and interpersonal skills.
CliniComp, Intl. is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. We comply with the Americans with Disabilities Act and consider reasonable accommodation measures that may be necessary for eligible applicants/employees to perform essential functions. EEO/AA/M/F/Veteran/Disabled","San Diego, CA",Data Scientist/ Data Engineer Intern,False
312,"Company Overview
Niche helps people find where they belong. Every month, millions of people use Niche to choose where to live, work, and go to school. We have in-depth reviews, rankings, and statistics on every college, K-12 school, and neighborhood in the U.S. and millions of monthly website visitors. Located in Pittsburgh, we're quickly becoming one of the largest websites in the U.S.
Job Summary
We are looking for a highly motivated Data Engineer. We want someone who loves designing database solutions. This person will utilize their skill set to improve upon SQL database flows, build processes to work with back end applications, and make maintaining our databases easier.
Responsibilities
Create and maintain SQL stored procedures, triggers, and functions to support tech requirements
Design data infrastructure that can be utilize by data analysts, back end engineers, and non-tech employees
Organize data in a way that optimizes querying
Work collaboratively with team members to improve upon existing practices
Write technical documentation


Required Qualifications
1-3 years relevant experience
Bachelor’s degree in Information Technology, Computer Science or related field
Strong knowledge of SQL and/or PostgreSQL databases
Strong programming skills including complex SQL programming, stored procedures and performance tuning
Experience designing database solutions to move and transform information to different teams and services
Hard worker who wants to learn and grow with the position and company
Passionate about all things data
Ability to work independently in a fast paced environment and handle multiple projects concurrently


Preferred Qualifications
Experience with geospatial and boundary data
Basic knowledge of R
Basic understanding of ETL processes
Experience building a data warehouse

Princpals Only. No recruiters or agencies, please.
Niche will only employ those who are legally authorized to work in the United States without sponsorship now or in the future for this opening.
Benefits and Perks
This is a full time, salaried position with competitive compensation and benefits including 20 paid days off per year, paid parental leave, stock options, SIMPLE IRA, and a comprehensive health plan, including vision and dental, provided at no cost to the employee. Niche is committed to the local community and offers three paid volunteering days per year. We’ve also been ranked as one of the Best Places to Work in Southwestern PA by the Pittsburgh Business Times. EOE.","Pittsburgh, PA 15232 (Shadyside area)",Associate Data Engineer,False
313,"PeerIQ is transforming the way lending and securitization markets work. Meeting the needs across the credit funding cycle - from loan purchasing to financing to securitization - we work with industry leaders to unlock capital at scale. We aim to bridge the gap between originators and the capital markets so that investors can invest with confidence. Our employees come from the technology and financial sectors, combining the best of both to change the game of consumer credit.
PeerIQ is looking to hire a software engineer passionate about data, data-driven insights, and building products designed to improve financial services and transform capital markets. You will be at the forefront of green field development in delivering our data and analytics platform, working closely with our product, research, and capital markets teams on business-driven development.
The ideal candidate is a self-starting engineer who enjoys working with data (analysis, transformations, analytics) with experience in multiple back end, front end, and visualization technologies.
Previous successful candidates for this role, even if lacking financial product experience, have shown themselves to be passionate developers through projects they have built from scratch for work and personal satisfaction.
RESPONSIBILITIES
Work closely with insights/analytics to build highly configurable, scalable, robust data processing infrastructure and applications
Implement and productionize data pipelines and automation to support product and business needs
Work closely with our product, data, research, and capital markets leads on data retrieval and analysis, as well as prototyping and iterative development
QUALIFICATIONS
B.S. in Computer Science, Engineering, Math, or equivalent experience
2-4 years of software engineering experience with focus on data analytics
Proficiency in Python and knowledge of REST API frameworks (Flask, Django a plus), workflow tools (Airflow)
Experience in other languages (Java, Scala, Go, R) is preferred
Strong relational and distributed database experience, familiarity with SQL is critical
Experience developing and building distributed and scalable ETL processes and workflows
Experience with automated build and continuous integration testing tools (jenkins or similar) and continuous deployment tools (kubernetes or similar)
Proven critical thinking and analytical problem-solving skills
Desire to learn more about the consumer credit ecosystem and how capital markets affect everyday consumers
BONUS QUALIFICATIONS
Experience with big data (spark, presto, hive) and stream technologies (Kafka, MQ)
Familiar with linux bash, dockerizing applications
Financial Services experience","New York, NY 10010 (Gramercy area)",Data Engineer,False
314,"Job Description
We are looking for a talented Business Intelligence Engineer (BIE) who is passionate about using data to drive critical business decisions for Amazon Studios. As a BI Engineer working on the Amazon Studios analytics team, you will be developing and supporting the analytic technologies to give our internal customers flexible and structured access to data that is timely, accurate and actionable. Moreover, you will partner with senior leaders throughout the organization to drive insights and measurable results.

The ideal candidate will not only be passionate about working with big data sets, but also be comfortable developing key analytical frameworks. In addition, this person will have strong business acumen, experience in developing reporting and analytical infrastructures, exemplary communication skills, an ability to work effectively with cross functional teams, and an ability to work in a fast paced and ever-changing environment.
Basic Qualifications
BA/BS in Computer Science, Engineering, Statistics, Mathematics or related fieldDemonstrated strength in SQL, data modeling, ETL development, and data warehousingExperience with AWS technologies including Redshift, RDS, S3, EMR2-3+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planningAdvanced skills in Excel as well as any data visualization tools like Tableau or similar BI tools (familiarity with Tableau preferred)Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required.Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.An ability and interest in working in a fast-paced and rapidly-changing environment
Preferred Qualifications
MBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related fieldExperience working in very large data warehouse environmentsExperience with Python and/or R2-3+ years of experience in a data engineer or BIE role with a technology companyExperience conducting large scale data analysis to support business decision makingStrong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.","Santa Monica, CA",Business Intelligence Engineer,False
315,"ContractJob SummaryPosition: Big Data EngineerLocation: Dallas, TXDuration: Long Term (It can be 1 or 2 years)Interview - In Person or Skype after PhoneVisa: H1B, USC, GC, GC-EAD onlyNeed 10+ Years Exp. Profiles onlyResponsibilities and DutiesDesign, engineer and build data platform solutions using Big Data Technologies.Establish and communicate fit for purpose analytical platforms for business prototypes.Lead innovation by exploring, investigating, recommending, benchmarking and implementingData-centric technologies for the platform.Be a proactive coding engineer. Interfacing with vendors.Qualifications and SkillsMinimum 6 + years of solid development experience on Hadoop, Java/Scala.Minimum 2+ years of solid experience on Scala, Spark, Kafka, Hive, etc.Experience on System IntegrationExperience on Linux and Big Data FrameworksShould have working knowledge of MySQL, etcPrior experience with Cloudera is an added advantageJob Type: Contract","Dallas, TX",Big Data Engineer,False
316,"Overview

BNY Mellon Asset Management North America (Mellon Capital ManagementAMNA (MCM)), a top tier global multi-asset investment manager, is seeking a talented, experienced individual to lead the design, implementation and maintenance of data and model technology infrastructure of the Asset Allocation (AA) investment team.

The AA team utilizes several proprietary multi-asset strategies based on fundamentals-based principles and quantitative investment models. AMNA has provided a wide variety of asset allocation and multi-asset solutions to investors since 1983.

Responsibilities
Design, build, and maintain the data infrastructure used by the AA investment team and serve as a liaison between researchers, portfolio managers and developers.

Make use of ETL knowledge to design tools for robust data management.Develop API for accessing data, for use by business users (i.e., researchers and portfolio managers)Contribute to the codebase for investment modelsMake recommendations for innovative or creative approaches for data management as neededAssist in the design and development of enterprise data standards and best practicesParticipate in the evaluation and implementation of vendor application software and tools
Qualifications
Requirements

5+ years experience as a data engineer, data scientist, software engineer or similarExperience with overall data architecture and infrastructure or similarExperience successfully managing large projects to completionDevelopment experience with Matlab, R or Python in a data-science or research settingExperience with relational database design (MS SQL Server or similar)Experience building ETL pipelines and knowledge of ETL best practicesDemonstrated ability to work with multi-disciplinary teams (Portfolio Management, Research, Information Technology teams)Availability to work during market hours
Nice to Have

Experience working in investment management and/or quantitative financeKnowledge/Experience with financial data provider API’s (Bloomberg & Datastream)

Knowledge/Experience with the following technologies:

DockerJavaMicrosoft .Net suitePentahoNoSQL (MongoDB)Cloud and distributed computing


For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments & safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.

As one of the world's leading investment management organizations and one of the top U.S. wealth managers, BNY Mellon Investment Management combines agility, insight and scale to create and deliver strategies and solutions to address our clients' needs. Encompassing BNY Mellon's investment management firms, wealth management organization and global distribution teams, we draw on deep expertise to collaborate with clients and tailor our best ideas and resources to meet their specific needs. We pride ourselves on providing dedicated service through our network of global professionals who have a deep understanding of local requirements. With our extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management is dedicated to helping clients reach their goals.

BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.
Minorities/Females/Individuals With Disabilities/Protected Veterans.

Primary Location: United States-California-San Francisco
Internal Jobcode: 45144
Job: Asset Management
Organization: NA Investment Boutiques-HR13428
Requisition Number: 1809765","San Francisco, CA",Data Engineer,False
317,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Building 8 brings together world-class experts to develop and ship groundbreaking products at the intersection of hardware, software, and content. We have a clear mandate to deliver products at scale that define new categories and advance Facebook's mission of connecting the world.

Our analytics team works closely with Product Managers, Product Analysts and Marketers to acquire and retain users and optimize the user experience — all while using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems at a scale that few companies can match.

This is a full-time position based in our office in Menlo Park.

RESPONSIBILITIES

Inform, influence, support, and execute our product decisions and product launches.

Manage data warehouse plans for a product or a group of products.

Interface with engineers, product managers and product analysts to understand data needs.

Partner with Product and Engineering teams to solve problems and identify trends and opportunities.

Build data expertise and own data quality for allocated areas of ownership.

Design, build and launch new data extraction, transformation and loading processes in production.

Support existing processes running in production.

Define and manage SLA for all data sets in allocated areas of ownership.

Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS

BS/BA in a technical field, Computer Science or Mathematics.

4+ years experience in the data warehouse space.

4+ years experience in custom ETL design, implementation and maintenance.

4+ years experience working with either a MapReduce or an MPP system.

4+ years experience with schema design and dimensional data modeling.

4+ years experience in writing SQL statements.

Experience analyzing data to identify deliverables, gaps and inconsistencies.

Communication experience including experience identifying and communicating data driven insights.

Experience managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS

4+ years experience using Python or Java.","Menlo Park, CA","Data Engineer, Building 8",False
318,"Work with business owners and development teams to define, develop and support data-driven applications and product prototypes.
Gather data insights related to user behavior, product usage, audience segmentation, social media, quality of services, and media consumption to drive business decisions.
Working with predictive analysts and data scientists, build scalable prototypes of machine learning algorithms, build production quality codes, manage and maintain the code base.
Work with product management and internal stakeholders to help dictate the requirements and technical design of Fandango data services and related intelligence tools.
Develop BI applications including transactional systems, mobile application development of BI products/services including Android, iOS, and/or cross platform.
Contribute to every phase of the BI product development life cycle (design, development, testing, iteration, deployment, support and maintenance).
Other duties as assigned.
Qualifications/Requirements
Master's degree in CS, MIS, Statistics, Machine Learning, Applied Mathematics, Data Mining or a related field.
3+ years of overall professional experience. Proven track record working on distributed, scalable, service-oriented platforms. Proven ability to deliver high quality, production ready code.
Hands-on experience with Hadoop eco-systems (Hive, Pig, Spark, etc.).
Hands-on experience in developing complex Talend ETL processes for integration between systems enterprise application and business analytics (Big Data / Hadoop) using Talend tools.
Hands-on experience in programming languages Python, Java, Scala, etc.
Experience in data extraction, data-driven statistical modeling, analysis and supervised learning using modeling languages such as R, Stata, MATLAB, etc.
Experience in using Cloud services such as Amazon Web Services, Microsoft, IBM and Google Cloud.
Experience with Machine Learning/Deep Learning APIs, frameworks, and AI tools.
Experience with BI reporting tools (Tableau, MicroStrategy, SAP BusinessObjects, SSRS, etc.).
Expert understanding of the agile software development life cycle.
Excellent knowledge and expertise with SQL and Relational/Multi-dimensional Databases.
Understanding of data warehousing concepts.
Strong customer service and communication skills (verbal and written) to effectively interact with a diverse team of people across business and engineering.
Must maintain regular and acceptable attendance at such level as is determined at the company’s sole discretion.
Must be available and willing to work extended hours (during crunch times!) per day including occasional weekends and holidays.
Desired Characteristics
Willingness to dig into technical concepts/details.
Proactive to improve products and business processes.
AWS certifications.
Naturally inquisitive and self-learner.
A team player.
Stay on top of the latest innovations in data and technologies.
Sub-BusinessFilm Fandango
Career Level
Experienced
CityBeverly Hills
State/Province
California
CountryUnited States
About Us
At Fandango, we love movies. From showtimes and ticketing, to engaging content and innovations in movie going- we strive to deliver the perfect movie going experience—anytime, anywhere. And to be the go-to destination for moviegoers, we think it’s all about collaboration. Anyone can build a website or app, but it takes a special group across many disciplines to create an experience that can live across multiple platforms and connected devices. Thanks to an exceptional team, we’re working hard to make Fandango a little bit better every day. Our 30 million (and growing!) online and mobile monthly visitors can now buy movie tickets at over 25,000 screens nationwide and stay for exclusive trailers, our award-winning original video series, movie reviews, celebrity interviews and more. So, if you’re looking to inspire, be inspired and work at the intersection of entertainment and technology look no further than Fandango. Visit Fandango.com/careers for a behind-the-scenes look at Fandango and follow us on LinkedIn for the latest news and updates. Fandango is an NBCUniversal Company.
Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.","Beverly Hills, CA",Big Data Engineer,False
319,"As a member of the Data Engineering team you will be required to leverage strong data engineering experience with cutting edge technologies in the NoSQL, Hadoop ecosystem and data lake/warehouse spaces to support and enable a true data driven company. The ideal candidate will have a track record of developing data pipelines to extract, transform and aggregate data that can scale to petabytes, elastically, with low latency and high availability.

Qualifications


A background in Computer Science, Engineering, Mathematics or appropriate industry experience
Experience building complex data pipelines for data integration from enterprise wide applications/systems into centralised big data lakes/warehouses
Advanced experience and understanding of distributed database technologies/concepts
Experience integrating with Big Data platforms including Spark and Kafka
Strong analytical competencies with the ability to turn requests from Business Intelligence and Data Analyst personnel into efficient and scalable data transformations
A proficient Python programmer
Experience processing data at scale with a concrete understanding of the tradeoffs and challenges of a variety of implementations
Experience building and enforcing strong data quality guarantees and anomaly detection into data pipelines

Nice To Haves


Experience with AWS internals and their suite of product offerings
Knowledge of database internals, transactions, clustering, sharding, and replication
Knowledge of optimised data storage and serialisation formats
Fundamental data science knowledge and experience

What we expect from you


You are a data visionary that believes in the power and potential of being data driven
Be willing to share your expertise and mentor less experienced engineers
Able to work quickly and independently within a fast pace agile team
Exhibit strong ownership of all aspects of assigned projects, tasks, issues, and code
Possess a natural tendency to analyze and optimize platform scalability and efficiency
Continually seek to improve your skills and keep abreast of industry trends
Able to evaluate and experiment with emerging technological trends and apply them to new and existing projects as well as certify their usage for wider adoption throughout MongoDB
Can adapt to new, different or changing business requirements, able to distill complex problems into concrete technical requirements and are comfortable identifying alternatives and making timely decisions

","New York, NY",Data Engineer,False
322,"$100,000 - $140,000 a yearOptomi, in partnership with the Global Technology team for a leading global asset management and financial services company, is seeking a Data Engineer for their Baltimore, MD location to play a key role in helping build the future of financial services.The Data Engineer will be joining a brand-new team that will help establish and build new capabilities for the organization’s research, data science, machine learning, and business analytics efforts. The goal is to transform large amounts of structured and unstructured data such as text, time series, and events into machine-readable knowledge fueling applications and the Buy/Sell investment decision-making process for Portfolio Managers, Analysts, and Quants, and Traders.What the right professional will enjoy!!Help establish best practices in big data technologies and data warehouse to collect and analyze large volumes of data advancing the state of the art in efficiency optimization, predictive analytics and BI self-reporting tools.Join an emerging technical team of data engineers delivering a wide array of big data and self-service reporting solutions that use cutting edge technologiesHelp guide the organization in efficient data and resource management best practices with cloud based big data, Data Warehouse and Reporting platforms and servicesApply today if your background includes: You have a Bachelor’s Degree in Computer Science (or related field) and have 3+ years of relevant work experience in business intelligence, data engineering, data warehousing, or a similar field.You remain active in learning emerging practices and technologies in the BI/DW and data science space.You are comfortable and confident in your knowledge of multiple database programming languages and your knowledge of relational and columnar databases.You have experience integrating data from multiple data sources and have processed large amounts of unstructured data.You have experience with AWS, Big Data Components (Apache Spark, Hadoop, Presto, etc.), and Continuous Integration tools (TeamCity, Octopus, Jenkins, etc.) is desired, but not required.Job Responsibilities: Provide significant contributions to the growth of our Data infrastructureDesign, develop, test and deploy business intelligence solutionsDesign and document data structures/models and data flowsInteract with various business units and technical teams to gather requirementsReview, optimize and document current ETL processesEnsure data quality, completeness, and accuracyCollaborate with team members in code reviews, discovering better practices and patterns and continuous improvementsJob Type: Full-timeSalary: $100,000.00 to $140,000.00 /yearLicense:US Citizen or Green Card (Preferred)","Baltimore, MD",Data Engineer,False
323,"Description:
JOIN US AS A PRINCIPAL DATA ENGINEER
Join Target’s Data Science & Engineering team in Sunnyvale as a Principal Data Engineer – Data Architect. Here we employ cutting edge technologies to help Target meaningfully engage with guests and make intelligent business decisions using huge data sets. Our Data Engineers partner closely with Data Scientists continuously innovating to harness valuable insights from data generated. You’ll work in an environment that provides the freedom and agility of a startup with the security and vast resources of a large established company.
As a Principal Data Engineer, you’ll get the chance to design and architect high quality, flexible, manageable and performant systems and services. You will architect solutions that will capture, manage, process and serve small to large data at scale. This role allows you to work with business partners to provide technical solutions for business problems on a large scale in a fast moving retail data industry. Responsibilities will include analyzing, designing, programing, debugging, modifying software for existing and new, proprietary data products used in distributed, large scale analytics solutions.
We’re looking for a highly motivated engineering professional who will partner with Lead Technical Product Owners and management at one end as well as lead and motivate Scrum development teams on the other.
Key Responsibilities:
Lead decision-making process for selection of software products and architecture solutions
Develop software systems using test driven development employing continuous integration practices
Mentor and partner with engineers to develop software that meets business needs
Follow agile methodology for software development and technical documentation
Innovate constantly and keep systems up to date with current technologies
Requirements:
MS or PhD degree in Computer Science or equivalent experience in data architecture
10+ years of experience in developing software applications
3+ years of experience working on Big Data products - preferred experience with Hadoop, Spark, Hive, Druid, etc.
Proficient in application/software architecture (Definition, Business Process Modeling, etc.)
Extensive understand of application/software development and design
Collaborative personality – ability to engage in interactive discussions with diverse engineering teams
Stays current and up to date on Big Data technology and trends
Experience mentoring and developing skillsets of data engineers

Qualifications:","Sunnyvale, CA",Principal Data Engineer,False
324,"Who We Are:
Omaze's mission is to transform lives by leveraging the power of storytelling and technology. Our model democratizes traditional auction-giving by offering people everywhere the chance to have a once-in-a-lifetime experience for as little as a $10 donation. To continue raising money for hundreds of nonprofits around the world, we're growing our team of smart, dedicated and passionate world changers! That's where you come in.

Who We're Seeking:
Omaze's engineering team is looking for a Data Engineer to help us grow in 2018 and beyond. We have a unique product, passionate customers, and global reach. We need a great engineer to help us marshall and wrangle data. If you are comfortable working with SQL, Spark, Redshift, Parquet, Python, Scala, Big Data and AWS, we want to talk to you!

Key Responsibilities:

Work on our existing data pipeline which is SQL, Bash, Python, Spark, Scala, S3, Redshift and Cron
Develop our new data pipeline using things like Alooma, SQL, S3, Redshift
Help consolidate our media spend across social networks, ad networks, affiliates into a holistic dashboard. This may require API integration work with third party companies or working with companies like Funnel.io
Help manage and administer our data warehouse
Support Business Intelligence and database administration
ETL performance optimization
Build aggregates

Our Ideal Candidate:

You have 5+ years of professional software experience; have worked with several programming languages, and deep knowledge of at least one object-oriented language
You do (or would) enjoy working in Bash, Golang, Python, and/or Scala
You're comfortable working with ad spend data from Facebook, Instagram, Twitter, Ad Networks and Affiliates
You have experience building and managing data pipelines, and with writing ETL jobs
You have designed and implemented optimized, scalable data warehouses
You are great at query optimization and have DBA experience
You have experience with cloud infrastructure
You have experience working with data marts and the star scheme
You're familiar with Tableau, Looker or Microstrategy
You take your work seriously, but not yourself

To Apply:
Tell us a little bit about yourself and why you would be an ideal fit.","Los Angeles, CA",Data Engineer,False
325,"Are you a Data Engineer/Data Scientist, with a solid data experience, who is seeking a new challenge Our downtown Seattle client would like to meet you! You will be working with a small team currently migrating data into AWS. Your day-to-day focus will be on programming. You are ideal for this team if you have experience working with Hadoop and related technologies, SQL, AWS, database engineering experience and programming experience. Ready to learn more

Expectations


You excel working as part of a team
You enjoy opportunities to participate in stand-ups and other sprint meetings
You are passionate about solving the toughest challenges
You are comfortable working with Hadoop related technology, databases, and cloud platforms
You are good at rapidly rationalizing and transforming data into useful form, and delivering their results via clean, maintainable code.
You have experience working with engineer and analyst teams to rapidly deliver insights in an iterative, agile development environment.
You enjoy working in a fast-paced collaborative team environment contributing to the high-performance team culture.

Requirements


Degree in in engineering, computer science or a related field.
3+ years of experience in the development of data engineering for non-academic problems.
5+ years of experience data engineering for real world problems using Hadoop and related technologies (Spark, Hive, etc.)
Experience developing software within the full software development lifecycle, from design through release.
Experience at a large company with complex business models, working directly with internal customers and facilitating or mediating across functions.
Knowledge of fundamental CS concepts, e.g. algorithms and data structures.
Expertise in programming (Python and Java)
Experience with Hadoop and related technologies for distributed, scalable storage and processing.
Expertise in SQL.

Thank you for considering Quardev. When you join Quardev consulting team, you join a team of industry veterans with a combined experience of over 30 years who are dedicated to creating a positive work environment that attracts and retains consultants through a combination of employee satisfaction, working conditions and company culture. Team members enjoy W-2 employment benefits, competitive salary, birthday off paid; affordable health, vision and dental insurance; 401K and fridge access at our corporate office. We pride ourselves on being a great place to work and strive to ensure our team members enjoy coming to work each day.

For more information and new job opportunities, visit www.Quardev.com

To check out our employee reviews on Glassdoor, click here","Seattle, WA 98105 (Ravenna - U District area)",Data Engineer-Data Scientist,False
326,"$50 - $65 an hourContractJob DescriptionTranslate complex functional and technical requirements into detailed designArchitect Data pipeline using Spark, Airflow & BeamProficiency in SPARK for technical development and implementationLoad disparate data sets by leveraging Kafka consumersAbility to utilize Hive, Spark, Cassandra, Mesos, PysparkContribute and adhere to the best engineering practices for source control, release management, deployment, etc.Participate and facilitate in production support, job scheduling/monitoringPerform machine learning(supervised & unsupervised), natural language, and statistical analysis methods, such as classification, collaborative filtering, association rules.QualificationsB.S. and M.S. in mathematics, computer science or engineering6+ years of Python, Java/ Scala.6+ years of demonstrated technical proficiency with SPARK, big data projects, and experience in data modeling.Writing high-performance and reliable codes, Spark scripts, and implement Kafka and Flume topics/processesGood knowledge of database structures, theories, principles, and practicesAnalytical and problem solving skills applying to Big Data domainUnderstanding and experience of utilizing Hadoop, Pig, Hive and Spark Elastic search etc.Good aptitude in multi-threading and concurrency conceptsJob Type: ContractSalary: $50.00 to $65.00 /hourExperience:Spark: 2 years (Preferred)Cassandra: 2 years (Preferred)Mesos: 1 year (Preferred)Location:Sunnyvale, CA (Preferred)","Sunnyvale, CA",Senior Big Data Engineer @Apple Inc.,False
327,"HomeLight is changing the face of real estate one homeowner at a time. We empower consumers to use real agent performance data to make a more informed choice on the biggest financial decision of their lives. Our proprietary, machine learning algorithms analyze over 30 million transactions from 2 million agents to determine the best agents to meet clients' specific home buying or selling needs.

We are backed by some of the most well known investors in Silicon Valley, such as Menlo and Google Ventures, and help consumers sell billions of dollars in homes annually. we are expanding our lean, versatile engineering team by hiring a Data Engineer!

What You'll Do Here:
We are building a Data Engineering team to tackle HomeLight's diverse, data challenges. You will develop and operate our data pipeline, which collects, processes, and distributes data to a suite of HomeLight products and teams. You will provide data to both our algorithms and internal users, refining our product and identifying new markets. Some projects you will work on:


Execute on requests to pull, analyze, interpret and visualize data
Partner with team leads to build out and iterate on team, and individual performance metrics
Participate in our data release process, and partner with team leads to iterate on and improve existing data pipelines.
Design and develop systems that ingest and transform our data streams using the latest tools.
Setup and integrate new cutting edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing and representing our data.
Research, architect, build, and test robust, highly available and massively scalable systems, software, and services.

You Have


3+ years of data engineering experience
Experience writing and executing complex SQL queries
Experience building data pipelines and ETL design (implementation and maintenance)
Experience with AWS or other IAAS or PAAS provider
Ability to produce high-quality software through unit & functional testing.
Experience with development in one or more of the following Python, R, Scala, SQL
Experience with data processing frameworks and data warehouses such as Hadoop, Spark, Redshift
Scrum/Agile software development process.

Bonus points for:

Real estate experience
Experience with Periscope, Looker, Tableau and other BI tools
Experience with building data pipelines
Experience with machine learning

The Perks
---------


Medical, vision, dental, and paternity/maternity benefits.
401(k)
Commuter benefits
Flexible time off policy
Catered lunches and snacks
Corporate gym membership
Company events: happy hours, bowling, bocce league, etc.

","San Francisco, CA",Data Engineer,False
328,"$70,000 - $90,000 a yearHelp data scientists and analytics modelers prepare poor quality data so it can be used in analytics models.Enable access to internal and external data sources and making sure data is available for those who need it.Give input to IT for the design and implementation of data management and/or architecture solutions.Set up and ongoing operational support of ETL environments including development, test and production.Design, implement, and deploy data loaders to load data into Hadoop/NoSQL.Assist in pulling, filtering, tagging, joining, parsing, and normalizing data sets for use.Work with the business analyst, data governance team, data steward to resolve data quality issues. Create analytical datasets from large data sources (multi-Terabyte/Hadoop) through the development of highly-efficient reusable code.Data engineers in the big data platform ( Hadoop, spark, scala, java, python) with hands on experience in supporting analytical projects.Cloud ( AWS, Google or Microsoft) experience is requiredJob Type: Full-timeSalary: $70,000.00 to $90,000.00 /yearExperience:Hadoop: 2 years (Required)NoSQL: 2 years (Required)Data Analysis: 5 years (Preferred)Google Cloud: 1 year (Preferred)AWS: 1 year (Preferred)Spark: 1 year (Preferred)scala: 1 year (Preferred)Python: 1 year (Required)Java: 1 year (Required)Azure: 1 year (Preferred)Education:Bachelor's (Preferred)Work authorization:United States (Preferred)","Quincy, MA 02169",Data Engineer,False
329,"ContractJob SummaryGreetings from Infotechspectrum. We have an immediate requirement for Big Data Engineer. Please share suitable profiles.Title: Big Data EngineerDuration: 12 monthsLocation: Phoenix, AZJob Details:Perfect in Big-Data & Hadoop conceptsShould have Knowledge of Big Data tools Hive,Sqoop,PigShould have experience working with Apache SparkKnowledge of Big Data ecosystemShould have experience in optimizing Hadoop ETL jobsVery good in advanced SQL queries (Join, Group by, Partition by, Indexing etc)Should have knowledge and exposure to NoSQL databasesAble to Explore and analyze datasets with tools like Excel.Good to have Knowledge in Banking Domain and risk Management system.Excellent communication, analytical and inter-personal skillsJob Type: ContractWork authorization:United States (Required)","Phoenix, AZ",Big Data Engineer,False
330,"$160,000 a year*Position Overview:Redefine How Washington WorksWhen government is at its best, diverse ideas are at work. The same is true of Bloomberg Government. Our ability to innovate and serve our clients means diversity and inclusion are essential. If you’re eager to join a multi-dimensional team that celebrates and leverages difference, if you have the vision to see how information can transform “business as usual” and if you are hungry to create-to build a service transforming how things get done—then stop reading this and join our team.Who Thrives Here?Individuals who embrace hard work, act with urgency and collaborate without reserve thrive at Bloomberg Government. You are on the front lines of revolutionizing the information services industry in Washington-as high as our expectations will be of you, yours must be higher. If you are dogged, innovative and interested in work that contributes to causes greater than yourself, you belong at Bloomberg Government.Bloomberg BNA also provides legal, tax and compliance professionals with critical information, practical guidance and workflow solutions. We leverage leading technology and a global network of experts to deliver a unique combination of news and authoritative analysis, comprehensive research solutions, innovative practice tools, and proprietary business data and analytics. Bloomberg BNA is an affiliate of Bloomberg L.P., the global business, financial information and news leader.We are looking for an experienced Web Application Architect to be responsible for business analysis, application design, development, integration and delivery and application maintenance and support. Individuals in this position must be a self-directed professional, who will also bring leadership skills and quickly learn new technologies and programming languages.Hiring RequirementsJob DetailsJob ProfileJob Profile*Web Application Architect (Project Supervisor), BGOV (BBNA)*Job Families for Job ProfilesJob Families for Job Profiles(empty)Worker Sub-TypeWorker Sub-Type*Regular*Worker TypeWorker Type*Employee*Time TypeTime Type*Full time*Compensation GradeCompensation Grade*M04*Primary LocationPrimary Location**Washington - 1101 K Street NW (BBNA)*Primary Job Posting LocationPrimary Job Posting Location**Washington - 1101 K Street NW (BBNA)*Additional LocationsAdditional Locations(empty)Additional Job Posting LocationsAdditional Job Posting Locations(empty)Scheduled Weekly HoursScheduled Weekly Hours37.5Work ShiftWork Shift(empty)Recruiting Start DateRecruiting Start Date08/27/2018Target Hire DateTarget Hire Date08/27/2018Target End DateTarget End Date(empty)Additional InformationUnion Membership from Job ProfileUnion Membership from Job Profile(empty)Allowed Unions from Job ProfileAllowed Unions from Job Profile(empty)Job DescriptionJob DescriptionPrimary Responsibilities:Proposes, develops and supports world-class customer facing web applications using a range of technologies.Participates in the analysis of system and business requirements to provide hands-on solutions to meet or exceed our customers’ expectations.Delivers high-quality code by defining and deploying best practices in unit testing and regression and testing frameworks.Performs integration and testing of software components across an entire team, as needed.Serves as the scrum master for a cross-disciplinary team on an as needed basis.Responsible for the delivery of discreet products and components, marshalling resources across a matrixed organization.Communicates with the Product Management and development teams to raise issues and identify potential barriers in a timely fashion.Participates in user-centered research through client focus groups, interviews, usage analysis, and rapid prototyping.Responsible for protecting our customers and brand by writing secure-by-design code.Leads, supervises, mentors, and trains other team members in order to develop a strong, best-in-class development bench.Directs the work of and provides technical guidance to less experienced staff.Participates in recruiting, hiring, onboarding and performance management of new team members.Participates in special projects and performs other duties as assigned.Job Requirements: Demonstrated commitment to high quality user facing and back end code that is usable, maintainable and well thought out.Strong track record of establishing best practices in software architecture and development.Experience developing applications using SOA (Service Oriented Architecture) interfaces and architectures such as REST.Strong record of project execution and completion with experience using Scrum and agile development practices.Excels at working with multidisciplinary teams to develop great user experiences.Ability to work with and mentor other developers and lead by example to develop highly proficient and productive teams.Must be a self-directed and highly motivated individual who embraces modification of their work based on customer feedback and other business factors.Works closely with designers and other developers in a tightly knit, agile team.Ability to work both independently and collaboratively across team and organizational boundaries.Displays enthusiasm for the challenge of pushing the limits of the web platform to deliver disruptive, innovative solutions to the world that will delight our customers.Must be focused on front-loading quality into the development process, ensuring that quality tests are not failing.Familiarity with new and emerging technologies across the full stack.Familiarity with web application security topics such as SAML, AD, and SSL.Excellent written, verbal, and interpersonal communication skills including the ability to interact with all levels of employees and customers throughout the organization.Ability to travel, as required, to meet with stakeholders.Willingness to maintain a flexible work schedule.Education and Experience:Bachelor’s degree in Computer Science or a related discipline, or equivalent experience. Master’s degree preferred.A minimum of 5+ years’ experience as a software engineer or architect for web applications.Minimum 3+ years’ experience developing web applications in a complex, multi-platform distributed environment with Object Oriented languages (Ruby on Rails, Java, etc.).Experience using a variety of languages and technologies to develop web solutions.Experience working in an iterative or agile development environment, preferably Scrum.Job Type: Full-timeSalary: $160,000.00 /yearExperience:JAVA: 5 years (Required)Education:Bachelor's (Preferred)","Washington, DC",Data Engineer,False
331,"Job Code: 4777
Grade: K
Stanford Graduate School of Business
Residing in Silicon Valley, the heart of innovation, Stanford GSB has built a global reputation based on its immersive and innovative management programs. We provide students a transformative leadership experience, push the boundaries of knowledge with faculty research, and offer a portfolio of entrepreneurial and non-degree programs that deliver global impact like no other. We strive to change lives, change organizations, and change the world.
The Data Insights team at the Stanford GSB supports the school’s ETL, API and analytics needs. We’re looking to add a strong hands-on Database Engineer to the team. The ideal candidate must have extensive hands-on experience an all relevant tech and tools; experience in dealing with business level conversations and interactions; juggling multiple and shifting priorities; in building environment-appropriate, practical solutions that appropriately balance between competing pulls of the immediate and future. Since the team is distributed across time zones, ability to collaborate across the distributed work environment is crucial.
Responsible for development of complex technical analysis and design, new programming, modifications, scheduling, tuning, testing and maintenance of systems in support of new and existing Business Intelligence(BI) and other projects.
Your primary responsibilities include:
Architect, design, implement and maintain secure, scalable BI data stores for structured and unstructured data for all domains of the school’s business - Operations, Learning, Admissions, CRM etc.
Collaborate with Integration Engineers to build ETL processes for ingesting data into BI stores and do data prep for BI dashboards.
Provide complex analysis, conceptualize, design, implement and develop solutions for critical BI components.
Plan and implement standards, define/code conformed global and reusable objects, perform complex database design, data modeling (EDW) and programming and streamline the systems.
Consult with client groups to assess user needs and understand business processes; convert requirements into technical solutions.
Contribute to data analysis, design and development of new and ongoing business intelligence (BI) projects involving complex EDW structures OR on OLTP systems.
Collaborate closely with internal and external teams to understand and apply changes/modifications impacting data warehouse.
Monitor industry for emerging technologies and trends; provide appropriate recommendations to team and leadership.
Provide technical mentoring to other team members, as appropriate.
Be flexible to completing any other additional assigned duties.

To be successful in this position, you will bring:
Bachelor’s degree and seven years of relevant experience in computer science, or engineering or combination of education and relevant experience.
Mastery in SQL skills and ability to support SQL based ETL processes.
Mastery in Business Intelligence and Data warehousing concepts and methodologies.
Expertise on Databases, data acquisition, ETL strategies and tools and technologies like Informatica, Talend etc.
Expert in Data Modeling data from heterogeneous OLTP sources into appropriate BI stores - structured warehouses and data lakes.
Experienced in designing Physical and Reporting Data models for seamless cross-functional and cross-systems data reporting.
Experienced in building robust and reusable Data Prep processes to support BI Analysts.
Experience working with standard SDLC processes.
Experience in one or more programming platforms like R, python etc. a strong plus.
Experience in relevant components of AWS and GCP a strong plus.
Experience in machine learning a plus.
Self-motivated, demonstrable quick learner of new technologies and/or processes. Experienced in working with distributed teams across time zones.Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of his or her job.

Why Stanford is for You
Stanford’s dedicated 16,000 staff come from diverse educational and career backgrounds. We are a collaborative environment that thrives on innovation and continuous improvement. At Stanford, we seek talent committed to excellence, driven to impact the future of our legacy, and improve lives on a global sphere. We provide competitive salaries, excellent health care and retirement plans, and a generous vacation policy, including additional time off during our winter closure. Our generous perks align with what matters to you:
 Freedom to grow. Take advantage of career development programs, tuition reimbursement, or audit a course. Join a Ted Talk, film screening, or listen to a renowned author or leader discuss global issues.
A caring culture. We understand the importance of your personal and family time and provide you access to wellness programs, child-care resources, parent education and consultation, elder care and caregiving support.
A healthier you. We make wellness a priority by providing access to world-class exercise facilities. Climb our rock wall, or participate in one of hundreds of health or fitness classes.
Discovery and fun. Visit campus gardens, trails, and museums.
Enviable resources. We offer free commuter programs and ridesharing incentives. Enjoy discounts for computers, cell phones, recreation, travel, entertainment, and more!
We pride ourselves in being a culture that encourages and empowers you.

How to Apply
We invite you to apply for this position by clicking on the “Apply for Job” button. To be considered, please submit a cover letter and résumé along with your online application.","Stanford, CA 94305",Data Engineer,False
332,"We, at Flywire, are looking for a smart, analytical thinker who's excited to empower data-driven decision making at an exciting and fast-growing organization! As our Data Engineer, you will work within the Data Analytics team to ensure that our organization has access to reliable, accurate, and timely data to be used in various reporting, business intelligence, and analytical solutions. Great data aptitude is a must for this role, but we're also looking for someone that's willing to work across Flywire teams to understand real business problems and design solutions that will ultimately provide insights into the performance of the company, improve process efficiencies, and contribute to our company's ability to provide a world-class cross-border payment solution

Key responsibilities:

Own the maintenance and ongoing development of Flywire's analytical data infrastructure
Develop production-grade data pipelines and ETL processes to support analytics projects, business intelligence reporting, and machine-learning solutions
Continuously identify and implement data process improvements (e.g., optimize data delivery, re-design for scalability, implement testing and alerting systems to assure data quality, etc.)
Own maintenance of documentation and data dictionaries for various internal data sources

Minimum Qualification Criteria:

BS in Computer Science, Mathematics, or related field
5+ years of experience of data engineering, database administration, or related work
Experience with AWS/Amazon Redshift and/or Google Cloud Platform is required
Experience in building data extraction and manipulation scripts in Python
General understanding of the broader data landscape, trends, and emerging technologies
Hunger and excitement for learning new tools and techniques
Excellent problem-solving skills: You may not know the solution to all the problems you'll face, but you have the ability to research available technologies/strategies and figure out a solution or a path forward
Strong communication skills: You can make even the most complex data and technical problems easy to comprehend

Preferred Criteria:

Experience in Business Intelligence development (Tableau, Looker, etc.)
Demonstrated ability of building streaming data applications
Practical understanding of classification, regression, and other statistical methods
Familiarity with Apache Spark
Proficiency with Spanish

Flywire is an equal opportunity employer.","Boston, MA",Data Engineer,False
333,"Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably Integrate with a variety of data metric providers ranging from advertising, web analytics, and consumer devices Build and maintain dimensional data warehouses in support of business intelligence tools Develop data catalogs and data validations to ensure clarity and correctness of key business metrics Drive and maintain a culture of quality, innovation and experimentation 3-5 years of experience developing in object orient Python Engineering big-data solutions using technologies like EMR, S3, Spark Loading and querying cloud-hosted databases such as Redshift and Snowflake Building data pipelines using Kafka, Spark, Flink, or Samza Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift Experience deploying data notebook and analytic environments such as Jupyter and Databricks Knowledge of the Python data ecosystem using pandas and numpy Experience building and deploying ML pipelines: training models, feature development, regression testing Experience with graph-based data workflows using Apache Airflow Bachelor’s degree in Computer Science or related field or equivalent work Disney Streaming Services is a place for the creative and the bold. We’re seeking talent across disciplines to join our team. Whether New York City, San Francisco, Manchester or Amsterdam we provide opportunities to elevate your career and transform an industry. Disney Streaming Services software engineers develop premium digital media products for Major League Baseball and our partners.

The products we build, such as MLB.TV, NHL.TV, HBO NOW and PlayStation Vue are paving the way for the next-generation media and sport technologies. BAMTECH engineering is headquartered in the Chelsea area of New York, NY with an office in the SoMo area of San Francisco, CA and team members based around the world. If you are interested in joining Disney Streaming Services in the pursuit of not only crafting new media products but enjoying the products you build, we are interested in hearing from you. At Disney Streaming Services data is central to measuring all aspects of the business, and critical to its operations and growth.

The data engineering team is responsible for collecting, analyzing and distributing data using public cloud and open source technologies and offers transparency into customer behavior and business performance. 600905","New York, NY 10036",Sr Data Engineer,False
334,"LEAD DATA ENGINEERHarmony Labs is on a mission to understand media influence at scale, and to experiment with using media to support an open, resilient, democratic society. Our community of researchers, practitioners, and industry partners is building basic knowledge and applied solutions to help get ahead of pressing media systems challenges, like business models that favor engagement over accuracy, partisan polarization, and information wars fought with manipulated media. We are currently seeking a Lead Data Engineer to help us build out our data warehouse and add new sources to it from our accelerator program.The LEAD DATA ENGINEER will take on the following roles and responsibilities:Architect and implement reliable ETL pipelinesWork with myriad sources to automate ingestion processes across internal and externalsystemsBuild alarms and health metrics to maintain high data availabilityDesign and create data schemas and models that can scaleFacilitate the growth of the engineering team at HarmonyQUALIFICATIONS AND REQUIREMENTS:CS degree or related work experienceFamiliarity with cloud database architectureExperience designing and implementing systems using PythonFamiliarity with schemas, tables structures, relational, and non-relational databasearchitectureStrong experience in improving performance of queries and data jobs and scaling thesystem for exponential growth in dataExperience leading small teams and working with stakeholders to design and build systemsHOW TO APPLYPlease submit your cover letter, resume, and salary requirements.COMPENSATIONSalary is commensurate with experience. There is an excellent benefits plan, including unlimited paid time off for sick leave and vacation; medical, dental and vision plans; pre-tax flexible spending accounts and transit program; and 401(k). As part of our dedication to the diversity of our workforce, Harmony Labs is committed to Equal Employment Opportunity without regard for race, color, national origin, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity, or religion.ABOUT HARMONY LABSHarmony Labs is on a mission to understand media influence at scale, and to experiment with using media to support an open, resilient, democratic society. Through the sharing of data, knowledge and other resources, we're enabling networks of researchers, practitioners and partners to rapidly generate and refine hypotheses about how media and society interact, to align around ideal outcomes, and to test outcomes-optimized interventions. Together, we’re building basic knowledge and applied solutions to help get ahead of pressing media systems challenges, like business models that favor engagement over accuracy, partisan polarization, and information wars fought with manipulated media.Harmony Labs is New York City based 501(c)3 that has evolved from nearly a decade of research and prototyping, in partnership with and/or funded by leading organizations like Google, The Ford Foundation, The Corporation for Public Broadcasting, MTV, and Columbia University.Job Type: Full-timeExperience:Python: 1 year (Preferred)Education:Bachelor's (Preferred)Location:New York, NY 10036 (Preferred)","New York, NY 10036",Lead Data Engineer,False
335,"$55,000 a yearPosition: Data EngineerLocation: Cincinnati, Ohio USADuration: Full-timeSalary: Starting at $55,000 / yearBenefits: Competitive benefits packageAbout Sphaeric.ai: Sphaeric.ai is an up and coming leader in developing and implementing AI solutions for corporate clients and funded startups. We are currently working with clients in the insurance, healthcare, and marketing industries and are looking to move into additional verticals as we continue to grow. Current solutions we offer include:ML/AI model building and deployment in flexible and scalable cloud environmentsWeb application development for process automation and AI interactionCloud-based data engineeringTechnical consultingThis is a great opportunity to join a small but growing company and quickly become a part of the leadership team.Position Summary: As a data engineer for Sphaeric.ai you will be entrusted with significant responsibility. The primary function of this role will be as lead data engineer for a sophisticated cloud-based data collection platform. This includes working with Apache Airflow to ensure that data pipelines are functioning properly, innovating and implementing improvements, and taking lead on recommendations. In addition, you will be responsible for supporting our data science services that could include model deployment and some front-end web app development.We are looking for a candidate who can grow into a leadership role with our company.Responsibilities: Managing workflows in Apache AirflowBuilding data pipelines to collect and store data in the cloudReworking existing backend infrastructure to optimize performanceDeploying of ML/AI models in the cloudBuilding and deploying UIs for ML/AI modelsInteract with clients regarding work productsRequirements: Ability to adapt to new programing languages or software products quicklyProven programming experience (preferred but not required: Python, Java, SQL, Airflow etc.)Knowledge of and experience with the software development lifecycleFamiliarity with cloud environmentsFamiliarity with data warehousingFamiliarity with big data tools (Hadoop, Apache Spark, mongoDB, etc.)Optional Requirements: Experience with dashboarding and data visualizationStrong quantitative and problem-solving skills (exposure to math, statistics, engineering, or physics)Benefits of working with Sphaeric.aiTeam members working with Sphaeric.ai will be entrusted with significant responsibility and room for growth. We love innovative thinking and working at the highest standard while maintaining a relaxed environment in which we actively help each other learn and share best practices that we discover.Specific benefits include:Work on industry-leading AI projectsParticipate in the entire project lifecycle from ideation to deploymentInvolvement in AI conferencesOpportunity to work with a growing startup which is set to expand significantly in the next 5 yearsLook us up: Website: sphaeric.aiLinkedIn: https://www.linkedin.com/company/sphaericJob Type: Full-timeSalary: $55,000.00 /yearEducation:Master's (Preferred)Location:Cincinnati, OH (Required)","Cincinnati, OH",Data Engineer,False
336,"InternshipData Engineer Intern - (00051473)
Description
Do you want to help analyze data and do the analysis needed to contribute to solving our nation’s most critical problems? Do you want to be mentored by engineers and scientists that are experts in their fields? Do you want to join over 300 other interns for a summer full of learning, networking and fun?

MITRE’s people are committed to tackling our nation's toughest challenges. We are different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&D centers we operate for the government create lasting impact in fields as diverse as cybersecurity, healthcare, aviation, defense, and enterprise transformation. We're making a difference every day—working for a safer, healthier, and more secure nation and world.

Our workplace reflects our values. We want you to come as an intern and then join us upon completing your degree so that you can experience the gratifying work, our competitive benefits, exceptional professional development opportunities, and a culture of innovation that embraces diversity, inclusion, flexibility, collaboration, and career growth.

The MITRE Corporation is seeking a Data Engineer intern to come join our team. Our Data Engineering & Biometrics Department applies machine learning, computer vision and biometrics capability to provide MITRE-unique value across multiple sponsors and driving multiple research initiatives and efforts across MITRE or in collaboration with academy, industry and sponsors.
Key Function: What do MITRE Data Analytics / Operations Research / Statistics / Math Interns do?
MITRE’s Data Analytics /Operations Research / Statistics / Math Interns help strengthen our sponsor’s effectiveness by using data analytics, statistical analysis, and modeling and simulations methods to drive analytically-defensible decisions. They apply advanced analytical and mathematical techniques to solve complex decision problems—those with multiple alternatives that require quantitative analysis to confidently select the best one. Analysts use high performance computing, cloud Computing, big data analytics, and data visualization tools and techniques to improve system designs, to assist in making difficult policy and acquisition decisions, to maximize operational effectiveness and efficiency, and to achieve the highest quality engineering solutions.

Qualifications
What does an ideal Data Science / Operations Research / Statistics / Math Intern or New Grad have:

Demonstrated understanding of how to retrieve data from databases and other systems and tools using queries, exporting capabilities, and other effective methods

Understanding of or experience with statistical software and data visualization tools such as R, SPSS, SAS, MS Access, Python, Visual Basic, UNIX, shell scripting, and/or SAS/JMP.

Proficiency in one or more of the following-Java, Python, XML, HTML, C#, Objective C; Database design & development including SQL.

On-going excellence in academic performance

High level desire to help their nation solve its most critical problems

Exhibits the characteristics of a continuous learner

Additional Information

*70% of MITRE’s full-time jobs require US government security clearances; therefore, many internships and full time positions require that the candidates be clearable, which requires US Citizenship. MITRE does not provide sponsorship for those that need it currently or in the future.*

**Many of our jobs welcome those students who have an interdisciplinary approach to problem solving.**

Primary Location: United States-Virginia-McLean
Work Locations: Washington 22102
Job: Student
This requisition requires a clearance of: None
Travel: No
Job Posting: Oct 16, 2018, 2:34:02 PM","McLean, VA",Data Engineer Intern,False
337,"$80,000 - $140,000 a yearFlowCommand is hiring a lead data scientist for our San Francisco office. We are a sensor company that aims to accurately capture the behavior of high throughput fluid systems. With this information we can help optimize allocations while reducing spills and theft. This technology could change the way the oil and gas industry, water municipalities, and anyone dealing with high volume fluid, operate. Our sensors send ultrasonic data directly to our server via cellular/satellite; we then use a mix of physics equations, signal processing, and anomaly detection to determine the behavior of fluid (volume and speed) in pipes.As the first data engineer at FlowCommand, you will work closely with our small team split across Houston, San Francisco and Mexico City. You’ll leverage our remotely controllable flow-loop test facility in Houston to generate training data from our sensors. As the owner of this process, you will be building the foundational models and algorithms that our future suite of products will run on.Responsibilities: Increase accuracy of our measurementsIncrease the accuracy of our confidence in our measurementsCollaborate with engineers to build models that enable a fundamentally new sensor technologyUnderstand, source and leverage tangential/contextual datasetsIncrease the use-cases of our products by identifying correlations with tangential/contextual datasetsRequirementsExperience building production grade Machine Learning modelsExperience building models with limited training dataStrong written and verbal communication skills.Expert knowledge and demonstrable experience with Python and SQL.Team oriented and able to take an idea from conception to launch.Preferencesdegree in Statistics or Computer Science with a Machine Learning focus.Note:If you have a very strong theoretical knowledge of building these models from a PhD or something similar but no experience, you should still apply for considerationFlowCommand is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.Job Type: Full-timeSalary: $80,000.00 to $140,000.00 /yearExperience:Data Science: 1 year (Required)Education:Bachelor's (Preferred)Location:San Francisco Bay Area, CA (Required)Work authorization:United States (Required)","San Francisco Bay Area, CA",Data Scientist,False
338,"$70,000 - $90,000 a yearBig Data Geek Needed at AWHAWH, a 24-year old software engineering firm in Dublin, Ohio, has partnered with Snowflake to take data to the cloud. We’re looking for a Data Engineer to work with next-gen data technologies to architect and deploy clients. Your primary role will be to support data integration processes that move enterprise data lakes into the Snowflake cloud.www.awh.netwww.snowflake.comYour skills will include:Bachelor’s Degree.Minimum 3 years of experience configuring, managing, and troubleshooting PostgreSQL or SQL Server or Oracle or Redshift.2+ years experience as an SSIS ETL developer in MS SQL Server or with using ETL/ELT tools, such as Pentaho, Talend, Business Objects, or Informatica/Data Stage.Minimum 3 years of experience with database backup and recovery, including implementing disaster recovery standards.Minimum 2 years of experience in an Agile development environment with experience with Version One and Jira.Minimum 2 years of experience developing in SQL, Python, R, Ruby, C#, C++, Java, Ansible, or Chef.Experience with Hadoop, Spark, Tableau, and other Big Data technologies.At least 3 years of experience with database design, optimization, and tuning.As a Data Engineer, you will have the ability to grasp new technologies quickly, break down complex data issues quickly, and resolve them. Your goal will be to work with a cross-functional team of client and internal resources to drive design and deployment from a client’s data lake into a Snowflake data warehouse solution. You’ll be an expert at grasping multi-tier, multi-platform frameworks and have hands-on experience in the design, development, and maintenance of relational databases for data storage and data mining.Job Type: Full-timeSalary: $70,000.00 to $90,000.00 /yearEducation:Bachelor's (Preferred)","Dublin, OH 43017",Data Engineer,False
340,"Casey's General Stores Corporate Headquarters in Ankeny, IA is currently seeking a Data Engineer to join our Business Intelligence & Analytics team! As a Data Engineer, you'll be responsible for identifying data sources, dimensional modeling of warehouse tables, extract transform and load (ETL) process, and working with other team members to troubleshoot and maintain the data warehouse.

Specific responsibilities will include:

Designs solutions that conform to the established data warehouse standards.
Evaluates existing data assets and understands how to extract and model the source data to a star or snowflake schema.
Develops data warehouse table architecture that adheres to Kimball best practices and/or specific needs of the team's BI tools.
Builds extract transform and load processes to extract data from a variety of source systems using SQL Server Integration Services and Transact SQL.
Provides thorough testing of all data warehouse objects and ETL processes.
Expands and maintains ETL and data warehouse architecture documentation.
Documents best practices for data modeling and integrations.
Required

Bachelor's degree in Computer Science, Management Information Systems, Data Analytics or related discipline or equivalent related experience.
2-5 years experience in related role.
Advanced SQL skills and experience. Experience writing Macros and stored procedures.
Possess a thorough and practical understanding of application design, development, and environment.
Knowledge of physical and logical database design.
Preferred

SSIS, Informatica or other data integration tool experience.
Ability to extend ETL process using C#, Python, Scala, SQL, or similar programming language.
Experience with multiple database platforms such as SQL Server, MS APS (Microsoft Analytics Platform System), Teradata, Oracle, and Hadoop.
Experience with reporting tools including MicroStrategy, BI Publisher, OBIEE (Oracle Business Intelligence Enterprise Edition), PowerBI, and Business Objects.","Ankeny, IA 50021",Data Engineer,False
341,"Overview
We are looking for a data engineer to work with a team of quantitative researchers in SIG’s Philadelphia office. This group will utilize machine learning techniques to capitalize on trading opportunities for our equities, futures, and options products. This team will be responsible for spearheading the application of deep learning to our daily trading activity.

In this role you will collaborate with other researchers, developers, and traders to improve existing proprietary strategies and develop new trading algorithms that analyze and optimize our performance in capital markets. You will use your software development and data mining skills to build data sets, data quality metrics, and automation tools to enhance our research and system development.

This work will be challenging, fast-paced, and competitive. Your interest and drive to apply cutting-edge machine learning techniques to large financial data sets will enable this team to expand quantitative research at SIG.

What we're looking for
Advanced degree in Computer Science, Statistics, Machine Learning, Applied Mathematics or related field.
Strong object oriented programming skills
Interest in applying machine learning/deep learning theories in a professional research environment
Visa sponsorship is available for this position.

We don’t post salary ranges externally so any salary estimate you see listed here was not provided by SIG and may not be accurate.

SIG is not accepting unsolicited resumes from search firms. All resumes submitted by search firms to any employee at SIG via-email, the Internet or directly without a valid written search agreement will be deemed the sole property of SIG, and no fee will be paid in the event the candidate is hired by SIG.","Bala-Cynwyd, PA 19004",Data Engineer - Deep Learning,False
342,"$120,000 - $160,000 a yearAs an integral part of our technology team, you will work on our data platform, including cloud databases, and ETL pipelines. You will enable our groundbreaking natural language platform, helping us develop, scale, and deploy our applications in a variety of contexts. You’ll wear many hats, touch many parts of our system, and have a significant impact on our products.
Do you love organizing data in ways that enable creative applications?
The kinds of problems you’ll be working on include:
Leveraging existing open source technologies like Kafka, Hadoop, Druid, Spark, RDBMS and other tools
Indexing, and summarizing large data-sets, enabling high performance analytics
Optimizing database queries
Scaling cloud databases and data processing pipelines
Developing data driven APIs for machine learning and application developers
When applying, please tell us about your real world large scale data platform experience.
Qualifications
BS, MS, PhD in Computer Science, Engineering, or related discipline, or 3+ years equivalent technology experience
3+ years of software development (Python, Java, or equivalent)
Familiar with complex database management, replication, and backup
Design and operation of robust distributed systems
Expertise with writing efficient complex database queries
Secure cloud development experience on AWS, GCP, or equivalent
Use engineering best practices – deliver high code quality, automated testing and build reusable components
Authorized to work in the United States
Salary range: $120K – $160K
Company benefits
Flexible leave policy
Health care insurance
Dental & vision insurance
Life insurance
Short-term & long-term disability insurance
Health care FSA
Transit & parking FSA
Free lunch at SF office
Flexible work hours
Holiday time off","San Francisco, CA",Data Engineer,False
343,"Nordstrom Data and Services Technology is at the core of Nordstrom Technology and is pivotal to the Nordstrom customer experience. The team designs, develops and maintains software applications and services that support all of Data Engineering and Analytics business needs across multiple channels. This opportunity also provides various avenues to collaborate and influence several other platforms within Nordstrom Technology and have a company-wide impact – both business and engineering.

Join the Nordstrom Cloud Data Pipeline Team! We believe that with our talented engineers, smart technology, and passionate customers we can deliver the best retail experience on the web. We’re looking for a cloud and data savvy engineer to help us maintain and improve our data pipelines that deliver data from source systems to a data lake in the cloud. Help us solve complex problems with data and automation, while ruthlessly pursuing incremental wins that scale our systems to the next level of sustained performance. While we don’t expect someone to know everything, we expect a great candidate is someone that is willing to learn new technologies where they lack knowledge. We value collaboration, innovation, and passion for delivery while sustaining work life balance.

A day in the life...
You will help us maintain and improve the data pipelines that acquire data from various data sources, such as transaction, customer, vendor, inventory, and land that data in the cloud to be used by various internal customers that vary from Supply Chain to Data Scientists. You will be working with various types of data sources that range from Oracle to ElasticSearch, and using technologies such as Kafka, NiFi, Spark, AWS, and more. This work enables the business to be more agile in decisions and serving the customer.

You own this if you…
Proficient in Java and Object-Oriented Programming
Basic knowledge of relational databases
Basic knowledge of AWS
Kafka experience a plus
Spark experience a plus
NiFi experience a plus
Oracle GoldenGate experience a plus

We’ve got you covered…

Our employees are our most important asset and that’s reflected in our benefits. We listen to what’s most important and continue to evolve our offering to support both our employees and their families.

Beyond strong health, retirement and time off benefits, Nordstrom is proud to offer:
Commuter Benefits
100% Paid Parental Leave
Charitable Giving and Volunteer Match
Merchandise Discount
Nordstrom Stock Purchase Plan

A few more important points...

The job posting highlights the most critical responsibilities and requirements of the job. It’s not all-inclusive. There may be additional duties, responsibilities and qualifications for this job.

Nordstrom will consider qualified applicants with criminal histories in a manner consistent with all legal requirements.

Applicants with disabilities who require assistance or accommodation should contact the nearest Nordstrom location, which can be identified at www.nordstrom.com .
© 2018 Nordstrom, Inc. | Nordstrom Careers Privacy Policy

Current Nordstrom employees: To apply, log into Workday, click the Careers button and then click Find Jobs.","Seattle, WA",Big Data Engineer,False
344,"The Data Engineer acts as a subject matter expert of the data repositories available within the organization by obtaining an in-depth understanding of each source system.These engineers are primarily focused on the creation, management, and availability of structured analytics. Requires the ability to conduct data mining and analytics structuring.

MINIMUM QUALIFICATIONS
Bachelor’s degree required, emphasis in Information Science, mathematics, business management or related discipline preferred.
5+ years in healthcare analytics/informatics and report development experience
3+ years’ experience with analytics in data warehouse environment to include ETL, SSIS, SSRS, API, coding and programming knowledge
3+ experience with visualization tools (Qlik, Tableau, PowerPivot)","Billings, MT 59101",Data Engineer,False
345,"$84,000 - $126,000 a yearDo you know ETL? Do you know it really well? Are you interested in working with some of the hottest new ETL tools, like Wherescape and Alteryx? Are you looking for a new challenge in a very cool company?

We’re helping a very forward-thinking financial institution locate the person who can make sure the right data is in the right places at the right time. You’ll be the key data manipulation resource and will report directly to the manager of BI / DW. Your efforts will directly affect the success of the company.

Is this you? If so, please submit your resume and fill out our questionnaire ASAP!

THE WORK: The data engineer will work on implementing complex data projects with a focus on collecting, parsing, managing, analyzing large sets of data to turn information into insights using multiple platforms. In this role you’ll identify where needed data exists, whether it be inside or outside of the organization. You’ll then be responsible for designing and building rock solid ETL applications to access and deliver that data to key business intelligence systems and analytic users. You’ll work hand in hand with business contacts and BI developers to determine what they need and then deliver on those needs. To accomplish this you’ll have to harness some of today’s hottest ETL technologies, including Wherescape and Alteryx.

In this role, the right person will have an unusual opportunity to make a huge impact on a cool, growing company.

LOCATION: Silicon Valley in the country! You’ll be working in a beautiful, brand new headquarters in Brighton, Michigan. Brighton is a centrally-located town that’s also only 20 minutes from Ann Arbor (home to the University of Michigan), 30 minutes from Lansing (home to Michigan State University) and 50 minutes from Detroit, a rapidly evolving technology hub.

The company recently built their new headquarters building around open workspaces and the latest technologies. In terms of location, this job has it all: a beautiful work environment, access to major metropolitan centers, and the ability to easily get back to nature when you want.

MODE: This is a permanent role

REQUIRED: Applicants must be very strong in:
ETL tools and techniques
SQL

DESIRED: In addition, it would be great if you had some background in:
SQL Server and T-SQL
Data architecture
BI tools
Wherescape
Alteryx
Cloud database architecture (especially Azure)
Big data / NoSQL data stores
The financial services / credit union industry

COMPENSATION: This organization is not afraid to invest in technology and the skills necessary to harness that technology. Thus, the right candidate for this role will earn a base salary of between $84,000 and $126,000 as well as a generous benefits package.

INTERESTED? If you're interested and have the skills, we'd love to hear from you. Please answer our questionnaire and submit your resume right away! Thanks!
U.S. Citizens and all those authorized to permanently work for any employer in the U.S. are encouraged to apply. We are unable to provide visa sponsorship at this time.

NOTE: Dataspace performs background and drug screens on accepted candidates prior to their employment or contract start dates.","Brighton, MI",004: Data Engineer,False
346,"$65 - $70 an hourContractOur client, one of the largest health insurers in the country, has asked us to provide them with a super-strong data engineer to work with their data scientists to provide critical data from Hadoop and SQL data sources.

Is this you? If so, please submit your resume and fill out our questionnaire ASAP!

THE WORK: The work includes manipulating data in big data sources and then extracting it in data-science-friendly forms. The chosen consultant will be working on a highly skilled team with some of the latest NoSQL, Big Data, and Data Science technologies. As a Senior Data Engineer, you’ll also be responsible for mentoring less-experienced staff.

LOCATION: This client is located in the heart of Detroit, Michigan. In case you haven’t heard, The Detroit area is experiencing a new renaissance of technology, food, arts, and culture. Building on its storied history as the “Motor City”, Detroit is now reinventing itself as a home for a vibrant community of well-funded startups with all the amenities demanded by a sharp, urban crowd. With its constant offering of music and theatre performances, several of the nation’s best known museums, a trio of new sports arenas, and distinct neighborhoods of every flavor, the Detroit region truly has something for everyone!

MODE: Contract or Contract to Hire

DURATION: The chosen consultant will be required for at least six months and likely longer.

REQUIRED: Applicants must be very strong in the following:
Hadoop and tools in the Hadoop ecosystem
Spark
Automation scripts (Batch Scripts, Apache Oozie, ...)

DESIRED: In addition, it would be great if you had some background in:
﻿ Python and PySpark
ETL processes

COMPENSATION: $65 - $70 / hr including all expenses

INTERESTED? If you're interested and have the skills, we'd love to hear from you. Please answer our questionnaire and submit your resume right away! Thanks!

NOTE: Dataspace performs background and drug screens on accepted candidates prior to their employment or contract start dates.

NOTE: Our client requires that we work ONLY with direct, W2 employees of our contracting partners. Contracting firms, do not submit resumes for candidates who are not your own, direct employees.","Detroit, MI",012 Senior Data Engineer - Hadoop & Spark,False
347,"dv01 is a data management, reporting, and analytics platform that brings transparency and insight to lending markets- making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.

As the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. To date, dv01 has offered institutional investors insight into $15+ billion of securitizations and more than $64 billion of consumer, small business, real estate, auto, and student loans from the largest online lenders, including LendingClub, Prosper, and SoFi.

YOU WILL:
---------

Be at the heart of dv01. You will operate as the bridge between the engineering and finance teams, contributing to a variety of integral processes that drive dv01 on a daily basis. Every new dataset that gets integrated within dv01 will have your fingerprints all over it.

Be an owner of dv01's most valuable asset. You'll own the business logic in our data pipeline, encapsulating all the knowledge we've accumulated across hundreds of datasets. The output from the pipeline powers all of dv01's customer offerings and is critical to the success of our business.

Be customer-facing. You will have direct exposure to high-level contacts at hedge funds, banks, and asset originators, providing valuable insights to help them answer complex questions.

Work with state-of-the-art technology. You'll work with popular, modern, and exciting open source technologies like Apache Spark. The skills you develop here will serve you well beyond dv01.

QUALIFICATIONS:
---------------

A well-rounded engineer. You have 2+ years of professional programming experience with Apache Spark, Scala, Java, R, or Python. You are able to write thought-out code while accounting for resource and performance constraints, and are also capable of performing ad-hoc data investigations with SQL.

Interest and experience in both engineering and finance. You're looking to grow your skills in both disciplines, and are excited about the synergies between finance and technology.

Knowledgeable about consumer credit. You understand how investors evaluate loan portfolios and the complexities of amortization, prepay, and default. You strive to further your knowledge in the credit market.

Excited about big data. You should have 2+ years of professional engineering experience working with large datasets, with exposure to large datasets related to loan products an added plus. You enjoy working with data, from expressing complex business logic as scalable data processing logic to configuring and debugging intricate big data pipelines. You love the intricate details of a thorough investigation, but also stay aware of the bigger picture while operating across multiple threads of work.

Undergraduate or graduate degree in Finance, Math, or Engineering. Note that we're not anti dropouts if you're a superstar.

dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law.","New York, NY 10010 (Gramercy area)",Data Engineer - Solutions,False
348,"The Global Digital Acquisition organization within Mobile & Web Engineering is looking for talented data engineers to join our Platform Excellence engineering group. We build the core capabilities that power the digital acquisition software platform and enable the journey of a prospective customer of American Express.

In this role, you will use your extensive knowledge of database systems and data processing paradigms to modernize and globalize the data layer of our digital acquisition platform. You will create perform and scalable data pipelines and processing software that integrate with our enterprise data warehouse and reporting and analytics systems.

Job locations: New York, SF Bay Area, Phoenix (remote okay)

If you were to join our team, these are the kinds of things you'd do:

Collaborate with software architects to design the next generation of the Global Digital Acquisition platform's data layer
Work in a cross-functional team with other engineers to design, build, test and deploy software components enabling data layer capabilities
Develop frameworks and approaches for measuring and testing performance and correctness of data layer components and systems
Review colleagues' code with an eye toward performance, reliability and maintainability
Help our production support team address issues encountered in production and fix defects when discovered
Continuously learn about new technologies and help keep the entire group abreast of industry developments and evolving best practices
Mentor other engineers and be mentored in turn
Qualifications
Requirements for this position:

7+ years of software development experience in multiple project environments
Strong fundamentals and production experience with multiple programming languages including Java and at least one scripting language such as Python or Ruby
Deep understanding of and strong opinions on data programming concepts and architectural patterns
Hands-on experience building data pipelines and ETL systems using tools such as Hadoop, Spark and NiFi
Hands-on experience with software engineering practices like source code control, code review, and continuous integration and delivery
Working knowledge of Unix system and shell programming
Also critical to your success:

Familiarity with and desire to work using agile methodologies and practices such as Scrum/Kanban, iterations, user stories and development flows using continuous delivery, and automated testing
A drive to stay up-to-date with the latest web architecture technology including language innovations, containerization, storage technology and runtime problem solving
The ability to see and to understand the larger context in which your team works and to craft solutions within that context
Adaptability to changes in product requirements, organizational structures and business conditions
A strong belief in your personal responsibility for ensuring quality craftsmanship
An open, collaborative spirit
Bonus points:

Strong computer science fundamentals
Experience with Domain-Driven Design (DDD)
Experience in a fast-paced startup environment
Why American Express
Talk to our people and you’ll find out what we’re really all about. Open, creative, risk-taking, collaborative and innovative are just some of the expressions you’ll hear. It’s our culture that makes American Express an outstanding place to work, and a big part of why we regularly win best workplace awards all over the world. If you’re ready to take on a challenge and make an impact, you owe it to yourself to launch or grow your career here.
Employment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for these positions.
ReqID: 18008463
Schedule (Full-Time/Part-Time): Full-time
Date Posted: Sep 27, 2018, 3:40:27 PM","New York, NY",Data Engineer,False
349,"PeerStreet is an award-winning, Andreessen-Horowitz backed, platform focused on democratizing access to real estate debt. What we're working on at PeerStreet will ultimately change mortgage finance! PeerStreet is a leading online platform for investing in real estate backed loans (www.PeerStreet.com).

PeerStreet is strongly data-driven. Data is the foundation of our operational processes, ranging from loan underwriting, lender acquisition, loan servicing, to product decision making. Having access to high-quality data, internal and external, allows us to continuously optimize our operations and maintain great insights into how we can continue to improve product on both sides of the marketplace.

We are looking for a highly talented data engineer with a strong technical background and one who is passionate about engineering elegant solutions to data problems and creating an environment that allows users to dive deep into financial and user data with cutting-edge analytics and data insights. You will have a huge impact on defining and implementing PeerStreet data strategy. Your day-to-day will involve close collaboration with Engineering, Product, and Business organizations.

Responsibilities:

Work closely with product managers, engineers, and business stakeholders to become a source data expert.
Develop and optimize ETL processes to meet the growing business needs.
Define technical requirements and data architecture for the underlying data warehouse.
Collaborate with subject matter experts across different business units to design, implement and deliver insightful analytic solutions.
Improve and maintain data access for our BI tools (Periscope, Tableau)
Automate data quality monitoring and improve auditing capabilities.
Analyze trends and work with development teams to develop a long-range plan designed to resolve problems and prevent them from recurrence

Basic Qualifications:

Bachelor's Degree or greater (technical or science degree preferred).
3+ years of backend or data engineering experience.
Knowledge of data warehousing concepts
Experience with scalable architectures and large data processing.
Expert understanding of SQL, RDBMS, and data modeling for scalability and performance.
Experience in ETL, data mining, and using databases in a business environment with large-scale and complex datasets.
Experience with AWS services (e.g. EC2, RDS, DMS, Redshift, etc)
Experience with Linux and Shell Scripting
Experience with ETL orchestration tools such as Airflow, Pentaho, etc
Proficiency in a leading programming language (Ruby, Python, Java, etc)
System-level DBA functions including: configuring replication, automating backups, performance tuning, etc
Excellent communication ability

Bonus Points:

Experience with BI systems (Periscope, Tableau, etc)
Experience with dimensional data modeling
Experience with Terraform
Experience with RoR or Salesforce
Experience or deep knowledge in finance

We offer a competitive salary & equity, medical, flexible vacation, and an awesome team.","El Segundo, CA",Data Engineer,False
353,"Who we are
Our Data Management team at gradient A.I. is building an industry leading data pipeline and infrastructure for data science. From modeling and transforming distributed client data stores, to providing high-performance cloud infrastructure, to building client facing business insights and analytics our data management team is at the core of the data at gradient A.I.
What you’ll do
You will shape and realize the vision of our data insights. You will learn new technologies and tools and expand your competence in multiple engineering areas. Examples of the kind of work you might do include developing tools to extract and process client data from distributed sources, transform large amounts of data to provide the building blocks for our data science or scale our data pipelines. To do all this work you will collaborate with machine learning researchers, software engineers and project managers on the team.
Who you are
You are a multi-talented engineer excited about taking a high-level problem and designing and owning the details of implementing the solution. You are independent and creative with tools such as Python, SQL, or AWS. If you are an engineer excited about data, systems engineering or the power of cloud computing this could be the job for you. You probably have
Working knowledge of SQL or experience with non-relational databases
Experience with high-level programming languages, ideally Python
Experience with predictive analytics, algorithms, or machine learning
Desire to learn new skills and tools (eg. Redshift, Spark, Tableau, etc)
Requirements
Experience with software engineering best practices
Bachelor’s degree or higher in Computer Science or related field
An aptitude for systems software design","Cambridge, MA",Data Engineer,False
354,"As a ML/NLP/Large Data Engineer you will be responsible for building and improving ML models, modeling & experimentation frameworks and building out technical core of AgentIQ. You will be given opportunity to drive positive technology improvements and develop state of the art NLP/ML applications.
Responsibilities
Build and improve NLP/ML models at hearth of AgentIQ product
Build and improve infrastructure to support NLP/ML modeling, experimentation and deployment
Take ownership of NLP/ML trainer tools and data processing pipelines
Brainstorm and prototype algorithmic improvements
Develop customer specific and general machine intelligence
Contribute to deploying/monitoring/debugging models in production
Take ownership of NLP/ML trainer tools and data processing pipelines
Collaborate with platform teams on developing new tools and features needed for NLP/ML development and deployment
Create and maintain documentation
Provide internal training on applicable topics

Requirements
Passion for improving ML/NLP models and making them more robust and scalable
Thrive in a diverse, dynamic environment that leverages multiple tools and languages
The ability to communicate effectively with thoughtfulness and maturity
Make technology decisions that are best for the business of Agent IQ
Experience building large, production-quality NLP, speech, or deep learning systems
Strong software engineering and interpersonal skills
Ability and desire to quickly pick up on new topics and techniques
Ability to take an idea from conception and prototyping to deployment in production
Masters degree or equivalent in ML/NLP or related field

Our environment
AWS/GCP hosted infrastructure
Linux
Python
Tensorflow
Node.js
Docker
Perks
Competitive salary + equity
Full medical/dental/vision benefits
Unlimited PTO policy
Tons of snacks in the office and all-you-can-drink coffee
Convenient office within a 3 minute walk from BART/Muni Underground
Google apps, Dropbox, Drive, Slack, Mac (or PC) everything
Agent IQ swag
Commuter benefits
Great teammates
Perks
Full medical/dental/vision benefits (for 3+ months)Tons of snacks in the office
Weekly team lunches; happy hours; tons of snacks in the office and all-you-can-drink caffeinated drinksConvenient office within a 3 minute walk from BART/Muni Underground
Powerful Apple productsAgent IQ swag","San Francisco, CA",ML / NLP / Large Data Engineer,False
355,"Title: Senior Data Engineer
Reports to: VP – Analytics and Data Science
Location: NY Office

Overview: We are looking for a savvy Data Engineer to join our growing team of analytics and data science experts. They will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for the analytics and data science teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Primary responsibilities include but are not limited to:
Create and maintain optimal data pipeline and predictive modelling architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools and processes that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the IT, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing models.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Senior Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience building and support data modelling infrastructure and processes
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

Education & Experience Requirements
We are looking for a candidate with 4 to 6 years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Experienced on AWS tools (Redshift, Hive, Glue and RDS)
Experience building data pipelines, ETL/ ELT type applications on AWS
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with machine learning, data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Familiarity with R modeling language and predictive modeling concepts
Strongly motivated to be a player in a team which is constantly working to improve themselves through discovering new analytics techniques and software tools to improve the quality of our work
Strong verbal and written communication skills
Adaptability and the capability of multi-tasking and strong time management
Thrive in a fast‐paced, entrepreneurial environment comprised of high achievers and high stakeholder expectations
Ability to work in a team environment and dynamically align to changing business conditions
Ability and willingness to travel and work long hours and weekends if necessary to meet stakeholder demands

J.Crew Group, Inc. is an Equal Employment Opportunity (EEO) Employer
We are committed to affirmatively providing equal opportunity to all associates and qualified applicants without regard to race, color, ancestry, national origin, religion, sex, marital status, age, sexual orientation, gender identity or expression, legally protected physical or mental disability or any other basis protected under applicable law.","New York, NY 10003 (Greenwich Village area)",Sr. Data Engineer,False
356,"As a Data Engineer for Business Intelligence at Compass, you will be responsible for helping to build the data-driven decision-making culture throughout the organization. You'll work as part of a rapidly growing team in a fast-paced environment. You will be responsible for managing large-scale business systems initiatives that impact multiple functions and teams across the organization. In this high impact role you will have an opportunity to work with emerging technologies, while driving business intelligence solutions end-to-end: business requirements, workflow instrumentation, data modeling, ETL, metadata, reporting, and dashboarding. You are someone who loves data, understands enterprise information systems, and has a strong business sense.
At Compass You Will:
Design, develop, and implement the infrastructure that elevates data-driven decision-making for our proprietary real estate technology
Work with the enterprise business systems that facilitate the end to end experience of real estate transactions
Work with company stakeholders and Product and Engineering teams to define analytics requirements
Deliver flexible and scalable solutions from end-to-end, harvesting process-level data and transforming it into normalized data marts from which operational and process metrics and analysis can be reliably generated
What We're Looking For:
Bachelor's degree in Computer Science, Information Systems, or related field
3+ years of SQL development experience
3+ years of Data modeling, ETL, and Data Warehousing experience
Familiarity with ETL tools such as Informatica, Pentaho, Talend, etc
Expertise in modern OO language (e.g. Java, C#, C++, Objective C)
3+ years or Python Scripting experience
Familiar with AWS as a Platform
Strong business communication skills
Experience with AWS technologies such as Redshift, RDS, EMR, etc.
Comfortable in a Linux environment
Capable of data processing and transference outside of ETL tools or databases (custom scripts to pull and load from APIs or Files)
Experience writing software requirements
Familiar with Enterprise Networking
At Compass, our mission is to help everyone find their place in the world. This means we continually celebrate the diverse community different individuals cultivate. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone's place.","New York, NY","Data Engineer, Business Intelligence",False
357,"Senior Data Engineer- KYC Entity Exchange

New York, NY - USA

Posted 2018-10-05 - Requisition No. 71150

As the Know Your Customer (KYC) industry evolves at a rapid pace, we are responding by building innovative and best-in-class enterprise software solutions that enable our clients to gain a competitive edge in their industry. Our products, Entity Exchange and Entity Intelligence, provide our clients’ ways to optimize workflows, satisfy KYC regulatory laws, receive proactive notifications, and ultimately reduce the friction of doing business.

As an engineer on the team, you will be contributing to our client’s Entity Exchange, a centralized, secure platform to enable trading counterparties to manage and share client data and documents. The product accelerates the onboarding process between brokers, hedge funds, and corporations while letting each party maintain control of their information.

In our dynamic and collaborative environment, you will design and build application services that are flexible, scalable, and easy to maintain. You will also help figure out the right solutions for our clients’ needs. If you are passionate about helping us build these solutions, we want to hear from you.

We’ll trust you to:

Design, architect, and develop application data solutions that solve business problems in innovate ways
Design and Develop robust fault tolerant ways to store and access data
Collaborate within an agile, multi-disciplinary, fast-moving team
Advocate for high quality, well-tested solutions
Take ownership and drive technical solutions from inception to production release

Need to have:

3-5 years proven experience with technical architecture/design and implementation of enterprise scale software projects
Experience in writing software in 1 or more languages, ideally Python
Strong technical problem solving skills, good ability to troubleshoot and debug
Experience in data modelling, data engineering, software design, ETL
Experience with Relational and NoSQL databases, including schema design, transactions, and performance tuning
Experience using ORMs for application/database integration (SQL Alchemy, Django, Hibernate, or other)
Experience designing applications that employ encryption when data is stored
Experience with software best practices including automated testing, continuous integration, and documentation
BA, BS, MS, PhD in Computer Science, Engineering or related technology field

We’d love to see:

Experience building asynchronous services and message queues (Rabbit MQ, Celery)
Experience building and defining API interfaces and building RESTful services using OAS 3.0
Experience with designing software for and deploying to private clouds

","New York, NY",Senior Data Engineer,False
358,"If you are an active Vivint employee, please apply through Workday by searching ""Find Jobs"".
Job Description
JOB SUMMARY:
As a Data Engineer on our Data Engineering team, you will be responsible for developing the core service that provides data to the entire enterprise. You will be responsible for building the processes that support the ingestion and consumption of data at Vivint. Working closely with our Data Ops and Business Analytics teams, you will design, build, and maintain a data warehouse platform that provides timely, accurate, and reliable data to thousands of users. Your role will be critical in defining the appropriate architecture and processes needed to build a data warehouse that is flexible, agile, reliable, responsive, and scalable.
JOB RESPONSIBILITIES:
Help build and maintain an enterprise data warehouse platform with its associated data pipelines and data architecture requirements
Responsible for designing, building, and maintaining robust, high-performing ETL processes
Implement best practices and innovative ETL solutions to provide timely, accurate, & reliable data
Evaluate, recommend, and implement proper tools & technology to achieve a high performing data warehouse platform servicing thousands of users and a broad set of use cases
Build cross functional relationships with data analytic teams and business leaders to understand their requirements and data needs
Develop data products for delivery via web and mobile technologies.
REQUIRED SKILLS:
Must have a passion for data and helping the business turn data into information and action
3+ years of data warehousing architecture & design experience
3+ years of ETL & data pipeline development experience
Ability to initiate, drive, and manage projects with competing priorities
Ability to communicate effectively with business leaders, IT leadership, and engineers
Expert in SQL, databases, and ETL development processes & tools (Cloud MPP like Snowflake or Redshift)
Proficiency in one or more scripting languages (Python, PHP, Perl, .net, etc.)
Familiarity with one or more web technologies (Javascript, Node.js, Angular, etc.)
Bonus Skills:
Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, etc...)
Experience with Tableau or similar data visualization tool
Experience with AWS or Azure data product offerings and platform
Experience with machine learning technologies (R, SparkML, AzureML, etc.)
MINIMUM QUALIFICATIONS:
BA/BS or higher in Computer Science, Information Systems, Math, or other technical field
Master’s Degree preferred","Lehi, UT 84043",Data Engineer,False
359,"ABOUT US
Lark is the world's largest A.I. healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with A.I. Nurses. We’re on a mission to improve people’s health and happiness through our digital health coach. We are the only A.I. nurse ever to become fully medically reimbursed to 100% replace a live nurse because we achieved equivalent health outcomes to live healthcare professionals - which allows for infinitely scalable healthcare. Since launch Lark has continued to receive awards and accolades for both our product, and our leadership.

✦Apple's Top 10 Apps in the World
✧ Business Insider 's most innovative companies in the world along with Uber and Snapchat
✦ Biz Journal ’s 100 Women of Influence

We are looking for a talented data engineer to join our growing team in Mountain View, CA, where you'll be building our next generation data pipelines.

** Open to Temp Contractors for immediate fill **

ABOUT THE ROLE

What You'll Do:

Build our next generation data pipelines into a fast and efficient big-data system
You'll be the first fully dedicated data engineer on the team, and will be able to call the shots on strategy

What You'll Need:

A love of data, and the make-or-break effect it has on startups
Default to coding efficient systems from large databases, both RDBMS and noSQL.
Familiarity with the following key technologies (or similar): * Spark
Yarn
Kafka
Python
AWS

JOIN US
Our team works with cutting edge tools and technology related to Artificial Intelligence and Machine Learning. We are using NLP to process millions of meals, and accelerometer data to compute activity and sleep amounts from users' phones. Our chat AI is the most sophisticated digital health engagement tool in the world. Join us and make it even better!

Lark is an Equal Opportunity Employer. Lark does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.","Mountain View, CA",Data Engineer,False
360,"Who You Are:
Getty Images is looking for a Data Engineer who enjoys working across the entire lifecycle of Machine Learning projects and takes pride in deploying high-quality ML and data workflows.
The mission of the Data Science team at Getty Images Inc. is to leverage internal and third-party data to inform other groups on how to interact with its customer base. We achieve this goal by 1) building automated solutions that apply best-in-class Machine Learning and Engineering practices and 2) continuous interactions with stakeholders to identify critical needs that deliver results relevant to the business.
As a Data Engineer on the Data Science team, you will have end-to-end autonomy and ownership of your projects, and will work closely with other business units to develop creative solutions to a variety of problems.

Your Next Challenge:
You will join a team of highly-collaborative and curious Data Scientists and Data Analysts that are comfortable working with a diverse set of tools, and willing to take initiative on their ideas. As a member of the team, you will have the chance to define the technical architecture that serves as the foundation for upstream analytical projects, and accelerate the delivery of a robust portfolio of Data Science models.
Your primary goal will be to catalyze the development and deployment of full-stack Machine Learning pipelines. You will have the opportunity to continuously develop and ship code in our production environment, and will be empowered to implement a variety of data-centric architectures that support critical operational initiatives.
You will also interact with the entirety of Getty Images Inc. technology stack, and collaborate with data infrastructure, platform and cloud engineers to design and build a production-level data ecosystem that aligns with business function requirements and capable of handling large-scale structured and unstructured data. You will also have the opportunity to continuously evaluate and provide guidance on the use of new technologies.
We value learning and development, and you will be given every opportunity to work on projects that excite you. You will get to lead and innovate as a thought-leader within Getty Images, and will sit at the intersection of Engineering, Marketing and Leadership to inform, influence, support, and execute on our decisions.

What You'll Need:
You have prior experience working as a Data Engineer, preferably in a product or customer-focused organization.
You are extremely comfortable working with Python and have a working knowledge of Cloud services and Tools, as well as standard engineering tools such as Git, Linux and SQL.
You have experience building streaming and batch data pipelines and are comfortable working within a large-scale distributed environment with open source tools such as Hadoop, Hive, Airflow and Spark.
You can independently execute on a project, from ideation to delivery to stakeholders, and can pro-actively interact with other engineers at Getty Images to access necessary resources or data.
You understand, or have interest learning about, the real-world advantages and drawbacks of various Machine Learning techniques, and have applied those to a variety of datasets.
Nice to Have:
A M.S. or Ph.D. in computer science, statistics, economics/econometrics, natural science or any other equivalent quantitative project is preferred. If you are self-taught and believe you are a good fit for this role, or have significant work experience, we would love to hear from you as well.
Previous experience in an analytical role, or experience working with teams of Data Scientists and Data Analysts.
Experience having managed or contributed to the use of Business Intelligence platforms.

#LI-MM1

Who We Are:

Getty Images is the most trusted and esteemed source of visual content, with over 200 million assets available through its industry-leading sites www.gettyimages.com and www.istockphoto.com. The Getty Images website serves creative, business and media customers in almost every country in the world and is the first place people turn to discover, purchase and share powerful content from the world's best photographers and videographers. Getty Images works with over 200,000 contributors and hundreds of image partners to provide comprehensive coverage of more than 160,000 news, sport and entertainment events, impactful creative imagery to communicate any commercial concept and the world's deepest digital archive of historic photography.
For company news and announcements, visit our Press Room. Find iStock on Facebook, Twitter, Instagram and LinkedIn, or download the iStock app where you can easily search, save and share superior images to create standout visual communications.

Getty Images is an equal opportunity employer and strongly supports diversity in the workplace.","New York, NY",Data Engineer,False
361,"$100,000 - $130,000 a yearMUST-HAVESAt least 5 years of data engineering or related experienceSQL & relational databaseExperience with data warehousesOur client is looking to add an experienced Data Engineer to their Technology Team in Philadelphia. The ideal candidate will be adept at problem solving, interested in pursuing new ideas, and will play a key role in the strategic initiatives of an innovative global investor. Responsibilities include gathering requirements, building out a data warehouse, establishing and maintaining data integrations, developing data governance best practices, and optimizing data flow.They are looking for candidates that are passionate about building and optimizing data systems. The Data Engineer will collaborate with developers, data scientists, and technology products. They will also support non-technical colleagues in the collection and use of structured and unstructured data. They must be self-directed and comfortable supporting multiple projects and teams. This hire will contribute to data transparency across the organization, driving operational efficiency and providing decision makers with actionable insights.JOB DESCRIPTION: Project Management – gather project requirements, establish timelines, track progress, and manage to milestone achievementsModeling – determine the most appropriate schema for storing structured and unstructured data Extract, Transform, Load – apply business logic to move data from one system to another and validate data qualityIntegration – determine the optimal methods for collecting and incorporating new data into data warehousesGovernance – establish and educate the organization on data governance standardsStrategic Reporting – collaborate with colleagues to scope out new data requests and methods to extract and present data from various data sourcesAutomation - implement internal process improvements with an aim to automate manual processes and optimize data deliveryNECESSARY QUALIFICATIONS: 5+ years experience in data engineering, preferably in financial servicesBA/BS in a related field (e.g., computer science, mathematics, engineering)Must have experience with object-oriented programming languages and agile software developmentMust have proficient technical skills in SQL and relational databases, exposure to data integration tools, and experience building and consuming APIsMust have experience building a data warehouse in a professional environmentExposure to statistical data analysis tools (e.g., R) and data visualization tools (e.g., Tableau) is a plusMust have proficient communication skills, be proactive and be able to comfortably lead projects independently that include cross-functional collaborationNICE-TO-HAVESAgile software developmentSALARY RANGE$100,000 – $130,000PERFORMANCE BONUS10-15%SIGNING BONUSNoneBENEFITSMedical Ins.Dental Ins.Vision Ins.Life Ins.RetirementEquityMORE INFORMATIONRELOCATIONNoneREPORTS TOStrategic Technology TeamREMOTE WORKUp to 20% of the timeTRAVELTravel not requiredVISAVisa sponsorship not supportedJob Type: Full-timeSalary: $100,000.00 to $130,000.00 /yearExperience:Tableau and R: 1 year (Preferred)Data Engineering: 5 years (Required)SQL: 1 year (Preferred)Software Development: 3 years (Preferred)Education:Bachelor's (Required)Work authorization:United States (Required)","Bala-Cynwyd, PA",Data Engineer,False
363,"$95,000 a yearTitle: Big Data EngineerLocation: Dearborn, MIType: Full TimeSalary: $95,000+benefits5 years of experience in a Data Engineer roleAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Proficiency building and optimizing ‘big data’ data pipelines, architectures and data sets.Background performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience supporting and working with cross-functional teams in a dynamic environment.Knowledge and Skills:Strong analytic skills related to working with structured and unstructured datasets.Project management and organizational skills.Experience with relational SQL and NoSQL databasesKnowledge of data pipeline and workflow management tools: i.e.: Knime, DataFlow, DataPrep, Airflow, etc.Familiar with object-oriented/object function scripting languages: i.e.: Python, Java, C++, Scala, etc.Familiar with big data tools. Examples include: Hadoop, BigQuery, Kafka, etc.Job Type: Full-timeSalary: $95,000.00 /yearExperience:DataFlow: 3 years (Preferred)NoSQL database: 2 years (Preferred)query authoring (SQL): 3 years (Preferred)Data Engineer: 5 years (Preferred)Bigdata: 4 years (Preferred)metadata: 2 years (Preferred)Education:High school (Preferred)","Dearborn, MI",Big Data Engineer,False
364,"Animoto is looking for a data-driven Software Engineer with a focus on analytics and data warehousing. You are highly analytical, ask the right questions, and are a true self-starter. You will be the “reality expert” for Animoto, helping leadership across functions translate their products and questions into actionable metrics, analysis, reports, and recommendations.

In the role of Data Engineer, you will be responsible for architecting, designing, and developing Animoto.com’s data infrastructure to support the wide variety of application and analytical needs. We want someone smart, quick, and creative: an engineer who will dig deep into the system and find various ways to improve it; who has not only an understanding of how web technologies work today, but where they are going in the future.
Why Animoto Wants You:
You love having a big impact and high visibility.
You are exceptionally detail-oriented.
You’re a great debugger and like finding and solving mysteries.
You are customer focused and eager to work with stakeholders to deliver value.
Excellent analytical skills to deliver meaningful and impact-driven insights using big data.
The Day to Day:
Design and implement data systems
Design and implement dimensional data models and systems that scale
Partner with product, engineering and analysts to explore structured and unstructured data to leverage business insights
What We’re Looking for:
Bachelors degree in CS or equivalent work experience
Solid experience in languages like Ruby or Python
Strong experience with SQL
Experience with Splunk or other Elasticsearch tools
Familiarity with ETL and BI concepts
Bonus Points:
Comfortable with AWS (Redshift, Athena, etc)
Comfortable working in application code when needed
Familiarity with Looker
Why You'll Want to Work at Animoto:
Be a part of a thriving engineering team that includes opportunities to collaborate with teams across the organization including, product and design.
Learn and grow within a role that is important to you- we encourage exploration and the chance to work on interesting projects that challenge you professionally.
You resonate with our company’s mission and value teamwork.
We have an inclusive and quirky culture here! Ask us about our values: Humbletude, Betterfication, and Oomphosity.
Great benefits. We offer competitive salary, bonus, equity, 100% paid medical, dental, and vision for you and your dependents, and paid time off to name a few.
At Animoto, we help our customers communicate who they are, what they do, and what they love through video. Our users are connected by the desire to use video to share what matters most to them, but come from all walks of life and are passionate about all sorts of different things. We’re proud to help them share these passions.
Similarly, we embrace the differences of our team members and actively seek diversity of beliefs, backgrounds, education, and all the other things that make us unique. We strive to create a space where employees can bring their true selves to work every day. By doing so, we’re building an inclusive culture where we can continue innovating for our customers. We too are united in the belief that our voices are even more powerful when using video to communicate, and we aim to have a workplace that reflects the variety of our users. Animoto is proud to be an equal opportunity workplace and affirmative action employer.","New York, NY",Data Engineer,False
365,"Summary:
The Data Engineer is responsible for developing systems to acquire, analyze, and gain actionable insight from data that promotes the SPLC's mission; preparing and implementing quantitative and qualitative models for analysis; collaborating with attorney and policy subject matter experts through the process of developing and using these systems and models; using data to lift up and tell stories to support litigation and policy transformation; and working closely with a variety of internal and external partners and stakeholders to these ends.

Strong analytical and reasoning skills that result in clear, robust, flexible architectures. Proven ability to drive complex design and development efforts in an agile environment.

Primary Job Functions:
Under the supervision of the Big Data Manager, the Data Engineer will:


Implement their knowledge of cloud, serverless, and hybrid-cloud technologies;
Provide oversight of data collection and analysis, synthesis of data and study outcome reports, and quality assurance and auditing of data in an Azure environment;
Find, import, transform, validate, and/or model data with the purpose of understanding or drawing conclusions from the data;
In collaboration with key SPLC staff, develop the necessary data and analysis to assist in the dissemination of information on successful and promising approaches, lessons learned, and other policy priorities to local and regional government bodies, partners, and other stakeholders;
Participate with the SPLC data team in the process of data governance, quality, and integrity, and manage the implementation of related policies;
Create and maintain detailed up-to-date system documentation;
Comply with all federal and state lobbying and ethical rules and requirements;

Qualifications –

Education and Related Work Experience:

A bachelor's degree and at least four (4) years of relevant experience are required; advanced degree in statistics, data science, public policy, and/or other relevant field is required except in unusual circumstances to be evaluated on a case-by-case basis;
Strong research and analytical skills applied to public policy issues, including an ability to synthesize and summarize large amounts of information, and to focus quickly on the essence of an issue;
Strong Knowledge of Microsoft Azure (SQL Server, Azure SQL Database, CosmosDB, Services Logic, Flow);
Demonstrated experience in indicator selection, quantitative and qualitative data collection and analysis, and multiple data stores (g., SQL & NoSQL) on premise, hybrid, and Cloud;
Strong knowledge of Data analysis languages (e.g., Python, R, etc.)
Demonstrated experience in data collection tools and mitigation for measurement errors as to ensure data reliability;
Demonstrated experience in solo and collaborative development of data triangulation, presentations, and report-writing;
Demonstrated experience with capturing, analyzing, and visualizing data from research studies, assessments, and evaluations;
Demonstrated experience communicating clearly and concisely orally, visually, and in writing;
Demonstrated experience assisting with or conducting briefings with internal and external partners and other stakeholders, when necessary;

Knowledge, Skills, and Abilities:

Ability to be an SME in multiple projects, all of which have competing deadlines and require cooperation of various people inside and outside the organization;
Ability to organize data and communicate to all levels of the organization and external partners and stakeholders at all levels of expertise;
Initiative, vision, and a commitment to social justice;
Excellent and consistent attention to detail and the ability to prioritize and meet deadlines;
Willingness to travel (amount of required travel varies depending on SPLC needs);
Willingness to work varied/flexible hours depending on SPLC needs; and

Other Special Considerations:
This job is performed under general office conditions and is not subject to any strenuous physical demands or dangerous conditions.

Disclaimer:
The statements herein are intended to describe the general nature and level of work being performed by the employee in this position. These statements are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of a person in this position.

An Equal-Opportunity Employer with a Commitment to Diversity

Southern Poverty Law Center (SPLC) is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, gender identity, color, sexual orientation, religion, marital status, disability, political affiliation, national origin, or prior record of arrest or conviction.",Alabama,Data Engineer,False
366,"The MIS Data Engineering team is responsible for designing and developing the enterprise database architecture, data warehouse, and reports on the WMIS space in AM. Our primary focus includes data lakes, automated data analysis, DevOps, and continuous integration of data visualization as well as reporting software. Individuals on this team are self-driven, excited to learn new technologies, and interested in solving challenging problems.

The main role will be to use different types of data gathered from our existing financial technologies to generate reports and test existing/PoC software to diagnose different failures in addition to measuring success. We work with cross-functional teams of software engineers, QA/Validation, designers, and infrastructure technicians. With the recent adoption of a data lake, this composition may change to be dominated by Hive (SQL on Hadoop), Shell/ Python scripts, and a Scala programs. On this team we act as responsible engineers for assigned products. Maintain highest standards of excellence, never settling for the status quo and dive deep into clients' issues and find efficient solutions.

Skills:
High degree of accuracy along with attention to detail, excellent oral and written communication skills, strong interpersonal skills.
Partner with non-technical stakeholders to understand their analytical needs, help frame the problem by asking the right questions, document and prioritize requirements. Clearly communicate results of complex technical work. High standards for code quality, maintainability, testing, and performance.
Knowledge of SQL/ NoSQL (Hive)
Experience with UNIX Shell Scripting
Experience with Git version control
Proficiency in one (or more) of the following: C/C++, Java, Scala, Python
Experience with all aspects of the software development life cycle (SDLC) in a professional team environment including requirements collection, solution design, implementation, code reviews, testing and operational support
Large data analysis and visualization experience


Preferred Background:
Strong understanding of/ experience with distributed, horizontally scalable systems such as Hadoop or Spark
Message queues such as Kafka
Experience with Qlik","Jersey City, NJ 07310 (Downtown area)",Data Engineer,False
367,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
Comcast shapes the future at the intersection of media and technology. We create world-class experiences that people love and trust and drive innovation that builds value. We bring millions TV, Internet, entertainment, sports and news, communications and home management, theme parks, television and movies. Comcast brings to life the best of what's to come.SUMMARY: Are you a Data Engineer that loves working with very large data sets? Are you skilled in using SQL, Hive, or Python to integrate large data sets into meaningful assets that can be used by the business for analytics? If you answered yes to any of the questions above, please read on!The Comcast Enterprise Business Intelligence team needs Data Engineers to help us architect and build robust data solutions that can be used by the Data Science & Analytics teams, as well as, analysts throughout the business. The role requires you to collaborate with both technical and non-technical folks so, unfortunately you won't be able to speak techie all the time. However, you will be involved in a variety of projects allowing you to grow your knowledge and skills beyond what you thought was possible.We spend a lot of time and effort architecting, building, and automating our solutions so, hopefully it's no surprise that we take data quality very seriously! We'll ask you to use your Jedi engineering skills on data quality efforts from time to time. It's fun and a great way to learn our data!WHAT YOU WILL BE DOING: - Transforming large, complex data into business assets that serve both the Enterprise Business Intelligence team and analysts throughout the organization- Providing appropriate data for a given analysis. This would require you to work with data modelers/analysts to understand the business problems they are trying to solve and create or augment data assets to feed their analysis.- Explore and recommend innovative solutions to complex problems. How cool is that?- Ensure our data assets meet our data quality standards. It's important!- Have fun in a fast paced energetic environmentWHAT YOU NEED: - 2+ years of relevant employment experience- Teradata experience preferred. We're looking for power users not administrators- Strong SQL, we mean REALLY strong. We want you to be excited about SQL scripts that are hundreds of lines- Experience transforming large datasets into consumable assets for self-service analytics and reporting- Experience designing, implementing and supporting Data Marts- Must be familiar with Linux systems, including basic shell scripting- Design, develop, and maintain data aggregation, summarization jobs (i.e. automation)- Experience with DevOps processes and principles- You need to be flexible to changing priorities and comfortable in a fast paced dynamic environment- Knowledge of Amdocs and/or CSG billing systems a plus- Good generalist experience is a plus, ideally working with all layers in the technology stack. If you're ""good"" in various technologies, we should talk.WHAT YOU GET: - An opportunity to work with an excellent and exciting engineering team- A fantastic work environment- An awesome boss and mentor- Work on challenging projects- Learn new stuff- Competitive salary- Comprehensive benefits package- Early exposure to new Comcast products and servicesWhat are you waiting for? Interviews are occurring immediately, don't miss out on this incredible opportunity!
Comcast is an EOE/Veterans/Disabled/LGBT employer","Philadelphia, PA 19103 (Belmont area)","Engineer, Data Engineering & Apps",False
368,"$60 - $80 an hourContractJob title: Sr. Data EngineerLocation: Atlanta, GADuration: 12 months+I-94 required for H1b'sSkill sets Required: SparkAWSPythonGood SQL,Snowflake Schema (Preferred)good database backgroundEnglish proficiency (spoken/written)Job Type: ContractSalary: $60.00 to $80.00 /hourExperience:Spark: 4 years (Preferred)AWS: 5 years (Preferred)Python: 4 years (Preferred)SQL: 4 years (Preferred)Work authorization:United States (Required)","Atlanta, GA",Senior Data Engineer,False
369,"ContractOverview
C009 Big Data Engineer
Location: Harrisburg, PA
Position Type: Contract
Length of Project: 8 weeks

Description
This position will be working with data scientists, helping with migrating structured and unstructured data sets into Hadoop using Sqoop, Hive and other ETL technologies. Will work with the Hadoop development team on data design, and data acquisition process.
Will be reporting to project engagement manager and will be responsible for data design, data wrangling and data security on Hadoop and S3-AWS Cloud. The project is related to risk prediction across various government entities based on both structured and unstructured data, with emphasis on text analytics, NLP (natural language processing).

Experience with the Following is Required
Working with Big Data including the following:
Data design
Data acquisition process
Data wrangling
Data security
Hadoop
Sqoop
Hive
Hortonworks
ETL technologies
AWS/Azure Cloud
Text analytics and NLP (natural language processing)","Harrisburg, PA",Big Data Engineer,False
373,"ContractWe are looking for a Data Engineer(PowerBI, Sql) for our client in Redmond, prefer local candidates. It's a 18 months contract role or Long term contract.ResponsibilitiesAnalyze data and help prepare dashboards that provide better insights of service usage.Work on and execute existing infrastructure.Do reverse engineering to understand existing Power BI charts and help prepare new reports by computing data through SQL queries.QualificationsBachelor's Degree in computer science or related field, or equivalent level of practical experience3+ years of big data-related software engineering experienceHigh usage of Kusto QueriesStrong SQL querying knowledge with Power BI experience is a must.Must be able to ramp up quickly.Company Benefits: Visa/Green Card SponsorSick LeavePTOHoliday401KAbout CSI Interfusion:Chinasoft International Limited (CSI, HKSE: 00354), founded in 2000, is an industry leader in globalized software and information technology services with branches in 28 cities across China and 18 cities around the world. CSI draws upon its complete ecology of industrial resources and provides multi-field technological services such as cloud computing, big data and etc. to help clients tackle challenges and accomplish digital transformation. The main business of CSI includes consulting, solutions and technological services targeting major clients and industries; online and offline operation of internet software crowdsourcing services centered on the self-owned ITS cloud platform “Joint Force”; IT education and training system composed of ETC, the offline training center, EEC, the experience center and Zker, the online study community.Job Type: ContractExperience:Power BI: 3 years (Required)SQL: 3 years (Required)Education:Bachelor's (Required)","Redmond, WA",Data Engineer 2/BI Engineer 2,False
374,"Data Engineer permanent job in New York City. Big Data background with experience in relational databases, SQL, NoSQL, Hadoop. Preferably someone with an analytics or digital advertising background, and some project management or leadership.

Duties:

Create technical product roadmap with input from the delivery team, stakeholders, and leadership
Develop and code the data management services
Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership

Required:

Java
Scala and Spark
Docker and images running on DCMS
Apache Spark for data ingestion
Kafka for streaming apps, or NoSQL DBs like Casandra or Postgress
5-10+ years of software development experience, as a developer or manager
1-2 years of experience as a development manager (including direct reports)
2-5+ years of experience with digital advertising technologies
OO design, data structure, and algorithm design skills
Fluency in at least 1 of the following programming languages (C, C++, Ruby, R, SAS, MapR, Python)
2-5+ years of experience with both relational database design (SQL), non-relational (NoSQL), big data, real-time technologies
Scalable computing mechanisms such as Hadoop and Amazon Elastic MapReduce
Web application development and associated skills (REST, HTTP, web services)
Bachelor¹s degree in Computer Science or related field

","New York, NY",Data Engineer,False
376,"About inMarket
--------------

The most sophisticated companies in the world, such as Procter and Gamble and Walmart, rely on inMarket to engage with consumers and create proven ROI. The most trusted news publications such as Wall Street Journal, Forbes, and Business Insider utilize inMarket's first party data to understand how consumer behavior is evolving and what it means for global brands. These brands rely on inMarket's third party verified data which is both IAB certified and NAI compliant.

Our competition has raised over $400 million dollars from VCs, we've raised $2MM and are thriving after 8 years of growth. How did inMarket become the leader in digital advertising and consumer insights? By finding the best people in the world!

Founded in 2010 in Venice, CA by award-winning tech entrepreneurs, inMarket strives to embody the team mindset needed to thrive in the modern, digital workplace -- including all the quirks and creativity of our Venice roots. We are a high energy, fast paced company and a place where entrepreneurial self-starters thrive knowing teammates have all the bases well-covered. With offices in Venice, Chicago, New York City, and Bentonville, inMarket is a growing company who has kept the spirit, culture and focus of an explosive tech startup. We are always in search of top talent like yours to join our family.

About You
---------

You are a good peep who has a blast going above and beyond to tackle daring challenges. Despite adversity, you somehow find a way to make stuff happen and are agile in a world of accelerating changes. You creatively brainstorm yet are data-driven in your decision making.

Job Description

At inMarket our number one priority is our customers and reaching them at the precise moment. The Data Scientist at inMarket combines deep data analysis and research of our rich user data to present a compelling vision around user retention, user behavior, and preferences across the vast ecosystem of product offerings and content.

We are looking for data scientists who are passionate about using data to drive strategy and product recommendations and is able to develop successful algorithms to help understand the customer. You will be engaged with senior leaders to design well-constructed analyses and work cross-functionally with analysts, product managers, and engineers to effectively deliver actionable results. You will work on a variety of domains such as data science, lead cutting-edge analytical solution development pipeline, and contribute to external research via attending conferences and collaborations. You will support data and insight needs across a wide range of functions and activities to help us better understand our data with cutting-edge analyses and interactive visualizations. The ideal candidate has a proven track record of analyzing large datasets to identify meaningful information and insights, and creating valuable products out of data.

Your Day to Day


Create statistics via data mining in a variety of areas including customer analysis and user behavior
Invent and fast iterate on novel solutions to challenging data related problems.
Develop scalable and efficient methods for large-scale data analyses and model development.
Collaborate with developers, program managers, and product managers in an open, creative environment.
Coaching and providing research and system guidance to a team of other researchers on a variety of areas including data analysis

Required Qualifications


Bachelor's degree preferably in Statistics or related quantitative field (e.g. Computer Science, Econometrics, Mathematics, Physics, Operations).
Master's Degree technical/scientific/analytical field preferred, but not required.
3+ years related experience such as analyzing data and/or building analytical models in a professional setting
Ability to draw conclusions from data and recommend actions.
Demonstrated self-direction.
Hunger to continue learning and developing as a data scientist. Willingness to help further build the team, including contributing ideas, establishing best practices, following trends, and attending conferences.
Perform statistical and other data analysis to inform decision making and drive content creation
The ability to present results in a clear and concise manner to non-technical teams.
An appreciation and understanding of good design in both software UI and slideshow presentations.

Perks


Competitive salary depending on experience
Opportunity to work for one of leading mobile startups in the US
Equity Appreciation Grants
Comprehensive benefits to include Medical, Dental, Vision, supplemental benefits, and a Flexible Spending Account (FSA) which includes Transit & Medical coverage
Paid Maternity and Paternity Leave
Company matched 401(k) Plan
Unlimited PTO
Continuing Education and Learning Program
Matching Charitable Contributions Program
Weekly catered lunches
Fully stocked kitchen
Free Shared Bike Membership (Breeze, Citi, Divvy, and Hudson)
Dog friendly offices with Zero-Gravity massage chairs
AND MORE!

","Venice, CA",Data Engineer,False
377,"VividCortex is a groundbreaking database monitoring platform that gives developers and DBAs deep visibility into the database. Our solution is delivered as software-as-a-service and helps our customers see and analyze the work their databases are doing in unprecedented detail. It addresses critical issues in measuring and managing today's large, distributed, diverse storage tiers composed of multiple different clustered products, all working together. VividCortex is headquartered in Charlottesville, Virginia with a soon-to-come office in the Arlington, VA area and remote team members in the US and abroad.

We have a fast-growing customer base of well-known companies, and a tremendous reputation in our market for delivering a high-quality, innovative solution for database performance problems that are common in thousands of enterprises.
About the Role

VividCortex is looking for an experienced Data Engineer to architect and build our next-generation internal data platform for large scale data processing. You are at the intersection of data, engineering, and product, and run the strategy and tactics of how we store and process massive amounts of performance metrics and other data we measure from our customers' database servers.

Our platform is written in Go and hosted on the AWS cloud. It uses Kafka, Redis, and MySQL for data storage and analysis. We are a DevOps organization building a 12-factor microservices application; we practice small, fast cycles of rapid improvement and full exposure to the entire infrastructure, but we don't take anything to extremes.

The position offers excellent benefits, a competitive base salary, and the opportunity for equity. Diversity is important to us, and we welcome and encourage applicants from all walks of life and all backgrounds.

Remote candidates will be considered depending on location and time zone alignment, with periodic travel to a VividCortex office.
Responsibilities:
Work with others to define, and propose for approval, a modern data platform design strategy and matching architecture and technology choices to support it, with the goals of providing a highly scalable, economical, observable, and operable data platform for storing and processing very large amounts of data within tight performance tolerances.
Perform high-level strategy and hands-on infrastructure development for the VividCortex data platform, developing and deploying new data management services both in our existing data center infrastructure, and in AWS.
Collaborate with engineering management to drive data systems design, deployment strategies, scalability, infrastructure efficiency, monitoring, and security.
Discover, define, document, and design scalable backend storage and robust data pipelines for different types of data streams.
Write code, tests, and deployment manifests and artifacts, using CircleCI, Git and GitHub, pull requests, issues, etc. Collaborate with other engineers on code review and approval.
Measure and improve the code and system performance and availability as it runs in production.
Support product management in prioritizing and coordinating work on changes to our data platform, and serve as a lead on user-focused technical requirements and analysis of the platform.
Help provide customer support, and you'll pitch in with other departments, such as Sales, as needed.
Rotate through on-call duty.
Understand and enact our security posture and practices.
Continually seek to understand and improve performance, reliability, resilience, scalability, and automation. Our goal is that systems should scale linearly with our customer growth, and the effort of maintaining the systems should scale sub-linearly.
Contribute to a culture of blameless learning, responsibility, and accountability.
Manage your workload, collaborating and working independently as needed, keeping management appropriately informed of progress and issues.
Preferred Qualifications:
You are collaborative, self-motivated, and experienced in the general development, deployment, and operation of modern API-powered web applications using continuous delivery and Git in a Unix/Linux environment.
You have experience resolving highly complex data infrastructure design and maintenance issues, with at least 4 years of data-focused design and development experience.
Experience building systems for both structured and unstructured data.
You are hungry for more accountability and ownership, and for your work to matter to users.
You’re curious with a measured excitement about new technologies.
AWS infrastructure development experience.
SaaS multitenant application experience.
Ablility to understand and translate customer needs into leading-edge technology.
Experience with Linux system administration and enterprise security.
Mastery of relational database technologies such as MySQL.
A Bachelor’s degree in computer science, another engineering discipline, or equivalent experience.",United States,Data Engineer,False
378,"About Accion:
Accion (us.accion.org) is a nationwide nonprofit community lender dedicated to helping entrepreneurs generate income, build assets, create jobs and achieve financial success through business ownership. Our network serves small businesses in communities across the U.S. and is made up of four certified Community Development Financial Institutions (CDFIs) and a U.S. Network office. Globally, for more than 50 years Accion has helped over 90 partners serve the financial needs of tens of millions of people in 40 countries.

Accion Serving Arizona, Colorado, Nevada, New Mexico, and Texas is looking for a Data Engineer to join our operations team either at our Albuquerque headquarters or remotely within one of the states in our region: Arizona, Colorado, Nevada, New Mexico, and Texas. Accion is embarking on a three-year project to overhaul our technology infrastructure and implement a state-of-the-art lending platform. It will be the only end-to-end lending platform in our industry, and is fully owned and operated by Accion, allowing it to be customized to meet our (and our customers’) needs. This will have a transformational effect on Accion, our clients, and the sector of lenders supporting underserved entrepreneurs. As our Data Engineer, you’ll be an integral part of this transformation, helping us create a data-centered culture we need to make data-informed decisions. Learn About Our Values.
What you’ll be doing:
You’ll work closely with our technology operations and engineering teams to understand business needs and design/maintain scalable data models.
You’ll make data-backed recommendations to the executive team to help inform business decisions and answer complex questions.
You’ll own the design, build, maintenance, quality, and expansion of a data warehouse, and support and scale the pipeline that relays data from Accion’s lending platform back to the warehouse.
Over time, you’ll act as a product manager, determining project timelines, goals, and deliverables for updates to our lending platform.
Skills and Experience:
Must have a passion for Accion’s mission and a strong commitment to Accion’s culture of exceptional customer service, excellence and accountability;
Knowledge of ETL processes and applications;
Experience in the financial services industry and familiarity with the lending process strongly preferred;
Advanced skills in SQL/Java/Ruby preferred;
Bachelor’s Degree in Computer Science, Engineering, Applied Mathematics, or related quantitative discipline plus four years’ relevant experience preferred.


Accion offers an excellent total compensation package, including competitive base salary, the opportunity for exciting incentive pay, health and dental coverage, retirement benefits, and generous paid time off.","Las Vegas, NV",Data Engineer,False
380,"Arthur Lawrence is urgently looking for Hadoop Data Engineer for our client in Hayward, CA. Kindly review project details and respond back at your earliest

Must Have:

6 to 8 years of experience as Hadoop Data Engineer
Experienced in Hadoop distributions
Experienced with Hadoop, Map Reduce, Yarn, and Hive
Experienced in NoSQL, HBase, Apache, or MongoDB
Experienced with Big Data eco-system


Nice to Have:

Relevant Certifications


For further details, please contact Adam at 832-562-4613 or email at adam@arthurlawrence.net","Hayward, CA",Hadoop Data Engineer,False
381,"Over $20 trillion worth of goods—the items we use, wear and consume every day—flow through increasingly complex global supply chains annually. Alloy is a supply chain synchronization platform that connects manufacturers, suppliers, logistics providers, distributors, and retailers, giving businesses end-to-end visibility with fast and actionable insights. Our customers—companies the make, move, and sell products—use Alloy to get the right products to the right place at the right time with greater agility, efficiency, and effectiveness than ever before. We work with companies of all sizes in many industries, ranging from Fortune 100 enterprises to fast-growing, innovative manufacturers.

We are early stage, well funded by leading VCs, and growing fast. Our small team studied at top institutions including MIT, Stanford, Waterloo, Caltech, ETH Zurich, Carnegie Mellon, and Harvard and has diverse backgrounds and experience in analytics, large-scale enterprise software, retail, and financial technology. There are many challenging problems to solve in this complex industry, and a huge opportunity for modern software to make the global supply chain operate more effectively.

About the role

As a Data Engineer at Alloy, you will oversee and expand our entire data integration layer, allowing Alloy to seamlessly communicate with a wide variety of companies across the supply chain. This includes retailers, distributors, logistics operators and e-commerce platforms.

About You

You thrive in a small team where you can build technology from the ground up. You love to pick up new tech, master it quickly, and do something creative with it.

You don’t shy away from even the most challenging problems and relentlessly strive for better solutions. You are self-motivated and enjoy working with others towards a common objective. When you know a better way, you voice your opinion. Building software is the means to an end—you want to change the way an entire industry operates.
What You'll Do
Build, automate and maintain integrations with retailer/distributor portals, e-commerce platforms, logistics providers, ERP platforms, manufacturers, and other relevant data sources
Build and improve internal libraries to streamline data integration across multiple sources, including web scrapers, EDI files, REST APIs and flat files
Be the internal expert on how each player in the supply chain shares and interpret data. Maintain internal Alloy logic to automate the interpretation of data across channels through the unified Alloy data model
What We're Looking For
Strong knowledge of Python and SQL, especially in data wrangling and ETL applications
Familiarity with Java is a plus
Experience in interpreting and manipulating supply chain-related datasets (Point-of-sale, logistics/EDI, product master)
Working knowledge of Selenium and other web-scraping tools
Our Stack
Google Cloud Platform
Postgres, Redis
Python, modern Java, React","San Francisco, CA",Data Engineer,False
382,"ContractSenior Data Engineer (Remote)
Apply Directly to creposa@syrinx.com

Our Engineers don't just make things, they make things possible. We are looking for someone that is ready to solve the most challenging and pressing engineering problems for our clients. Join our engineering team that builds massively scalable software and systems, architect low latency infrastructure solutions, proactively guards against cyber threats and leverage machine learning alongside financial engineering to continuously turn data into action.

What we look for:
We are working with a global investment company that is building solutions in risk management, big data, mobile and more. We are looking for creative collaborators who evolve, adapt to change and thrive in a fast paced environment.

Technologies:
Spark, Hadoop, SQLPython (must be able to code in Python)Familiarity with AWSTableau (nice to have)

This is a 12 month REMOTE contract.",Massachusetts,Sr. Data Engineer (REMOTE),False
383,"Design and build reusable components, frameworks and libraries at scale to support analytics products
Design and implement product features in collaboration with business and Technology stakeholders
Identify and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Support the implementation of new data management projects and re-structure of the current data architecture
Implement automated workflows and routines using workflow scheduling tools
Understand and use continuous integration, test-driven development and production deployment frameworks
Participate in reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues

Qualifications
Bachelor's degree in Computer Science, Information Systems or related field or equivalent work experience
2+ years of experience with data engineering with emphasis on analytics and reporting
Experience with relational SQL
Experience with scripting languages such as Shell, Python
Knowledge of workflow scheduler systems such as Airflow, Oozie or AWS data pipeline
Experience developing on Hadoop eco-system with tools like Pig, Hive, Sqoop and Spark is preferred
Knowledge of file formats including JSON, Parquet, and Avro
Experience developing solution within AWS Services framework (EMR, EC2, RDS, Lambda, etc.) is preferred
Experience with source control tools such as GitHub and related dev process
Willing to learn new skills and technologies
Has a passion for data solutions
Strong understanding of data structures and algorithms
Strong understanding of solution and technical design
Has a strong problem solving and analytical mindset
Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders
Able to quickly pick up new programming languages, technologies, and frameworks","Beaverton, OR",Intermediate Data Engineer - Nike Technology,False
384,"Apple's Map Service Team builds necessary infrastructure which are the foundation for many customer facing maps services for our millions of awesome customers. This is an exciting role for someone who loves realtime huge data processing pipeline. We bring our new ideas to the table, and we are excited to create solutions for the Apple Maps Developer Communities. We're looking for a talented and passionate person to join this amazing team, if you feel this is you, we'd love to hear from you.

Key Qualifications
You have experience with architecting, designing and developing Big-Data processing pipelines.
You possess proficiency in MapReduce development and experience with Hadoop and spark data processing technologies required.
Significant experience with distributed kev/value store.
Build instrumentation experience.
Performance Metrics Reporting.
Strong Core Java programming experience.
Description
You will architect, design and build Big-Data Frameworks that automate the creation of spatial a data warehouse. These data services enable maps developers to build new features at greater speed. We are part of a larger Maps organization which strives to provide foundation services for developers. As daily activities in this role, you will:
Works quickly to deploy necessary spatial data solutions as requested.
Recommend best practices of architectural and design performance for distributed data systems.
Provide infrastructure and service team members solutions for efficient data-processing and data delivery.
Build a geospatial index using realtime feeds which many teams can access.

Education
BS/MS or anyone with relevant industry experience will be considered

Additional Requirements
Nice to have, but not necessary: Apache Kafka and geo spatial database experience, Python and Scala programming background.","Santa Clara Valley, CA",Software Data Engineer- Maps Software,False
385,"About AQR Capital Management
----------------------------

AQR is a global investment firm built at the intersection of financial theory and practical application. We strive to deliver concrete, long-term results by looking past market noise to identify and isolate the factors that matter most, and by developing ideas that stand up to rigorous testing. By putting theory into practice, we have become a leader in alternative strategies and an innovator in traditional portfolio management since 1998.

At AQR, our employees share a common spirit of academic excellence, intellectual honesty and an unwavering commitment to seeking the truth. We're determined to know what makes financial markets tick – and we'll ask every question and challenge every assumption. We recognize and respect the power of collaboration, and believe transparency and openness to new ideas leads to innovation.

The Team:
The Python Data Engineer will work hand in hand with the product management teams playing a key role in AQR's portfolio management.

The two primary objectives: portfolio monitoring and client communication.

You will build tools to aid in many stages of the investment process, including new business presentations, new account onboarding, performance monitoring and analysis, and portfolio review meetings with clients. Product management teams are ultimately responsible for the accuracy and delivery of information to clients and are expected to do so in a professional, poised, and consultative manner. As a result, the teams interact closely with the rest of the firm including research, portfolio management, trading, business development, legal, compliance, risk and marketing. Through this collaboration, product management teams have in-depth knowledge of the strategies they cover and are expected to be in-house experts of their respective products. They are integral part of initiatives to create and sell new products, run competitor analyses, and produce market commentary as needed.

Your Role:
Portfolio monitoring:

Analyze portfolio exposures and performance using existing tools, and drive the development of new tools for monitoring
Conduct portfolio analysis and present results to product and portfolio teams in a logical, comprehensive manner
Be familiar with fund's investment parameters, and review these on a regular basis
Additional department specific tasks such as: oversee fund's onboarding, including launch and post-launch analysis, and build, maintain and present competitor analysis reports

Client communication:

Prepare review materials on quarterly and yearly performance
Create, review and approve presentations used by business development
Complete bespoke portfolio analysis to answer client queries
Develop tools to enhance and streamline data used in client portfolio reviews

Sample projects:

Build-out tools for, and run, returns-based style analysis using multivariate regression
Create a dashboard of tactical asset allocation views for client use
Analyze the impact of a surprise currency devaluation on portfolio returns and risk

Skills / Requirements:

2+ years' of working with or analyzing data
2+ years' of working experience in finance
2+ years' of programming experience
Proven track record and portfolio of successful work

Experience with the following:

Python (pandas, NumPy, SciPy) or any other programming languages
Experience with Tableau (preferably) or any other BI tool
Strong knowledge of SQL with advanced analytic functions
Experience with consuming and building APIs
Ability to design database schemas
Excellent problem solving and communication skills
Knowledge of visualization best practices is a plus
Strong coding skills with knowledge of software design patterns
Experience in an Agile environment is a plus
Master's Degree or Bachelor's Degree with equivalent experience in Computer Science, Data Science, Statistics or equivalent quantitative field

AQR is an Equal Opportunity Employer. EEO/VET/DISABILITY","Greenwich, CT",Python Data Engineer,False
386,"About the Role

Associate Data Engineers at BlueLabs have a passion for problem solving at the intersection of data and engineering. Whether it’s architecting a new data processing pipeline, building out internal tooling, scaling our modeling work, or feature engineering collaboratively with Data Scientists, the Associate Data Engineer works closely with both the Data Science and Engineering teams across a wide-variety of BlueLabs’ work. They have strong technical skills and are also creative thinkers who are always looking to innovate and deliver value to our clients. They aren’t afraid of messy data, and are comfortable working in a fast paced, production-oriented environment.
Duties and Responsibilities
Scope, design, and implement data pipelines and processes.
Develop/deploy data visualizations and internal tools to facilitate data analysis and reporting.
Collaborate with Data Science to support and optimize BlueLabs’ modeling work.
Strives to support team excellence by documenting processes and evangelizing new approaches.
You probably have...
An undergraduate degree in computer science or a quantitative field, or significant personal programming experience.
Proficient understanding of a general programming language such as Python, Ruby, or Java.
Proficient understanding of a statistical programming language such as R, Python, or Julia.
The ability to effectively communicate technical concepts to a non-technical audience, both in writing and verbally.
You may also have...
The ability to manipulate data with SQL.
Experience working with messy data or building ETL data pipelines.
Experience creating informative and engaging data visualizations using industry leading tools.
The ability to create user interfaces for new products using frameworks such as Shiny or Django.
We know that the best candidate may not fit neatly into the boxes we define here so if this sounds like a place you want to work, even if you're not confident you perfectly match our posting, we still encourage you to apply! We welcome diverse, out-of-the-box thinking, and we strive to provide an ecosystem for innovation and development. If you want more information about who we are as a team, check out our Facebook page, Twitter, or Instagram.

About BlueLabs

BlueLabs was formed in early 2013 by senior members of the Obama for America analytics team. We help organizations personalize their engagements with individuals, optimize communications, and achieve their strategic goals. Our team includes more than 40 data scientists, engineers, and strategists from diverse backgrounds who share a passion for using data to solve the world’s greatest social and analytical challenges. Through our work, we’ve directly and measurably improved the health and financial security of millions of Americans.

Since 2013, we’ve served more than 300 organizations, run more than 1,000 randomized experiments, built hundreds of models, generated over 6 billion touch points, reached virtually every contactable person in the United States, and driven significant improvements in some of the highest-profile private sector, advocacy, and government programs around the world. Along the way, we’ve developed some of the most innovative tools available in media optimization, reporting, and influencer outreach.

Our clients range from political campaigns to advocacy groups, unions, government agencies, and international groups, as well as global companies in the automotive, travel, consumer packaged goods, entertainment, healthcare, media, telecom, and other industries.

BlueLabs is headquartered in Washington, DC, and has offices in New York City.
Equal Opportunity and Diversity Policy:

BlueLabs believes a diverse, inclusive staff makes us a stronger team and more impactful partner for our clients. We’re committed to a diverse team, and qualified people of all races, ethnicities, cultures, ages, sex, genders, sexual orientations, gender identities and expressions, languages, social class, marital status, religions, veterans status, and disabilities are strongly encouraged to apply.","Washington, DC",ASSOCIATE DATA ENGINEER,False
387,"ContractJob SummaryHello,Hope you are doing well!My name is Kumar from Softcom Systems, Inc.I am aggressively recruiting for one of the positions “Data Engineer” for Location “New York City “with one of our direct client. Please have a look at the job description below and if interested feel free to call me at 609-759-9004 or revert to this email with your updated resumePosition: Data Engineer Location: New York CityDuration: Long term contractSkill: Data engineer with Spark, Scala, Hadoop & Java skills.with Regards..*Softcom Systems, Inc. Your trusted technology partner....,Office: 609-759-9004, Fax: 609-751-9077475, Wall Street, Princeton, NJ-08540,Under Bill s.1618 Title III passed by the 105th U.S. Congress this mail cannot be considered as ""spam"" as long as we include contact information and a remove link for removal from our mailing list. In order to not be in the recipients-list for this mail, please revert to us with ""REMOVE"" either in the subject or in the mail body.P*Please don't print this e-mail unless you really need to.*Job Type: ContractExperience:data engineer: 4 years (Preferred)","New York, NY",Data Engineer,False
388,"$87,000 - $131,000 a year (Indeed Est.) ContractData Engineer @ Downtown Dallas, TXThis is a data engineer role.  * Bachelor's degree  * 8-12+ years of experience  * Advanced SQL skills - Adept at queries, report writing and presenting findings  * Expertise in Data Analysis, Data Profiling, and SQL Tuning  * Expertise in translating business requirements to project design, development, and execution  * Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy  * Ability to clearly communicate capabilities, opportunities, and recommendations to both technical and nontechnical audiences  * Experience working in Data warehouse ETL & BI platforms and have a good understanding of related development activities and challenges  * Strong knowledge of and experience with reporting, databases (SQL etc), programming ( ETL frameworks)  * Experience in understanding the source data from various platforms and mapping them into Entity relationship model(ER) for data integration and reporting.  * Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platformsResponsibilities: Interpret and analyze data from various source systems to support data integration and data reporting needs within CBRE.  * Identify, analyze, and interpret trends or patterns in complex data sets  * Work with team leads to prioritize business and information needs  * Prepare high-level ETL mapping specifications.  * Develop complex code data scripts (Primarily SQL) for ETL  * Data Quality Control, Metrics  * Troubleshoot & determine best resolution for data issues and anomalies  * Manage exploratory data analysis to support database and dashboard development, as well as advanced analytics efforts  * May assist in development of software technical documentation.Job Type: Contract","Dallas, TX",Data Engineer - MSBI Developer,False
389,"This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics
For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.

Equal Opportunity Employer","Florham Park, NJ",Quantitative Data Engineer,False
390,"RFA Engineering (www.rfamec.com) is seeking several growth oriented entry level to experienced candidates to be part of our engineering team at our customer's facility in Dubuque, Iowa. You will work with our experienced engineering staff ato provide highly engineered data solutions for off-road equipment using state of the art engineering tools.


Our customer's facility is a world-class manufacturing center. These are full time positions that are indefinite in duration with opportunity for professional growth and direct hire by our customer.


Telematics Data Engineer


Job duties for these positions are associated with integrating telematics data collection in off-road machinery:
Collaboration with various departments to assist the integration of machine telematics into their projects
Analyzing machine telematics data for custom information reporting
Developing and testing telematics data accuracy to provide high quality information to Internal Engineering teams
Developing and documenting requirements for telematics data to be collected from off-road machinery
Telematics data generation and summary from various sources to support custom projects


Requirements
BSEE, Computer Science, or Mechanical Design or related degree with experience

A proven mechanical aptitude through employment, personal experience, or education, including machinery operation, maintenance, repair, metal fabrication, and other “hands on"" experience.

Candidates must have excellent communication, teamwork, and analytical skills.

Previous design experience or knowledge of off-road mobile equipment is a plus.

High level of attention to detail and accuracy.

Travel: Minimum to None
Desired Attributes
Design experience or exposure to off-road mobile equipment

Ability to communicate clearly and interact with multiple engineering groups

Work autonomously with minimal direction while supporting a team atmosphere
About RFA Engineering


RFA Engineering has provided machine design and engineering services to industry leading customers since 1943. Our primary focus is engineering of off highway equipment including agricultural and construction equipment, engine and drive train development, consumer, recreational, industrial, and special machines.


RFA Engineering is domestically owned and operated exclusively by engineers who have spent their careers in the industry. Our engineering staff is located both at our Engineering Center in Minneapolis, and at numerous customer sites throughout the U.S.


Why work for RFA?
We offer:
Health and Dental Insurance Programs
Company Paid Life and Long-Term Disability Insurances
Retirement Savings Account (401k)
Flexible Spending Plan for Medical Expenses and Dependent Care
Paid Time Off (PTO)
Employee Assistance Program (EAP)
Education Assistance
Equal Opportunity and Veteran Friendly","Dubuque, IA 52003",Telematics Data Engineer,False
391,"About the company

The name ThousandEyes was born from two big ideas: the power to see things not ordinarily possible and the ability to collect insights from a multitude of vantage points. As organizations rely more on cloud services and the Internet, the network has become a black box they can't understand. ThousandEyes gives organizations visibility into the now borderless network, arming them with an accurate understanding of how the network impacts their applications, users and customers. ThousandEyes is used by some of the world's largest and fastest growing brands, including all of the top 5 global software companies, 5 of the top 6 US banks, and 45 of the Fortune 500. ThousandEyes is backed by Sequoia Capital, Sutter Hill Ventures, Tenaya Capital, Google Ventures and Salesforce Ventures, with headquarters in San Francisco, CA.

About the position

We are looking for a Data Engineer superhero who will take the Analytics team's data infrastructure to the next level. You will work directly with our data infrastructure, datasets and analytics tools that are used by the Product, Marketing, Sales, Sales Engineering, Finance and Customer Success teams every day.

You will contribute to a variety of exciting projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing and operating stable, scalable and efficient solutions to flow data from production systems into the data warehouse. You are also a self-starter who is comfortable with ambiguity, pays close attention to detail and has the ability to work in a fast-paced environment.

In short, you will play a critical role in shaping our Analytics foundation. On the Analytics team we know new analytics technologies are emerging every day and we are excited about the impact they will have – we hope you share our enthusiasm!

Required Skills

BS/MS with quantitative focus (e.g. Economics, Computer Science, Mathematics, Physics, Statistics) or equivalent practical experience.

2+ years experience designing, implementing and maintaining production grade ETL processes and data pipelines.

2+ years experience operating databases (e.g. Redshift, MySQL, MongoDB) and advanced query authoring.

2+ years of dimensional data modeling & schema design in data warehouses. Development experience in at least one scripting language (e.g. Python, JavaScript).

Knowledge of industrial grade data architectures and reporting tools (e.g. ChartIO, Tableau).

An eye for automation and instrumentation in all data-related aspects. Work experience in an inter-disciplinary/cross-functional field.

Preferred Skills

Working experience in SaaS companies.

Experience performing quantitative analysis and using data visualization tools to deliver dashboards at scale.

Strong cross-functional and interpersonal skills with demonstrated ability to communicate technical content to general audiences.","San Francisco, CA",Data Engineer,False
392,"Automattic is the company behind WordPress.com, Jetpack, WooCommerce, and more. We are looking for a full-stack data engineer.

You’ll work with business leads, analysts, data scientists and fellow engineers to build data-powered products that empower better decision making. You’re committed to data quality. You’ll understand how to manage a cluster to deliver performance and reliability. You’ll evaluate and help to craft technology choices, and you’ll implement systems that tackle business use cases.

What we’re looking for:

Hands-on, production experience with the Hadoop family of big data technologies (Hive, Impala, HBase, etc.).
Collaboration with business partners to craft and iterate on solutions that extract value from data.
Experience with Spark, Python, and Java.
Strong analytical skills and a fervor for data integrity and accessibility.
The curiosity and determination to understand and improve data flows.
We’re serious about growing diversity in the tech industry. We want to build Automattic as an environment where people love their work and show respect and empathy to those with whom we interact. Diversity typically includes, but is not limited to, differences in race, gender, sexual orientation, gender identity or expression, political and religious affiliation, socioeconomic background, cultural background, geographic location, disabilities and abilities, relationship status, veteran status, and age. To work on diversity means that we welcome these differences, and strive to increase the visibility of traditionally underrepresented groups. Read more about our dedication to diversity and inclusion.

HOW TO APPLY
Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a résumé. Let us know what you can contribute to the team. Include the title of the position you’re applying for and your name in the subject.

Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.

If you’re reading this on a site other than automattic.com please ensure you visit automattic.com/work-with-us for the latest details on applying.
Please answer the following questions in your cover letter. Applications without these questions answered will not be considered:

Tell us some details about an interesting data problem you’ve worked on. What made it interesting?
Include a link to a recent favorite blog post or paper about working with lots of data.
What questions do you have for us?","San Francisco, CA 94110 (Mission area)",Data Engineer,False
393,"Data Engineer


Position Title: Data Engineer
Location: Chicago
Department: Information Technology
We are looking for a Data Engineer to join our team. You will have the opportunity to be involved in all aspects of a performance-driven database infrastructure geared towards a fast paced trading environment. On the technical front, the Database team touches every layer of the database stack from hardware to application layer, so be ready to leverage your strengths while learning a lot to improve your weaknesses. The team manages every aspect of the database environment from server hardware to arrays to SQL development and administration. On the business front, this team works directly with traders, other engineers, and business stakeholders. You will interact closely with both team members and stakeholders alike. The ability to have a keen technical understanding but communicate in layman’s terms is important. Day to day tasks can vary between database design, support, tuning, SQL report writing, scripting and anything else that could touch the database layer of the firm.
Required Qualifications
Bachelor’s degree in an applied science3.0+ GPA0-3 years of experienceExcellent problem solving skillsEfficient T-SQL (and ANSI-SQL) coding abilityPrevious work or internship experience
Recommended Qualifications
Financial/Trading industry experienceSQL Server experienceCHashtag development skillsProblem solving skillsDatabase design, normalizationKnowledge of database operations (backups/restores, security, HA)Ability to connect business problems with technological solutionsDatabase performance tuning/indexingKnowledge of networking/DNS/AD, SAN, RAID
About Wolverine
Founded in 1994, the Wolverine companies comprise a number of diversified financial institutions specializing in proprietary trading, asset management, order execution services, and technology solutions. We are recognized as a market leader in derivatives valuation, trading, and value-added order execution across global equity, options, and futures markets. With a focus on innovation, achievement, and integrity, we take pride in serving the interests of both our clients and colleagues. The Wolverine companies are headquartered in Chicago with offices in New York and San Francisco and a proprietary trading affiliate office located in London.
Visa sponsorship is not available.






Are you a returning applicant?


Previous Applicants:

Email:

Password:





If you do not remember your password click here.","Chicago, IL 60604 (Loop area)",Data Engineer,False
394,"This position is for a role in TEDRA department. TEDRA (Trade Enrichment Data Reporting & Allocations) is part of the Institutional Securities Technology (IST) Division. It is responsible for maintaining, distributing, and reporting on trading, revenue, risk, and reference data (client, product, and pricing). As the authoritative source of key data sets, we are at the forefront of database technology and are heavily involved in data engineering, data science, data visualization, and machine learning efforts across the Firm.This position is for a data engineer role in the Analytical Databases team. Our team designs, develops and manages a variety of data containers, ETL tools, real time and batch driven systems. Our technology domain spans relational, NoSql, data lakes, and ultra low latency worlds. We are experts in advanced data engineering. When you join our team you will be exposed to all the latest data technology in FinTech. Development will utilize an agile methodology, which is based on scrum (time boxing, daily scrum meetings, retrospectives, etc), and XP (continuous integration, refactoring, unit testing, etc) best practices. Candidates must therefore be able to work collaboratively, demonstrate good ownership and be able to work well in teams. Work will include designing, enhancing and developing databases across different database environments. The job will involve considering all aspects of the project life cycle and includes proof-of-concept evaluations, coding, designing, testing, implementing, deploying, and continued support of project releases as well as on-call Level 2 support. Collaboration with the Firm?s engineering teams is expected. We are a team of highly technical individuals who manage a large number of databases that include Big Data volumes. We deliver multiple projects for multiple business areas in parallel. The business owners and subject matter experts are globally distributed, making strong communication skills important to the position. The candidate will be expected to work closely with our IT partners in analyzing and delivering on business requirements.

3-5 years of experience in database management/engineering role. * Knowledge of relational or NoSql databases - any experience will count. * The candidate must be familiar with some scripting language such as: Python or Perl. * The candidate must have strong knowledge base of database performance and tuning* Versioning tools such as perforce and git is a plus, but not required. * Knowledge of financial instruments would be a great benefit, but not required. Personal skills: Integrity & ownership, good team player, ability to work under time and resource dependencies and constraints, ability to find simple and effective solutions, high degree of motivation to expand technical and business knowledge.

","New York, NY",Big Data Engineer/Analytics Developer,False
395,"Job Overview:
We are looking for a dedicated Data Engineer to help kickstart and grow business intelligence services at Aftershock Studios/FoxNext Games for our premier mobile game titles including James Cameron’s Avatar.
Responsibilities:
Design, implement, debug, document, test, and maintain code and systems for ingestion, transformation, storage, and consumption of data from multiple games and millions of players
Collaborate with game teams, product managers, data analysts and technical stakeholders to craft the best solutions for our data driven business
Estimate engineering effort to contribute to sprint planning and keep our delivery on track
Ensure data quality is delivered timely and consistently with active alerting and notifications for escalation
Bachelors in Computer Science or related field or equivalent work experience
4+ years of experience with software engineering
Deep understanding of functional and object oriented programming
Expert ETL techniques and best practices to handle large data volumes
Expert SQL and NoSQL database experience
Experience with data warehouse architecture and data modeling best practices
Experience with relevant scheduling, batch and stream processing frameworks (e.g. Apaches Spark, Kafka, Airflow)
Experience with relevant managed services (e.g. AWS Kinesis/Lambda, GC Pub/Sub)
Cloud infrastructure devops to support deployment and data management
Excellent communication skills both written and verbal
Preferred Qualifications:
Python, Javascript, Node.js, MongoDB, BigQuery
Infrastructure management on Amazon Web Services and Google Cloud Platform
Docker container deployment on Kubernetes
RESTful Web Service API development
Mobile development in Android and iOS
Experience in games or fast paced company such as growth phase startup
Git, GitHub, Perforce, Jenkins, Splunk","Los Angeles, CA",Data Engineer,False
396,"We have the great privilege of helping patients and families re-build their lives. It’s extraordinarily meaningful work and the reason we greet the day with optimism and anticipation. When patients “Ask for Mary,” they experience a culture that has been sculpted for more than a century. Our hallmark is to carefully listen to patients and innovatively serve them.

Mary Free Bed is a not-for-profit, nationally accredited, rehabilitation hospital with 167 inpatient beds – 119 acute and 48 sub-acute. There are numerous outpatient programs as well as home and community services. With the most comprehensive rehabilitation services in Michigan and an exclusive focus on rehabilitation, Mary Free Bed physicians, nurses and therapists help our patients achieve outstanding clinical outcomes.

Mission Statement
Restoring hope and freedom through rehabilitation.

Diversity and Inclusion
Mary Free Bed values diversity and inclusion among patients, families and staff. We strive to hire people who reflect the communities we serve. Our employees will serve all patients, families and each other with dignity and respect.

Summary:
As a member the data architecture team, the Data Engineer develops data solutions for advanced analytics and operational support across the organization. This role provides expertise in the design, implementation, and maintenance of data objects and models using supported enterprise IT platforms and guidelines. Data Engineers gather and integrate data from the enterprise data warehouse (EDW), which is stocked by multiple source systems and data streams. To support these needs, Data Engineers work in conjunction with data architects to research, prototype, and pilot emerging analytics tools and platforms. This role further promotes the analytical operations of the organization by providing expertise in report automation, and the development of self-service dashboard solutions using enterprise supported platforms.


Essential Job Responsibilities:
Support the Enterprise Business Intelligence Platform by acting as the primary steward of the EDW presentation layer.
Participate in data modeling and data source creation– provide and support technology that allows data analysts to meet the organization’s reporting needs.
Participate in optimization efforts – within both the back-end and front-end of the BI platform.
Enforce BI platform governance. Administer platform technology and maintain user permissions and security.
Participate in BI product evaluations, RFP’s, POC’s and business decisions to ensure fit and scale into the platform and the organization.
Understand the business objectives and provide solutions that efficiently meet the objects in a timely manner with a focus on data integrity and quality.
Assist Data Architects with ETL and warehouse development when necessary.
Performs other duties in support of departmental and corporate objectives and initiatives, as assigned.
Essential Job Qualifications (Knowledge, Education, and Training Requirements):
A Bachelor’s Degree or equivalent background and work experience in computer science, application programming, software development, information systems, database administration, mathematics, engineering, or other related field.
At least 2 years’ experience providing data solutions, analytics, and or data development.
Requires strong and very effective, verbal and written communication skills. Ability to express complex technical concepts effectively, both verbally and in writing.
Proven ability to handle multiple tasks and projects simultaneously.
Excellent analytical skills, attention to detail and problem-solving skills.
Willingness to learn and stay current on technology trends and upcoming features.
Ability to work in a team-oriented environment.
Must be self-motivated and demonstrate strong initiative.
Demonstrated ability to establish priorities, organize and plan work to satisfy established timeframes.
Physical Demands
Able to exert up to 10 pounds of force occasionally (up to 1/3 of the time)
Able to lift, carry, push, pull, up to 20 pounds occasionally
Able to sit for the majority of the time, but may involve brief periods of time involving walking or standing.
Able to use keyboard frequently (3/4 of the time)
Technical Skills:
Knowledgeand experience with SQL programming (T-SQL preferred)
Knowledge of software development methodologies, relational and dimensional database skills, and design skills.
Experience using a modern Business Intelligence stacks (MS SQL Server preferred).
Experience with data visualization technologies (Tableau preferred).
Mary Free Bed is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, age, genetic information, veteran status, disability or other legally protected characteristic.","Grand Rapids, MI",Data Engineer,False
397,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
At Facebook, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Analytics team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Facebook’s Data Center organization. You will be responsible for creating the technology that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise and provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team and located in our Menlo Park office.
RESPONSIBILITIES

Apply proven expertise and build high-performance scalable data warehouses

Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)

Securely source external data from numerous partners

Intelligently design data models for optimal storage and retrieval

Deploy inclusive data quality checks to ensure high quality of data

Optimize existing pipelines and maintain of all domain-related data pipelines

Ownership of the end-to-end data engineering component of the solution

Collaboration with the Data Center SMEs, Data Scientists, and Program Managers

Support on-call shift as needed to support the team

Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data
MINIMUM QUALIFICATIONS

BS/MS in Computer Science or a related technical field

7+ years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)

7+ years of experience in custom or structured (i.e. Informatica/Talend/Pentaho) ETL design, implementation and maintenance

7+ years’ experience in data engineering, experience in applying DWH/ETL best practices

7+ years of Java and/or Python development experience

2+ years experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)

2+ years experience working with enterprise DE tools and experience learning in-house DE tools
PREFERRED QUALIFICATIONS

Technical knowledge of data center operations.","Menlo Park, CA",Data Engineer,False
398,"As the fastest growing, highest-performing charter school network in New York State, Success Academy has reimagined public education. Nationally recognized for achieving outstanding academic results for students from all backgrounds and zip codes, we have created an innovative K-12 school model that is preparing current and future generations of children with the subject mastery and skills to succeed in college and life. We now need talented, highly motivated Data Engineers to help us build the best possible data products and insights that will propel us to our public goal of building and managing 100 schools in NYC.
Reporting to the Director of Data Management & Analytics, the Data Engineer will help to enhance the learning experience for thousands of children across the five boroughs of NYC by becoming the glue that binds together hundreds of disparate data sources and third party APIs that our schools use every day to improve curriculum across our network of over 50 schools. Imagine working for a company that has the iterative, constant improvement practices of a startup applied at scale to curriculum and schooling operations. Our data team is at the heart of how this organization operates.
Key responsibilities include:
Building scalable and reliable data pipelines in Python by leveraging data processing technologies like AWS, Spark, Airflow, etc.;Writing modular, reusable code in Python to build and enhance our data framework;Writing complex, high performing SQL queries against Postgres, MySQL, and MSSQL databases; andInterfacing with third-party APIs to collect data from various sources and transform them into our central data warehouse.

A successful applicant will have the following experience and characteristics:
Experience with data warehousing, streaming architecture, machine learning pipeline and workflow scheduling with Airflow;Experience with service oriented architectures and interfacing with third-party APIs;A high level of comfort using Python data libraries, particularly Pandas and SQLAlchemy;Experience with Python web development frameworks (Flask, Django, or Pyramid);Experience with data analysis and visualization methods;Seeks a fast paced, collaborative environment with capacity to work with multiple teams to complete a variety of projects;
Has 3-5 years of experience on a data science team; andHas a strong interest in education reform and working for a mission driven organization.

To join our team, please upload a cover letter and resume that outlines your candidacy. Your cover letter should explain in detail your qualifications for the position. Resumes without cover letters will not be reviewed.

Success Academy Charter Schools is an equal opportunity employer and actively encourages applications from people of all backgrounds. Compensation is competitive and commensurate with experience. Success Academy offers a full benefits program and opportunities for professional growth.","New York, NY",Data Engineer,False
399,"Company Mission and Highlights:
mPulse Mobile, the leader in mobile health engagement, drives improved health outcomes and business efficiencies by engaging individuals with tailored and meaningful dialogue. mPulse Mobile combines technology, analytics and industry expertise that helps healthcare organizations activate their consumers to adopt healthy behaviors. With 9 years, 60+ healthcare customers, and more than a hundred million messages sent annually, mPulse Mobile has the data, the experience and the technology to drive healthy behavior change.
Our Core Values:
Model Integrity and Collaboration
Drive Innovation and Thought Leadership
Support Decision Making at All Levels
Create Value for Clients by Empowering Consumers
Improve Customer Experience Through Simple Design
Celebrate Success… Often
Purpose of the Role:
The mission of the Data Science and Analytics (DSA) team at mPulse is to uncover insights from data in order to help drive better patient engagement and health outcomes. We are looking at everything from tactical optimizations to broad level strategic direction that is grounded in data evidence and heavy analytical rigor.
This requires a multidisciplinary blend of data science, behavioral science, and business strategy, all applied in tandem to discover key insights that lie hidden in our data sets. The Data Engineer will help build a research-based and data-driven approach to optimize mobile customer engagement. This role will focus on deep diving into a broad variety of exploratory initiatives to improve segmentation, tailoring and personalization of mobile engagement.
Duties and Responsibilities:
Querying and processing data using ETL tools, generating reports, and visualizing data
Working with the data science team to refine and develop data science and analytics (DSA) product roadmap
Improving team data report quality by cross-checking, cross-validation, documentation and code reviews
Follow our HIPAA compliant data policies
Building rich and interactive data visualizations used for internal analysis, reports, and in client presentations
Understanding and participating in analytics group’s data mining techniques and analytics efforts
Ability to deliver reports and visualizations within tight timelines
Skills, Abilities, and Experience:
Required:
Strong background and solid skills in interactive data visualization (Shiny, D3.js)
1-2 years experience with Python (Pandas, NumPy, sciKit-learn), SQL and R
Strong academic record, ideally in Economics, Mathematics, Computer Science, Engineering, Operations Research, Statistics or other quantitative field
Strong team player with organizational skills, attention to detail and ability to collaborate
Preferred:
2-5 years of experience in a corporate, start-up, or research environment
Experience in research methods and exploratory data analysis, and familiarity with machine learning approaches
Intense intellectual curiosity – strong desire to always be learning
Analytical, creative, and innovative approach to solving difficult problems and a can-do attitude when outside your comfort zone
Able to work in a demanding deadline-oriented startup environment
The Perks:
Enjoy Unlimited PTO and flexible work hours
Full Vision, Dental and Healthcare - all individual premiums paid by mPulse!
401K Program with a 4% match
Weekly team lunches to celebrate victories
Paid Parking as well as Car Pooling incentives
Laptop fitness stations
Ping pong conference table and Foosball
Free snacks and drinks
Contact Information:
mPulse Mobile, Inc.
Attn: HR Dept.
16530 Ventura Blvd, Suite #500
Encino, CA 91436
careers@mpulsemobile.com
(mPulse is an Equal Opportunity Employer)","Encino, CA",Data Engineer,False
400,"InternshipIntern - Data Engineer / Data Sciences Developer (
Job Number:
 043033 )
Description
 Summer 2019 Paid Internship

Zions Bancorporation is currently accepting resumes for our Data Engineer Internship position. The intern will have the opportunity to:
Work with other developers and data scientists to code proof-of-concept projects on large scale data sets
Assist in developing data processing and system integration applications
Assist in constructing web based user interfaces and visualizations
Document design decisions, code and work flows
Assist in designing, developing, and testing ETL applications consistent with application architecture guidelines.
Qualifications
Preferred candidate will be pursuing a degree in Computer Science, Software Engineering or Computer Engineering.
Strong analytical, organizational and problem solving skills
Ability to elicit, gather and analyze user requirements
Ability to work independently and provide updates to management
Requires strong interpersonal skills
Must be able to meet deadlines
Technical Knowledge in the following is preferred:
Programming languages, including R, TCL, JAVA, Ruby, and Python
SQL and NoSQL data stores and solutions
Knowledge of Big Data Technologies (e.g Apache Hadoop, Spark)
Work Locations
 Utah-Salt Lake City-Zions Bancorporation - HDQTRS
Business Operations
Sep 12, 2018","Salt Lake City, UT","Intern, Data Engineer / Data Sciences Developer",False
401,"Job Description
We are looking for candidates who want to help shape the future of Ops Finance. Specifically, we are looking for an outstanding business intelligence expert and data engineer who is able to partner effectively with both business and technical teams to drive the growth of our rapidly expanding global business. From your first day, you will own the architecture and build out of new data infrastructure using MySQL and ETL. In this role, you will also develop and support the analytic technologies that give our teams flexible and structured access to their data, including partnering with other software and business teams, in order to build robust and scalable solutions for the entire NA Ops Finance organization.

The successful candidate will be an expert with SQL, ETL (and general data wrangling) and have a demonstrated ability to quickly translate business requirements into technical solutions. The candidate will need to be a self-starter and team player, demonstrate exemplary communication skills and able to think big while paying careful attention to detail.

Responsibilities

You know and love working with business intelligence tools, can model multidimensional data sets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:

Design, implement, and support a platform providing ad hoc access to large datasetsInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQLModel data and metadata for ad hoc and pre-built reportingInterface with business and finance customers, gathering requirements and delivering complete reporting solutionsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisionsRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersParticipate in strategic & tactical planning discussions, including annual budget processesLeverage large data sets to form recommendations on forecasting, along with web app product experimentation.
Basic Qualifications
Bachelor’s or advanced degree in Math, Finance, Statistics, Engineering, Computer Science or related discipline.Experience in data mining (SQL, ETL, data warehouse, etc.) and using databases in a business environment with large-scale, complex datasetsAdvanced SQL and Excel skills, familiarity with statistics or other analytical techniquesProven ability to solve complex quantitative business challenges; experience in the development of pricing analysis is a plusExcellent written and verbal communication skills
Preferred Qualifications
Knowledge of data visualization and exploration tools (Looker, Tableau, etc.)Strong data extraction, analytical and problem solving skillsThe right candidate thrives in a high energy environment where tactical and strategic activities are expected to be driven in parallelHighly innovative, flexible and self-directedAdvanced degree in Math, Finance, Statistics, Engineering, Computer Science or related discipline.
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation",United States,"Business Intelligence Engineer, Field FP&A",False
402,"$130,000 - $170,000 a yearOur Client is looking for a Data Engineer to work on fast data infrastructure leveraging data streaming, batch processing, and machine learning to personalise experiences for their customers.We would hope that you have:-Bachelor’s degree-At least 1+ years’ experience with leading big data technologies such as Cassandra, Spark, Hadoop, PostgreSQL, Redshift, and MongoDB-2+ years experience with AWS cloud-2+ years of experience in Java, Scala, or Python-2+ years of experience building data pipelines-1+ years of experience working with Cloud Technologies (AWS, Google, Azure).Job Type: Full-timeSalary: $130,000.00 to $170,000.00 /yearEducation:Bachelor's (Preferred)","New York, NY",Data Engineer,False
403,"$135,000 - $160,000 a yearBuild data pipelines using Apache Spark, Scala, Python, Apache Airflow etc.Collaborate with User Experience and Engineering teams in the planning of new productsWrite unit tests and get close to 100% code coverageWork on AWS – S3 for storage, EC2 and EMR for processing/analysisFollow Agile methodology for the software developmentIdentify problems and propose resolutions*6+ year of engineering experience; focus on back-end development and/or data engineeringIn-depth programming knowledge with Java, Python, and ScalaExperience with Spark, Hadoop, or HIVEStrong experience with AWS; including EC2, EBS, RedShift, EMR, ELB, SNS, RDS, CloudFormation, and moreExperience with tools like Maven, Jenkins, GitAble to perform as an effective member of a geographically dispersed team across multiple time-zonesEducation: Bachelor’s degree in computer science or closely related fieldClearance: Must be eligible to obtain a Public TrustLocation: Rockville, MDJob Type: Full-timeSalary: $135,000.00 to $160,000.00 /yearExperience:AWS – S3 for storage, EC2 and EMR for processing/analysis: 3 years (Required)Java, Python, Scala: 5 years (Required)Spark, Hadoop or Hive: 3 years (Required)Engineering focused on Backend Developement: 6 years (Required)","Rockville, MD",Data Engineer,False
404,"Jebbit is looking for mid- to senior-level data engineers that are problem solvers looking for a fast-paced and flexible environment to build cool things with a fun team. We have a design for scaling out our infrastructure and need a dedicated and knowledgeable engineer to build, troubleshoot, and optimize on current and future iterations of the Jebbit Platform’s infrastructure from ETL to analytics, reporting, and data science.

Our base technology stack consists of PSQL, Ruby on Rails, EmberJS, and NodeJS, but not knowing some or all of these is not a show stopper for a bright candidate who can learn the intricacies of the data formats and infrastructure that works best with these technologies. Most of our infrastructure resides on AWS and we also employ Kubernetes, ElasticSearch, and Redshift. Having a firm grasp of fundamentals and having experience scaling architecture is more important than being a master of any singular domain.

Originally founded at Boston College, Jebbit was named one of the Top 25 Most Promising Companies in the World by CNBC and our co-founders are Forbes 30 Under 30 honorees. We’re a graduate of Techstars and located in Boston's Fort Point near South Station.

The Jebbit platform enables marketers and others to create branded, personalized mobile experiences to engage, profile, and convert any audience while allowing them to leverage the user-given Declared Data to activate and personalize marketing for their customers.

Responsibilities
Work with department and team leads to expand our architecture’s scalability
Build POCs for ETL and other high-bandwidth services to help discover problems early
Help design, instantiate, and maintain data storage setups for analytics and data science
Troubleshoot all parts of the data pipelines as necessary
Interact with Product and Client teams to better understand the market and what roles our technology plays in it
Qualifications
Enjoys solving complicated technical puzzles/issues
Effective verbal and written technical communication
5+ years as a software developer working on movement and storage of large data sets
Previous work with Redshift, ElasticSearch, and AWS Datapipeline a plus
Ability to program in Ruby and Javascript also are pluses
Willingness to learn and try new things
Perks
Relaxed office culture
Weekly lunch and learns to encourage developer and team growth
Flexible schedule and WFH policy
Side projects are encouraged

No Recruiters Please","Boston, MA",Data Engineer,False
405,"$110,000 - $130,000 a yearAre you interested in working for a cool startup focusing on using data that helps our unique client base make better business decisions? Are you looking for an innovative, creative, fast-moving environment that lives and breathes data using the newest tools?

What is the job?

As a member of our growing engineering organization, you will work very closely with our Data Scientists to reach our product goals by owning and maintaining our ETL and data pipeline processes. You will build out extensive high-performance and secure data extraction processes as well as the reporting platform that delivers actionable insight to our clients.

Within this role you will work heavily with APIs and leverage AWS, as well as, third-party tooling to solve challenging problems. The distributed systems you will develop will take on issues such as scale and performance. The platform you and your team will build will have a direct impact on our product and customers.

Who are we?

We are one of the fastest growing startups in the data space. We use sophisticated machine learning and AI for data extraction to provide predictive analytics and market insight to our client base.

What Skills Do You Need?


Must be experienced in a Spark environment in a commercial setting
Everything we do is in the cloud, so you must be experienced in it, preferably using Spark clusters and S3.
You should have a working knowledge of Scala, Kafka, and Python

Compensation


We offer a fully comprehensive and competitive compensation package that includes Base + Bonus + Equity
Salary Range: $110,000-$130,000
100% fully paid Medical, Dental, and Optical Coverage
Too many other sweet perks to list!

Why should you join?

This is phenomenal opportunity for an experience Data Engineer to join a great organization and make an impact immediately. If you want to work for a company with an amazing technology culture that puts people first, then come and see what were all about!","New York, NY 10002 (Lower East Side area)",Data Engineer - Technology Startup,False
406,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you.

What you'll do:

As a Data engineer you will work with Data Scientists to take their prototypes and turn them into scalable, production code. These datasets will be used to train Machine Learning Algorithms that are core to our business.
Work with data science, product management and development teams to understand requirements and technical specifications and work as part of an agile product team

Who you are and what you have:

It's critical that you have experience with Big Data and have a strong understanding of ETL and Batch concepts.
Experience with Java, Python or Scala
Hadoop or similar Big Data tools
Curiosity about new approaches like Stream Processing
Agile Software Engineering

What puts you over the top:

Prior Ad-Tech experience
SAFe Methodology
Prior experience with Statistical Analysis
Masters or PhD in Quantitative discipline
AWS

About Integral Ad Science

Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions to empower the advertising industry to effectively influence consumers everywhere, on every device. We solve the most pressing problems for brands, agencies, publishers, and technology companies by verifying that every impression has the opportunity to be effective, optimizing towards opportunities to consistently improve results, and analyzing digital's impact on consumer actions. Built on data science and engineering, IAS is headquartered in New York with global operations in ten countries. Our growth and innovation have been recognized in Inc. 500, Crain's Fast 50, Forbes America's Most Promising Companies, and I-COM's Smart Data Marketing Technology Company. IAS was also named to Crain's Best Places to Work in NYC for three years running, Great Companies to Work For in NYS, and AdAge's list of Best Places to Work in the US.

Equal Opportunity Employer:
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, or disability status. EEO/AA/M/F/Disabled/Vets

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) or http://bit.ly/glassdoorIAS/ ( http://bit.ly/glassdoorIAS/ )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com ( recruitingagencies@integralads.com ). We will get back to you if there's interest in a partnership.","New York, NY 10003 (Greenwich Village area)",Big Data Engineer,False
407,"$110,000 - $180,000 a yearJob SummaryAre you a high-performing software engineer passionate about building production-quality applications using cutting-edge machine learning algorithms? Our client experiments and innovates leveraging the latest technologies to engineer breakthrough customer experiences and bring simplicity and humanity to banking. At the Center for Machine Learning (C4ML), you'll be part of an elite team accelerating machine learning solutions within by building enterprise-class applications that solve big problems and meet real customer needs.As a Data Engineer in C4ML, you will build fast data and machine learning solutions to address some of the many complex problems in the financial services industry. You'll leverage full stack technology solutions including streaming big data, state of the art machine learning, micro-service architecture, distributed computation engines, and intuitive visualizations in the cloud. We work with several cutting-edge technologies and actively develop and contribute to the open source community. You will work alongside highly technical peers, with deep domain expertise (from cyber threat prevention to sophisticated NLP), and partner with product and business teams to deliver game changing solutions for our customers.Qualifications and SkillsBachelor’s Degree or Military ExperienceAt least 2 years of experience with Python, Java or ScalaAt least 2 years of experience in deploying scalable, distributed systems or multi-node database paradigms.Preferred Qualifications:Master’s Degree or PhDAt least 2 years of experience with cloud software design using micro-services and distributed cachingAt least 2 years of experience delivering applications from architectural design to production implementationAt least 1 year of experience working with Machine Learning, Deep Learning or Artificial IntelligenceJob Type: Full-timeSalary: $110,000.00 to $180,000.00 /yearExperience:Machine Learning: 4 years (Required)","New York, NY",Sr. Data Engineer With Machine Learning,False
408,"Take Yahoo and AOL, fuse them and you get a media and technology company operating at massive global scale. It takes powerful technology to connect our combined media brands and partners with an audience of 1 billion. Nearly half of Oath’s employees are building the code and platforms that help us achieve that. And we’re only getting bigger. Whether you’re looking to write mobile app code, engineer the servers behind our massive ad tech stacks, or develop algorithms to help us process 4 trillion data points a day, what you do here will have a huge impact on our business—and the world. Want in? Yahoo Sports connects fans to the sports and fantasy games they love most and is the first screen for the next generation of sports fans worldwide,who crave real-time personalized quality content and superior fan experiences every day.


Data Engineer - Yahoo Sports


A Little About Us:

We are sports fans. We love our teams. We love our code, our data, our products and our people. We are also relentless about improving ourselves and are looking for someone who shares our values to join our team and make us even better.

A Lot About You:
You get data. You have a thirst for knowledge and insight. You thrive and strive to present data in ways that product, design, engineering, marketing, and executive teams understand and act upon. Your data is 100% accurate and credible. Your reports are always clear and actionable.
You get growth. You are a consumer-focused, data-driven, and growth-enabling analyst who has supported growth strategies, roadmaps, scrums, and final product rollouts, across the analytics/insights, acquisition/referrals, activation/onboarding, and adoption/retention loop.
You get mobile/digital. You have significant industry experience – and a strong understanding of the mobile/digital ecosystem – from apps to advertising and analytics. You have successfully applied the latest mobile/digital tools to help drive reach, retention, and revenue growth.
You get it done. You have successfully worked with product, design, marketing, and executive teams to understand requirements, translate business needs into data requests, develop methodologies/plans, analyze data, and present findings that are embraced/enacted.


Your Day:
Use deep analytical capabilities to transform data into actionable insights and effectively present your findings to partners and executives to help make data-driven product and business decisions.
Provide data tracking requirements and support reporting needs.
Maintain a relentless focus on data quality, always striving to identify logging issues and improve the accuracy of the data.
Work with teams to understand business problems, frame the problem into questions that can be answered through data analysis, formulate and implement analysis plans, analyze data and deliver actionable insights.
Analyze and present a/b test results with launch recommendations
Define, create, monitor, and improve key performance indicators (KPIs) to aid in decision making and overall strategy.
Building positive relationships and trust through strong cross-team interactions, on-time delivery, high quality products and innovation.


Requirements:
BS/MS in highly-quantitative field (Analytics, Computer Science, Mathematics) or equivalent is required.
Technical Experience and Expertise:
Big data technologies such as Hive, Hadoop, MapReduce, PIG, Google BigQuery, Oozie, etc.
Scripting with programming languages such as Perl/Python
Comfort working in a Unix/Linux environment
Proficiency with business intelligence tools (Looker, Tableau, etc.)
Solid understanding of computer science fundamentals like algorithms and data structures
Familiarity with MySQL (or other RDBMS) is a nice to have
3+ years as a data analyst, generating insights for consumer-focused (mobile + web) products
3+ years experience playing Fantasy Sports or active knowledge of major sports (Football, Basketball, Baseball, Hockey, Soccer, etc)
Track record of proactively establishing and following through on commitments
Demonstrated use of analytics, metrics, and benchmarking to drive decisions
Excellent interpersonal, organizational, creative, and communications skills
Team player in driving growth results combined with a positive attitude
Strong work ethic and strong core values (honesty, integrity, creativity)
Problem solver who never stops thinking about ways to improve


Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","Los Angeles, CA",Data Engineer - Yahoo Sports,False
409,"If you are you passionate about big data, want to be part of a great team, and love building new technology, we want to speak with you. As a Data Engineer you will work with our technology team to build and maintain our suite of data pipelines, stores, and databases that power sophisticated marketing products used by many of the world’s largest advertisers. We are looking for smart and hardworking individuals who have the DNA needed to build world class software. The right candidates will be creative thinkers who can design and deploy professional applications using the newest technologies to solve real business problems.
Who you are...
You have bachelor’s degree in computer science or similar.
You’re a whiz in SQL.
You have experience with querying and loading data (ETL), database design, and query optimization.
You have basic command line knowledge and Git.
You have dealt with large databases that contain millions of rows and know how to architect databases for performance.
You can write engines that generate wicked-fast SQL statements.
You can write SQL statements and stored procedures in your sleep.
You have a natural commitment to quality and thoroughness.
You thrive in ever-changing environments.
You’re organized, you manage your time well, and you stand behind your work.
You communicate well with coworkers of all levels.
You’re confident, forward-thinking who’s well versed in web and object-oriented development processes and concepts.
You have a passion for learning new technologies.
Your drive to innovate is inspiring.
What you'll do...
You’ll work full-time, for a great salary, in a collaborative and growing privately held company.
You’ll work within robust data systems and develop custom solutions while consulting with external customers.
You will recommend and implement improvements to data processes/warehouses that improve supportability, usability, and scalability.
You’ll optimize and refactor existing code.
You’ll Improve efficiency, scalability, and reliability of applications.
You will use your top notch collaboration skills with other team members across all departments.
You’ll participate in lively technology discussions where your opinion is truly valued.
Who we are…

We’re connecting travelers with experiences through best in class technology and innovation.

Koddi is an advanced reporting, bid management and campaign intelligence software platform for metasearch publishers like Google Hotel Ads, TripAdvisor, Kayak, Trivago, and other travel products that enables hotel supplier, OTA, and agency clients to reach more customers at higher returns. Koddi was built with the digital marketer in mind and provides a fast, intuitive interface, custom, on-the-fly reporting, bulk and granular bidding tools, and alerting functionality.","Ann Arbor, MI",Data Engineer,False
410,"Design and execute state-of-the-art data systems for input data for various energy models, including electric grid operations and capacity expansion and transportation and fuels modeling. Design, implement, and document data workflows. Ensure quality of both data and archival records. Perform complex operations on large datasets, especially resource data time series. Work in a dynamic, cross-disciplinary team using cutting-edge analysis and visualization techniques. Create new capabilities to analyze scenarios and design solutions for complex challenges in sustainable energy development to solve real world energy system design problems.

The successful candidate will help to develop data architecture, quality control systems, datasets, and archival systems, especially for time series resource data inputs to electric grid operations models. Duties will include:
Integrating multiple data sources, models, and software tools with scientific and engineering workflows for decision support and data analysis. These workflows will include the use of distributed computing and utilization of both cloud and high-performance parallel computing (HPC) resources.
Developing new methods to help researchers, clients, and stakeholders analyze and evaluate design strategies to implement sustainable energy solutions at various geographic and temporal scales.
Modeling complex systems by integrating large spatio-temporal datasets of economic, demographic and energy related information.
Creating visualization interfaces that enable real-time examination of design scenarios as desktop, immersive, and web frameworks to allow for design processes by stakeholders around the world.
Evaluating and communicating results through written research reports for publication and presentation at seminars, participating in group meetings and seminars, and assisting in developing grant proposals for new research directions.

The ideal candidate will bring a deep background in data analysis and programming skills to integrate various models and tools to solve complex design problems. The candidate will demonstrate experience in
defining project requirements that balance constraints from the users, available hardware, software, framework complexity, etc. The candidate must be willing to work in an interdisciplinary field, together with computer scientists, policy analysts, and engineers, and will require excellent interpersonal and communication skills.

Other required skills include:
Highly proficient and extensive experience with time series resource data analysis and modeling techniques; demonstrated experience in management of large datasets, especially spatio-temporal data
Strong scientific programming and algorithm development skills and demonstrated use of Python
Other required programming languages: SQL, NoSQL, PostgreSQL, R for statistical analysis
Demonstrated ability to produce scientific visualizations

This position is in NREL’s Strategic Energy Analysis Center. See http://www.nrel.gov/analysis for more general information about our work and impact on the energy sector both domestically and internationally.
.
Required Education, Experience, and Skills
Relevant Master's Degree. Or, relevant Bachelor's Degree and 2 or more years of experience. General knowledge and application of scientific technical standards, principles, theories, concepts and techniques. Training in team, task or project leadership responsibilities. Intermediate abilities and knowledge of practices and techniques. Beginning experience in project management. Good technical writing, interpersonal and communication skills.
.
Preferred Qualifications
.
Submission Guidelines
Please note that in order to be considered an applicant for any position at NREL you must submit an application form for each position for which you believe you are qualified. Applications are not kept on file for future positions. Please include a cover letter and resume with each position application.
.
EEO Policy
NREL is dedicated to the principles of equal employment opportunity. NREL promotes a work environment that does not discriminate against workers or job applicants and prohibits unlawful discrimination on the basis of race, color, religion, sex, national origin, disability, age, marital status, ancestry, actual or perceived sexual orientation, or veteran status, including special disabled veterans.

NREL validates right to work using E-Verify. NREL will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee’s Form I-9 to confirm work authorization.","Golden, CO",Data Engineer,False
411,"Los Gatos, California
Data Engineering and Infrastructure
Netflix makes up 1/3 of internet traffic, and we're proud to deliver entertainment that over 100 million global customers enjoy. Behind the scenes, Netflix is delivered by Open Connect, a custom-built content delivery network that connects our content to thousands of ISPs around the world.

Decisions on how to improve streaming performance for our customers and evolve Open Connect's architecture are driven by data. We're looking for someone to transform petabytes of incoming data on video performance and network efficiency into well-designed, high-quality data structures that empower critical decision-making for teams within Netflix.

We track every customer action and each byte of data transferred, so you'll work with data at incredible scale and collaborate with best-in-class data engineers and analytic experts. You'll become an authority in the world of video streaming delivery (no prior knowledge necessary, but curiosity to learn is a must), and the projects you'll work on will be truly impactful.
In the meantime, learn more about the Streaming Data Engineering team.
What you'll do
You’ll take ownership and increase automation and scale of complex data sets that drive use cases by our analytical partners such as hardware capacity planning and failure prediction, understanding network topology and forecasting network traffic flow, and monitoring/improving efficiency of deployment and turnover of Netflix-encoded assets to the network.
You’ll build robust data pipelines of high data quality in a scalable fashion (both data and maintainability).
Every video or audio file you stream from Netflix started as a file living on an editor’s hard drive and became a Netflix-encoded asset sitting on a server ready to be played. You’ll create the data pipelines that will let us quantify and understand that life cycle by merging system activity and customer behavior.
We need to process data more quickly than ever to enable rapid experimentation in an increasingly nimble engineering organization. You’ll help implement our business logic to be compatible with real-time/stream processing frameworks.
Who you are
Have several of the characteristics/skills listed below and have passion and self-drive to quickly learn in areas of less familiarity. We believe the experience in your years is more important than your years of experience.
Enjoy a high level of autonomy in managing cross-functional engineering projects. We enjoy a culture of Freedom & Responsibility.
Have experience building production data pipelines using one or more frameworks such as Spark, Flink or Hive/Hadoop. Have hands on experience with schema design and data modeling.
Have programming proficiency in at least one major language such as Java, Scala or Python. You have a software engineering mindset and strive to write elegant, maintainable code and you're comfortable working in a variety of tech stacks. You may even be a software engineer with a focus or passion for data-driven solutions.
Have strong SQL skills and knowledge and familiarity with other distributed data stores such as ElasticSearch or Druid.
Have excellent communication in sharing context to effectively collaborate with analytical partners, domain experts and other consumers of your work, preferably in supporting an engineering or product function. We like to collaborate across teams and so should you.
Ambitious and willing to take action, but not stubborn. Awareness to recognize when you're wrong and move past your own mistakes. We are humbly confident in ourselves and our work.
Netflix Culture

Our culture is unique, and we live by our values. You will need to be comfortable working in the most agile of environments. Requirements will be vague, and iterations will be rapid. You will need to be nimble and take smart risks. Learn more about Netflix’s culture.","Los Gatos, CA",Data Engineer,False
412,"Overview:
---------------

Snagajob is working to transform the hourly job seeking experience. We have an unparalleled level of access to America's hourly workforce -- and the employers who are desperately looking for their help to make their businesses grow. As a Data Engineer, your job is to drive the connections between these complex data entities and their underlying systems which power our marketplace.

Data Engineers work to build and maintain systems that process amounts of generated platform data optimizing ingest pipelines and database systems that support our data platform. Data Engineers are the glue between our latent data and our machine learning systems which build rich intelligence to power our business.

What We'll Expect:
------------------


Work on a high performance Big Data environment processing 100 million events per day
Share ownership of an app portfolio with a highly collaborative development team that includes dedicated API and QA resources
Explore new technologies and frameworks to drive our architecture and processes forward
Use Agile development practices to focus on engineering craftsmanship, quality and best practices
Teach and learn with the team (check out the Snagajob Engineering Blog ( http://engineering.snagajob.com/ ))

What You'll Bring:
------------------


2+ years writing software (we currently use java, c# and python)
2+ years working on Big Data platforms such as hadoop, spark, flink, beam or other similar systems.
2+ years working with NoSQL databases such as MongoDB, Cassandra, Dynamo or other similar systems.
2+ years working with relational databases such as MSSQL or Postgres.
Experience with machine learning / data processing toolkits such as scikit-learn, pandas, juypter notebooks a plus.
Willingness to collaborate, explore and share
Desire to build inspiring data centric experiences that thrill our users
Commitment to remaining curious, open and active in your pursuit of the best practices
Degree in Computer Science or equivalent experience

What You Can Expect from Snag:
------------------------------

Snag offers a highly competitive compensation and benefits package including medical, dental, vision, and life insurance, 401k plan, health and fitness incentives, 20 days of PTO to start and 2 days of paid community service time, and a casual fun work environment with an award winning culture. At Snag, we don't just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our team, our products, and our community. Snag is proud to be an equal opportunity workplace.

About Snag:
-----------

Snag is the largest platform for hourly work with 90 million registered hourly workers and 450,000 employer locations nationwide. With Snag, employers staff up faster, hire smarter and keep shifts filled. Snag's platform for hiring and managing teams allows people to land the right work while ensuring employers find the right workers when and where they need them. Snag's flexible work platform, Snag Work, launched in 2017 and provides a network of workers the opportunity to select the shifts they want, when they want, from a variety of employers and locations, and helps employers optimize their shifts.

With offices in Arlington, VA; Richmond, VA; and Charleston, SC, Snag has been named to Fortune Magazine's Great Place to Work® list for eight years in a row.","Richmond, VA",Data Engineer,False
413,"Sun Basket is the #1 healthy meal kit in the US, which is backed by top-tier investors and led by one of San Francisco's top chefs. As the company continues to explode in size ($0-300M ARR in 3 Years), we are adding a Data Engineer to our BI and Analytics team and to continue this momentum. We are looking for a Data Engineer who is passionate about analytics, has solid experience working with data warehouse, possesses an innate curiosity about our business, and is eager to dive into large, complex data sets and ultimately create actionable business insights.
Sample Projects:
Centralize data and support the analytics team to deliver world-class insights.
Set up analytics data warehouse and data infrastructure, and develop a flexible data model for unstructured tracking data.
Perform all of the necessary data transformations to populate data into a warehouse table structure that is optimized for BI reporting.
Work on data integration projects for onboarding new vendors, such as CDPs or attribution vendors; can inform best practices on data taxonomy, hierarchy or naming conventions
Own data quality throughout all data life cycles, including acquisition, cleaning, processing, and validation.
Designing, integrating and documenting technical components for seamless data extraction and analysis on big data platform.
Build an experimentation platform to help facilitate A/B testing and model selection.A portion of your time will be used to maintain automated test suites using advanced frameworks.
Build analytics tools utilizing the data pipeline to provide actionable insights for our product and data science teams.
Be able to directly communicate with senior business leaders, to embed yourself with business teams and to present solutions to business stakeholders.

Education and Experience:
Bachelor's degree in a highly quantitative field including data science, computer science, math, engineering, statistics, economics, and hard sciences.
3-4 years experience with SQL and relational databases such as Postgres or MySQL.
Experience with DevOps tools (GitHub, JIRA).
Great knowledge of data warehousing principles, schema design, data governance, data pipeline automation, and query + database tuning techniques with excellent debugging and troubleshooting skills.
Ability to automate data pulling and reporting process.
Working knowledge of Amazon AWS services (Redshift, Aurora, DynamoDB, S3, RDS).
Experience with visualization software such as Looker or Tableau.
Python or R coding skill is a plus.
Experience with Google Analytics API and Facebook Business Manager API is a plus.
Personality and Values:
Ready to Grow: The BI team is the engine driving Sun Basket's stupendous growth, and you are eager and ready for this job to get bigger over time.
Problem Solver: You have the ability to answer unstructured business questions and work independently to drive projects to conclusions.
Effective Communication: Present and communicate analysis to stakeholders in order to drive business decisions.
Passion for Food: Sun Basket brings healthy nutritious meals to thousands of people every week. Ideally, you like food and are excited to be part of that.

Sun Basket is an Equal Opportunity Employer and does not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability with respect to employment opportunities. We value diversity and encourage all qualified candidates to apply.

Pursuant to the San Francisco Fair Chance Act, individuals with a criminal background are encouraged to apply.","San Francisco, CA",Data Engineer,False
414,"ContractProven expertise/success building robust data pipelines for batch and real-time analytics at scale, using a modern “lambda” style architecture, ideally with the AWS toolset.Architecting enterprise data-warehouses, migrations, data cleansing, modeling and normalization of raw data into prepared star-schema. Experience with Snowflake a big plus. Tacit knowledge of Redshift and Google Bigquery. Expert level SQL and JSON handling. Experience working with binary columnar data formats e.g. Parquet.Orchestration and monitoring of complex ETL and ELT workflows using AWS step-functions, Lambda coordinators and runner functions, Glue, and/or other tools (e.g. Airflow). Spark experience a plus, ideally with EMR or Glue. Python w/PySpark a plus.Implementing custom Kinesis applications using the Kinesis API, KCL and KPL, using lambda polling to transform/aggregate in-stream data, and building tooling to monitor and dynamically re-shard as needed. Light API development using AWS API Gateway, Kinesis and Lambda to process push-based events at scale (200K+ records per second)Serverless AWS architectures leveraging AWS Lambda with state management using ElastiCache, Memcached, Redis, DynamoDB, etc. with careful consideration for monitoring, logging and error handling.Experience automating 3rd party CLI tools, shell scripting, and integrating with API’s for data acquisition.Cloudformation or Terraform and modern CI/CD pipelines for automated code deployment/testing.Tacit knowledge of one or more cloud-centric COTS ETL tools e.g., Informatica, Talend, Matillion.10+ Years experience in data-architecture-related roles (data pipelines, warehousing, productizing analytics), preferably for large SaaS or online D2C business.Solid references that can speak to expertise/success with above.At least BS in Computer Science or EE. MS preferred.Job Type: ContractSalary: $150,000.00 to $200,000.00 /yearExperience:Data Warehousing: 5 years (Required)Software Development: 5 years (Required)Analytics: 5 years (Required)Education:Master's (Preferred)Location:Santa Monica, CA 90404 (Required)Language:English (Required)","Santa Monica, CA 90404",Cloud Data Engineer,False
415,"InternshipOur Software Data Engineers build systems and analytics used by health care organizations around the country for managing risk. You will gain a deep understanding of structured and unstructured data sets from the health care domain as well as proprietary and industry-standard analysts calculated on this data. Your technical challenge will be to design, develop, and test a system that ingests, aggregates and presents millions of records of complex health data to our customers. If you enjoy programming, data analysis, and quickly learning your way into new areas, then you are probably a good fit for this role.

There will not be work sponsorship offered with this position.

Experience Level:

Bachelor’s candidate in a pertinent degree (e.g. Computer Science, Software Engineering, Applied Statistics / Mathematics etc.)

This is an entry level position intended for, but not limited to, college students. We will give preference to students closer to graduation as we wish this internship to serve partly as an extended interview for full time employment.

There will not be work sponsorship offered with this position.

Business Overview:

Milliman MedInsight is one of the healthcare industry’s most highly regarded platforms for data warehousing and healthcare analytics, and has been adopted by payers, purchasers, providers and other healthcare clients.

Milliman PRM Analytics, located in Indianapolis, IN, is a product group that operates in parallel with the MedInsight practice. Our products are used by health care administrators, medical directors, health care providers, and others to manage the clinical and financial risk in a patient population.

Milliman, Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, age, protected veteran status, disability status, or other characteristics protected by law.

Position Description:

Our Software Data Engineers build systems and analytics used by health care organizations around the country for managing risk. You will gain a deep understanding of structured and unstructured data sets from the health care domain as well as proprietary and industry-standard analysts calculated on this data. Your technical challenge will be to design, develop, and test a system that ingests, aggregates and presents millions of records of complex health data to our customers. If you enjoy programming, data analysis, and quickly learning your way into new areas, then you are probably a good fit for this role.

The Software Data Engineer is required to work in the Indianapolis, IN office of Milliman.

This position is intended to represent a summer internship opportunity during a college break. For candidates not attending college, this could represent an approximately six month long internship.

Responsibilities:

On a daily basis you will:

Write code to maintain and enhance data/analytics pipeline(s)
Strive for fault tolerant processes and scalable solutions
Understand and work with complex data structures and advanced analytics
Work with team-members to propose technical solutions to business problems
Contribute to the growth of your team by sharing knowledge
Minimum Qualifications:

Bachelor's candidate in a pertinent degree (e.g. Computer Science, Software Engineering, Applied Statistics/Mathematics, etc.)
Communication that is clear, logical, and cordial
A helpful, collaborative, and team-oriented attitude
Insatiable appetite to learn
Professional poise
Grit to make it through the difficult problems
Pride and ownership to want to make everything better
Desire to work with code and data
Basic statistics and an intuition for data
Basic software development principles (e.g. ""Don't Repeat Yourself"")
Tools/techniques we utilize and will teach as needed (i.e. candidates are not required to know any of these):

Git and GitHub
Python
R
Apache Spark
Jenkins
SaltStack
KanBan Workflows
Why Milliman?

Milliman is a global consulting and actuarial firm and a recognized leader in the markets we serve. We owe our standing to the quality of our consultants and employees, who are among the most satisfied in the industry.

Milliman offers talented and self-motivated individuals a place to achieve and grow. Our entrepreneurial environment rewards excellence and innovation. Our unique culture creates an atmosphere where bright employees have the opportunity to produce superior work and can chart their own careers. With our client-focused approach and unmatched depth and breadth of expertise, Milliman delivers practical solutions to challenging, future-oriented business problems.
Qualifications
Behaviors

Preferred
Dedicated: Devoted to a task or purpose with loyalty or integrity
Team Player: Works well as a member of a group
Motivations

Preferred
Growth Opportunities: Inspired to perform well by the chance to take on more responsibility
Education

Preferred
Bachelors or better in Applied Statistics or related field.
Bachelors or better in Computer Science or related field.
Bachelors or better in Software Engineering or related field.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.","Indianapolis, IN",Software Data Engineer Internship,False
416,"Job Description

We are looking for an ensusiastic Data/Big Data Engineer to join our growing team of Data Services experts.
The hire will be responsible for building and optimizing our data provisioning architecture around initiatives supporting Visa Products.The candidate must have the ability to think both strategically and tactically, to enforce global architectural principals while tuning SQL performance at the detail level. An understanding of data warehousing, OLAP, OLTP, naming standards, data federation, governance and metadata is key to this position. The ability to work independently and learn quickly is essential. The role offers great opportunities to learn various tools and technologies used in a sophisticated data architecture within the Business Intelligence and Analytics Data Services team in Data Products Development organization.
Responsibilities/Duties
Hands-on execution as well as leadership of the following:
Assemble large, complex data sets that meet functional / non-functional business requirements
Create and tune complex SQL for views across federated sources including Hadoop, DB2 and Oracle
Collaborate with product management / product owners on defining user experience, demos and training
Collaborate with BI report developers on design for optimal performance
Assist on data analytics projects involving data modeling and architecture
Assist on building integration with other data / metadata tools in the architecture
Assist on some database administration tasks for the data virtualization platform
Use business requirements to document clear and concise technical designs
Maintain design and naming standards
Contribute to project planning discussions, provide status updates for development progress and be a critical resource for issue resolution

Qualifications

Bachelor's degree in Computer Science or related discipline
Experience with Big Data tools: Hadoop, Spark, Kafka, etc.
Minimum 3 years of experience writing and tuning SQL
Experience with RDBMS technologies (DB2, Oracle)
Experience with SQL-on-Hadoop preferred (Hive, Impala)
Familiarity with web services, APIs and related architectures
Excellent written and oral communication skills
Experience with basic Windows and Linux administration is a plus
Experience in Agile development methodology / Scrum is a plus
Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of Article 49 of the San Francisco Police Code.All your information will be kept confidential according to EEO guidelines.","Palo Alto, CA",Data Engineer,False
417,"About the Role. . .
Do you like helping customers and partners solve technical problems? Would you like to do this using the latest cloud computing technologies? Do you have a knack for helping people understand technology?
If the answer is Yes, Logic20/20 is looking for a Data Engineer to join our Seattle-area-based team. This is a great opportunity to have a positive impact on a variety of different key areas in the Company. The role will be responsible for providing technology services to a major local client through a managed cloud service offering.
About You. . .
The ideal candidate is someone who:
Enjoys engaging with people
Relishes taking on challenges
Has a continuous thirst for knowledge and is a perpetual learner
Strives to make themselves and everyone around them better
Has a passion for technology
Improves processes and approach, in large IT organizations.
Quantifies the efficiency of your own work and works towards success.
Validates consistency, accuracy, and completeness of the data, identify patterns in systematic data issues and propose robust solutions.
Applies your 6 Sigma skills in IT.
Qualifications. . .
Identified data issues, performs RCA and provides optimal solutions
Creates an overall picture of old, new and potential data issues - across the organization and conceptualizes solutions - then works with the team to implement.
Quantifies the impact of the data cleansing efforts
Master certified in Six Sigma
Consulting experience
MDM - master data management
DMM / CMMI certification
SQL
About the Team. . .

Work within a team focused on overall corporate strategy, in shared service engineering teams.

About Logic20/20. . .

Logic20/20 is one of Seattle’s fastest growing full service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.

The past three years, we’ve been in the top 10 “Best Companies to Work For” ….. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity.","Seattle, WA 98115 (Wedgwood area)","Data Engineer - Quality, Cleansing, Process",False
418,"$110,000 - $135,000 a yearUS Citizens & Green Card Holders ONLY
NO C2C
CANDIDATES WILL WORK ON CAPGEMINI W2
Pay can range from a $110k base to $135k base
Title: Data Engineer – Big Data/Python
Location: NYC, NY

Job Responsibilities:
Must Have Skills:
Senior Developer who has In-depth knowledge of big data predictive analysis including: relational data mining, neural network, nearest k-neighbor, association rules, time series, regression trees, and sequence clustering.
Extensive experience in Python programming Worked extensively on AWS Hadoop ecosystem tools and concepts like Hive, NoSQL, Spark, Scala, Hbase, Sqoop, Map Reduce, Pig Excellent command over SQL and relational databases including writing complex queries, data transformation, conversion on Big Data.
Good hands on development of ETL packages
Nice to have Skills:
Tableau for data visualization.","New York, NY",Data Engineer – Big Data/Python,False
419,"Company: SpotX, Inc.
Requisition ID: 24222

We’re SpotX, one of the top-rated companies to work for in Colorado. If you would like to learn more about us, our team is inviting you to join us for one of our events:

SpotX Engineering Recruiting Open House
Sustaining our Communities Hackathon

SpotX is seeking a talented Data Engineer who can contribute brilliantly to our Data Intelligence team located in Broomfield, CO. Our team solves problems important to the business giving the opportunity for high impact.

The data engineer role will interface with business, product owners, and the Data Analytics team to deliver timely and quality data sources in support of reporting through software like Tableau and Superset.

We don’t believe in “culture fit”. We believe in you being a genuine human being and pleasant to work with. Sure, we might share a beer at work or after work but you don’t have to. You just need to be a contributing member of our team who is thoughtful and wants to do quality work.

Making an immediate impact:
Develop and maintain accurate and robust data sources from very large datasets
Work closely with the data analyst team in development and support of reporting
Root cause and corrective action assessments of data discrepancies, often with minimal information
Support software creation, maintenance, and enhancements
Work with management and product team in accurately planning and executing sprints
Perform additional duties assigned by management

Needed SpotX’er talents:
Bachelor's Degree (BA) from four-year college or university
2 to 3 years experience working in big data
Strong knowledge in Python and MySQL/HQL/SQL
Knowledge of Hive and Spark
Comfortable working in a Linux/Mac development environment
Possess strong analytical skills, with the ability to analyze raw data
Effectively manage multiple tasks, large and small, delivering accurate and timely data
Work independently in a fast-paced environment, often without formal requirements and with minimal supervision
Be a highly motivated self-learner
Have excellent verbal and written communication skills

Bonus SpotX’er talents:
Ad Tech experience
Druid ingestion experience
Airflow DAG creation experience
Scala experience

SpotX Perks:
Work-life balance
Unlimited PTO(work it out with your team first!), company closed from Christmas to New Years
Make a real difference - your code reaches millions of people as soon as you release it
Work in a fun, casual, team environment – flip flops OK
Frisbees, foosball table and ping pong
Benefits such as medical, dental, and 401(k)

SpotX is the leading global video advertising platform that enables media owners and publishers to monetize premium content across desktop, mobile and connected TV devices.

Visit our About Us page to learn more.","Broomfield, CO 80021",Data Engineer,False
420,"$150,000 a yearContractWe are looking for talented software engineers to join our bigdataservices development team. Your past experience matters, but more important to us is what you can do going forward. If you are technically talented and have the tenacity to build upon your current skill set, then we want to talk to you. The ideal candidate has both a willingness and desire to work in a dynamic environment, and is a self-motivated developer who mentors and shares knowledge.ResponsibilitiesDefine ideal Architecture, Evaluating tools and Frameworks, Standards & Best Practices for implementing scalable business solutionsImplement Batch and Real-time dataingestion/extraction processes through ETL, Streaming, API, etc., between diverse source and target systems with structured and unstructured datasetsDesign and build datasolutions with an emphasis on performance, scalability, and high-reliabilityCode, test, and document new or modified datasystems to create robust and scalable applications for dataanalyticsBuild datamodel for analytics and application layersWorking closely with multiple teams and Business partners, for collecting requirement and providing optimal solutionRequired Knowledge and SkillsProven experience on Hadoop cluster components and services (like HDFS, YARN, ZOOKEEPER, AMBARI/CLOUDERA MANAGER, SENTRY/RANGER, KERBEROS, etc.)Ability to participate in troubleshooting technical issues while engaged with infrastructure and vendor support teamsExperience in building stream-processing systems, using solutions such as Kafka, Storm or Spark-StreamingProven experience on BigDatatools such as, Spark, Hive, Impala, Polybase, Phoenix, Presto, Kylin, etc.Experience with integration of datafrom multiple datasources (using ETL tool such, Talend, etc.)Experience building solutions with NoSQL databases, such as HBase, MemsqlStrong experience on Database technologies, DataWarehouse, DataValidation & Certification, DataQuality, Metadata Management and DataGovernanceExperience with programming language such as, Java/Scala/Python, etc.Experience implementing Web application and Web Services APIs (REST/SOAP)Summary: 1. HIVE – Must have2. HBASE (MO SQL) – Nice to have3. SPARK SQL – Must have (required 100%)4. Python – Nice to have5. Java background a plusJob Types: Full-time, ContractSalary: $150,000.00 /year","Long Beach, CA",Big Data Engineer,False
421,"ContractJob Title: Data Engineer



Terms: 6+ month contract

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Job Description
Need to have 3+ years of experience

Strong data engineer able to:
Strong SQL and relational database experience
Strong Python
Customer has PySpark environment so you need to have Spark experience as well. you can also have Spark/Scala as long as you have the Python experience
Those who are shortlisted will be given a a problem and need to walk through the solution

In person Discussion at Sanjose,CA


We are Growing Rapidly: 2017/2018 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.

Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).","San Jose, CA",Data Engineer,False
422,"$250,000 - $300,000 a yearA million people a year die in car collisions around the world. That number can be zero. You can help us build an InsurTech company that uses the latest technology, data science, and behavior modification methods to save lives by preventing car collisions before they happen. To this end, we helped launch hiroad.com, a cloud native insurance solution that rewards people for the act of driving well. We are a well funded team of elite developers, data scientists, and business people who truly care about making a difference in the world, located in the financial district of San Francisco. The field is rich with data and we will be pushing the boundaries of what is possible with it. If this sounds like a match for you and what you are up to, please apply. We'd love to hear from you.
Skills and requirements:
You've built streaming data applications using open source tools
You're deeply familiar with the SMACK stack and Scala
You've deployed machine learning models in production
You are a solid software engineer
Ideal, but not required:
You have been responsible for supporting large-scale, data-intensive deployments and have the scars to prove it!
You know how to put together a machine learning model
You have wrangled trip data (location, accelerometer, gyroscope, etc).
More details:
Salary: We invest in first-rate people and pay top-of-market salaries for most positions, factoring in experience and talent. We are unable to offer equity.
Benefits: Full medical, dental, vision coverage, 401k, daily catered lunch, wellness reimbursement & on-site shower, four weeks of vacation, six weeks of parental leave, panoramic views, and more.
Location: Near Montgomery Street BART station, San Francisco, California. Locals preferred, but relocation within the US considered for outstanding candidates.
All are welcome at Blue Owl. We are an equal opportunity and affirmative action employer who values diversity and inclusion and looks for applicants who understand, embrace and thrive in a multicultural world. We do not discriminate on the basis of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.","San Francisco, CA",Senior Data Engineer,False
423,"About Scoop

Scoop brings co-workers and neighbors together to enjoy a smooth carpooling experience—unlocking new opportunities to create friendships, improve their well-being, and make the most of their valuable time.

Learn more in Forbes: https://www.forbes.com/sites/miguelhelft/2017/11/08/with-36-million-in-financing-scoop-wants-to-make-carpooling-mainstream/

Engineering @ Scoop

Few companies get to face such diverse technical challenges as Scoop, and we’ve built a team of people excited to face these challenges together while investing in each others’ growth.

Scoop’s engineering team may move bits and pixels, but we also put real, live human beings in cars together. We’re touching problems academics have written about for years, and have data that no other company has ever collected.

But Scoop knows engineering is not a lone discipline. We’re a small team with varied backgrounds: big companies, VC-backed startups, bootcamps, academia. We like to build together, and we like to learn together. Our entire team and process are built around helping you grow and be successful. And we’d love to tell you more about the impact you could have at Scoop.
In this role, you will:
Architect, develop, and deploy infrastructure on which data moves
Operationalize machine learning—from research into fault-tolerant, production-scale deployments
Apply grit and inventiveness, both in writing new software and also deploying existing tools like Airflow, Spark
Work closely with data scientists and Scoop’s product team to understand their needs and create a platform that empowers them
You should:
Have experience with building and deploying large scale ETL pipelines
Previously have worked in a data-driven company
Know the many pain points of AWS
Be proficient in Python and/or Scala
Life @ Scoop

Founded in 2015 and based in downtown SF, our team mixes technology and elbow grease every day, with one statistic in our crosshairs: 80% of Americans drive alone to work. At Scoop, we envision a world where commuters feel empowered — starting with a choice to make their commute a meaningful part of their day. We embody that same spirit within our own culture, empowering every team member to make this the most meaningful experience of their career.

Walk into Scoop and you’ll find a furry, tail-wagging welcoming committee. In many ways these fluffy faces exemplify the energy that flows through our office. They are a reminder that while we’re focused and driven, we shouldn’t take ourselves too seriously. They also help bridge the gap between our homes and our workplace, just like a Scoop carpool.

The atmosphere overall is dynamic and unique. It’s influenced by our backgrounds at successful startups, big tech companies, and premier consulting firms — blended and crafted into what feels natural and right for this company. It plays out in our balance of scrappy and strategic, frameworks and fast thinking.

At Scoop, we’re all united by our desire to change the way people get to work — and committed to enjoying the journey together along the way.","San Francisco, CA",Data Engineer,False
424,"The Role
Come help us and our customers learn about the world through data. With industry leading companies using insights gleaned from our data to understand how purchase and travel behavior is changing in America, the ability to marshall the volume and complexity of our data is paramount.

As the data science team’s first data engineering hire, you will be responsible for scaling our operations for deep internal analysis and customer deliveries to handle the next stage of growth while prioritizing data security and user privacy. You will design, implement and maintain the infrastructure on which our data move. You will develop predictive modeling pipelines for scientists and data models that enable non-technical colleagues to ask questions of our data. You identify and vet new technologies that will help us ingest and transform data.

You
You are deeply curious and are passionate about building tools that become vital. You creatively work through problems and have the natural itch to follow obstacles to their root cause. You are expert at building data products that empower users. You think deeply about the problems ahead and are comfortable moving between tools in your toolbelt. And you are quick to learn, implementing new approaches when facing questions that demand it.

You are comfortable being the decision maker and relish the opportunity to take ownership of challenges. You are passionate about helping others make sense of data and are excited to shape our long term data strategy. You are a strong advocate for data best practices within the company.

Requirements

Expertise in designing and building pipelines in a distributed environment (spark, hadoop), managing data flow (kafka), loading data into warehouse platforms (redshift, snowflake) and traditional data stores (sql)
Strong experience in a scripting language (python, scala)
Familiar with working in a cloud environment (aws)
Bachelors in Computer Science, related discipline or equivalent experience
3+ years of experience managing and manipulating large data sets
Excited by a high learning curve
Excellent written and oral communicator

Compensation and Benefits

Highly competitive salary and benefits
Stock grants pre-IPO at a company backed by top investors
Take unlimited, responsible vacation

About Us
Edison provides intelligent email solutions for users and competitive intelligence for businesses. The largest, most valuable and as yet untapped data on earth is in mail; 3x larger than the worldwide web. Through our user base of more than 3 million users, we empower investors, brands, and technology companies to understand trends in the marketplace and gain deep insights into consumer behavior patterns.

As a team, we’re collaborative, engaged, and committed to continually improving as we serve our mission. None of us are on an island-- we trust our teammates to lend a hand when we’re stuck and our egos take a backseat to figuring out the best approach to tackling problems. We’re energized by tough problems and are excited to know that a challenge ahead of us does not have a textbook solution. Finally, we’re always in a posture of learning-- there is a lot we do not know but that does not hold us back from making an attempt at solutions. We lead thorough blameless postmortems to become better analysts, scientists, and leaders.","San Jose, CA",Data Engineer,False
425,"Niantic’s Engineering Team seeks a Data Engineer to create a finance data analytics warehouse that supports Pokémon GO, Ingress, and Harry Potter: Wizards Unite and the hosted real world platform underpinning these. Niantic Engineering leads the advancement of AR and other immersive technologies while creating engaging apps for a user base in the billions.

Responsibilities
Create and maintain data warehouses.
Work with Finance to overlay calculations.
Provide technical implementation and support.
Use ETL technologies to cleanse, load and transform data for financial analysis and accounting purposes.
Qualifications
You have a Bachelor’s degree or above in Information Systems, Computer Science or related field.
You have previous experience with ETL solution.
You have 2+ years of experience with SQL / query logic.
You have 2+ years of work experience building software in a professional team setting.
You have a proven record of being comfortable with large volumes of complex data.
You are proficient at coding in python.
Plus If...
You have prior experience with financial analysis and/or accounting systems.
You have experience with data modeling.
You have experience with NetSuite.
Join the Niantic team!
Niantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.

Originally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, and record-breaking AR game Pokémon GO. Our third title, Harry Potter: Wizards Unite, is currently in development.

Niantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.

We're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, Tokyo, and Hamburg.","San Francisco, CA","Data Engineer, Finance",False
427,"Job Description
Would you like to be part of a team focused on improving customer experience as well as helping Amazon save lots of money? Are you passionate about data?

As a Data Engineer with Global IT’s Finance Business Operations team, you will be working in a large, extremely complex, and dynamic, data-driven environment. We are looking for data engineers with expertise and passion for analyzing data, designing and building predictive and decision models, and designing metrics to measure the performance of the business. You will interact with business groups that rely on the metrics and the decisions produced using predictive models.

Key responsibilities of the role include:
Interface with business customers, gather requirements, and deliver complete reporting solutions
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
Develop a deep understanding of our vast data sources and know exactly how, when, and which data to use to solve particular business problems
Work with internal stakeholders to root cause identified defects

Basic Qualifications
Bachelor's degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering, or similar
Demonstrated ability in data modeling, ETL development, and data warehousing
Strong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams.
Experience with Big Data
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist), with a track record of manipulating, processing, and extracting value from large datasets
Coding proficiency in at least one modern programming language (e.g. Python, Java, Scala)
Experience building data products incrementally, and integrating and managing datasets from multiple sources
Linux/UNIX including to process large data sets.
Strong ability to interact, communicate, present, and influence within multiple levels of the organization
Master's degree
Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions","Seattle, WA",Data Scientist,False
428,"Why Work Here?****Arkatechture began in 2012 with a passion for data, business, and getting things done. We are a team of data lovers and technical experts who use our skills to help businesses big and small harness, utilize, and optimize their data. At Arkatechture, we work hard and we play hard--we genuinely love what we do and that's what makes us different.We take great pride in being New England's Data Resource. We've been based in Maine and New Hampshire since our founding and have no plans on leaving. We love being a part of our communities, and many of our team members are involved in local activities and organizations. We offer a competitive benefits package that includes health and life insurance, an annual bonus based on company performance and paid time off!The PositionArkatechture is seeking candidates for a Data Engineerposition to support various Data initiatives for our clients, particularly in the Data Warehousing and BI world. We are looking for a self-starter who is passionate about data and excited to work on new and emerging technologies. The ideal candidate is someone who understands and has implemented full end-to-end Data Warehousing solutions including data architecture, data provisioning, data integration, data publishing, and can execute effectively as part of a team. Must have hands-on experience working on BI and Data warehousing Projects. Should be able to work effectively with the Business and Technology teams while collaborating with Project/Program Managers.Responsibilities- Source-to-target mapping including detailed business and transformation rules- Data modeling, data profiling, data quality analysis/remediation, database table, index, and view creation- Designing, developing, testing and review of code [ELT or ETL, views, stored procs, etc.]- Communicating with both technical and non-technical collaborators- Requirements elicitation- Estimation and working with Project manager on task allocation- Additional responsibilities as assignedMinimum Qualifications**- Bachelor's degree in Computer Science or related major- 1 to 3 years of experience in a similar individual contributor role- 2 to 4+ years of experience in a leadership role- Implemented several Data Management projects for Data Lakes/Data Warehousing- Hands on experience using ETL tools like Talend, Alteryx, Alooma, Matilion, Informatica, Datastage, etc- Experience in any one Programming language like Python, JavaScript, C#, Java, etc.- Experience working on any one Database such as Snowflake, SQL Server, Oracle, Aurora, PostgreSQL, MySQL etc- Excellent SQL skills and understanding of Database Models and tuning- Experience working with APIs, specifically REST APIs, SDKs and CLI tools as part of ETL provisioning- Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.- Exposure to Cloud Technologies such as AWS, Azure or GCP (Certified Practitioner at any level is a nice to have)- Strong understanding of Microservices architecture- A strong understanding of Agile software development life cycle and methodology using toolsets such as Jira, Confluence, etc.- Exposure to NoSQL databases like MongoDB or DynamoDB (nice to have)-This position is based out of our Portland, ME office (this is not a telecommuting or hardware IT position).How to ApplyPlease send a Cover Letter along with your Technical skills, Resume (must have prior project experiences listed along with technologies used), and Salary requirements with your application. If you do not currently live in the Portland, ME area please explain your relocation plans in your cover letter. You must submit all requested documents to be considered for the position.Job Type: Full-timeExperience:REST APIs, SDKs and CLI tools: 2 years (Required)Microservices architecture: 2 years (Required)Excellent SQL skills and understanding of Database Models: 2 years (Preferred)JSON, XML, CSV, Flat, etc: 2 years (Required)AWS, Azure or GCP: 1 year (Preferred)leadership: 4 years (Required)Python, JavaScript, C#, Java, etc: 2 years (Preferred)similar individual contributor role: 3 years (Required)Snowflake, SQL Server, Oracle, Aurora, PostgreSQL, MySQL etc: 2 years (Preferred)Talend, Alteryx, Alooma, Matilion, Informatica, Datastage: 2 years (Preferred)Agile software (Confluence, JIRA): 2 years (Preferred)Education:Bachelor's (Required)Location:Portland, ME (Required)Work authorization:United States (Required)","Portland, ME",Data Engineer,False
429,"Description

About Us:
Founded in 1962, Raymond James Financial, Inc. is a Fortune 500 diversified holding company providing financial services to individuals, corporations and municipalities through its subsidiary companies engaged primarily in investment and financial planning, in addition to capital markets and asset management. Headquartered in Florida, Raymond James Financial has approximately 7,500 financial advisors in 3,000 locations throughout the United States, Canada and overseas. With 119 consecutive quarters of growth and Service 1st culture, Raymond James Financial aims to be the premier alternative to Wall Street.

About the role:
As a Data engineer, you will be part of the Data Engineering team building Enterprise Data Integration solutions and working on enterprise class data integration initiatives. Our vision in Information Technology is in parallel with the firm’s vision. We strive to be the premier provider of financial services technology and support through innovative solutions, reliable performance and a Service 1st culture. Besides our headquarters in St. Petersburg, FL, we also have presence in Southfield, MI; Memphis, TN; and Denver, CO.


Responsibilities:
Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data LakeDesign and develop new systems and tools to enable folks to consume and understand data fasterSupports ETL Batch processing.Provides on-call support of Data Integration Batch processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.

Qualifications

Qualifications:
Minimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Big Data Platforms.

Licenses/Certifications:
None required.

Competencies and Behaviors:
Analysis: Identify and understand issues, problems and opportunities; compare data from different sources to draw conclusions.Communication: Clearly convey information and ideas through a variety of media to individuals or groups in a manner that engages the audience and helps them understand and retain the message.Exercising Judgment and Decision Making: Use effective approaches for choosing a course of action or developing appropriate solutions; recommend or take action that is consistent with available facts, constraints and probable consequences.Technical and Professional Knowledge: Demonstrate a satisfactory level of technical and professional skill or knowledge in position-related areas; remains current with developments and trends in areas of expertise.Building Effective Relationships: Develop and use collaborative relationships to facilitate the accomplishment of work goals.Client Focus: Make internal and external clients and their needs a primary focus of actions; develop and sustain productive client relationships.","Saint Petersburg, FL",Data Engineer,False
432,"TemporaryOne of our largest retail client is looking for experienced Data EngineerDescription:Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ years’ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.Must HaveStrong analytical backgroundSelf-starterMust be able to reach out to others and thrive in a fast-paced environment.Stron background in transforming big data into business insightsTechnical Requirements Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance OptimizationExperience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)Advanced SQL (preferably Teradata)Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeExperience in ETL/ processesReal time data ingestion (Kafka)Nice to Have Development experience with Java, Scala, Flume, PythonCassandraAutomic schedulerR/R studio, SAS experience a plusPrestoHbaseTableau or similar reporting/dash boarding toolModeling and Data Science backgroundRetail industry backgroundEducationBS degree in specific technical fields like computer science, math, statistics preferredAdditional Job DetailsPotential CTH Must HavesExcellent knowledge and experience with Hive and SQLExperience with Spark SQLProficientwith one programming language Java/Scala/PythonGeneralunderstanding of how to build end to end data pipelines Good to HaveExperience n architecting data pipelines – from Data model to the jobs and the sequence of jobsAbility to build dashboards with Tableau or ThoughtSpotSoftwareEngineering knowledge – ability to build web applications using JavaJob Type: TemporaryEducation:Bachelor's (Preferred)","Sunnyvale, CA",Data Engineer,False
433,"How would you like a career where you get to use your best analytical skills to make a substantial difference in the well-being of people across the globe? Bring YOUR skills and talents to Lilly and our Advanced Analytical and Data Sciences organization where you’ll have the opportunity to make an impact on the lives of patients.
As an innovation driven company we work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism.

Our Advanced Analytical and Data Sciences organization is growing to support the entire Lilly enterprise, from Discovery to Development to Manufacturing and Commercialization of our medicines. To solve the complex problems of a global business and the ever-evolving data and analytics landscape, the organization generally requires advanced degrees in statistics, mathematics, econometrics, operations research and computer science. We are playing a leading role in transforming the way the company discovers and develops new treatments, identifies personalized treatment regimens, drives efficiency in our operations and optimizes our commercialization of new products. WE are doing this with an emphasis in the areas of machine learning and artificial intelligence, natural language processing and other approaches to unstructured data, advanced mathematical and predictive modeling, visual analytics and more. Whether you are intrigued by the research and development of new medicines or optimizing our commercialization/business, or driving efficiency into our operations, we have a position for you. You will be encouraged to identify important business problems and to further your own research interests in these areas including presentations and publications at professional meetings. Come join us on our amazing journey to make life better!

Key responsibilities include:
Partner with key business partners and work within team to identify, scope, and execute analytic efforts that leverage data to answer business questions, solve business needs, and add business value
Maintain a broad understanding of the pharmaceutical industry (from discovery to commercialization) and be fully engaged with teams, bringing an objective voice to the table, and facilitating decisions grounded in data
Collaborate with other analytics team members to review and provide feedback on the analytics work being done, and be willing to seek feedback from other team members about your own work
Streamline and prepare data for analysis through understanding of data flow and integration
Creating and driving standards for data capture, storage, and transformation.

Company Overview
Lilly is a global health care leader that unites caring with discovery to make life better for people around the world. For more than a century, we have stayed true to a core set of values—excellence, integrity, and respect for people—that guide us in all we do: discovering medicines that meet real needs, improving the understanding and management of disease, and giving back to communities. We also are committed to investing in our employees and supporting a culture of well-being —through competitive pay, comprehensive employee benefit programs, and training and development resources.

We’re doing extraordinary things. Join us and you could be, too!

Basic Qualifications
Master in Statistics, Computer Science, Biostatistics, Operations Research, MIS or related areas
Qualified candidates must be legally authorized to be employed in the United States. Lilly does not anticipate providing sponsorship for employment visa status (e.g., H-1B or TN status) for this employment position
Additional Information
Lilly is an EEO/Affirmative Action Employer and does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status
Additional Skills/Preferences
Proficiency with one or more relevant programming languages such as R, SQL, SAS, Python, C++
Deep knowledge of database structures
Strong communication skills
Bring an insatiable desire to learn, to innovate, and to challenge yourself for the benefit of patients.
Has a passion to learn new things, such as machine learning, artificial intelligence algorithms
Lilly is an EEO/Affirmative Action Employer and does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.
RegionNorth America
Country
USA
CityIndianapolis
Workplace Arrangement
Local
Job Expires14-Feb-2019","Indianapolis, IN",Data Engineer - Advanced Analytics and Data Science,False
434,"Passionate about data? Come noodle with us!

We are accelerating our growth as our company gains increasing traction in the exciting ""AI for the Enterprise"" market. We are looking for talented technologists who want to be part of a world-class team and bring with them a healthy mix of intellectual curiosity, desire to learn and passion for excellence.

As a Data Engineer, you will collaborate with the Noodle Client Service team, Data Scientists, SW Engineers, and UX Designers, as well as industry-specific experts from our clients. You will be responsible for developing, maintaining, and testing data solutions with a wide variety of data platforms including relational databases, big data platforms and no-sql databases. You will develop various data ingestion & transformation routines to acquire data from external data sources, manage distributed crawlers to parse data from web sources, and develop APIs for secure exchange of data. You will be involved in securing access to the data based on appropriate rights, implementing data quality routines and mechanisms to flag bad data for correction, and building QA and automation frameworks to monitor daily ingestion of data and provide alerts on errors and other problems.

Qualifications:
Must haves


3-6 years of experience with engineering data pipelines
BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields)
Excellent knowledge of relational databases like SQL Server, PostgreSQL or MySQL
Proficient with writing SQL queries, Stored Procedures and Views
Strong fundamentals in any programming language like C#, Python or Java
Familiarity with any ETL tool like SSIS, Informatica Power Center, Talend or Pentaho
Excellent at writing code to parse JSON / HTML / Javascript etc.
Passion for learning and a desire to grow – Noodlers are life-long learners!

Nice to haves


Strong knowledge of what works and what doesn't. This includes common pitfalls and mistakes when designing a data pipeline.
Comfortable working with both high performance on-premises SQL installations and cloud instances.
Familiarity with Hadoop and Spark
Demonstrated energy and passion that extends beyond your field of study – Are you a computer engineer who writes poetry? A mathematician who loves psychology? An engineer passionate about public policy? We want to build something with you.
Experience with (and excitement for) interdisciplinary collaboration

Want to help shape the future of Enterprise Artificial Intelligence?

Let's noodle.","San Francisco, CA",Data Engineer,False
435,"Overview
The Media Solutions Team at MWG is seeking a Data Engineer to help build out a new Ad Tech team. We are seeking experienced, full stack, ad tech rock star unicorns to join us in this journey. Real Time Bidding, Big Data, Machine Learning - we want to take our product offering to the next levels.
Responsibilities
Extract, transform, and analyze large volumes of structured and unstructured of data from various sources
Architect and maintain data stores for Big data
Write and optimize complex SQL queries for these data sets
Create dashboards for visibility into these datasets
Stays abreast of industry trends and tech
Lead architecture and tech decisions
Mentor junior team members
Qualifications
3+ years of development experience focusing around BigData
3+ years of development experience (Java, Scala, C#, Golang, R)
Experience building and deploying large scale ETL pipelines
Degree in Computer Science, Math or equivalent experience
Strong understanding of math, algorithms, data structures and design patterns
A desire to learn and teach others along the way
A desire to constantly push the bar to the next level
Experience working on a fast pace, agile team
Willingness to participate on on call rotations
Experience with RTB frameworks & platforms (OpenRTB, RTBKIT, Beeswax, etc)
Experience with Machine learning (Tensorflow, Caffe2, Torch)
Strategy and planning: an ability to think ahead and plan over a 12-24 month time span.
Problem analysis and problem resolution at both a strategic and functional level.

Required Skills:
RDMS Experience (MySQL, percona)
NoSQL Experience (MongoDB, Elasticsearch, Redis)
BigData Experience (Hadoop, Spark, Hive)
Windows, Linux, Docker","New York, NY",Data Engineer,False
436,"As a Sr Data Architect at Engage3, you will lead a technical team that architects, builds, maintains, scales, monitors, administrates and secures Engage3's retail pricing platform. You will actively work in a multi-disciplinary fast-paced environment. Your ultimate goal is to create a solid, flexible, stable system that enables us to deliver best-in-class analytics products to retailers and brands in the face of massive growth.
This role requires a broad range of skills and abilities; you will be the functional lead, manage staff and do the work. Your primary responsibility is to enable data access, data processing and data products by architecting, maintaining, scaling, monitoring & securing.
ML production system (AWS, Python)
Data Warehouse (Snowflake)
ETL system & data pipelines
BI system (Tableau Online)
As a qualified applicant:
You have planned, built & managed data infrastructures in a public cloud
You have strong experience with working with tools & platforms within the AWS ecosystem (EC2, S3, Aurora, Lambda, API Gateway, etc)
You have in-depth experience with MySQL databases and Snowflake's data warehouse
You have managed a business intelligence system
You have demonstrated experience of ETL developments
You are proficient in at least one programming language like Python, Scala and Java
You have familiarity with big data technologies like Hadoop, Spark, Hive
You are comfortable with setting and meeting SLAs for data availability and quality
You have an understanding of Machine Learning / AI principles in data engineering
You are a mentor to your team & colleagues and have passion in sharing your knowledge
You've worked in an Agile environment. You thrive on iteration. You make opportunities to bring value sooner rather than later.
You value data-driven decisions. You are always looking for opportunities to quickly produce the right data to make decisions quickly. You keep cool under pressure.
You are a self-driven, highly motivated technologist who can work with a high degree of autonomy, is able to prioritize effectively and drive the data architecture vision.","Davis, CA",Data Engineer,False
437,"Join our Data Engineering team and help build a scalable real-time analytics platform that processes streaming data to make our product even more intelligent! Own and extend our data pipeline, perform data modeling, and improve data reliability and quality. Become part of a team focused on creating innovative real-time analytics and machine learning feedback loops.

Responsibilities
Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.



Required Qualifications
2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.


Desired Qualifications
Working experience with Machine Learning and predictive analytics.
Familiarity with Hadoop framework.
Experience with Spark is highly desirable.
Familiar with data visualization through Tableau, or other tools.","Redwood City, CA 94065",Data Engineer,False
438,"Are you adept at transforming and organizing dynamic, complex data? Do you have experience in data engineering with various types of data structures and formats? Our Data Analytics team is seeking a data engineer with strong technical knowledge and a real passion for addressing business needs through data analysis. As an individual contributor on this team, you will build data pipelines that use existing tools while also exploring advances in data engineering to address business questions and support enterprise level efforts. You will work side-by-side with internal partners from our Data Analytics, Data Science, and functional teams to tackle problems and derive insight from data. We’re looking for self-starters with a solid sense of urgency who thrive when operating in a fast-paced environment. The ideal candidate is comfortable wearing multiple hats – data modeling/architecture, SQL / Python coding, data analysis and business analysis – all while looking for creative ways to solve business challenges.

Key Responsibilities:
Perform data analysis to understand how well data is aligned and identify possible data quality issues, present results in a way that is easy for business users to understand and consume
Partner with the Data Analytics team and various functional partners to understand the data conversion necessary to drive analytics
Design and develop relational database objects including tables, views, indexes, etc.
Develop and Maintain PL/SQL code including Packages, Procedures, Jobs, Tables, Triggers, Indexes, Constraints, DB Links, and functions
Partner with ETL team to build target tables, design logical source to target mapping processes and quality test the data after ETL processing
Fix data and reporting issues, propose solutions to meet emerging business needs
Optimize the performance of the solution from coding to automation routines and identification of optimal methods for processing data
Propose new solutions and technologies that could transform the way we handle data and contribute to the business
#LI-RS1
Qualification:
Bachelor’s degree or higher in Computer Science or related discipline
5+ years of experience with SQL database queries and programming (PL/SQL)
Experience programming in Java or Python
Familiarity with data quality, cleaning and masking techniques
Conceptual understanding and experience with Data Warehousing, Operational Data Stores, and ETL is key, including Fact-Dimension modeling and hierarchies
Understanding of basic functional business concepts including Headcount, Balance Sheet and P&L Accounting, Journal Entry & Sub ledger
Ability to communicate regarding data definitions and data quality with business users
Strong alignment to data privacy standards and ethics
Strong interpersonal and communication skills and a proven ability to work and collaborate in a team environment
Preferred Qualifications:
Strong communication, interpersonal, and collaborative skills
Demonstrated analytical and problem-solving abilities
Knowledge of distributed data processing and management systems
Demonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively","Boston, MA 02114 (Back Bay-Beacon Hill area)",Data Engineer,False
439,"MORE ABOUT THIS JOB
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.

Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.

Who We Look For
Goldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.
COMPLIANCE TECHNOLOGY provides technology solutions to help Compliance manage the firm’s regulatory and reputational risks, and enables them to advise and assist the firm’s businesses.
LCA Tech Data team is seeking Java, Python, RDMBS, Angular JS, and C++ developers with deep knowledge of distributed computing principles. The candidate is responsible for design, development, deployment and support of products and platforms that leverage big data technologies and enable large scale event processing where events are inspected by machine learning and statistical policy based models. Successful candidate will join a team of talented and motivated developers working on strategic and highly visible projects within the firm.
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Play an key role in architecting the Compliance Big Data Platform to handle storage of very large datasets, design pipelines, and business metrics to scale with growing trading volumes
Build disruptive solutions using cutting edge technologies with measurable commercial outcomes
Develop technical specifications, high level/detailed design, testing strategies, and implementation plans from business requirements
Manage end-to-end systems development cycle from requirements analysis, coding, testing, UAT and maintenanceWork in a fast paced environment

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
JAVA, Python, RDBMSSQLBig Data Visualization Tools

ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2018. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.","Jersey City, NJ",LCA Technology - Data Engineer,False
440,"$60 - $85 an hourContractJob SummaryBig data engineer with Scala/Spark experience Nashville, TN (12 months contract)Mandatory skills: Experience coding in Scala & Spark,Deploying on and sing multiple components of AWS cloudJob Type: ContractSalary: $60.00 to $85.00 /hour","Nashville, TN",Big data engineer,False
441,"Innovate. Collaborate. Shine. Lighthouse – KPMG's Center of Excellence for Advanced Analytics – has both applied data science, AI, and big data architecture capabilities. Here, you'll work with a diverse team of sophisticated data and analytics professionals to explore the solutions for clients in a platform-diverse environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and tools. Be a part of a high-energy, diverse, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. For you, that translates into the chance to work on a wide range of projects – covering technologies and solutions from AI to optimization – and the power to have a real impact in the business world. So, bring your creativity and pioneering spirit to KPMG Lighthouse.
KPMG is currently seeking an Associate to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics to work with our Healthcare team.
Responsibilities:
Rapidly prototype, implement, and optimize architectures to tackle the Big Data and Data Science needs for a variety of Fortune 1000 corporations and other major organizations; Develop modular code base to solve real world problems while conducting regular peer code reviews to ensure code quality and compliance following best practices in the industry
Work in cross-disciplinary teams with KPMG industry professionals to understand client needs and ingest rich data sources such as social media, news, internal/external documents, emails, financial data, and operational data
Develop and maintain D&A solutions on premise, cloud, KPMG-hosted, or hybrid infrastructure; Be the team champion of some mainstream BI/EDW/Big Data toolsets like Tableau, Alteryx, Informatica, Pentaho, Er-Win, and Power Designer
Help in research and experiment of leading and emerging BI/EDW/Big Data methodologies such as serverless data lake, AWS Redshift, Athena, Glue, GCP Bigquery, and MS PowerBI and apply them in solving real world client problems
Help drive the process for pursuing innovations, target solutions, and extendable platforms for Lighthouse, KPMG, and client
Participate in developing and presenting thought leadership, and help in ensuring that the Lighthouse technology stack incorporates and is optimized for using specific technologies
Qualifications:
Minimum of one year of relevant software development experience in multiple programming languages and technologies; preferably related to professional services; Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures; Proficiency
with healthcare analytics and data structures is preferred
Bachelor's Degree or Master's Degree from an accredited college/university in Computer Science, Computer Engineering or related field
Ability to pick up and learn new technologies quickly; Experience or knowledge of RDBMS design, data modeling, MPP EDW system implementation; hands-on experience and knowledge in distributed computing architecture, massive-parallel processing big data platforms ( such as Hadoop, MapReduce, HDFS, Spark, Hive/Impala, H-Base/MongoDB/Casandra, Teradata/Netezza/Redshift)
Hands-on experience and knowledge in BI/EDW/Big Data toolsets (Tableau, Alteryx, Informatica, Pentaho, Er-Win, Power Designer); Hands-on experience and strong knowledge in mainstream cloud infrastructures (AWS, MS Azure and GCP including their D&A-related Microservices), and ability to implement data lake and serverless data lake
Market-leading fluency of SQL; hands-on experience of Linux/Unix/Windows/.NET. Market-leading fluency in several programming languages (Bash/ksh/Powershell; Python/Perl/R) and understanding of programming methodologies (version control, testing, QA), and development methodologies (Waterfall and Agile); Full-stack development capability is preferred
Ability to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.","New York, NY 10001 (Chelsea area)","Associate, Data Engineer, Healthcare",False
442,"Job Description:
Summary
Provide basic project or application level team support to include business and technical solutions planning, design, and support to ensure the information technology (IT) solutions strategy and architecture align with business strategy.

Essential Job Functions
Acts as the subject matter expert for a project or application and serves as the system architecture authority within that scope.
Defines technology-based business solutions within scope of project. Assists project teams in the appropriate use of technology.
Reviews and approves design decisions, within established level of authority, prior to implementation.
Assists in advising client company management regarding IT vision and strategy, technology innovations and enterprise architecture services. Provides production problem diagnosis and technical, offshore strategy and processes support to project team. Assists in implementing resolution.
Leads system design activities and/or reviews system designs to assure that applications solutions will exhibit expected levels of performance, security, scalability, maintainability, appropriate reusability and reliability upon deployment.
Assists in the preparation of IT vision and strategy work products, technology and product analysis, white papers, and responses to management queries on technology and product related topics.




Basic Qualifications
Bachelor's degree or equivalent combination of education and experience
Bachelor's degree in computer science, information technologies or related field preferred
Three or more years of information technology experience
Solid knowledge of structured query language and stored functions
Knowledge of Python, Avro
Knowledge of Elasticsearch, Hadoop, Redis
Knowledge of Kafka or other stream-processing platform
Knowledge of Luigi, Azkaban, Airflow, or others for data workflow
Knowledge of machine learning and analytical workloads, nearest neighbor and other algorithms, transaction theory
Ability to present technical information to various audiences
Understanding of the security requirements for handling data both in motion and at rest
Other Qualifications
Excellent communication skills
Interpersonal and presentation skills
Analytical and problem-solving skills
Organizational and time management skills to prioritize work
Ability to work in a team environment
Ability to set technical direction for a project or application
Ability to perform within defined direction
Work Environment
Office environment
May require occasional evening or weekend work","Tampa, FL 33602 (Downtown area)",Data Engineer,False
443,"Who we are

Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 244 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.

Note:
Provide full legal first Name/Family Name
DO: Capitalize first letter of First and Last Name. Example: John Smith
DON'T: Capitalize entire First and/or Last Name. Example: JOHN SMITH
NOTE: Use correct grammar for Names with multiple cases. Example: McDonald or O'Connell

Provide full address details
Resume is required
Multiple attachments can be uploaded including Resume and Cover Letter for each application


Job Description Summary:
The Essbase Application Team is looking for a full time experienced, customer-oriented, business-focused, and analytically-driven Data Engineer to support the delivery of Essbase operations and project portfolios commitments.

Job Description:
The Essbase Application Team is looking for a full time experienced, customer-oriented, business-focused, and analytically-driven Data Engineer to support the delivery of Essbase operations and project portfolios commitments. The Data Engineer will interface with teams, customer and leaders alike to ensure seamless technological support of business process by ensuring the technologies we support are not an inhibitor to the business process.

Ensure Quality - Base the delivery model on best practices to drive efficiency / manage risks
Operational Excellence - Deliver on all commitments with no business disruption
Be accountable for continuous improvements.
Communicate effectively to instill confidence, as well as build trust and strengthen partnerships

The role will be responsible for supporting global stakeholder departments and working domain leads and data citizens across different GEO’s such as Americas, EMEA, and Asia Pacific. Because the position requires interaction with geographically-disbursed teams so the candidate must be a self-starter with strong leadership qualities, project/task management skills, and analytical skills and comfortable working with distributed teams without direct management oversight day to day.

He/she must be able to contribute to programs/initiatives and collaborate with the financial stakeholders and users as well as EDS analytical teams to drive alignment around Essbase Application Services and Support for PayPal. These accountabilities will be achieved through developing high trust relationships with internal and external customers, critical data analysis, attention to detail and a customer focus. Process development and optimization, as well as deliverables management will be a focus for the role.

Essential Functions (Tasks and Responsibilities):

Works with PayPal business units and Product Dev teams to design, develop and deliver data solutions on one of the largest data platforms in the world.
Supports PayPal business units by providing data in a ready-to-use data financial analysts, data scientists and leadership for to drive decision support around critical financial business processes.
Owns and is accountable for the design and development of a Data solution (Essbase) or a Data Pipeline (example: Cross team integration such as Essbase, Peak and TMIS to bring Innovation in or Supporting Data aggregation for an Executive Dashboard or Report on TPV growth trend).
Code is well-commented, easy to maintain, and can be reused across a sub-system or feature. Code may persist for the lifetime of a software version. Code is thoroughly tested with very few bugs, and is supported by unit tests.
Leads feature or sub-system design reviews and code reviews and be recognized as the go-to developer for that component.
Recognized as the go-to developer for a product or major sub-system and is seen as a leader in their specialized field.
Leads feature or component design reviews and code reviews and is fully recognized as the go-to developer for that component.
Participates in architecture discussions, proposes and discusses solutions to system and product changes that are directly related to their area of focus.
Responsible for managing multiple domains within an applications area, providing necessary support and maintenance activities.
Should be comfortable working in an agile environment and with cross-functional teams, should have appetite to learn and be flexible to pick up new technology.

Subsidiary:
PayPal

Travel Percent:
0

Primary Location:
San Jose, California, United States of America



Additional Locations:






We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.


Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.","San Jose, CA",Data Engineer 3,False
444,"Position Description:Persivia is seeking Data Engineer who will help support our customer base as well as our development team. This position requires extracting data from Meditech, Mckesson, Epic and other hospital systems. The qualified candidate will be a Data specialist who exhibits expertise in extracting data from Meditech, and possess strong knowledge focus on data extracts for a Quality Reporting perspective.Key Activities:Perform System setup and extraction of medical data from hospital’s databasesMaintain XSLT, SQL, and Java scripts for mass loading and rendering of XML filesTroubleshoot problems reported by customers and help solve technical issuesPerform coding tasks using C#, .Net and SQL, JavaAnswer questions from customers as well as prospective customers about the features and capabilities of our solutionsDevelop customer-facing documentation for our solutions on an as-needed basisCommunicate customer needs and wishes to our product team.Work in highly secure environmentsRequired Skills:Expertise in extracting data from multiple hospital systemsAbility to maintain and execute XSLT, SQL and Java scripts for batch loading and rendering of XML filesHands-on experience working with HL7 (V3 preferred), XML, and web servicesKnowledge of relation databases (SQL Server, Oracle) a huge plusExperience working with hospital EMRs such as Meditech, McKesson, Epic and Cerner.Experience working in Java, C#, .Net and MS-SQLThe ability to be a good listener, and to really understand a customer problem or question and help them solve itBachelor’s degree in Computer Science, technical field, or equivalent experienceMinimum 4 years of relevant work experienceExcellent written and verbal communication skillsStrong customer service skillsStrong organizational skillsJob Type: Full-timeExperience:SQL & XSLT: 3 years (Required)Data Engineering: 4 years (Required)EMR: 2 years (Required)Education:Bachelor's (Required)Work authorization:United States (Required)","Marlborough, MA",Data Engineer,False
445,"ContractAkraya is looking for a Data Engineer - Application Support Engineer for one of our clients. If the job description below is a fit, please apply directly or call Ruhana at 408-512-2363. If this position is not quite what you’ re looking for, visit akraya.Com and submit a copy of your resume. Our recruiters will get to work finding you a job that is a better match at one of our many clients.

Primary Skills: Big Data, Business Intelligence, SQL, python, UNIX
Duration: 0-12 Months
Contract Type: W2 Only

Responsibilities:
Bachelor' s degree in Computer Science, Software Engineering or other technical degree
3+ years experience developing business critical big data solutions, including data-modeling, data architecture, data platform development and optimization for same.
3+ years architecting & building Business Intelligence/ OLAP solutions using SQL or similar
Familiarity in one or more scripting languages (eg. Python) and UNIX.
Self starter who can meet critical deadlines in a fast paced environment with little direction and guidance


Please apply directly with your update resume or call Ruhana at 408-512-2363

About Akraya
Akraya, Inc. Is an award-winning staffing firm that works with many of the leading, technology-based companies around the world. We have been ranked as one of the “ Best Staffing Firms to Temp for” by Staffing Industry Analysts on multiple occasions and are a preferred staffing vendor within numerous staffing programs. Please visit akraya.Com to search through all of our current openings or to submit your resume to our recruiting team.","Sunnyvale, CA",Data Engineer - Application Support Engineer: 18-03458,False
446,"Job Category
Employee Success


Job Details
We are looking for a Senior Analyst to support our Employee Success Strategy & Analytics organization. In this role, you will work with regional leaders across Salesforce functions to understand business needs, wrangle data from various systems, and perform sophisticated analysis to develop insights and intelligence. The role focuses on developing employee/organizational insights and applications for the company, leaning towards our Recruiting Organization. It will provide the opportunity to learn the requirements and needs of a growing business focused on optimizing our organization and talent.


You will have to get your hands dirty and love working with data. You must be able to design an analysis approach, mine the data, and provide an output which can be easily understood and used by the business. You will need to pull data from multiple systems to produce the analysis requested, with little direct guidance and an understanding that the results may be iterative. You must be able to illustrate complex analysis in a concise and simple fashion using tables, charts, and graphs. Most of all, you must have curiosity and a drive to create new knowledge, ways of doing things, and/or applications.


Responsibilities:
Work with business partners and recruiting to conduct ad hoc queries and analysis
Develop and build sustainable statistical models and analysis to better understand employee behavior and data
Drive the maturity of the people analytics function in the region

Required Skills/Experience:
5+ years of experience in applied statistics, data mining and analysis, and/or management consulting
Proficiency with Excel, SQL, R, Einstein Analytics or Python to conduct analysis
Excellent PowerPoint skills with a deep understand of graphs and visuals
Strong analytical and problem-solving skills
Excellent verbal and written communication skills; ability to communicate effectively with different levels of management, as well as the business and technical communities
Extremely adept at integrating disparate information and understanding data trends
Must be able to proactively communicate status and identify risks
Team-first mentality
Must be comfortable with changing requirements and priorities
Must be results oriented and able to move forward without complete information and with minimal supervision
Bachelor’s Degree

Desired Skills/Experience:
Experience with HCM (ex. Workday, SAP, etc.), and data visualization tools ( ex. Tableau)
Experience with data quality assessment and implementing solutions to improve the data quality
Experience with HR technology system optimization and implementation





Posting Statement

Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay fees to any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org.","Indianapolis, IN",Employee Success Data Engineer,False
447,"Fusion is currently seeking a Senior Data Engineer to join our fast-growing software development team. The data engineering team is responsible for developing and maintaining the data processes for our clients. Tasks will include import and export of data, data migration from legacy systems, interface development, reporting development, as well as maintenance and support for all our existing data projects.

About Us:

Fusion was founded in 2006, and has since become a major disruptor in the Corrections and Public Health sectors of government. Recognized by INC magazine as one of the fastest growing private companies in the United States, Fusion is looking to expand its hand-picked team to include a candidate through this job placement.

From a company culture perspective, we are a vibrant and young group who have come together to be leaders in healthcare IT and software for government agencies. The office provides open working spaces, several meeting areas as well as a café & gym on premise.

Because of the niche Fusion belongs in as well as the business model we operate with, we are looking for not only skilled and qualified candidates, but also candidates who have an outgoing personality and fit well with our other team members.

To date, Fusion has a phenomenal retention of our team members. Our fundamental belief is that employee satisfaction is critical to achieving our mission, so we provide competitive compensation, professional development, career advancement opportunities, and a supportive team-based atmosphere. We also provide a full range of health related benefits, including medical, dental, vision and 401K. And we offer work-life enhancements like flexible hours, business casual dress code, and an easy-going corporate structure.

Fusion has been recognized by Inc. 5000 List of Fastest-Growing Private Companies – thanks to the tireless efforts of our team. If you are a talented professional and our mission speaks to you, please speak to us!

Job Roles:

Communicate with partner companies to develop and support bi-direction data communication.
Migrate data for new clients from old systems to new systems.
Create meaningful insight with data to help our customers meet compliance standard.
Develop reports to allow customers to view their data in the format they request.
Meet with government clients to understand their environment and work with Project Managers to determine the optimal solution for their needs.
Work with Project Managers to create and execute a technical implementation plan for larger client roll outs.
Work closely with product management to understand current and new product features so they may be implemented correctly.
Act as a role-model for junior employees to help them learn industry best practices


Required Experience:

Javascript
SQL (Plus if it is SQL Server)
Crystal Reports or any comparable reporting tool
C#
Windows Network Experience
Familiarity with most common structured or delimited file formats (CSV, XML, JSON, etc)
Familiarity with data transfer methods (sFTP, HTTP, TCP, SOAP, REST, etc)
Qualifications:

8+ Years of professional Software Development experience
Bachelor’s Degree in Computer Science or any IT-related field.
Working Hours:

Standard hours for this role are M-F start between 8 and 9, expected to put in 8 hours.
Willingness to provide weekly on-call coverage, rotationally.
Additional Notes:

It is not expected that applicants have any familiarity with Fusion’s proprietary applications, GE Healthcare software, or Corrections/Public Health business processes. Qualified candidates will be able to demonstrate experience in this role as well a demonstration of working well with the Fusion team.
This is an On-Site, Full-Time salaried position.","Woodbridge, NJ 07095",Senior Data Engineer,False
448,"$110,000 - $130,000 a yearWe are in search of a high energy, self-motivated Product Development Analyst with strong analytical and technical skills. This role is an ideal opportunity for someone with experience and interest in a fast-growing big data analytics environment. The selected candidate, by having deep understanding of our data sources, will support the Cimba product owner in ensuring that our platform delivers value. You will work closely with teams across the company and around the world, applying strong analytical skills to add definition to problems and solutions. Types of detailed analyses will include product value-add discovery, supplemental partner discovery support, implementation team data compliance.The Product Development Analyst belongs to the MCP Business Intelligence team (also known as the Cimba team). Cimba is responsible for creating a world-class big data analytics platform for Cimpress, serving a diverse set of audiences throughout the company and its partners around the world. The ideal candidate will be an excellent collaborator with all company functions, particularly finance, manufacturing, technology groups and the MCP Business Intelligence team itself. This position will be a key role in building the Cimba team’s reputation as a trusted partner of our internal customers across the company, evangelizing the Cimba platform throughout Cimpress.ResponsibilitiesPerform data forensics to discover, validate and propose solutions of data anomalies and data outliersDesign, develop and deploy data integrity validation and reporting solutions in support of Cimba internal monitoring as well as partner notificationSupport the Cimba product owner, as well as other teams, with ad hoc projects and initiatives as requestedDefine required metrics and dimensions and work with engineering or other teams to improve relevant data collection and “best practices” to maintain high standards of data integrityExtract appropriate information from rich data sources, analyze results and present findings and recommendations when neededProduct Owner responsible for managing and extending the Cimpress data catalog environment to include training development, product evangelizing, vendor management, and partner engagementBecome the go-to expert in use of Cimba for functional business units leveraging Cimba data for analysisProfessional QualificationsBachelor’s degree in computer science, database engineering or equivalent – MCS a plus6-10 years’ professional experience as a data analystIntense curiosity, ability to learn quickly and apply new tools and techniques, to identify and define problems and work with business owners to resolveAble to handle multiple tasks and possesses excellent problem-solving skillsOccasional travel (approximately 10%) to global facilitiesStrong written and oral communication and presentation skills, confidence communicating openly with stakeholders in a growing global organizationExcellent communication (written and verbal), analytical and interpersonal skillsAbility to handle multiple tasks under tight deadlinesAbility to multi-task and excellent problem-solving skillsAbility to work in a fast-paced team environmentTechnical QualificationsStrong understanding of statistical inferenceStrong knowledge of SQL, ideally experience with multiple database platformsAdvanced understanding of visualization technologies such as Tableau, Qlikview, Tibco/Spotfire or LookerExperience with management and support of application to include applying upgrades, custom modifications, extending functionality using open source or 3rd party integrationsFamiliarity with R, SAS, SPSS or other statistical packages a plusDeveloper skills sufficient to build POC solutions for data validation and notification systemsWe produce millions of affordable, highly customized, personalized physical products for small businesses and consumers. We're boldly going where no one has gone before here at Cimpress: the scale, complexity and sheer scope of what we do requires us to innovate and solve problems that haven't been solved before. With over $2.1BB in revenue and 40+ offices and manufacturing facilities across the globe creating more than 46 million uniquely designed items annually that serve over 17 million customers worldwide, we're a unique combination of stability, strength, growth and innovation. We're also a place where ideas matter, whether they come from our newest or most senior team members. There's also a lot of fun that goes with doing things no one has ever done before and you can feel it.Job Type: Full-timeSalary: $110,000.00 to $130,000.00 /yearEducation:Bachelor's (Preferred)Required travel:25% (Preferred)","Waltham, MA 02451",Product Development Data Engineer,False
450,"ContractJob Title : Big Data Engineer - Oct 20th Drive - Locals PreferredLocation : Fort Worth, TX.Duration : 6+ Months ContractSkills required:Spark with Python, spark streaming and SQL.Sqoop, Hive and Big data concepts.Shell scripting.NiFi is good to haveJob Description:The big data and predictive analytics team is seeking an individual to standardize and automate open source processes and functions. Primarily, this position will focus on automating quality assurance, testing, and provisioning processes. Extended areas of concentration will be creating end user applications intended to expedite and automate data acquisition, integration, and control processes.Duties/Responsibilities:Responsible for identifying and designing automation opportunitiesBuild a working knowledge of all existing operational analytics and data pipeline methods and functionalityCreate effective and automated processes to drive efficiency and consistency across all areas of big data and advanced analytics projectsWork closely with peers and leadership within Technology Services to develop an automation frameworkCreate an infrastructure to support continued growth in the open source environmentBasic Qualifications:Bachelor’s in Business, Information Technology, Computer Science or other related discipline or equivalent work related experienceStrong personal and organizational skills to execute project deliverablesExcellent verbal and written communication skills to lead a cross functional teamProficient with automation tools and framework designExperience in automation scripting and web application developmentApache Hadoop software library knowledgePreferred Qualifications:MBA or equivalent degreeWorking knowledge of Hortonworks 2.3 or greaterExperience in using Java, Python, Scala, Spark, Shell scriptBackground in ETL, data warehousingJob Type: Contract","Fort Worth, TX",Big Data Engineer - Oct 20th Drive- Locals Preferred,False
451,"Work closely with Data Scientists to identify and develop methods to collect and integrate a wide variety of data to be used in predictive analytics, machine learning or other data science use case
Develop data pipelines and iterative models for data experimentation using a variety of tools such as Kafka, Hadoop and Cassandra, Storm and Spark
Apply big data technologies such as Hadoop, Spark or Streams with NoSQL data management and related programming languages for analytics and experimentation with large, multi-structured data sets
Design and maintain Hadoop Workflows/ETL for all the data products
Design, Implement or translate data science to Hadoop ecosystem for scale
Act as a big data consultant in recommending the right tools/libraries to solve big data problems
Work closely with customer’s business, engineering, and executive teams, using data to drive iterative analytical models
Design, develop, Quality Assurance (QA) and maintain application code
Provide thought-leadership and dependable execution on diverse projects
Position requires unanticipated travel throughout US up to 50% of time

QUALIFICATIONS:

Master’s Degree or foreign equivalent in CS, Comp Engineering, Comp Applications, or a related field and two years of experience in the following: Linux Operating Systems, Hadoop, Oracle, Informatica, and Teradata
Use tools/technologies like Kafka, Hadoop and Cassandra, Storm and Spark, and Spark or Streams with NoSQL
Email resume with cover letter to careers@cloudwick.com.","Newark, CA 94560",Hadoop Data Engineer,False
452,"-----------------------------
Senior Data Engineer (REMOTE)
-----------------------------

About Us:
---------

With over 15 million active users and $90 million in venture funding, Life360 is the world's largest mobile app for families. Today, we are very focused on location sharing and safety, but our mission is to become the must-have Family Membership that gives families peace of mind anytime and anywhere.

Our team is focused on building technology that helps families feel safe and together even when they are outside of the home and apart. From personalized location-based alerts that help make daily coordination easier, to advanced sensor tech that can detect if you are in a car crash and automatically send you an ambulance, we are leveraging smartphones to their fullest extent to reinvent how families get through the day.

You will be joining Life360 at a key moment in our history. We doubled active users and tripled revenue in 2017, and we are scaling our team to accommodate this rapid growth. We currently have 95 full time employees, with offices in San Francisco, Las Vegas, and San Diego.

About the Job:
--------------

At Life360, we collect a lot of data: 60 billion unique location points, 12 billion user actions, 8 billion miles driven every single month, and so much more. As our first dedicated Sr Data Engineer, you will be responsible for helping build out our data processing and storage pipelines and workflows. You should have a strong engineering background and even more importantly a desire to take ownership of our data systems to make them world class.

Responsibilities:
-----------------


Design and develop resilient pipelines using a variety of different technologies
Manage our data from ingestion through ETL to storage and batch processing
Automate, test and harden all data workflows using tools like Apache Airflow
Architect logical and physical data models to ensure the needs of the business are met
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure
Be part of an awesome infrastructure team handling massive scale with tons of automation

Requirements:
-------------


Always be learning and staying up to speed with the fast moving data world
Minimum 4+ years of experience working with high volume data infrastructure
Extensive experience programming in one of the following languages: Python / Java / Scala
Experience with writing SQL queries, performance tuning, and data modeling
Knowledge and proficiency in the latest open source and commercial data frameworks

We're a growth company, which means that you'll be part of a tightly knit and highly driven team. Some days will be busy, other days will be even busier. But it's never boring, and the work is stimulating. We're looking for someone who is a star in their own right and can inspire others but at the same time, roll up their sleeves and own key components of the business.

Perks:
------


Competitive pay and benefits
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
Whatever makes you stronger makes us stronger. We buy you the things you need to improve yourself and get your job done.

",California,Senior Data Engineer (REMOTE),False
453,"At Wag!, we build products that help dogs and their parents live more joyful lives. Working with our technology and engineering team is part art, part skill—and all heart. We like to surround ourselves with other smart people that will challenge us and bring new thinking to the team to help us build better products. We're all about automation and we're constantly pushing the limits on what can be delivered auto-magically—from QA to deploying code in production. Our patent-pending Woof Pack Scrum structure allows us to work more independently, faster, and with the right level of autonomy and accountability for producing results. If there's a way to integrate it into Slack, we've probably done it. We love music, food and drinks—anything that helps us be the team possible. And maybe most important—we don't like to leave our dogs home alone, so be ready to spend your days in the office hanging with the pups.We are moving a mile a minute to make sure that Wag! customers, walkers, and partners are able to experience the magic of Wag!. We're looking for a talented dog lover to help our Data Engineering team build out our Trust & Safety data pipeline. You will help us move our data systems to the next level. You will not only be working with data but also help define the way Trust and Safety performance is calibrated and what questions should be asked. You should be excited to contribute new ideas and articulate them to a variety of stakeholders. You should also be committed to learning, seeking to innovate and raise the bar on how we use data for our Trust and Safety efforts. Specifically, you will be responsible for the following:What you'll be doingBuilding out a robust T&S data platform from how we define T&S issues to developing reporting mechanisms to share that data with key stakeholdersMonitoring and managing incident rates globally, and working proactively to identify opportunities for improvements in data quality and/or Trust and Safety efforts.Collaborate with cross-functional agile teams, including product, marketing, analytics and data science, to understand, design and develop data models to support business reporting and analysisAssist the Data Services Team Lead in maintaining optimal data pipeline architecture and performanceResearch, design and share your ideas in technical design and architecture discussionsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etcBe subject matter expert of our data warehouse stack; MySQL, Alooma, Snowflake, dbt and TableauEnsure compliance with international data storage regulations (e.g. GDPR)Ship code!Provide coaching to junior data engineers and data modellers and share and learn from your peersDevelop your craft and build your expertise in data engineeringRepresenting Trust and Safety in company-wide data efforts with engineering, business intelligence, and others.Designing analytical and reporting frameworks to make complex data easy to understand and drive decision making for stakeholdersWhat you’ll bringAdvanced working knowledge and experience working with relational databases (MySQL preferred)Experience modelling, building and optimizing Data Pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong and deep refactoring skills, and the ability to impart them to other developersExperience working with large codebases and writing robust and testable codeExperience supporting and working with cross-functional teams in a dynamic environmentMust require masters degree or higherBonus points forExperience with safety or risk dataA desire to stay at the forefront of data pipeline technologyExperience with MySQL, Alooma, Snowflake and dbtWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with dbt and/or willing to learn it. (getdbt.com)Experience with AWS cloud services: S3, RDS, EMR, DataBrickExperience with Docker, Kubernetes, Ansible, and Terraform, and other DevOps and infrastructure as code technologies.Experience with PythonAbility to tolerate questionable dog puns and a sense of funWhy Work Here: Competitive SalaryMedical, Dental, Vision, Life InsuranceUnlimited PTOCatered daily lunches, snacks galore, and endless coffeeOffice dogs!Get in on the ground floor of a fast-growing startup!About UsAt Wag! we're crazy about dogs and the people who love them, and everything we do is intended to bring them joy and keep them safe. Our company was founded in 2015 and born from a love of dogs and an entrepreneurial spirit to help make pet parenthood just a little bit easier, so dogs and their humans can share a life full of joyful moments.We invented on-demand dog walking by connecting an already passionate community of local dog walkers with pet parents. Launching in Los Angeles in 2015, Wag! services are now available in 43 states and more than 100 cities nationwide. Our walkers and sitters are vetted and pass a robust screening process—and our services are bonded and insured. We know your dogs are members of your family and taking care of them is the highest honor you can give Wag! walkers and sitters.Job Type: Full-time","West Hollywood, CA",Data Engineer,False
454,"$120,000 - $200,000 a yearMy CONFIDENTIAL Pre-IPO client is hiring a Senior Data Engineer in Manhattan, NY. This candidate is ready to bring new ideas to the table and loves to dig into the data to find the source of a problem or validate an assumption. They need someone who can quickly understand, discuss, and optimize the performance characteristics of a complex offline data pipeline.Qualifications3+ years of proven experience working with Hadoop MapReduce and/or other big data technologies and pipelinesYou consider yourself both a Data Scientist and a Senior Developer, and are just as happy working on challenging data problems as you are tinkering with the clusterYou have a solid foundation in computer science fundamentals with particular expertise in data structures, algorithms, and designYou obsess over data: everything needs to be accounted for and be thoroughly testedYou are constantly thinking of ways to squeeze better performance out of the pipelinesStrong Java or other object-oriented programming experience or, even better, experience and/or interest in functional languagesJob Type: Full-timeSalary: $120,000.00 to $200,000.00 /yearExperience:Data Engineering: 3 years (Required)","New York, NY",Senior Data Engineer,False
455,"Your core responsibility will be to maintain and scale our infrastructure for analytics as our data volume and needs continue to grow at a rapid pace. This is a high impact role, where you will be driving initiatives affecting teams and decisions across the company. You’ll be a great fit if you thrive when given ownership, as you would be the key decision maker in the realm of architecture and implementation.

Responsibilities
Architect systems and end-to-end solutions that provide fast, efficient and reliable interfaces to heterogeneous data, meta data for internal users of the analytics infrastructure.
Automate existing processes and create systems that favor self-service data consumption.
Own the quality of our analytics data.
Implement a robust monitoring & logging framework that guarantees the trace-ability of inevitable incidents.
Evaluate whether the best solution for each problem at hand is to build, buy or contract the work.
Interface with data scientists, analysts, product managers and all other customers of the analytics infrastructure to understand their needs and expand the infrastructure as we grow.


Requirements
BS/BA in Computer Science/Engineering, or relevant technical field with 2 – 3 years of experience as software engineer and/or data engineer and/or front-end engineer and/or full-stack engineer
Ability to manage data warehouse plans and communicate them to internal clients.
At least 4 years of experience as a Data Engineer, or in a role that required expertise in data pipeline technologies.
Strong overall programming skills, able to write modular, maintainable code and high quality code
Strong in web-based programming (CSS, HTML, PHP)
Experience in one or more data visualization libraries like Tableau, Plotly, InfoGram, Material Design
Strong python programming specially in python machine learning and data mining libraries (SciPy, Panda, Numpy)
Strong in Linux and shell-scripting
Specialized experience with at least one of HDFS, EMR, Redshift, Spark, Flink, or Presto.
Experience with SQL RDBMS is required.
Experience in client/server, RESTFul architecture and tools like Jenkins, RunDeck
Experience in basics of data mining, clustering, classification and comfortable to work with large matrices of data effectively
Familiar with CUDA, Blast and machine leaning engineers like Tensorflow, Torch, PyTorch, DIGITS","Mountain View, CA",Data Engineer,False
456,"“We focus on results, encourage mastery, and enjoy the benefits of being part of something big without the frustrations of corporate work culture”.

Intriguing, right?! That’s how one of our very own Developers described Stonecrop Technologies.

We are a recognized leader in transforming cellular and microwave deployments through an innovative technology platform that aligns RF designs, supply chain, and installation processes.

Since 2001, we have worked with large corporations to deliver solutions by speeding the build and upgrade of carrier networks while maintaining a culture that encourages peer collaboration and diverse expertise to drive our mission for relentless innovation to improve performance.

We believe in being agile as a way to focus on delivering real business value within a culture that gives an opportunity to be innovative and move fast.

This is an excellent opportunity for an experienced Data Engineer who wants to help build our infrastructure and support our analysts, business users, and clients with access to timely, accurate data. In this role, you will develop, construct, test and maintain the data architecture that supports our BI platform, and manage data flow through our web and mobile applications.

To excel in this role, you should have experience working on and building data pipelines and ETL processes that are reliable and scalable. You should have a solid understanding of how to access and extract data, have worked on cloud-hosted platforms such as AWS and have the soft skills to work with technical teams at different levels.

You Are:

Enthusiastic about data, its uses and best practices.
A habitual learner who is always looking to grow and add new skills.
Able to translate business requests into reliable, scalable infrastructure and database design.

You Have:

Experience in Airflow, Git, the Linux command line and working with ETL tools.
Strong knowledge of AWS platform and technologies, particularly AWS streaming data solutions and Redshift.
Several years experience writing Python for data processing and API integration. Ruby / Rails knowledge is a bonus.
Experience optimizing SQL queries in Postgresql / Redshift.
BS or MS in Computer Science.

You Will:

Help plan, build and maintain the next iteration of our data pipeline.
Recommend and implement ways to improve data reliability, efficiency and quality.
Collaborate with data architects, modelers and other team members on project goals.

To join our team, you should:

Be motivated to excel individually and as part of a small team.
Be looking to constantly grow and learn and enjoy bring others along with you.

","Petaluma, CA 94952",Data Engineer,False
457,"Blackwood Seven brings artificial intelligence to media analytics and planning. We have developed the world's first automated media allocation platform which maximizes the effect of a company's media spend using our advanced predictive analytics framework built on machine learning and artificial intelligence (AI). Using real-time modeling and advanced KPI prediction the platform produces optimal on and offline media plans to maximize business performance.

Revolutionize the media industry with us as we lead the industry with our innovative artificial intelligence and use of analytics and data to deliver measurable results to our impressive roster of clients. We value trust, transparency and team - we continuously train, mentor and invest in our people.

Blackwood Seven is looking for data engineers who has experience on AWS, Python and SQL to work on a variety of media data sets in various formats.

Responsibilities:

Work closely with Analytics (business intelligence) and Marketing Sciences (data sciences) teams to understand the data needs of the agency
Design, build and deploy new data models and ETL pipelines into production
Be accountable for operational efficiency and be proactive in monitoring data pipelines
Experience contributing to full life cycle deployments with a focus on testing and data quality
Define and manage overall schedule and availability of all data sets
Work closely with other engineers to enhance infrastructure, improve reliability and efficiency
Make smart platform and engineering decisions based on data analysis and collaboration
Make recommendations regarding standards for code quality and timeliness
Participate in agile development process

Skills & Qualifications:

Degree in Computer Science or a related field or a minimum of 4 year's working as a Data Engineer
3+ years' experience in custom ETL design, implementation, and maintenance
3+ years experience with Spark/PySpark or equivalent distributed processing systems
3 + years experience with AWS Services (Athena, Glue, Redshift, DynamoDB, Fargate, S3) and python
In-depth knowledge of how to write and optimize SQL statements
Experience with schema design (logical and physical)
Drive to analyze data to identify deliverables, anomalies, and gaps
Ability to quickly learn complex domains
Accountable, Curious and Organized
Experience in data analytics tools such as Tableau a good to have

As a Blackwood Seven employee you will enjoy:

Competitive compensation package
Unlimited paid time off policy
Flexible working hours
Benefits (Health, Dental, Vision, Life Insurance, 401k, Flexible Spending Account and more)
Fitness reimbursement
Catered lunches and stocked kitchen with fresh fruit, snacks, premium coffee & tea, and cold brew coffee
Ongoing learning and classes for employees
Team events and outings

About Blackwood Seven:
Blackwood Seven brings artificial intelligence to media analytics and planning. With an innovative proprietary media platform, we calculate each client's ""media effect formula"", which allows attribution of all channels – online such as search, YouTube and Facebook as well as offline such as TV, print and OOH. Our platform optimizes clients' media mix and provides a predictive forecast of expected results, in real time. Using a holistic planning approach we combine analytics with industry expertise to build profitable scalable campaigns for our clients from strategy development to execution and optimization.

Blackwood Seven has 175 employees in Munich, Copenhagen, Los Angeles, New York and Barcelona.

For more information, please visit www.blackwoodseven.com ( http://www.blackwoodseven.com ).","Los Angeles, CA",Data Engineer,False
458,"We are looking for a Big Date Engineer to create and manage solutions that turn data into knowledge and drive insights from growth. He/she will help us build leading edge analytics solutions and platforms to solve big data, data science, and traditional reporting needs. This individual should be analytical, have a background in data and analysis, and a strong communicator. This role will be responsible for writing well designed, efficient code using best software development practices to build and enhance our business intelligence and big data systems.
Function Specific Activities:
Function Related Activities/Key Responsibilities
Write application and database codes based on business requirements or user stories, architectural requirements, and established coding standards
Validate code against business and architectural requirements
Translate business needs to technical specifications
Design, build and deploy traditional BI solutions and big data solutions leveraging the data lake
Maintain and support data analytics platforms (e.g. MS Azure BI / Analytics Platforms & Tools)
Create tools to store data (e.g. OLAP cubes)
Conduct unit testing and troubleshooting
Evaluate and improve existing BI systems
Collaborate with teams to integrate systems
Develop and execute database queries and conduct analyses
Create visualizations and reports for requested projects
Develop and update technical documentation
Provide technical support under Dev/Ops model
Education Requirements:
Bachelor's Degree Required, BSc/BA in Computer Science, Engineering or relevant field
Related Work Experience:
0-5 years of work experience in relevant field
Proven experience as a BI / Big Data Developer
1+ years of Spark, PySpark coding experience
Background in data modeling and/or data mining
Experience with one or more of the following strongly preferred:
NoSQL databases (Azure Cosmos DB, Document DB, Mongo DB, etc)
PySpark, Python, Scala, Spark
Azure Functions
Azure Data Factory
Azure SQL DB
Knowledge of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework
Familiarity with BI technologies (e.g. Microsoft Power BI, Tableau)
Knowledge of SQL Server, SQL Server Integration Services (SSIS), Informatica ETL solution, and/or Azure cloud analytics platforms preferred
Development Methodology: Experience using Agile methods such as Extreme Programming (XP), Scrum, Crystal, Dynamic Systems Development Method (DSDM), Lean Development, and/or Feature-Driven Development (FDD)
Dev/Ops: Experience and/or training in Dev/Ops methods and tools such as Jira, GitHub, Docker, Kubernetes, Bamboo, BitBucket.
Development languages: Experience coding in one, or more, of the following desired: C#, Erlang, GFM, Go, IOS, Java, JavaScript, Perl, PHP, Python, R, RESTful web services, Ruby, SQL, XML (Python and Java a plus)
Proven abilities to take initiative and be innovative
Analytical mind with a problem-solving aptitude
Job Requirements:

Years of Experience:

Leadership Behaviors:

DRIVE INNOVATION: Generate new or unique solutions and embrace new ideas that help sustain our business(encompassing everything from continuous improvement to new product and package innovation).
COLLABORATE WITH SYSTEM, CUSTOMERS, AND OTHER STAKEHOLDERS: Develop and leverage relationships with stakeholders to approximately stretch and impact the System (Company and Bottler).
ACT LIKE AN OWNER: Deliver results, creating value for our Brands, our System, our customers, and key stakeholders.
INSPIRE OTHERS: Inspire people to deliver our mission and 2020 Vision, demonstrate passion for the business and give people a reason to believe anything is possible.
DEVELOP SELF AND OTHERS: Develop self and support others' development to achieve full potential.
Growth Behaviors:
GROWTH MINDSET: Demonstrates curiosity. Welcomes failure as a learning opportunity.
SMART RISK: Makes bold decisions/recommendations.
EXTERNALLY FOCUSED: Understands the upstream and downstream implications of his/her work. Tracks and shares external trends, best practices or ideas.
PERFORMANCE DRIVEN AND ACCOUNTABLE: Has high performance standards. Outperforms her/his peers.
FAST/AGILE: Removes barriers to move faster. Experiments and adapts. Thrives under pressure and fast pace.
EMPOWERED: Brings solutions instead of problems. Challenges the status quo. Has the courage to take an unpopular stance.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.","Atlanta, GA 30301 (Buckhead area)","Big Data Engineer (PySpark, Python, Spark)",False
459,"As a Senior Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of ""Agility"", including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.

There's no typical day or engagement for our Senior Engineers. Here's what you'll do:


Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

Regardless of what you do at ThoughtWorks, you'll always have the opportunity to:


Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you're passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.

Not quite ready to apply? Or maybe this isn't the right role for you?

That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (tick 'contact me about recruitment opportunities' to hear about jobs in the future).

#LI-NA","New York, NY 10016 (Gramercy area)",Senior Data Engineer,False
460,"Shippo lowers the barriers to shipping for businesses around the world. As free and fast shipping becomes the norm, better access to shipping is a competitive advantage for businesses. Through Shippo, e-commerce businesses, marketplaces, and platforms are able to connect to multiple shipping carriers around the world from one API and dashboard. Businesses can get shipping rates, print labels, automate international documents, track shipments, and facilitate returns. Internally, we think of Shippo as providing the building blocks of shipping.

Join us to build the foundations of something great, roll up your sleeves and get important work done everyday. Founded in 2013, we are a diverse set of individuals based out of San Francisco. Shippo’s investors include Bessemer Venture Partners, Union Square Ventures, Uncork Capital, VersionOne Ventures, FundersClub and others.

As a Data Engineer, you will be responsible for building systems to collect, process and store events at massive scale to gain operational and business insights into the performance and optimization of shipping services.

RESPONSIBILITIES


Implement and maintain data extraction, processing and storage processes in large scale data systems (data pipelines, data warehouses) for internal and customer facing analytics, and reporting features.
Implement and maintain machine learning systems (feature generation, learning, evaluation, publishing) primarily using Spark for our data scientists.
Integrate data from various data sources, internal and external, to ensure consistency, quality, integrity, and availability of data sets and insights.
Work closely with engineers, product managers, data scientists and data analysts to understand needs and requirements.
Design, build and launch new data models and datasets in production.
Define and manage SLA for datasets across the different storage layers.
Maintain and improve existing systems and processes in production.

REQUIREMENTS


2+ years working experience as a data engineer.
Ability to implement ETL processes using batch and streaming frameworks such as Hadoop, HDFS, MapReduce and Spark.
Work experience with RDBMS, such as PostgreSQL or MySQL, NoSQL and columnar data stores.
Investigate, analyze, identify, and debug data related issues to ensure stability, quality, and integrity of datasets.
Familiar with columnar data warehouse technologies, in particular Redshift.
Understand business processes, overall application components, and how data is gathered; and design a data model that ties the application telemetry data to metadata, and transactional data.
Build expertise and own data quality for various datasets.
Fluent in scripting languages such as Python, Ruby or Perl.
Collaborate with multiple teams in high visibility roles and own the solution end-to-end.
Self-starter individual who truly enjoys a fast-paced, innovative software start-up environment with a focus on delivering business value in a teamwork centric environment, groundbreaking technology.
Excellent written, oral communication, and presentation skills.
BS or MS in Computer Science or related technical discipline or equivalent job experience.

PREFERRED


Building, monitoring, managing and maintaining large data processing pipelines using frameworks and patterns such as MapReduce, Spark and Pig; and distributed columnar data warehouses including but not limited to Redshift and Druid.
Batch and streaming data transport using traditional ETL, AWS Kinesis and Kafka.
Workflow management tools such as Airflow; and data serialization formats such as Avro and Parquet; ; and data modeling concepts, methodologies and best practices.
Machine learning infrastructure such as Tensorflow or MXNet.
Cloud environments and devops tools; working experience with AWS and its associated products.

BENEFITS


Benefits: medical, dental, vision (90% covered by the company, incl. dependents)
Take-as-much-as-you-need vacation policy + flexible work hours
Free lunch / drinks / snacks
Fun team events outside of work hours - happy hours, “escape the room” adventures, hikes, and more!

","San Francisco, CA",Data Engineer,False
461,"$110,000 - $140,000 a yearBICP, a dedicated BI, Analytics & Big Data consulting firm, is currently looking to hire 2x Data Engineers for an engagement at our longstanding and direct Fortune 100 retail client in Portland, OR. We’re looking for candidates with 1+ years of experience with data engineering with emphasis on data analytics and reporting. Must possess strong experience with SQL and Relational database engineering (Oracle, SQL Server, or Teradata) and have expert-level SQL abilities. Any experience with AWS EMR, Snowflake, Hadoop (Pig, Hive, Sqoop, Spark), Shell, Python, Tableau or Cognos is ideal. We’re not looking for people that have done all of the aforementioned but should have tangible experience in at least a couple of these areas. Any Supply Chain or Retail experience is a huge plus. We’re looking to hire candidates that can thrive in an Agile environment and who possesses great interpersonal and communication skills to work effectively with parallel technical teams and can closely collaborate with the business. We’re looking to hire self-starters that can thrive in a highly Agile environment and who possesses excellent interpersonal and communication skills to work effectively with parallel technology teams and the business.If you’re looking to join an organization where there is tremendous growth opportunity, that operates with transparency and with a highly collaborative approach then BICP could be a great career choice for you. We offer excellent compensation and we will reimburse the cost of relocation to Portland (telecommuting is not an option).Please Note:  U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor H1-b visas at this time and this is a salaried or W2 position.Job Type: Full-timeSalary: $110,000.00 to $140,000.00 /yearEducation:Bachelor's (Required)Work authorization:United States (Required)","Portland, OR",Data Engineer | Fortune 100 Retailer,False
462,"Job Description
Amazon is one of the world’s most highly trafficked sites and with a data footprint multiple business areas. Delivering impactful advertising using Amazon data to help customers find the right products and help brands connect with their customers is one of our key business initiatives. The Advertising Measurement team is building a big data platform to provide machine learning based solutions to measure and optimize advertising spend across media channels. The platform and measurement services are consumed by different Amazon brands across Device, Retail and Online Video businesses of Amazon.

You will play a key role in building the appropriate data ecosystem and leverage big data solutions to deliver insights and optimize ROI for marketing dollars. The ideal candidate will have extensive experience with traditional Data Warehouse (DW) concepts and have the aptitude to incorporate new approaches and methodologies while dealing with big data. Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing reporting solutions.

This position will need to have substantial interaction with the analytics teams located in Seattle, London, NYC and Bangalore, as well as stakeholders in US, EU and Asia. International travel may be required.
Basic Qualifications
4 plus years of relevant experienceExposure to Big Data Ecosystem and hands-on knowledge on hadoop, hive, pig, sparkExposure to AWS Ecosystem and hands-on knowledge onDeep hands-on knowledge in using advanced SQL queries, experience in writing and optimizing efficient SQL queriesExperience in BI projects converting business needs to data warehousing and reporting solutionsExcellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions
Preferred Qualifications
Strong unix shell scripting abilitiesStrong in any programming languages such as Java, PythonStrong ability to interact, communicate, present and influence within multiple levels of the organizationExposure to managing and writing data processing jobs such as EMR/Spark or similar.","Seattle, WA","Data Engineer, Advertising Measurement",False
463,"Overview
Be Here. Be Great. Working for a leader in the insurance industry means opportunity for you. Great American Insurance Group, a member of American Financial Group, is a Fortune 500 company consistently recognized as a top place to work. We combine a ""small company"" culture where your ideas will be heard with ""big company"" expertise to help you succeed. With over 30 specialty property and casualty operations and a variety of financial services, there are always opportunities here to learn and grow.
Great American's Predictive Analytics division is looking for a Data Engineer to join their growing and dynamic team.
Responsibilities
Work with project team and business stakeholders to determine data requirements for analysis
Acquire and manipulate internal and external data to create clean reproducible data sets to facilitate predictive modeling
Build comprehensive data sets from various source systems including Hadoop, Oracle warehouses/marts, SQL Server, API’s, XLS, etc.
Work with Information Technology to develop production solutions to bring predictive analytics to the enterprise
Research and evaluate new methods and tools to improve data gathering processes
Design and implement database structures for modeling solutions
Complete descriptive analyses on various data sets
Research business unit queries regarding model outputs; this includes score shifts, missing items, reason messages, etc.
Qualifications
Superior organizational leadership skills.
Integrates multiple concepts across job functions with a goal of overall benefit to the organization.
Ability to communicate, develop and leverage strategic business relationships across the organization and externally.
Requires advanced technical and business knowledge.
Self-motivated team player who excels in a collaborative environment.
Contributes beyond job role and responsibilities.
Excellent problem solving skills.
Required:
Strong SQL and database knowledge (Oracle preferred)
Understanding of ETL techniques and processes
Strong Excel knowledge/experience including Macros and VB development
SOAP and REST web service experience testing and development
Preferred:
Previous experience in the P&C insurance industry
Report development/design experience (Tableau / Cognos preferred)
Strong Software Engineering practices developing enterprise applications – Java, Spring, XML, JDBC/JPA/Hibernate
Familiar with approximate string matching techniques (fuzzy matching)
Hadoop Development– interfacing with data stored in Hadoop environment (Familiar with technologies including: Hive, Pig, Spark, HDFS, Sqoop, Flume, HAWQ, Zeppelin)
Experience with Informatica Data Quality Suite & Infomatica Data Integration Suite (PowerCenter)
Experience in Linux
Experience with R/R-Studio
Text Mining experience utilizing Python or R a plus
Education: Bachelor’s degree or higher in Information Technology, Informatics, Computer Science, Information Systems, or equivalent experience
Experience: 0-6 years of relevant experience","Cincinnati, OH",Data Engineer,False
464,"Oath, a subsidiary of Verizon, is a values-led company committed to building brands people love. We reach over one billion people around the world with a dynamic house of 50+ media and technology brands. A global leader in digital and mobile, Oath is shaping the future of media.


The Marketing Systems team builds systems and data pipelines that enable the Marketing org to grow our audiences through multi-channel ad campaigns, user journeys and personalized newsletters. We deal with data at massive scale, leverage cutting edge technology to engineer applications for effective campaign targeting, optimized ad spend and measuring campaign performance.

Responsibilities

You are a self driven and motivated individual that wants to make an impact by offering innovative solutions in the big data and custom applications space. You are a free thinker, have a hunger to learn, willing to try out new tools and technologies and willing to look for the best and most practical solution to any given problem (rather than the quickest or easiest). You take on challenges head on and work well under pressure. No task is too big or small or you and you treat everything with equal merit. You are a good team player and thrive as part of a team of equally talented and motivated individuals. You are a strong communicator.
Work on development initiatives as part of a scrum team on sprint cycles.
Closely interact with our stakeholders (Product Owners/Managers, Business Analysts, others) for clarity on sprint items and for verification of developed solutions.
Participate in team activities such as sprint grooming sessions, project or product discussions, brown bags as well as the occasional team outing.
Follow appropriate coding standards and best practices as applicable.
Document your work well.
Participate in code reviews for your peers.
Collaborate with your peers for finding solutions to complex problems. Share knowledge with your peers and also learn from them as required.
Work on operational and production support for the applications we build and maintain.
Work towards quarterly team and organizational goals that should be result oriented and measurable.

Minimum Qualifications
MS in Computer Science or related field (or BS with 3 years of relevant experience).
Able to produce test-driven, modular and efficient code in Java.
Able to tame scalable distributed software systems.

Preferred
Experience with NoSQL and big data systems and tools (HBase, Redis).
Experience with Javascript, HTML5, CSS.
Experience with Cryptography, HTTPS/TLS.
Experience with API design.
Experience with AI and Machine Learning.
Experience with Unix OS.


Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","Sunnyvale, CA",Data Engineer,False
465,"The mission of our data team at Red Ventures to make data easy-to-use for everyone. We are hiring Spark Engineers who are excited to overcome challenges, work with the latest AWS technologies, and continue making our data better.

Our Spark Engineer will be a key part of building our data infrastructure; this includes using Spark for streaming applications and/or as an ETL tool, Data Aggregation, Column Operations, User Defined Functions, Caching and using the Spark UI to analyze behavior and performance.

You will join a team of highly skilled engineers who design, develop and automate high-quality, scalable solutions across the entire data lifecycle, from raw data to powerful insights and analytics. Basically, we are using Spark as our universal program on how we are transferring data across multiple platforms.

If you want to be a part of a dynamic team solving business problems using data, this is the role for you.

Skills:

Spark, working in RDDs and DataFrames/Datasets API (with emphasis on DataFrames) to query and perform data manipulation
Spark Structured Streaming
Experience building large scale Spark applications, ideally with either Batch processing and/or Streaming processing
Scala would be ideal but a solid knowledge of Java is also acceptable
Experience in SparkSQL (Broadcast Joins)
Experience with cloud computing platforms, we use AWS (Kinesis, S3, Lambda, DynamoDB)
Has experience with ANSI SQL relational database (Oracle, SQL, Postgres, MySQL)

Nice to haves:

Linux common working knowledge, including navigating through the file system and simple bash scripting
General knowledge of distributed systems and distributed data processing frameworks
Experience with Storm, Kafka, or Cassandra is a plus
Knowledge about agile software processes

About Red Ventures:
Red Ventures is a multi-billion-dollar portfolio of digital companies that specializes in bringing consumers and brands together. Through bespoke technology, integrated digital commerce and sales, distinguished partnerships, data science, and original content from the company's proprietary brands and marketplaces, Red Ventures provides better end-to-end consumer experiences throughout the buying cycle. Headquartered in the Charlotte metro area, Red Ventures has more than 3,600 employees globally in offices across the US, UK, and Brazil.","Pasadena, CA",Spark Data Engineer,False
467,"We are a small start-up on a mission to revolutionize the world of digital advertising. We are currently looking to add an experienced and dynamic Scala Data Engineer to our growing team! The ideal candidate for this position is excited, innovative, and is not afraid to handle new challenges within the programmatic media-buying space. If you have experience working on a large volume, low-latency data platform, this job is for you.

This position uses the following technologies (but is not limited to): Spark, Druid, Kafka, Cassandra, PostGres, Finagle, HDF, and Airflow

What you’ll do:

Contributing new reporting features to our system (e.g. HTTP services, various custom reporting solutions)
Constructing and scaling both new and existing Spark streaming applications.
Constructing new ingest pipelines, such as S3, Kafka, etc.)
Collaborate with your data science colleagues productionize any ML pipelines
Work with other engineering teams to organize various data models
Collaborate with our DevOps team to grow out monitoring and alerting coverage as necessary; scale NoSQL appliances
Tune all Spark applications to accommodate a quickly evolving platform

So what are we looking for


Someone who has worked with Spark, Hadoop, and/or other big data processing platforms within high volume environments
Familiarity constructing and maintaining ETL pipelines
Fluent in SQL and a good understanding of relational data models
A high comfort level working in a Linux environment
A driven mentality and is unafraid to approach new challenges and the latest technologies

#LI-AR1","Santa Monica, CA",Scala Data Engineer,False
468,".
This role can be based in Chicago, IL or Mount Olive, NJ

This is an exciting time at Mars. We’re using digital, data and user insights to transform our business by finding answers to problems that we’ve often never asked ourselves before. From joining the dots to improve our Petcare data ecosystem, to streamlining the efficiency and automation of our supply chain and quality operations, we’re already seeing some brilliant results. In fact, we’ve built so much momentum that we’re now looking for industry leaders in Business Translation, Data Science and Data Engineering with different and complementary skills to influence how we operate and grow beyond anything we’ve achieved before. Join us, and discover a company set up to develop your capabilities and ambitions and a group of colleagues ready to support and inspire you. Working together, we’ll create a better world for our planet, our communities and our pets.

What you’ll do

Keeping the data flowing and readily available to solve problems as and when they need solving is core to what you’ll do. You might do that through Agile sprints or self-service analytics - it’s up to you. The day-to-day? You’ll be working within a sprint team building both new data pipelines and establishing or improving new platform capabilities. Without people in this crucial role treating data as an asset, we won’t be able to become the business we want to be.

Working in a small Agile team alongside Global Analytics, Data, Source Systems, Business Translation and Integration colleagues, you’ll partner with Product Owners to establish and maintain the data pipelines (ETL/Batch/Streaming) within and between different technology platforms, such as SAP/SAP BW and Microsoft Azure. You’ll also work closely with users from our segments and markets around the world as part of SCRUM teams. This exposure will also give you the exciting opportunity to influence and contribute true ‘firsts’. That’s new and emerging approaches that Mars, and maybe even the industry, hasn’t seen before.

Data integration, orchestration and automation – you’ll build data flow components of MVP analytic solutions. And you’ll do it across the entire Mars data landscape. That’s on premises, cloud, internal, external, formal & informal data. You’ll look to the future too. That means thinking about how data assets yet to come can be integrated into the end-to-end Mars data landscape.

In everything you do, you’ll be thinking about the bigger business picture and making sure your solutions address specific business challenges. On top of that, you’ll work closely with the Enterprise Architecture function and other Mars Digital Technologies capabilities to ensure alignment with the Enterprise Architecture initiatives and capacity and infrastructure planning.

What you’ll need

To do all this, you’ll need:

Bachelors degree in IT, or similar experience
At least 3 years data integration and a year data orchestration experience with Microsoft Azure
In-depth experience of designing and implementing data flows and pipelines
2-3 years' experience as a Data Engineer is highly desirable
Experience of working in an Agile (ideally SCRUM) team
Comfortable being hands on with data, data modelling, query techniques
Background in the Data management Space
Experience of the FMCG/CPG industry
A good understanding of and adherence to data security standards

So, if you’ve got the skills we need and you’re looking for the opportunity to really make a mark in a world-renowned and supportive business going through a period of fast and massive digital transformation, this could be the role you’ve been waiting for.

Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.

Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.","Chicago, IL 60642 (Portage Park area)",Data Engineer,False
469,"Trend Logic is currently seeking an experienced Data Engineer for a full time position in Richmond, VA.Position DescriptionThe Data Engineer will work on projects to transform big data into curated data sets feeding analytics initiatives and helping organizations operationalize new insights quickly to increase the agility and responsiveness of the analytics that drive the business.You will be responsible for working with the DevOps team on tools for continuous integration, builds, and monitoring of solutions.You will join an established team in an open, collaborative environment using agile delivery methods. Any experience in converting legacy ETL into the newer technologies is highly valued, as well.Above all, we are looking for applicants who will thrive in an open, energetic, flexible, fun-spirited, collaborative environment and desire creative freedom and an opportunity to work on high performing teams.Required Skills & ExperienceBachelor's degree in Computer Science or relevant area of study2+ years of experience and knowledge of: Apache SPARK, Apache Kafka, Java and PythonExperience using ETL (extract, transform, and load tools)Experience with Scala and Apache Camel is also a plusThis is a full time position located in Richmond, VA. Remote candidates may be considered with semi-regular travel to Richmond expected.Excellent Salary and Benefits including employer paid insurance, bonuses and lots of perks!This is a direct-hire position. Applicants must be available for employment without sponsorship.Job Type: Full-timeEducation:Bachelor's (Preferred)","Richmond, VA",Data Engineer,False
470,"Data Engineer

Bluestem Brands, Inc. is the parent company to 13 dynamic, eCommerce retail brands. We have one mission: to build a dynamic retail enterprise that wins with direct-to-consumer excellence and entrepreneurial-minded employees focused on serving our customers’ unique needs. Our leadership is responsive and supportive, empowering those smart and passionate employees who drive our success. We are continually innovating and improving: we take risks, learn from mistakes and celebrate success as a team.

You work hard, and you deserve more than just a paycheck. Bluestem works to do what’s right for employees. From the big things (great benefits, employee discounts and incentive plans) to small touches (jeans-every-day dress code), this is the place you’ll want to be. And we don’t just talk about work-life balance, we try to live it. Join the Bluestem Brands team to make an impact, be inspired and be valued, every day.

We are seeking a Data Engineer for Bluestem Brands to be located at our office in El Segundo, CA.

What you’ll do:

Implement technology solutions to manage large amounts of data.
Deploy state of the art machine learning algorithms to production environments for real time credit adjudication.
Build algorithmic experimentation framework.
Design and develop code and data pipelines that leverage structured and unstructured data integrated from multiple sources.
Collaborate with Data Scientists to help tell meaningful stories and evaluate continued needs to drive innovation and add significant profitability to the bottom line.


What you’ll need:

Advanced degree (MS or PhD) in computer science, physics, applied mathematics, engineering or other technology/quantitative/hard science.
Expertise in general purpose programming languages such as: Python, Java, C, C++, etc.….
Expertise in shell scripting.
Experience with distributed computing tools such as: Map/Reduce, Hadoop, Hive, Pig, Spark.
Experience with building complex and non-interactive batch/distributed systems.
Extensive experience working the full length of the data pipeline.
Experience using Git.
Experience using Amazon Web Services (AWS) for cloud computing.
Experience with extract, transform, load (ETL) processes, and SQL expert.
Experience working with various data serialization forms: XML, JSON, etc.….
Experience with any of the following: R, Matlab, SAS or other mathematical/statistical programming environment, is a plus.
Experience with graphical processing units (GPU) computing for improving performance, is a plus.
Exposure to popular machine learning techniques is a plus.


Required Skills

Required Experience","El Segundo, CA",Data Engineer,False
471,"The Audience director is responsible for the management of audience first marketing strategies across Initiative’s key markets. The role is predominately US focused, however opportunities across global clients exist across markets including US, UK, Germany and Australia. Your role as a key player within the Initiative global analytics team is to support Initiative client and client teams to maximize data management technology solutions to fulfill audience marketing objectives. Specifically, this role is responsible for being the primary contact, providing direction to all stakeholders for the successful delivery of Merck Pharmaceuticals recently adopted Salesforce DMP.


Essential Functions
Oversee the end-to-end successful execution of Merck’s DMP scope of work
Establish goals, KPIs and delivery timelines with client and Initiative teams for the successful delivery of the DMP scope of work
Manage DMP workload across the account providing recommendations for project prioritization
Document and communicate requirements to meet program objectives
Help Initiative media teams manage use case impacts across the Merck portfolio
Provide Initiative with direction and instructions on how to deliver on DMP needs – make it as simple as possible from a work flow perspective
Provide consultation on best practice audience targeting, using both initiative in-house technology and client owned technology (i.e. DMP)
Step in and trouble shoot tasks where assistance is needed
Provide governance and communication strategy for key stakeholders and senior leadership
Partner with Merck and DMP vendors to develop a roadmap and timeline for key deliverables
Work with special business units (including programmatic, search, social and data teams) across media brands to successfully on-board client DMP technology
Monitor and analyze DMP data ensuring delivery against client goals
Prepare and distribute DMP performance summaries to clients and Initiative teams
Provide ongoing troubleshooting with data and media platform/partners. Validation of data from tags on a regularly basis ensuring that variability between platforms and Ad servers in a reasonable range

Other Responsibilities
Identify delivery gaps and work with Initiative and SBU teams for viable solutions
Manage delivery expectations with senior management across Initiative and Merck
Actively present potential project conflicts or work that compromises Initiative in any way
Cultivate relationships with current client directors and ensure analytics vision is being delivered
Work closely with Initiative analytics team to ensure a united front for any work put in front of clients
Foster a collaborative relationship with all stakeholders, providing support and direction where required
Support Diversity and Inclusion goals for CIA and Initiative


Qualifications
Education:
Bachelor’s degree

Work Experience:
5+ years industry experience, including 5+ years of management experience
Experience managing small teams– including those directly managed vs teams that are responsible for your product but not under your control

Skills:
Track record of successfully managing medium/small accounts and markets
Strong experience with data management platforms and the application in media
Strong experience with modeling, research, business intelligence platforms
Ability to communicate complex concepts at varying levels (from superficial to detailed)
Excellent communication, organizational, interpersonal and problem solving skills
Ability to proactively drive the business forward (i.e. being able to take the initiative rather than rely on direction)","New York, NY","Initiative - Director, Data Engineer (DMP)",False
472,"$85,000 - $125,000 a yearJob DescriptionAn EdTech companya company located in Bethesda is looking to onboard a Data Warehouse Engineer to join their Data Engineering team. This team reports directly to the Chief Product Officer, and is responsible for the future direction of the company's flagship anlytics product. On top of this, this team is also at the intersection of various facets of the business, and is developing new ways the company can utilize its massive amounts of data. Ideal candidates are those who are familiar with a Big Data environment, have experience working with ETL, SQL, Python, the Hadoop Ecosystem, and Redshift.Required Skills & ExperienceB.S. in Computer Science or related field2+ years of Java1+ years of PythonExperience with ETLExperience with Hive, Netezza, and Uzi2+ years of SQLDesired Skills & ExperienceM.S. in Computer Science or related fieldExperience with Redshift1+ years of NoSQL databasesExperience with AWSJob Type: Full-timeSalary: $85,000.00 to $125,000.00 /yearEducation:Bachelor's (Preferred)","Bethesda, MD",Data Engineer,False
473,"THE EARNEST RESEARCH COMPANY

Earnest Research is a VC-backed data innovation startup driven to change the way professionals understand consumer and business behavior. Working with world-class data partners, we transform raw data into a source for business and investment professionals to ask better questions so they can make better decisions. We believe, in the right hands, data has the power to change the way we work.

DATA ENGINEER

Earnest is seeking a data engineer to help scale up our data infrastructure. You will be part of a data-driven decision-making culture and collaborate with software engineers in building out the data tools and processes to support the creation of insights that will drive our business. The role will involve the design and implementation of the entire data pipeline, from capturing and storing disparate data sources to processing that data and making that data available to other team members. You will be working across the company to understand their data needs, and creating systems that provide consistent and complete information to help solve various business problems.

RESPONSIBILITIES


Maintain and implement tools and systems that ingest, transform, organize, and expose data insights
Collaborate with other engineers to help implement and design our next generation data warehouse system
Work together with our data analyst team to gather technical requirements and provide support on analytics processes
Develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources
Implement new product features and performance improvements to existing products
Help drive optimization, testing and tooling to improve data quality across the product line

QUALIFICATIONS

Required Skills:

Proficiency in one of Python, Java, Scala, or a similar programming language
Experience with Hadoop and related technologies (Hive, Pig, Spark, Presto, Impala)
Strong SQL experience (MySQL, Redshift/Postgres)
Comfort with source control (GitHub) and working in a Linux environment
Experience with handling and processing large data sets in a business environment
Understanding of structured and unstructured data design/modeling
Strong analytical, quantitative, problem-solving, and critical thinking skills
Excellent interpersonal, verbal, and written communication skills
Extremely committed, hard working and thoughtful with the ability to work both independently and in a collaborative team environment

Additional Preferred Skills:

Experience with AWS tools, i.e. especially EMR, Redshift, Data Pipeline
Experience working with large volumes of time series financial data
Exposure to Data Science
Knowledge of machine learning and natural language processing
NoSQL experience: HBase, MongoDB
Familiarity with BI and analytics tools (e.g. Looker, Tableau)

Benefits & Perks:

100% company paid medical plan options (additional medical, dental and vision plans available too!)
Health & fitness reimbursement program
401K retirement plans with employer matching
Flexible and generous time off
Pre-tax savings plans for public transportation and parking expenses
Fully stocked kitchen and cold brew on tap
Regular company happy hours, lunches & events

Earnest Research is an equal opportunity employer, and we encourage people with a diverse range of backgrounds to apply.","New York, NY",Data Engineer,False
474,"The Company

We're a tech company that's changing how people bank and think about their finances. We value empathy, curiosity, craft and efficacy. Our mission is to help people feel confident with their money. We do that by bringing humanity, elegance and ease to the consumer banking experience. And we make banking beautiful.

The Team

The Data Engineering team builds and operates the pipeline that feeds Simple's data needs, currently using Postgres, Kafka, and Redshift. The team is composed of data management specialists working together to reduce operational defects and increase the capabilities of Simple's internal data customers in Product and Platform Engineering, Risk Management, Analytics, and Customer Support.

About You:
You love both the human and technical challenges of building data solutions that create awesome business impact. You're hungry to learn about technology & data, and what it can unlock for our customers. You thrive in a collaborative environment that values discussion and empirical reasoning, and believe these components lead to a successful outcome for the team and the business.

What You'll Do All Day

As a Data Engineer you will contribute to the team's initiatives and projects. You'll participate in development and operational work that meets standards of practice for both. You'll interact across the platform and engineering teams in order to have an impact on the business at large.


Ensure our data pipeline is healthy and operational
Respond to alerts and customer requests for data and analytics support
Participate in design and implementation of pipeline features and enhancements

We'd Like To See:

Minimum 3 years of relevant experience
Experience with SQL and Python
Familiarity with data modeling, warehousing and analytics concepts and technologies
Experience with AWS or other cloud services
Experience designing and implementing data systems for business impact

Details:
We recognize the dire lack of diversity in our industry, and we're not okay with it. We actively seek to address it with our hiring and retention processes, as well as our office culture. If you're on the fence about whether you're a fit, we say go for it, and apply!

Why Simple's a Great Place to Work:

Based in Portland, Oregon-- a beautiful place to live and work.
Competitive salary and benefits package.
A supportive and nurturing place to work. We actively consider how we can improve employees' quality of life--both inside and outside the office.
Committed to hiring quality human beings. Simple is a place where others will watch out for you and help you learn. We actually like and respect each other.
We give a damn about what we do, both as individual contributors and as a company on a mission to change banking. We're passionate and nerdy about our work; in fact we're kind of that way about things outside of work, too.

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. Email our team at careers@simple.com ( careers@simple.com ) if you need an accommodation in the application process.

A background check will be required for this opportunity.

Simple provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability,​ or genetics. In addition to federal law requirements, ​Simple ​complies with all ​applicable state and local laws governing nondiscrimination in employment in every location in which the company has ​employees. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

By submitting this application, you certify that the facts contained in your application are true and complete to the best of your knowledge. If you are employed, false statements on your application will be grounds for termination.","Portland, OR",Data Engineer,False
475,"Gridwise, a mobility solutions startup headquartered in Pittsburgh, PA, is looking for a highly motivated individual to join our team as a Data Engineer.
Gridwise (Techstars Mobility '17) is a mobility startup that is creating solutions to improve the efficiency of on-demand rideshare fleets. As mobility is rapidly changing, Gridwise is aiming to improve the ecosystem through its data-driven solutions created to help human & autonomous rideshare drivers best know when and where to drive. We are seeking motivated individuals to help us capture this massive opportunity as a leading provider of on-demand mobility solutions. We encourage you to join us for the ride if you value the opportunity to make a huge impact on the mobility ecosystem of the future!
As an early member of the Data team at Gridwise, you will have an opportunity to contribute significantly to our success by helping us shape our data strategy, processes, and infrastructure as we continue to scale our technology. The ideal candidate for this position thrives at the intersection of Data Science and Software Engineering.
As a Data Engineer at Gridwise, you will:
Assist in designing, building, and deploying infrastructure necessary to implement ML-based product features, analytics jobs, and manage large geospatial datasets
Develop processes/algorithms to clean and refine imperfect datasets
Work with Data team to refine and productionize various models for use in our applications
Perform exploratory analyses on Gridwise's data to extract new insights and product ideas
Take on a leadership and mentorship role within the Data team as the organization continues to grow
Required Experience/Skills/Traits:
Graduate degree in engineering, machine learning, or computer science field
3+ years experience designing and building software/data infrastructure to support ML-based software features and other data science projects
Skilled in working with large, imperfect, real-world, datasets
Experience working with a variety of database technologies: SQL, NoSQL, geospatial, graph, etc.
Experience with big data & streaming infrastructure/frameworks, such as Hadoop, Spark, Kafka, etc.
Comfortable with data science tools/libraries such as numpy, pandas, scikit-learn, etc.
Able to select and apply appropriate machine learning methods for a given use case
Passionate about entrepreneurship and the mobility industry
Exceptional Candidates will have one or more of:
Ph.D. in computer science, data science, machine learning, or related field
Strong experience implementing production software and infrastructure for machine learning systems
Experience designing and deploying data pipelines, ETL processes, and distributed systems
Experience storing, querying, and working with geospatial and time-series datasets and databases
This position is full-time and is located in Pittsburgh, PA. Remote work is unfortunately not an option at this time.
Compensation & Benefits include:
Competitive salary & stock options
Medical, dental, and vision benefits
Unlimited vacation days
Maternity/paternity leave
Relocation if applicable
If you’re interested in this opportunity and feel that you’d be a great fit for our team, please apply and tell us why.
We look forward to hearing from you soon!","Pittsburgh, PA 15212",Data Engineer,False
476,"InternshipWant to join a team of passionate engineers who are excited to work at the intersection of fitness and mobile technology? Come join our team, a Boston-based mobile company with more than 55M users worldwide. A little bit about our app, Runkeeper transforms your phone into a personal trainer, helping you track your activities, set your goals, and stay motivated to go that extra mile.

We are looking for a Data Engineering Co-op to work on consolidating key data for the entire ASICS organization into our cloud-based Data Warehouse, Snowflake. During this initiative, you will get to work with several interesting technologies and solutions to bring our ETL framework into production. You will work in an integrated Scrum team with other full-time Data Engineers, Data Analysts, and Data Scientists.

And lastly, while we are a fast moving company, many of us are not fast (or even moderately fast) runners. The idea of helping make the world a healthier place is the most rewarding piece for us.
Potential projects you could work on
Consolidating ASICS data with Runkeeper data into our Data Warehouse
Learning and playing with the latest DevOps techniques like containerized solutions
Designing solutions on how to consolidate multiple sources of data for consumption
Collaborating on personalization initiatives for engaging users with our new fitness app and other ASICS services
What you'll do
Hands-on design and development in our cloud native Data Warehouse, Snowflake
Hands-on development in Amazon Web Services services and solutions for delivering software
Develop ETL (Extract Transform and Load) processes for Data Ingestion purposes
What you bring to the table
Currently enrolled in a BS/MS program in Computer Science or equivalent
Experience with a couple of the following: Java, Python, SQL, Containers
One other perk of the job?? 50% of ASICS gear. Best in class sneakers, athletic apparel, and training gear .. or just for building up a huge sneaker collection.

 ASICS Digital is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or fitness level.","Boston, MA",Data Engineer Co-op,False
477,"Who is Trace3?

At Trace3, our mission is to remain the leading Transformative IT Authority, providing unparalleled IT solutions and consultation services to our clients.

We began our humble journey in 2002 – as a traditional VAR. Fast forward to today: we have evolved to be a leader in the IT space – solidifying us as a force to be reckoned with. We integrate IT products and services with insightful consultation in order to provide total business transformation for our clients. Equipped with elite engineering and dynamic innovation, we empower executives and organizations to keep pace within the IT/corporate landscape through the transformative power of IT.

Our culture at Trace3 embodies the spirit of a startup with the advantage of a scalable business. We offer competitive compensation and awesome perks like onsite workout classes, stocked kitchen with snacks/beverages, and a focus on a healthy work-life balance. As a part of Trace3, we want you to have the opportunity to grow your career and have fun while doing it.

Trace3's HQ campus is located in Irvine, CA with office locations in San Diego, Los Angeles, Denver, Northern California and Phoenix.

Ready to discover the possibilities that live in technology?

Ideal candidates will have qualities true to our core…

Street -Smart You are flexible and resilient in a fast changing environment. You know how your job affects the whole mission. You get the bigger picture. You understand why your job matters to Trace3 and how to help grow the business. You exercise good business judgment in making high quality decisions in a timely manner.

Entrepreneurial Spirit You think like an entrepreneur. You constantly innovate, come up with solutions and drive change. You solve problems for the betterment of the company. You look for new and productive ways to make an impact. You find better ways to sell or provide solutions and are good at it.

Juice You are a well-respected achiever that gets things done and drives results. You ""bring the weather"" by demonstrating leadership, character and passion. You lead without a title, empowering others and inspiring trust. You treat others with respect, admit mistakes, give credit where it's due and demonstrate transparency. You hug people in their trials, struggles and failures, not just their success.

About the Role:
The Data Engineer will be responsible for leveraging new and emerging technologies to solve key technical challenges for our clients. The Data Engineer will act as an expert and trusted advisor who develops, implements, troubleshoots, and optimizes data solutions across many platforms. This role will work closely with clients, partners and other business units to ensure consulting engagements are successful.

What You'll Do:

Responsible for design, development, and hands-on implementation of data intelligence solutions including data platform build-up, proof of concepts or pilot implementation, software development, software integration, and documentation
Perform hands on development of apache, big data technologies, and framework
Serve as a data intelligence technical resource in team's efforts to determine the needs of our client's businesses that will simplify and automate the applications as well as make them more efficient
Align solutions with standards and best practices working with cross-functional engineering and consulting teams
Collaborate and communicate with Sales and Account Management team to ensure smooth and successful delivery and assist with the identification of additional Advanced Services and Sales opportunities within the customer's environment
Establish strong and lasting relationships with key stakeholders and decision makers in client organizations
Contribute to the development of internal best practices as well as new innovative consulting services offerings that we can take to market
Build a community and following around our company solutions and brand awareness

Qualifications & Interests:

Bachelor's degree from an accredited university required
Previous experience working for a consulting or services organization strongly preferred
5+ years of software development experience in distributed systems and building large-scale applications
5+ years of experience in building large scale, high performance, high availability systems and Strong Computer Science fundamentals (algorithms, data structures)
Hadoop, NoSQL or other Big Data certifications are a huge plus
Experience with Big Data technologies (SPARK, HDFS, HBase, Cloudera, MAPR, Hadoop and other frameworks in Hadoop ecosystem
Deep knowledge of Hadoop tools (MapReduce, SPARK, Oozie, ELK, KAFKA, HUE, HBase)
Fluency in several programming languages such as Python, Scala, or Java, with the ability to pick up new languages and technologies quickly
Intermediate knowledge with software engineering best practices
Must be able to quickly understand technical and business requirements and be able to translate them into technical implementations
Ability to mix deep technical expertise with simple, everyday language to deliver a story that is memorable, educational and useful
Highly organized, detail-oriented, excellent time management skills and able to effectively prioritize tasks in a fast-paced, high-volume, and evolving work environment
Ability to approach customer and sales requests with a proactive and consultative manner; listen and understand user requests and needs and effectively deliver
Comfortable managing multiple and changing priorities, and meeting deadlines in an entrepreneurial environment
Motivated self-starter who loves to troubleshoot and solve challenging problems and feels comfortable working directly with customers

The Perks:

Competitive Compensation
Comprehensive medical, dental and vision plans for you and your dependents
401(k) retirement plan, 529 college savings plan, life insurance, and AD&D
Training and development programs
Stocked kitchen with snacks and beverages
Collaborative and cool office culture
Work-life balance (where we don't encourage fun and relaxation time; we actually require it)
Unlimited vacation to relax, restore and refresh

***To all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company location. Trace3 is not responsible for any fees related to unsolicited resumes/CVs.","Irvine, CA",Big Data Engineer,False
478,"Who You’ll Work With
How connect with people every day? Do you chat? Video calls? Meetings? Have you ever had trouble connecting to audio, or video from a meeting room? Have you had a hard time coordinating with your team around a common goal or initiative? We all need to connect with people to get our work done. We are uniquely positioned to make it amazing across hardware and software with a leading market position in meetings. Our next goal is to redefine how people work where communication is at the core of all productive collaboration.

We are an experienced group of product managers, engineers, and designers who are finding new ways to leverage data from communication, people interaction, and collaborative content creation to craft new collaboration experiences in ways that nobody but Cisco can. These new collaboration experiences will give a stronger voice to the remote worker, make distributed teams as efficient as co-located teams, and drive the new generation of everyday collaboration between teammates.

What You’ll Do
Webex Teams is looking for a Data Engineer with Data Modeling experience who can help us reimagine and reinvent our Data Strategy. We need someone who is passionate about leading change, exploiting and optimizing customer engagement, driving experiences across our Collaboration stakeholders, growing our platform, incorporating new workloads and expanding our capabilities. You’ll be working alongside a team focusing on how people communicate across messaging, calling, and video as well as collaborate on documents, schedules, and tasks. We want to make everyone more productive every day.

Who You AreYou can understand and translate business needs into data models supporting long-term solutions.Work with the Application Development team to implement data strategies, build data flows and develop conceptual data models.Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.Optimize and update logical and physical data models to support new and existing projects.Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.Perform reverse engineering of physical data models from databases and SQL scripts.Evaluate data models and physical databases for variances and discrepancies.Validate business data objects for accuracy and completeness.Analyze data-related system integration challenges and propose appropriate solutions.Develop data models according to company standards.Guide System Analysts, Engineers, Programmers and others on project limitations and capabilities, performance requirements and interfaces.Review modifications to existing software to improve efficiency and performance.Examine new application design and recommend corrections if required.

Basic QualificationsExperience in designing and delivering enterprise-grade, high transaction volume, Data Platform as a Services (dPaaS) and experience with Data Lakes, Analysis Services, SQL, Cosmos or an equivalent set of cloud capabilities.6+ years of experience in building data platforms, data engineering and/or software engineering, including experience working with agile methodologies and cloud technologiesExperienced in leading and building solutions under Agile/scrum methodologies.Curiosity with a desire to continuously learn, share and collaborate to improve yourself and your teammates.Strong communication, negotiation and consensus building skills when dealing with stakeholders and team members.Strong problem solving and algorithmic thinking capability.Strong cross-group collaboration and effective communication skills to drive issues to resolution with ability to communicate insights to both technical and non-technical audiences.BS in data engineering (Master’s preferred), computer science or related field of study for software development is required.

Why Cisco
At Cisco, each person brings their unique talents to work as a team and make a difference.

Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.We connect everything – people, process, data and things – and we use those connections to change our world for the better.We innovate everywhere - From launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.We benefit everyone - We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.

Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us! #WeAreCisco","Seattle, WA","Data Engineer, Webex Teams",False
479,"Description:
JOIN US AS A LEAD DATA ENGINEER – SALES TRANSACTION PLATFORM
The Data Science and Engineering team at Target is a hyper-growing, dynamic and collaborative team. Data Engineers work closely with Data Scientists to create valuable insights using voluminous data collected from internal and external systems on a large-scale. Business operations are empowered with these insights to achieve Target’s strategic initiatives while providing world-class shopping experiences.
About this Opportunity
As a Lead Data Engineer, you will have the opportunity to create software solutions using Agile practices and DevOps principles. Responsibilities will include designing, programming, debugging and supporting high quality distributed systems and large-scale solutions on the latest Big Data tech stack (Java/Scala, Hadoop, Spark, Druid, Kafka, etc.).

We’re looking for a highly motivated and talented Big Data programmer who’s a team player, wants to work on large-scale data products, grow their skills/career as well as have fun in an exciting retail data environment!
Key Responsibilities:
Develop software systems using test driven development employing CI/CD practices
Partner with other engineers and team members to develop software that meets business needs
Follow Agile methodology for software development and technical documentation
Innovate constantly and stay current with latest technologies while staying focused on solving problems
Requirements
MS degree in Computer Science or relevant experience
5+ years of experience in developing software applications
3+ years of experience working on Big Data tech stack like Hadoop, Spark and Hive
Proficiency in at-least one of the following languages: Java, Scala or Python
Excellent understanding of software design and development at scale
Worked on building and supporting Web Services end to end
Experience with REST services preferred
Experience/knowledge of Kafka streaming solutions
Team player who collaborates and enjoys solving technical challenges with huge data sets

Qualifications:","Sunnyvale, CA",Lead Data Engineer - Sales Transaction Platform,False
480,"Our entrepreneurial group of technologists and analytics experts is looking to add an experienced Data Engineer to our Technology Team in Philadelphia. The ideal candidate will be adept at problem solving, interested in pursuing new ideas, and will play a key role in the strategic initiatives of an innovative global investor. Responsibilities include gathering requirements, building out a data warehouse, establishing and maintaining data integrations, developing data governance best practices, and optimizing data flow.
We are looking for candidates that are passionate about building and optimizing data systems. The Data Engineer will collaborate with our developers, data scientists, and technology products. They will also support non-technical colleagues in the collection and use of structured and unstructured data. They must be self-directed and comfortable supporting multiple projects and teams. This hire will contribute to data transparency across the organization, driving operational efficiency and providing decision makers with actionable insights.
JOB DESCRIPTION:
Project Management – gather project requirements, establish timelines, track progress, and manage to milestone achievements
Modeling – determine the most appropriate schema for storing structured and unstructured data
Extract, Transform, Load – apply business logic to move data from one system to another and validate data quality
Integration – determine the optimal methods for collecting and incorporating new data into data warehouses
Governance – establish and educate the organization on data governance standards
Strategic Reporting – collaborate with colleagues to scope out new data requests and methods to extract and present data from various data sources
Automation - implement internal process improvements with an aim to automate manual processes and optimize data delivery
NECESSARY QUALIFICATIONS:
5+ years experience in data engineering, preferably in financial services
BA/BS in related field (e.g., computer science, mathematics, engineering)
Must have experience with object-oriented programming languages and agile software development
Must have proficient technical skills in SQL and relational databases, exposure to data integration tools, and experience building and consuming APIs
Must have experience building a data warehouse in a professional environment
Exposure to statistical data analysis tools (e.g., R) and data visualization tools (e.g., Tableau) is a plus
Must have proficient communication skills, be proactive and be able to comfortably lead projects independently that include cross functional collaboration
We offer a competitive salary, annual discretionary bonus and a comprehensive benefits package which includes: Medical, Prescription, Dental, Paid Time Off, 401k plan, Life and Disability Insurances, Tuition Reimbursement, Health Club Reimbursement and Flexible Spending Accounts.
Hamilton Lane is an Equal Opportunity Employer. All qualified applicants will be considered for employment without regard to their race, religion, ancestry, national origin, sex, sexual orientation, age, disability, marital status, domestic partner status, or medical condition. As a registered investment adviser, employees of Hamilton Lane may be subject to certain limitations on political contribution and personal investment activities.","Philadelphia, PA",Data Engineer,False
481,"Job Description
At Amazon, our goal is to be earth’s most customer-centric company and to create a safe environment for both our Customers and our associates. To achieve that, we need exceptionally talented, bright, dynamic, and driven people. If you'd like to help us build the place to find and buy anything online, this is your chance to make history. We are looking for a talented data engineer to join the Restricted Products team based in our Seattle office.

This role will be a key member of the RP Business Intelligence team, focused on supporting the Restricted Products Supplier Engagement team and responsible for developing robust, scalable data models to support new supplier risk detection, assessment, and enforcement programs and tools. You’ll be challenged to innovate using a variety of technologies and data sources, and you will work with top-notch technical and non-technical professionals across multiple organizations and regions in order to develop and deliver complex solutions that will sustain operational excellence. We are looking for someone who is motivated by thinking big, moving fast, and inventing new ways to create and use data and available technologies at Amazon.


This role requires strong attention to detail, excellent analytical abilities, deep knowledge of business intelligence solutions, solid work ethic and drive, the ability to work independently and with primary stakeholders to solve complex medium and large scale issues, the ability to recommend solutions to unstructured problems, and the skill to manage both with and without authority. The successful candidate will be a motivated self-starter, comfortable with ambiguity, and will be comfortable extracting data from various sources to design/construct/execute complex data architectures that help to solve business problems.

Responsibilities

Design, implement, and support analytical infrastructureInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL, Python, and AWS big data technologies.Explore and learn the latest technologies to provide new capabilities and increase efficiency.Collaborate with Business Analysts and Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning.Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersProvide wing-to-wing data engineering support for project lifecycle execution (project planning, execution, risk assessment and system availability)Participate in developing strategy recommendations
Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or equivalent professional experience in a related technical discipline4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasetsDemonstrated strength in data modeling, ETL development, and data warehousingExperience using big data technologies (Hadoop, Hive, Hbase, Spark, etc.)Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos etc.)Knowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience working with AWS big data technologies (Redshift, S3, EMR)Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringFamiliarity with statistical models and data mining algorithmsKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative fields.A proven track record of delivering initiatives from conception through completion on time, within budget and on or beyond scope","Seattle, WA",Data Engineer - Global,False
482,"$73,052 - $148,967 a yearData Engineers focus on the design, implementation, and operation of data management systems to meet the CIA's business needs. It includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems. Data Engineers work together with data consumers to determine, create, and populate optimal data architectures, structures, and systems. Data Engineering requires an extensive knowledge of data manipulation, databases, data structures, data management, and best engineering practices.

Data Engineers must also plan, design, and optimize for data throughput and query performance issues. This requires constantly updating expertise in areas such as platform, network and storage technologies, bandwidth management, data bus implications and design.

Additionally, Data Engineers play a key role in the selection of backend database technologies (SQL, NoSQL, HPC, etc), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.

Offices of the CIA - Directorate of Digital Innovation

The Directorate of Digital Innovation (DDI) is at the forefront of defining the future of digital expertise within the CIA. DDI focuses on developing the workforce with cutting-edge skills, investing in IT infrastructure, and modernizing the way the Agency does business. DDI officers help accelerate the integration of innovative methods and tools to enhance the CIA's cyber and digital capabilities on a global scale and ultimately help safeguard our nation. Learn more about the Directorate of Digital Innovation

Life at CIA:

In addition to a comprehensive benefits package, the CIA offers exciting career opportunities and a dynamic environment. We're on the forefront of world-altering events - as they happen. So working here isn't just a job, it's a mindset and a lifestyle.

Minimum Qualifications:

Bachelor's degree, preferably in Mathematics, Computer Science, Engineering, Management Information Systems or related fields
GPA of at least 3.0 on a 4.0 scale
The following items must be attached to your on-line application (PDF format preferred):

Your resume.
A cover letter in which you specify your qualifications for one or more positions.
Unofficial transcripts for all degrees.
ALL POSITIONS REQUIRE RELOCATION TO THE WASHINGTON DC METROPOLITAN AREA.

All applicants must successfully complete a thorough medical and psychological exam, a polygraph interview and an extensive background investigation. US citizenship is required.

To be considered suitable for Agency employment, applicants must generally not have used illegal drugs within the last twelve months. The issue of illegal drug use prior to twelve months ago is carefully evaluated during the medical and security processing.

Important Notice: Friends, family, individuals, or organizations may be interested to learn that you are an applicant for or an employee of the CIA. Their interest, however, may not be benign or in your best interest. You cannot control whom they would tell. We therefore ask you to exercise discretion and good judgment in disclosing your interest in a position with the Agency. You will receive further guidance on this topic as you proceed through your CIA employment processing.

To Apply:

Save the position(s) that interest you in the job cart. You can add up to four (4) positions. Job cart selections will only be retained during this site visit, so be sure to click “Apply Now” before closing the browser window. After clicking ""Apply Now"" you will be taken to the application account creation page. The positions will appear in the cart once you have created an account. DO NOT submit multiple applications; this will only slow the review of your application and delay processing. Please read the Application Instructions carefully before you begin the online application process.","Washington, DC",Data Engineer,False
483,"About the Job
Can you envision working closely with all aspects of data from application to cloud services?
Come Join our team as a Data Engineer !!!

At 8x8, we have many opportunities to work with data on a daily basis. Can you envision working on data and build the infrastructure that are critical to drive valuable business insight. How about building a scalable data pipelines to drive critical data analysis for our end users and enable data science experimentation? Our Enterprise data analytics team works closely with all aspects of data from our application to our cloud services. We’re looking for Data Engineer with experience building efficient data pipelines reliably moving data across various and eager to build the scalable data tools for data exploration. Your work will greatly influence our program teams and analysts. You’ll take an active role in driving our analytics strategic projects that takes on some of the most challenging problems to solve. This is a full time position in our office in San Jose.


Build data expertise and own data quality for all data pipelines you build
Track events that are critical to drive business values
Work with new data models that provide intuitive analytics
Move data from large scale data warehouse and data storage both internally and externally
 Develop new systems and tools to enable folks to consume and understand data faster
Use your expert coding skills across a number of languages from Python, Java and PHP
Work across multiple teams in high visibility roles and own the solution end-to-end
Requirements

2+ years of Java and/or Python development experience is necessary
2+ years of SQL (mySQL, Hive, etc) experience is required
3+ years of experience with dimensional data modeling & schema design in Data Warehouses
2+ years of experience in ETL design, implementation and maintenance (Pentaho, Elastic Search)
100% passionate for reliable data pipelines and intuitive user interfaces
Ability to write well-abstracted, reusable code components
Excellent communication skills including the ability to identify and communicate data driven insights
BS or MS degree in Computer Science or a related technical field

#LI-RM1","San Jose, CA",Data Engineer,False
484,"Rated 32nd Most Successful SaaS Company in the world by Montclare, GoodData is on a mission to fundamentally change how analytics are used and adopted throughout the organization. With more than 50 percent of the Fortune 500 using GoodData, millions of end users and a massively scalable and secure Enterprise Insights Platform, GoodData works with customers to drive a business outcome focus allowing data to finally drive meaningful change for the business..
Today, some of the most well-known brands are using the GoodData Enterprise Insights Platform to disrupt their industries, improve productivity and make business critical decisions, to create revenue generating Smart Business Applications. According to Forrester, insights-driven businesses will steal $1.2 Trillion/year from traditional market players by 2020. This is the transformation GoodData is enabling globally.
GoodData is headquartered in San Francisco and backed by top-tier investors like Andreessen Horowitz, General Catalyst Partners, Intel Capital, TOTVS and others. For more information visit our website and follow GoodData on Twitter and LinkedIn.

JOB DESCRIPTION
As a key member of our Consulting team within Professional Services, The Solutions Engineer will be the data engineer that helps guide our customers through their data product implementation. Working directly with customers, a team of data product managers, solution architects and engineers drive requirements gathering, the project design, and the QA process. You will implement data warehouse data models and dimensional data models, deliver ETL in SQL, and write multidimensional queries in MAQL.
The right candidate for this position is customer oriented with strong communication and technical skills. You must be able to demonstrate deep technical expertise and clearly articulate technical topics even to non-technical audiences.
This role is best suited for someone who has 2+ years of work experience, has a quantitatively-oriented degree (of any variety) from a top-tier school, and who thrives in extremely analytical, technically-oriented, consultative role. This role is located in San Francisco, CA and no relocation assistance is offered.

RESPONSIBILITIES
In collaboration with architects, implement data solutions that solve business problems
Design conceptual, logical and physical data models
Implement effective and scalable end-to-end data pipeline solutions
Optimize SQL queries for scale
Build metrics and reports
Address functional requirements and quality attributes
Possess a comprehensive understanding of the GoodData products and services

QUALIFICATIONS
2+ years of overall experience
Fluency in SQL and experience in Extract, Transform, and Load (ETL) development in SQL
Understanding of dimensional and normalized database models and their applications
Having a programming mentality and problem-solving skills

BONUS POINTS
Experience in the Business Intelligence (BI) / Data Warehousing (DW) industry
IT consulting skills and experience
Experience with cloud based SaaS platforms
Experience working in professional services organization
Experience with columnar databases
Experience with Ruby
Experience with JavaScript
Experience with REST APIs
Interest in machine learning

ADDITIONAL INFORMATION
At GoodData, you won’t just grow your career. You’ll be a part of a movement that is changing how people work and make decisions.
Why you should join GoodData:
Help drive the future of business
Solve meaningful problems
Build and promote great technology
Work with brilliant people
We are committed to creating a diverse work environment and proud to be an Equal Opportunity Employer. All your information will be kept confidential according to EEO guidelines.
No relocation assistance is offered for this opportunity at this time, and local candidates will be given priority consideration.
Agency Recruiters Take Note:
Please do not spam us with unsolicited candidate resumes. Unless you have a fully executed agreement with us, we will consider them a gift. Do not send resumes to any of the executives.","San Francisco, CA 94107 (South Of Market area)",Data Engineer,False
485,"About LeafLink:
LeafLink is a marketplace that provides licensed cannabis retailers the ability to order from their favorite brands, as well as a suite of software tools for those brands to manage and scale their operation.

With over 2,000+ dispensaries and more than 600+ leading brands in Colorado, Washington, California, Oregon, Nevada, Maryland, and Arizona, LeafLink is setting the industry standard for how cannabis brands and retailers work together.

Our team, backed by funding from leading VC's, is poised to define the cannabis wholesale market. This year, LeafLink was named one of Fast Company's ""Top 10 Most Innovative Companies in Enterprise"", joining the ranks of Amazon, Slack, and VMWare - and we're just getting started!

The Role:
LeafLink seeks a Senior Data Engineer to join our growing Product and Engineering team to contribute to our industry and internal data insights efforts. We have some exciting data efforts in front of us which include:


Situate the organization with a Business Intelligence tool to help us drive business decisions.
Own the design and development of automated dashboards.
Drive internal business decisions and directions through establishment of core KPIs
Create industry-defining metrics and standard reporting to help drive the cannabis industry.
Support ad-hoc data analysis or reporting needs from teammates or customers.
Proactively conduct ad-hoc analyses to discover new product and business opportunities.
Develop new metrics to better identify trends and key performance drivers within a particular area of the business.
Empower teammates to develop reports by assisting them with concepts and SQL

Qualifications:

5+ years working at an established SaaS or E-Commerce company
5+ years working with SQL and other data insight tools
Proficiency in at least one programming language such as Python
Strong grasp of statistics and experience with open source big data technology
3+ years of working crossing functional to create KPIs which drive or support strategy decisions
Has owned business intelligence platforms such as Periscope, Looker, Tableau, or Domo
Has published industry reports or worked closely with a marketing team to showcase powerful data insights to establish thought leadership

Benefits:

Healthcare matching
3 weeks paid vacation a year
Fun office environment
Competitive salary
Benefit Matching (medical, dental, vision)
Generous stock options
Team events
Cool Swag
Brand spanking new office in FiDi

","New York, NY 10004 (Financial District area)",Senior Data Analyst,False
486,"Engineer - Data focus

Door is expanding its Engineering team and is searching for a full-time Software Engineer passionate about data. We are looking for cross-functional software practitioners who are involved in delivery across the full stack and the full lifecycle, with a focus on data retention, access, and reporting. Door is heavily invested in AWS, and this role will be responsible for:


Developing schemas and storage solutions for structured and semi-structured data
Deploying data solutions via CloudFormation
Working with various storage backends, possibly including Postgres, Redshift, DynamoDB, and Snowflake
Contributing to Docker services in node.js and Python
Modeling and documenting a consistent view of persistent and transient data
Ensuring the security, maintainability and robustness of the data and schema
Ensuring the quality and correctness of data access

The ideal candidate


Must have SQL and NoSQL database experience
Must have experience designing and changing data models
Must have experience with a reporting and visualization tool
Should have node.js or Python experience
Should have exposure to AWS services, AWS certification a plus
Should have exposure to Security practices, IAM experience a plus
Should be a polyglot developer, interested in new languages, tools, and technologies
Should be passionate about delivering quality software to quality people

This role reports to Engineering Lead in our Dallas, TX office.","Dallas, TX",Data Engineer,False
487,"The Data Engineer is responsible for working with and improving the quality of our clients’ data, coordinating with our technical and client teams to continually refine our data management and integration processes. Ideal candidates have several years of experience working with data and utilizing MS SQL, and a good understanding of data processing, cleansing, and integrating techniques to enhance our globally-used SaaS platform.A technical screen will be conducted.RESPONSIBILITIESContribute to leveraging big data, machine learning, and advanced analytics to deploy analytical solutionsDesign and improve data-integration procedures to collect distribution and store dataHelp drive optimization, testing, and tooling to improve data qualityIdentify inefficiencies in queries and ETLs, and investigate solutions for performance tuningWork with business teams to assist with data-related technical issues and support their data needsEnhance and maintain our business intelligence platform to deliver improved analytics for end usersCoding, testing, and troubleshooting features and enhancements in the current database environmentMonitoring and fine-tuning databases for optimal performanceQUALIFICATIONS & EXPERIENCE2+ years of Data modeling, ETL, and Data Warehousing experience2+ years of development experience with MS SQL ServerStrong computer science fundamentals including data structures and algorithmsBachelor’s Degree in Computer Science or relevant educational background (including certifications)Experience with MS SQL and related toolsExperience in Data WarehousingExperience in designing Data Integration / ETLKnowledgeable about data modeling, data access, and data storage techniquesAbility to manage a workload based on shifting prioritiesAbility to work in a fast paced and dynamic environmentExcellent written and verbal communication in EnglishExperience in Agile or SCRUM practices a plusJob Type: Full-timeExperience:Data Modeling: 2 years (Required)SQL: 2 years (Required)Education:Bachelor's (Preferred)Location:Atlanta, GA (Required)","Atlanta, GA",Data Engineer,False
488,"Functional Area:
IT - Information Technology


Estimated Travel Percentage (%): No Travel


Relocation Provided: No


American General Life Insurance Company


AIG Life & Retirement Group seeks a Data Engineer to work closely with software/application development teams and business users to design database structures and solutions based on the data storage and retrieval needs within each solution. The Data Engineer will be responsible for developing our target state data storage, processing, and analysis architecture with a focus on cloud and hybrid on premise/cloud architectures.
The Data Engineer is responsible for choosing the right technology and proper data design and implementation in various projects and solutions. Candidate is expected to demonstrate hands-on skills in processing large data sets, familiarity with data structure, Big Data concepts, cloud computing, open-source tools and optimization techniques, excellent oral and written communication skills, client focus and the ability to support and enable teamwork. Must be self-motivated and able to operate independently with limited guidance and direction.
Data Engineer Responsibilities
Design, build, and deploy database applications
Design and implement effective monitoring on the different database technologies to proactively identify performance issues
Serve as expert on the Data Lake, Relational and NoSQL implementations
Help maintain the integrity and security of the company data
Provide insight into the changing database storage and utilization requirements for the company and offer suggestions for solutions
Analyze database implementation methods to make sure they are in line with our data strategy and goals
Manage the retrieving and analyzing of large volumes of data
Attend key design meetings and provide support and expertise
Work closely with vertical development teams ensuring proper implementation and following of the design

Data Engineer Requirements
Bachelor’s Degree in Computer Engineering or related field required
Experience of IT platform implementations in a highly technical and analytical role.
Strong SQL skills and proficiency with Oracle required
Must be able to develop creative solutions to problems
Experience with the full development life cycle of an application stack - from architecture through test and deployment.
Experience with key reporting solutions including Cognos, Qlikview, and Power BI will be highly valued
Familiarity with Big Data concepts, cloud computing, open-source tools, and optimization techniques is preferred
Implementation and tuning experience specifically using Talend, AWS EMR, Spark
Strong analytical skills; ability to analyze raw data, draw conclusions, and develop actionable recommendations
Organized, detail-oriented, quality-focused
Prior experience in financial services industry
Demonstrated ability to think strategically about business, product, and technical challenges in an enterprise environment.
Strong verbal and written communications skills and ability to lead effectively across organizations
Hands on experience leading large-scale global data warehousing and analytics projects.
Demonstrated industry leadership in the fields of database, data warehousing or data sciences.


It has been and will continue to be the policy of American International Group, Inc., its subsidiaries and affiliates to be an Equal Opportunity Employer. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories.

At AIG, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.","Farmington, CT",Data Engineer,False
489,"Automattic is the company behind WordPress.com, Jetpack, WooCommerce, and more. We are looking for a full-stack data engineer with a head for business.

You’ll work with business leads, analysts, data scientists and fellow engineers to build data products that empower better decision making. You’re relentless about data quality in business metrics. You’ll understand business drivers and analytics use cases, you’ll evaluate and help to craft technology choices, and you’ll implement the data tools required to tackle business use cases.

What we’re looking for:

Hands-on, production experience with big data technologies (Hadoop, Hive, Impala, HBase) and data pipelines.
Experience in developing business analytics and data visualization tools for business metrics.
Strong analytical skills and a fervor for data quality.
The curiosity and determination to understand and improve data flows.
We’re serious about growing diversity in the tech industry. We want to build Automattic as an environment where people love their work and show respect and empathy to those with whom we interact. Diversity typically includes, but is not limited to, differences in race, gender, sexual orientation, gender identity or expression, political and religious affiliation, socioeconomic background, cultural background, geographic location, disabilities and abilities, relationship status, veteran status, and age. To work on diversity means that we welcome these differences, and strive to increase the visibility of traditionally underrepresented groups. Read more about our dedication to diversity and inclusion.

HOW TO APPLY
Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a résumé. Let us know what you can contribute to the team. Include the title of the position you’re applying for and your name in the subject.

Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.

If you’re reading this on a site other than automattic.com please ensure you visit automattic.com/work-with-us for the latest details on applying.
Please answer the following questions in your cover letter. Applications without these questions answered will not be considered:

Tell us some details about an interesting data problem you’ve worked on. What made it interesting?
Include a link to a recent favorite blog post or paper about working with lots of data.
What questions do you have for us?","San Francisco, CA 94110 (Mission area)",Business Data Engineer,False
490,"Do you want to make an impact on the future of eCommerce and blockchain technologies?
Who We Are:

We are located just minutes away from Salt Lake City, Utah and several world-class ski resorts, and within hours of five national parks . Overstock is an original resident of "" Silicon Slopes "", one of the fastest growing technology hubs in the country. We’re a passionate group of collaborative problem solvers and creative innovators, working on cutting-edge technology like our award-winning retail app (with amazing AR functionality ) and leading blockchain and machine learning technologies. Our team embodies unique values and diverse perspectives, making Overstock a hidden treasure in the tech industry.

Our Mission:

Overstock’s mission is to use, build, and find cutting-edge technology that helps connect people with products and services in new and unexpected ways. Our website offers millions of brand name products at discount prices to inspire people to make their dream homes a reality.

A Data Engineer role provides direct support to the Data Science group as well as the company at large.


As with any data related position, this role would include integrating data from multiple sources

via a variety of tools. This would necessitate using but also possibly researching

the appropriate technologies for such integrations.

Job Responsibilities
Aid data scientists and others in transforming their work into scalable production solutions

Set up and manage Big Data or Hadoop-related environments

Become familiar with existing big data, development, and database systems at Overstock.com

Preparation and cleaning of data for analysis

As needed, research and select proper architecture of and uses of big data tools
Perform other duties as required and assigned by manager and upper management.
Follow legal policies as directed.

Job Requirements
5+ years IT industry experience
Proficiency in SQL
Experience with Hadoop ecosystem technologies
Some proficiency in tools such as Spark, Pig, MapReduce or Hive
ETL experience
Ability to work with a variety of roles such as DBA's, Big Data Engineers, Data Scientists and ETL Developers
Experience in at least one programming language, preferred languages: Python, Java, Scala
Administration and Tuning of linux servers and applications a plus
Skills
SQL, Python, Spark, Hadoop, HDFS, Pig, Hive, Cloudera, Java, Scala, Linux
Education
BS in Computer Science, Information Systems, or related field
Certifications
Physical Requirements
Equal Employment Opportunity It is our policy to provide equal employment opportunity for all applicants and associates. This policy includes our commitment to ensure that all employment decisions are made without regard to race, color, religion, gender, national origin, disability, pregnancy, veteran status (including Vietnam era veterans), age, sexual orientation, gender identity, or any other non-job-related characteristic protected by law.


What We Offer:
LEED Gold Certified 19-acre Campus & Global HQ
Onsite Daycare Center
401k (6% match)
Onsite Health Clinic
Flexible Schedules
Tuition Reimbursement, Leadership development Program, & Mentorship Program
Onsite Fitness Center with group fitness classes and trainers
Onsite Cafe with additional Coffee Shop and Juice Bar
Indoor Bike Storage
Summer Party at Lagoon, Utah's largest theme park
Employee Fall Concert (Past performers include: Flo Rida, Snoop Dogg and Jason Mraz)
Medical, Dental, Vision coverage
Onsite Greenhouse, providing fresh fruits & vegetables for our cafe
Life Insurance, Short and Long-Term Disability coverage
Onsite salon services, massages, & auto-detail services
Discounts on ski passes, cell phone plans
Overstock Women's Network (OWN)

*Benefits vary based on location, position, tenure, and employee election

What We Value:
Wellness & Balance
Sustainability
Corporate Social Responsibility
Innovation - Discussed on Medium , Digital Trends , and Digital Commerce 360 .

Physical Requirements:

This position requires you to sit, stand and perform general office functions. You may also be required to lift up to 25 pounds occasionally. Bending, stooping and reaching are also frequently required.

Equal Employment Opportunity:

It is our policy to provide equal employment opportunity for all applicants and associates. This policy includes our commitment to ensure that all employment decisions are made without regard to race, color, religion, gender, national origin, disability, pregnancy, veteran status (including Vietnam era veterans), age, sexual orientation, gender identity, or any other non-job-related characteristic protected by law.","Midvale, UT",Data Engineer,False
491,"Job Description

CapTech Big Data Engineers are tasked with designing and implementing big data solutions for our clients. CapTech employees enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other CapTech developers, architects, and our clients.
Specific responsibilities for the Big Data Engineer position include:
Design, develop, document, and test big data solutions.
Understand the challenges being addressed by an engagement and collaborate with team members and clients to deliver a technical solution that meets the unique needs of our clients.
Create quality deliverables to communicate technical solutions to appropriate audiences.
Learn continuously, leveraging CapTech’s training resources and self-directed training, sharing knowledge and skills with others.
Provide mentoring and leadership to more junior resources.

Qualifications

Specific qualifications for the Senior Big Data Engineer position include:
Demonstrated growth over 5+ years’ experience working as a data engineer.
High level understanding of big frameworks.
Development experience with Big Data/NoSQL platforms, such as Hbase, MongoDB or Apache Cassandra.
Expert knowledge of SQL and NoSQL tools.
Knowledge of MapReduce and MapReduce generating tools like Pig or Hive.
Experience with message buses or real-time event processing platforms is a plus.
Java development experience.
Scripting language experience (Perl, Python etc.).
Understanding of NoSQL data modeling.
Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance.
Additional Information


We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture that filled with opportunities to have fun along the way.
At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career

Candidates must be eligible to work in the U.S. for any employer.

CapTech is an equal opportunity employer.
CapTech is a Drug-Free work place.
Candidates must have the ability to work at CapTech’s client locations.
All positions include the possibility of travel.","Reston, VA",Big Data Engineer,False
492,"MORE ABOUT THIS JOB
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.

Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.

Who We Look For
Goldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.
The Data Intelligence organization aims to make data a strategic asset for the enterprise by providing a platform that enables the structuring, management, integration, control, discovery, usage, and governance of our Data Assets.
The team leverages a wide variety of cutting edge technologies including Hadoop, HBase, Spark, Apache Beam, Apache Flink, Kakfa, SQL, OLAP platforms, Presto, Hive, Java and Python. Your impact will be to Curate, design and catalog high quality data models to ensure that data is accessible and reliable. Build highly scalable data processing frameworks for use across a wide range of datasets and applications. Provide data-driven insight and decision-making critical to GS’s business processes, in order to expose data in a scalable and effective manner. Understanding existing and potential data sets in both an engineering and business context
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
• • Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
• • Evaluate, select and acquire new internal & external data sets that contribute to business decision making
• • Engineer streaming data processing pipelines
• • Drive adoption of Cloud technology for data processing and warehousing
• • Engage with data consumers and producers in order to design appropriate models to suit all needs

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
2-3 years of relevant work experience in a team-focused environmentA Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)Extensive knowledge and proven experience applying domain driven design to build complex business applicationsDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processesIn-depth knowledge of relational and columnar SQL databases, including database designGeneral knowledge of business processes, data flows and the quantitative models that generate or consume dataExcellent communications skills and the ability to work with subject matter expert to extract critical business conceptsIndependent thinker, willing to engage, challenge or learnAbility to stay commercially focused and to always push for quantifiable commercial impactStrong work ethic, a sense of ownership and urgencyStrong analytical and problem solving skillsAbility to collaborate effectively across global teams and communicate complex ideas in a simple manner

Preferred Qualifications
Financial Services industry experienceWorking knowledge of more than one programming language (Python, Java, C++, C#, etc.)Experience with the Hadoop eco-system (HDFS, Spark)
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2018. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.","New York, NY 10282 (Tribeca area)",Data Intelligence - Edge Data Engineering - Data Engineer (D...,False
493,"Want to be part of shaping the future? Our breakthrough ability to unlock insights from the web radically improves intelligence and cyber threat visibility. We're a high-energy, fast-paced, and fast-growing company that partners closely with our customers. This role is integral in delivering best practice threat intelligence solutions to our customers and building lasting, productive relationships between users and the company. Our ideal candidate has outstanding communication skills, proven history in customer community building or engagement, and is excited about building collaboration around cyber threat intelligence problem sets.

The Data Science team takes ownership of this unique dataset. We implement crucial harvesting capabilities, manipulate our data structures to obtain solutions and insights, and explore new use cases and capabilities for the data. We're looking for smart and ambitious people with a strong engineering foundation and a quantitative mindset to help us get the most out of our unparalleled collection and analysis technology. You will join a team of motivated people working on a tough problem, and help us shine a light into the dark corners of the internet.

Qualifications:

Technical education: BA, BS, MS, or PhD in Computer Science or related discipline, with a strong academic record.
Programming: Solid programming in Python and at least one other language. Comfort with developing production code. JavaScript experience is a plus.
Data skills: Comfort working with complex data structures. You love shaping data into a usable product, and you know how to write the code that makes that happen.
Excellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations.
Bonus if you have experience or interest in the area of cybersecurity.

We realize we can only succeed with a team of very smart and passionate people. If you're looking to work in a unique environment with ambitious, dedicated colleagues, the chance to collaborate with fantastic users and customers, then we have a lot in common! You'll also be equipped with top technology, enjoy trips, ""the best"" coffee, great food and fun. We offer competitive compensation, including stock options and a full range of benefits, as well as a great culture, commitment to professional development and social responsibility.

Don't forget to check out our Podcast! ( https://boards.greenhouse.io/embed/recordedfuture.com/podcast )Join the Recorded Future team, special guests, and our partners from the CyberWire to learn everything you want to know (and maybe some things you'd rather not know) about the world of cyber threat intelligence. All episodes are free and available on iTunes, GooglePlay, and Stitcher.","Boston, MA",Data Engineer,False
494,"J.P. Morgan is a premier corporate and investment bank with a full suite of global financial services and capabilities. The world’s most important corporations, governments, financial institutions, pensions, sovereign wealth organizations, states and municipalities entrust us with their business in more than 100 countries. We offer strategic advice, lend money, raise capital, help manage risk, extend liquidity, buy and sell securities and provide many other banking services in markets around the world.

We are looking for an experienced data engineer to join a small group of technologists who will work hand in hand with scientists on a daily basis to enable them to deploy their ideas into production quickly and

This role requires a wide variety of strengths and capabilities, including:
 BS/BA degree or equivalent experience
 Advanced knowledge of application, data and infrastructure architecture disciplines
 Understanding of architecture and design across all systems
 Working proficiency in developmental toolsets
 Ability to collaborate with high-performing teams and individuals throughout the firm to accomplish common goals
 Proficiency with Hadoop and Big Data
 Understanding of software skills such as business analysis, development, maintenance and software improvement
 Design, architect and build data platform solutions using Big Data Technologies
 Work with Architecture and Engineering team to define Reference Architecture for Big Data/Distributed computing.
 Access stakeholder requirements, prototype and iterate solutions to solve business problems using Big Data technologies
 Proven track record selecting appropriate data storage technologies and ETL technologies and building out ingestion pipelines
 6+ years of hands on Java development experience. Experience working on Java server side frameworks - Spring, Spring Boot, etc.,
 2 to 4 years’ experience on Hadoop Platform (HDFS, Hive, HBase, Spark, oozie, Impala, Cassandra etc.)
 8+ years of professional experience with an established track record as a full stack architect.

Our Corporate & Investment Bank relies on innovators like you to build and maintain the technology that helps us safely service the world’s important corporations, governments and institutions. You’ll develop solutions for a bank entrusted with holding $18 trillion of assets and $393 billion in deposits. CIB provides strategic advice, raises capital, manages risk, and extends liquidity in markets spanning over 100 countries around the world.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.

At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.","New York, NY 10179 (Midtown area)",Data Science Data Engineer,False
495,"Description:
We are looking for a data engineer to help us build our data pipeline, data warehouse and technical data structures across the data landscape.

Responsibilities:

Partner with the product and engineering teams to develop scalable, extensible data solutions for data transformation, movement, and manipulation
Normalize transactional data, disparate systems, and transfer of data in a way that creates flexible and scalable data solutions
Builds applications and scripts for specialized data processes to acquire, manipulate, and supply data to systems for various purposes
Understands master data elements to be used when joining datasets and creating data maps
Creates processes and data management systems including manual and automated processes, such as cleansing and maintaining master data sets
Builds automated fault-tolerant processes to acquire and cleanse data routinely
Develop new dimensional data models where highly complex data relationships and data flows exist
Design and maintain data pipelines to simplify end user reporting and analytics
Maintain the security, data integrity, and availability of data in the data warehouses and data hubs
Build and maintain data pipelines between 1st- and 3rd-party products and the data warehouse used for company-wide analytics
Collaborate with all teams across the company to ingest data and apply appropriate business logic
Uses existing data assets to build new datasets for the purpose of gaining new insights.

Requirements:
Technical/Business Experience


Bachelor’s degree in Computer Science or related field
5+ years overall experience working in development and enterprise data
Experience with writing SQL scripts and Python or Ruby apps for purposes of data manipulation
Experience and background in building data platforms
Experience with EasyMorph or Alteryx preferred
Minimum 3+ years of experience with data architecture/design
Minimum 3+ years of experience in software development with deep experience in Object-Oriented, Functional, Object-Functional Language and one of the major SQL relational datastores (we are a SQL Server shop)

","Warrendale, PA 15086",Data Engineer,False
496,"iHeartRadio is looking for a talented Data Engineer to help us in our data-driven mission to reshape the world of music and the spoken word. You will work in a highly collaborative team of engineers, and alongside data scientists and analysts, to distill existing data processes, import new external data sources, and create complex data mashups. Your work will provide valuable insights and power important music data products. Expect to build high throughput data pipelines and improve the existing big data infrastructure. You will also improve performance, squash bugs, and increase visibility across the data ecosystem. You will have end to end ownership of your code, though ideally you also relish reviewing a good pull request. If you enjoy working with large sets of data and the challenges associated with them this is the role for you.

You Like:

Working in an Agile development methodology and own data driven solutions end-to-end
Experimenting with various frameworks in the Big Data ecosystem to identify the optimal approach for extracting insights from out datasets
Identifying performance bottlenecks in data pipelines and architect faster, more efficient solutions when necessary
Creating new data warehouse solutions and define and demonstrate best practices in schema and table design in varied databases like Hive, Redshift, Spectrum etc.
Developing end-to-end batch and real time pipelines for large data sets to our Hadoop/Spark clusters, and bring summarized results back into a data warehouse for downstream business analysis.
When needed, performing data housekeeping, data cleansing, normalization, and implementation of required data model changes
Increasing efficiency and automate processes by collaborating with our SRE team to update existing data infrastructure (data model, hardware, cloud services, etc.)
Designing, building, launching and maintaining efficient and reliable data pipelines in production
Designing, developing, and owning new systems and tools to enable our consumers to understand and analyze the data more quickly.

You Have:

2+ years of experience ingesting, processing, storing, and querying large datasets
2+ years of experience working in the Hadoop/Spark ecosystem
Ability to write well-abstracted, reusable code components in Python, Scala or similar language(s)
Ability to investigate data issues across a large and complex system by working alongside multiple departments and systems
A self-starter who thrives in owning the products and pipelines they develop
Experience with configuration management tools (Ansible, Chef, Puppet, etc) is a plus
Experience with Spark, Kafka, or similar is a plus
Experience with AWS technologies (S3, Redshift, EC2, RDS, EMR, Dynamo) is a plus
Proven proficiency with Scala is a plus

iHeartRadio, iHeartMedia's digital radio platform, is the fastest growing digital audio service in the U.S. and offers users thousands of live radio stations, personalized custom artist stations created by just one song or seed artist, and the top podcasts and personalities. iHeartRadio is a great environment for people who like to innovate and have the power to influence decisions. We have 120+ million registered users across over 200 different platforms, and outside the US, we are in New Zealand, Australia, Canada, and Mexico!","New York, NY",Senior Data Engineer,False
497,"Who we are

Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 244 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.

Note:
Provide full legal first Name/Family Name
DO: Capitalize first letter of First and Last Name. Example: John Smith
DON'T: Capitalize entire First and/or Last Name. Example: JOHN SMITH
NOTE: Use correct grammar for Names with multiple cases. Example: McDonald or O'Connell

Provide full address details
Resume is required
Multiple attachments can be uploaded including Resume and Cover Letter for each application


Job Description Summary:
PayPal’s Global Data Science organization is in charge of developing unique, best-of-breed ML/AI algorithms using cutting-edge technologies. We are looking for a motivated and experienced big data engineer, to join a highly skilled team of experienced professionals, which believe in best-of-breed software craftsmanship, clean and elegant coding, using the right tool for the job, and always exploring and learning new technologies and approaches.

Job Description:
We are looking for an experienced Data Engineer to help develop new projects on Hadoop big data platform..

Be responsible for practicing technical design, developing new functionality and maintaining existing components
Deep dive into PayPal’s world class risk systems and algorithms and work side by side with data scientists and analysts on data centric analytic solutions
Contribute to PayPal’s efforts to fight Fraud, using advanced algorithms and technology, while deploying the code you develop into PayPal’s live systems, and having a true global effect

Requirements:
B.S. in computer sciences or equivalent
At least 8 years solid server-side development with Java
Solid knowledge of basic algorithms
A passion for developing robust, scalable software systems
Strong OOP skills, ability to analyze requirements and prepare design
Hands on experience with common open-source and Java EE technologies
Spring
SQL, JDBC/ORM, JMS
App servers / Servlet containers
Maven/Git/Continuous integration

Advantage:
Experience working in agile methodology
Experience working on Big Data platform
Experience working on front end technologies
Experience working both in startups and in large companies
Good communication skill to global business and technical partners
Highly motivated, goal driven, Can-do approach
Innovative, entrepreneurial, team player, ability to multi-task

Subsidiary:
PayPal

Travel Percent:
0

Primary Location:
San Jose, California, United States of America



Additional Locations:


Masters Degree or Equivalent




We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.


Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.","San Jose, CA",Big Data Engineer,False
498,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile computing. Build groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who built the intelligent assistant that helps millions of people get things done — just by asking. Join the Siri Speech team at Apple.
We are looking for exceptional data engineers passionate about customer experience, who love working with data for measuring and improving accuracy of ML models in products used by millions of customers all over the world.

Key Qualifications
Experience in the development and maintenance of tools for analyzing and processing large amounts of data under Apache Hadoop and Spark
Strong programming skills in C/C++ and/or Java/Scala as well as scripting languages such as Python, Perl, and Bash
Prior experience working on ASR/MT training and test data sets in multiple languages is preferred
Strong analytical skills and a real passion for data and data quality Proficiency in visualizing data and statistics
Description
You will be a part of a small, collaborative team of talented researchers and engineers responsible for data analysis, training and test data set creation, and related tools development. Your focus will be on providing the speech recognition and machine translation teams with the best training and test data sets. Directly contribute to the accuracy of Siri in all supported languages and devices. You should thrive in a fast paced environment with rapidly changing priorities, and collaborate well with other engineering teams at Apple.

Education
B.S or M.S in Computer Science, or equivalent experience","Santa Clara Valley, CA",Siri - Speech Data Engineer,False
499,"Data Engineer
Engineering, Los Angeles, CA, Full-time

ABOUT THRIVE

Wholesome products at wholesale prices. Thrive Market is a membership e-commerce platform on a mission to make the world’s highest quality natural and organic products affordable for every American family. For $60/year, Thrive members get access to their favorite healthy snacks, supplements, home, beauty, and baby products at 30-50% off retail value—all shipped to their front door. As part of our Thrive Gives initiative, each paid membership on the site also sponsors a free membership for a low-income family.

THE ROLE

As Thrive Market continues to grow at an incredible rate, you'll have the opportunity to build the data infrastructure at the heart of the business. At Thrive, we've created a transparent, data-centric environment from the beginning and we'll need your help building out our existing data warehouse to provide reporting and data for our Business Departments and Data Science team.

Resposibilities
Own data instrumentation and data quality, and code enhancements for the Thrive Market web site, APIs and backend platforms for data collection and data integrity.
Participate in all aspects of a project life-cycle utilizing Scrum methodology
Performance tuning of JSON APIs, and frontend frameworks.
Works with dirty and messy data and leads instrumentation, logging and validation of data analytics across the site.
Architect, Design, and Develop ETL processes between multiple systems using different tools
Work with other Data Engineers and Analysts to meet business needs in a timely manner

Qualifications
Bachelor’s degree required (computer science or related field), master’s a plus
Proficiency in at least one high level programming language like Java / Scala / Python / C++
Proficiency in a Data Visualization Tool (Tableau preferred) is a plus
Experience with Apache Spark, Kafka, Elasticsearch, and large scale data analysis is a plus
Ability to learn quickly and multi-task in a fast-paced, dynamic environment
Experience with distributed systems and MapReduce architectures a plus
Excellent communication skills, fun personality and a strong sense of curiosity
Excellent analytical and problem-solving skills

We Offer
Competitive Salary (DOE) + equity
Flexible vacation time
Free Thrive membership
Medical, dental and vision plans to choose from
Stocked kitchen and fridges with Thrive-type foods
Catered lunches in the office
Optional Yoga on Wednesdays, Kombucha Thursdays and weekly events
Casual atmosphere and great people to work with","Los Angeles, CA",Data Engineer,False
500,"Job Description

Our Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business critical data sets that allow us to realize the value of all the data we collect.
A Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge.
The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.
You will accomplish this by:
Building and enhancing data ingestion pipelines using tools like Python, SQL, AWS
Leveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment.
Deploying and utilizing tools to ensure the curation of datasets that meet the needs of the business
Processing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for analysts and Data Scientists.
Assisting in data warehousing, data analytics, and data science efforts
You’d be a great fit if your current track record looks like this:
3-5 years of progressive experience data engineering
Experience with Python development, particularly for data manipulation and API integration
Experience with data management platforms (e.g. Hadoop, EMR, PostgreSQL, RedShift, BigQuery, Cassandra)
Fluency with data acquisition and relational database platforms
Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language
Ability to problem solve independently and prioritize work based on the anticipated business value
Qualifications:
At least 3 years experience building software for high traffic websites, using languages such as Javascript or Python
Full stack experience preferred
Experience with cloud services such as AWS and APIs
Deep understanding of relational databases, NoSQL databases and other types of data stores
A firm grasp of modern testing principles
Ways we work:
Software Craftsmanship - we want to be proud of our work. come to utahsc.org
Test-Driven Development - we take responsibility for our code without QA engineers
Agile Development - we deliver business value quickly
Pair programming - we value collaborative development
Continuous Delivery - teams independently ship code to prod every day
Continual improvement - we take time to sharpen the saw and adjust how we work
Autonomous & responsible teams - making their own product & dev choices
Cross-functional teams - collaborating through all phases of the product dev process
Customer research - we build what our customers actually want
Trusting leaders - who trust us to create and don’t impose deadlines or features
Solid technology - of the team’s choice, for the right job

Qualifications

null

Additional Information

Be Yourself. Pluralsight is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.","South Jordan, UT",Data Engineer,False
501,"WHO WE ARE

Zume is on a quest to be the most powerful source of health and well-being on the planet. To achieve our objective, we must facilitate the provision of wholesome, affordable food on a global scale and in a sustainable manner. We are meeting this challenge by providing an end-to-end, scalable platform that reduces the time and distance between clean food sources and dense population centers, using cutting-edge automation and transportation logistics. By developing better tools and processes, we can feed people healthier, sustainably-grown food, delivered fresh and free from chemical stabilization.

THE ROLE

We're looking for an experienced data engineer to own our science and modeling platform. You will own the platform that powers our data science, model training and evaluation, and API's E2E.

What You'll Do
--------------


Build infrastructure to train and evaluate a variety of models
Build tooling to normalize data, data pipelines, feedback infrastructure
Stand up the API's used for production prediction and modeling
Evaluate and improve model selection algorithms
Deploy algorithms and models to improve prediction and ranking performance
Generalize model training to include diverse data stores
Develop, scale, and maintain our distributed systems
Participate in the pizza production process

This role has the potential to evolve into a lead role as the team grows. It is located in Seattle, WA.

WHO YOU ARE

A generalist in data engineering and data science experienced in building production applications for data science and machine learning.


Degree Computer Science or other technical degree and equivalent work experience.
Experience with Hadoop, Cosmos, Spark, or the equivalent.
Understanding and familiarity with common ML techniques. Deep knowledge of optimization, stochastic processes / markov chains, or the like a plus.
Software development experience in Python.

BONUS POINTS


Familiarity with hardware / robotics
Understanding of supply chains and/or operations
Graduate course work in Optimization
Previous experience at a startup

WHAT WE OFFER


100% company-paid Medical, Dental, and Vision for you and 75% for your dependents
Ownership via Stock Options
Flexible Time Off
Daily catered lunch
Free and discounted pizza!
The opportunity to work with an incredibly supportive team of thinkers and innovators

","Seattle, WA",Data Engineer,False
502,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
At Facebook, we have many opportunities to work with data each and every day. We are looking for talented Data Engineers to build systems that will support fast and thorough analysis of our infrastructure products. You will work cross-functionally to define and track metrics that are critical to measure the adoption and efficacy of these products. As the products evolve, they will trigger new business questions - you will leverage the state of the art to build systems that will answer these questions quickly and comprehensively.
RESPONSIBILITIES

Work with engineers, product managers and product analysts to understand their data needs.

Automate frequently requested analyses.

Evaluate and define critical business metrics and identify new levers to help move these metrics.

Design and evaluate A/B experiments.

Monitor key product metrics and identify root causes behind anomalies.

Build and analyze dashboards and reports.

Influence product teams through presentation of data-based recommendations.

Communicate state of business and experiment results to product teams.
MINIMUM QUALIFICATIONS

2+ years of Python development experience

2+ years of SQL (Hive, Oracle, or MySQL) experience
PREFERRED QUALIFICATIONS

2+ years experience with data visualization and data-mining

Experience analyzing data to identify deliverables, gaps and inconsistencies

Experience initiating and driving projects to completion with minimal guidance

Experience communicating the results of analyses","Menlo Park, CA","Data Engineer, Infra Product Analytics",False
503,"Mulligan Funding is a technology-driven business-financing company that caters to the unique financial needs of small to mid-sized businesses. Headquartered in San Diego, we have a passionate group of software developers, analysts, underwriters, and sales and marketing professionals. We have funded over $150 million and have helped drive business transformation nationwide.We are seeking an enthusiastic and curious Software Developer to join our team to develop and manage data pipelines, but we’re a new team with lots of opportunities to contribute to application design and infrastructure as well. In this role, you’ll work in a highly collaborative environment where communication with the Marketing, Data Science, and Backend Engineering teams is key to your day-to-day success.Responsibilities: Write reliable software to ingest, transform and distribute data into our internal applications.Automate current processes and add logging and monitoring.Build data access platform for key stakeholders and data scientists.Design, develop, and launch ETL processes between multiple systems.Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleansed and stored.Partner with Marketing and Data Science teams to deliver data solutions that meet internal requirements within agreed upon timelines.Scope points of failure and create alerts to ensure continual data availability, accuracy and cleanliness.Provide ad hoc queries and analysis as needed.Build and maintain reports/dashboards to meet the needs of internal users.Monitor and remediate data quality issues.Process semi-structured data into a form suitable for analysis.Qualifications: Strong organizational skills and attention to detailDemonstrated proactive approach to problem-solving with strong decision-making capabilityAbility to balance high-priority, long-term projects with immediate deadlines.Excellent verbal and written communication skillsAbility to work independently in a fast-paced, rapidly-changing entrepreneurial environmentRequirements: Bachelor’s degree in computer science, mathematics/statistics or closely related field.2+ years of experience working with back-end data systems to create and process clean data.2+ years of experience in developing ETL solutions.2+ years programming, manipulating/analyzing data in Python, Scala, or Java (Python preferred).Proficient in writing scripts with Python, Ruby, or Javascript (Python preferred).Familiarity with manipulating Dataframes and Pandas.Proficient in SQL, (currently SQL Server) with ability to write complex SQL for data manipulation.Experience maintaining and updating a data warehouse.Experience accessing data via Web Services and external/non-SQL datasets.Proficient with Data Visualization Tools (e.g. - Tableau or PowerBI).Experience with AWS or Azure products: EC2, RDS, AWS Data Pipeline, S3, RDS, Dynamo or Logic Apps, SQL Server, and Functions, etc.BenefitsCompetitive compensation packageGenerous bonus incentivesMedical, Vision and Dental Benefits401KPaid Sick, Vacation, and HolidaysAn entrepreneurial, fast-paced and growing environment, with the ability to contribute meaningfully to the success of the enterpriseMulligan Funding is an Equal Opportunity Employer (EOE) and takes great pride in building a diverse work environment. Qualified applicants are considered for employment without regard to age, race, religion, gender, national origin, sexual orientation, disability or veteran status.Job Type: Full-timeEducation:Bachelor's (Required)","San Diego, CA",Data Engineer,False
504,"ContractJob SummaryTitle: Big Data Engineer/Consultant/AdminLocation: Phoenix, AZ/ OhioClient: HCLDuration: ContractWe are looking for a candidate who can work under our payroll i.e W2.Responsibilities and DutiesMinimum 1 year of relevant experience into Big Data and related tools.Job Type: Contract","Phoenix, AZ",Big Data Engineer,False
505,"$100,000 - $145,000 a yearWe are a growing Data Engineering team looking to build up their team with strong full-stack data engineers. We are a leader in the financial news and information space, and need great engineers at the core of our business.

Come apply to us to learn more so we can tell you about this great opportunity!

What's the Job?

The Data Engineer will be joining our 6-person team here in Downtown Manhattan. The role focuses on three things:

1) Data Projects: You will be working on ETL, building data pipelines, data cleaning, ingestion, and working on everything to do with getting our data into our data lake.

2) Infrastructure: You will be helping to maintain the data infrastructure supporting all of our analytics. We use Spark clusters, Kubernetes, and AWS for example. You will be helping to build out, upgrade and maintain these systems.

3) Tech Liaison: We need you to be able to work with our data scientists and analysts to make sure we're providing them the data they need to do their jobs. You will be finding out what they need, then building out the systems they need.

What Skills Do We Need?


We are looking for true engineers. You should have good engineering fundamentals and open to working on different systems using the right tools.
You should have a strong Linux background.
Plus: If you have experience with cloud systems, data infrastructure, and/or data engineering.

Who are We?

We are a large company that has been around for decades providing news, information and insights to the financial industry. We are known for our work-life balance, stability and room for growth for our employees. We have offices around the world, and you will be working in our office in Downtown Manhattan.

Compensation


$100,000 - $145,000 (depending on experience)
Full Benefits (medical, dental, vision)
401(k) Plan with 3% Match
Generous Paid Time Off (PTO) Policy with 3 Weeks Vacation from the Beginning
Great work/life balance
Travel reimbursement

What's In It For You?

For true engineers looking to flex their engineering experience and ability with a growing division, this will be a great job for you. We will provide the tools, resources and direction to further your experience in the engineering space. Also, you get to work with large, interesting data.","New York, NY 10014 (West Village area)",Data Engineer,False
506,"You can't improve what you can't measure, so we've built the best-in-class analytics product for content marketers. We're looking for a strong data engineer to help us build and expand our analytics/data science ETL and API. This is a top initiative within the company and with our best in class talent network and content production platform we have a lot of interesting data to analyze - come help us guide the industry with new insights.

You should love working at a startup because it means driving initiatives with little supervision, solving tough, yet exciting problems each day, working with a passionate and smart team in an agile environment, and making our customers happy and successful.

What You'll Do:

Collaborate with product management, data scientists, and engineers to define and translate feature requirements into sound, creative software designs
Creatively solve problems by bringing a unique perspective
Produce solid, thoroughly tested features (including automated tests)
Plan and execute software releases using agile methods
Big data modeling
Build and expand our ETL using technologies like Apache Spark, Airflow, AWS Lambda, Kubeless, and AWS Kinesis
Work closely with data scientists and architect solutions that enable them to do their jobs.

Who You Are:

3 - 5+ years software engineering experience, ideally with Python or Ruby
Strong software engineering & architecture fundamentals
BS in Computer Science or equivalent experience
BDD & TDD proficient with test frameworks such as Python's NoseTests or Ruby's RSpec
Experience building RESTful APIs, ideally with Python or Ruby
Experience with Big Data technology – Apache Spark/Hadoop, Redshift, Elasticsearch, and more
Advanced working SQL knowledge and experience working with relational databases (Postgres, MySQL, Oracle, etc)
Experience in building infrastructure required for optimal extraction, transformation, and loading of data from various resources
You want to be part of a growing and entrepreneurial company with proven users and business model
You want to learn more, build your experience, and contribute within a fast-paced agile environment

Bonus:

You understand machine learning, NLP, and other advanced analytical techniques
Experience working with AWS and/or Google cloud technology

","New York, NY",Data Engineer,False
507,"Working as a member of our data, platform, and analystics team, Gladson is looking for a hands-on Data Engineer who will focus on designing and implementing optimal data engineering solutions to scale with unpredictable data patterns while maintaining and monitoring them. This is an individual contributor position, requiring the ability to take on complex requests and transform them into clean data solutions and integrating that data with the architecture used across the company. The role will be also be evaluating and implementating new tools and frameworks, or extending existing ones by leveraging the on-premise and cloud services in support of the company's data science, data warehousing and visualization initiatives.

This full time role will be based in Gladson's Chicago office. The interview process for this position will require the completion of a case study which will be sent to selected candidates after an initial phone screen with the recruiting team.

RESPONSIBILITIES
Design and implement end to end automated data pipelines from data ingestion to delivery while selecting and integrating tools and frameworks required to provide the requested capabilities as per business requirements.
Perform capacity planning required to create and maintain enterprise relational and NoSQL databases and processing demands while providing all facets of database administration to production, development and quality assurance systems.
Implementing ETL process and monitoring performance and advising any necessary infrastructure changes (defining data retention policies, database tuning parameters)
Designing and building self-improving software by leveraging machine learning techniques and technology at scale.
Application of modern data processing technology stacks, streaming data architectures and technologies for real-time and low-latency data processing.

SKILLS
Proficient understanding of distributed computing principles and integration of data from multiple data sources and formats
Leverage data mining, statistics, and machine learning to develop best-in-class analysis techniques & data visualizations that answer strategic client / category questions
Knowledge of various ETL techniques and frameworks, such as Flume, Talend, Spoon and various messaging systems, such as Kafka
Understanding of how to build solutions for data science and client delivery while productionizing any machine learning models and collaborating across various teams
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Experience with SQL & NoSQL databases, such as MSSQL, PostGres, MongoDB, Cassandra, Neo4J
Good understanding of cloud platforms ( GCP/AWS ) and Lambda Architecture, along with its advantages and drawbacks
Knowledge of High Availability (HA) and Disaster Recovery (DR) options.
QUALIFICATIONS
Bachelor's degree in Computer Science, mathematics, engineering, or equivalent.
4+ years in BI or data engineering, working with structured and unstructured data formats along with batch and streaming framework design & implementation.
Demonstrated knowledge and experience with software development and deployment in on-premise and cloud environments.
Strong consultative skills to establish relationships across the broader organization
Comfort communicating and interacting with cross functional teams, as well as understanding and translating the science of data to a more general audience
Desire to ""roll up the sleeves"" and get into the heart of the business
Demonstrable ability to work across a global matrix and ability to put strong communication, leadership and influencing skills to work to be successful","Chicago, IL 60604 (Loop area)",Data Engineer,False
508,"Role: Data Engineer/Data Scientist/ Business Data AnalystLocation: Corte Madera, CADuration: 6 MonthsSkills:SQL, Python and RJob Type: Full-timeExperience:java: 7 years (Required)","Corte Madera, CA",Looking for Data Engineer/ Data Scientist at CA,False
509,"InternshipPurposes
This is a 40 hour per week internship that is expected to last approximately 4 months. Internships will be for winter (starts in January) or for summer (starts in May) of 2019. Normal office hours are between 6:30am-5:30pm, Monday through Friday. Get experience supporting systems and delivering services to users at Church Headquarters and millions of users throughout the world. This internship position represents an unusual and exciting opportunity to work for one of the largest Information Technology centers in the Wasatch Front. This individual works with divine guidance to provide or support technology that furthers the mission of the Church and reflects the eternal impact of the gospel.
This position will be located in Riverton, UT.
Responsibilities
Works with teams to build process modelsCompile reports on implementation and dataMonitors processes and makes recommendations for efficiency and accuracyCraft management reports to communicate statusSeeks opportunities to improve processesGood organizational skillsAttention to detail
Benefits:Work with state-of-the art tools to help develop enterprise solutionsMentor with highly experienced IT professionalsWe Hasten the Lord's Work in an important wayGreat payBe a member of a creative, spiritual, and highly motivated team and cultureYou will gain practical experience
Qualifications
We are looking for an upbeat and dedicated individual who loves to work on new technologies and is capable of working independently or in group settings.You should be currently enrolled in an accredited college or universityInternship is for current temple worthy members of the Church of Jesus Christ of Latter-day SaintsSolid business presentation skills (articulates technical concepts clearly - visually and verbally)Technology background is highly preferredExcellent communication skills for interacting with and providing information to management levels will be neededAptitude for database management technologies such as relational databases (Oracle, SQL, and MySQL), noSQL databases (MongoDB, Cassandra, MarkLogic, and data processing (SQL, Hadoop).Aptitude to learn business intelligence tools (Business Objects, Crystal, Corda).Strong aptitude for understanding and associating data.Ability to effectively communicate information with data.Ability and real passion for analyzing large amounts of information
Worthiness Qualification
Must be a member of The Church of Jesus Christ of Latter-day Saints and currently temple worthy.
Posting Notice/More Info.
Please Note: All positions are subject to close without notice.
Find out more about the many benefits of Church Employment at http://careers.lds.org.","Riverton, UT 84065",2019 Data Engineer or Database Engineer Internship,False
510,"What You'll Do
You will be focused on enabling a data-driven approach to optimization by sourcing, maintaining and ensuring the availability of data used to drive marketing insights to optimize Cisco's marketing investments. You'll integrate both batch and streaming approaches to support standard business intelligence, as well as decision automation and machine learning requirements.
You will Provide leadership and support for the development of an integrated digital marketing data foundation that will enable extensive business intelligence and machine learning for digital marketing and extended user communities.
You'll focus on data comprised of digital marketing and other supporting data (e.g., weblogs, sales-related data, customer and contact data, social data, third party purchase data, etc.)
You will assist in planning, designing, building, and documentation of Digital Marketing Data Foundation
You'll work with Digital Marketing business and IT teams to ensure high quality, on-time deliverables that meet usability, scalability, quality and performance standards.
You'll provide hands-on technical support for development, research, and quality assurance testing. Dedication to performing due diligence checks to ensure quality.
You will support change management efforts, including proactive communication with other teams and users.
Who You'll Work With
Cisco’s Digital Marketing organization will lead the next evolution in customer experience through the promotion of Cisco products and software through digital channels (web, social, media, mobile, direct mail, point of sale, etc..). The organization builds omni-channel, personalized, real-time customer experiences that engage and inform Cisco customers and accelerate the purchase process. The team has the unique opportunity to shape, influence, and craft digital experiences that build the Cisco digital brand and accelerate Cisco business.

Who You Are
You have a Bachelor's Degree and 5+ years of relevant work experience. You have previous experience with Real-Time Streaming Architectures such as Lambda Architecture using Apache Kafka, Solr. You have experience building Data Pipelines, with hands-on development of scripts using various programming languages including SQL, Python, Unix Shell, HQL, Spark Programming, Java. You possess a strong understanding and experience with structured/unstructured database environments (e.g., Oracle) and Big Data environments (e.g., SAP HANA, HADOOP). You are very familiar with Microsoft products (Excel, Word, PowerPoint, Access).

Desired Skills:
Dashboards & visualization tools (specifically, DOMO), and underlying tool data formats
Statistical analysis products, specifically R, Python.
Prior experience with marketing and sales systems (e.g., SFDC, Eloqua, etc.)
“Can do” / “Get it done” / “Let’s just do it” attitude to run projects end-to-end
Good understanding of digital marketing and systems concepts, processes and data
Accountability mindset taking responsibility for work and deliverables
Strong verbal and written communication skills
Why Cisco
At Cisco, each person brings their unique talents to work as a team and make a difference. Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.
We connect everything – people, process, data and things – and we use those connections to change our world for the better.
We innovate everywhere - From launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.
We benefit everyone - We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.
Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us!
#WeAreCisco","San Jose, CA",Data Engineer - Marketing,False
511,"WHAT CAN YOU TELL ME ABOUT THIS POSITION?

The Hartford’s Enterprise Data Office - Business Intelligence & Analytics Enablement team is seeking a Business Intelligence Associate to work closely with our businesses and technical partners across the enterprise to enable the implementation of end to end Business Intelligence solutions. In this full-time position, the Associate will help drive Community engagement, train customers and provide technical support, assist with Proof of Concept evaluations using modern self-service BI tools, and act as a BI&A subject matter expert.

This role will provide the opportunity to research, learn and master the Business Intelligence + Analytics tools we support as they engage with various members of our community. We regularly participate in evaluations of new tools, which means that we are often some of the first people to use them and become the experts to roll it out across the company.

The ideal candidate will have strong customer service skills, technical aptitude, natural curiosity, a willingness to learn and work towards making technical concepts understandable to business users.

The candidate will need to think critically to tackle complex challenges, thrive in a fast-paced, dynamic environment and help drive a culture of delivering actionable data insights rapidly in a self-service manner.

RESPONSIBILITIES:
Provide resident ‘super user’ BI&A tool expertise
Train and advise analysts in areas including:
Tableau development and visualization best practices
Building data wrangling workflows
Tools can include but are not limited to Alteryx, Trifacta, etc.
Assist with Community Engagement activities
Consult with business customers to develop strategies and governance practices that help move the business forward
Provide process and technical BI&A solutions to the lines of business to drive best in class usage of the appropriate tools
Own the creation of enterprise training materials for supported tools
Provide line of business technical support to expedite the effective usage of tools
Identify connection points across lines of business BI teams to drive enterprise problem solving
Assist with the demand intake process and work queue
Design, build, and maintain internal reports, processes, and analyses with a variety of business intelligence technologies (e.g. Tableau, MSBI, Business Objects, etc.)
Ongoing assessment of the enterprise Business Intelligence & Analytics (BI&A) landscape
Partner with various members of the BI&A community to understand and document tool usage, total cost, pain points, value achieved through modern BI processes and tools
Assist with new self-service tool Proof of Concepts and demos for business partners
Assist with evaluation and rollout of new BI monitoring and/or self-service tools
Evangelize modern BI execution and tool strategies to create a data driven culture
Perform other job-related duties as assigned
Qualifications
WHAT ARE THE QUALIFICATIONS?
0-2 years of hands on experience with business intelligence and analytics tools such as Tableau, Alteryx, Qlikview, IBM Cognos, Microsoft Business Intelligence (SSIS, SSAS, SSRS), SAS, Business Objects, R, R-Shiny, Python, etc.
Knowledge of data warehouse design and usage
Experience with agile BI project delivery methodology desirable
Intermediate level SQL skills highly desirable
Training material creation and delivery
Natural curiosity with a strong desire to learn, maintain, and apply knowledge of emerging BI&A tools, strategy, methodology, and general best practices
Strong team player that has a direct approach and is solution oriented
Comfortable working independently and collaboratively in an ambiguous environment on multiple concurrent projects
Highly analytical person with exceptional conceptual thinking skills equally comfortable coding and interacting with business partners
Outstanding verbal and written communication skills, with an ability to express complex technical concepts in business terms to effectively interface with all levels of management.
Entrepreneurial spirit - self-motivated, strong sense of ownership/accountability, and results oriented with the ability to manage time and schedules effectively
Customer focus: Strives to give customers the best service and takes the initiative to add value
Property & Casualty insurance industry experience a plus
What Else Can You Tell Me?
The Hartford is committed to the education and growth of our Information Technology Professionals. A number of IT Certifications are available to enhance your career and growth potential. IT Professionals at The Hartford may qualify for a stipend up to $1000 per year for additional certifications

Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age

Job Function
: Data Engineering
Primary Location
: United States-Connecticut-Hartford
Schedule
: Full-time
Job Level
: Individual Contributor
Education Level
: Bachelor's Degree (±16 years)
Job Type
: Standard
Shift
: Day Job
Employee Status
: Regular
Overtime Status
: Exempt
Travel
: No
Job Posting
: Sep 27, 2018, 5:38:55 PM
Remote Worker Option : No","Hartford, CT",Business Intelligence Data Engineer,False
512,"This position holder will be responsible to purchase price change including info record, configurable parts pricing table and PO line item in SAP and give the pre-alert on price increase. He / She monitors supplier material data creation and change are follow the process and ensures the price accuracy in SAP.
Supplier master data creation for new suppliers
Purchase Mater Data (Info Record) maintenance
PO price adjustment for configurable parts
PO line item price adjustment for standard parts
Monitoring purchase price and PO price increase and make sure each increase has been approved through process
Providing analysis, recommendation and developing compliance reporting to support Purchase Order integrity

Mobility needs you
Requirements:
Bachelor’s degree
3+ years of SAP, especially MM martial management module and ERP experience required.
3+ years of elevator engineering experience required.
3+ years of Data Analytics and Big Data Management experience required.
Strong communication and interpersonal skills.","Morristown, NJ",SAP Purchase Master Data Engineer,False
513,"Company Overview

Mosaic is building upon its success in solar and expanding into home improvement, making it easy by leveraging technology and financial innovation, with a goal of providing access to clean energy for everyone. We are looking to collaborate with passionate, thoughtful people who want to make a real social impact while working to solve climate change. Come join our team centrally located in beautiful downtown Oakland and help build the movement towards 100% clean energy for all.

The Opportunity

To complement our rapid growth, we are actively seeking a bright and talented Software Data Engineer to join our growing Data Engineering team. In this role, you will work with a team of outstanding Data Engineers impacting the growth of renewable energy through building e-commerce and financial services applications.

Your day-to-day


Design and build robust data pipelines using scripting in SPARK, Airflow, Python and SQL.
Design data warehouse/data marts in AWS Redshift and other databases as appropriate.
Use Optimization techniques in data load and query processing
Validate and build audit, balance, and control of mission-critical data pipelines
Develop cool viz using Tableau and other open source Viz tools as needed
Identify best data sources among multiple sources to use for data pipelines to improve trust in data
Fix bugs, work collaboratively with team members

What you bring to the team


Masters or equivalent in CS/Engineering or another comparable discipline
You have at least 6 years of technical experience and strong data warehouse & data modeling skills
Very strong skills in Python, SQL, SPARK, Redshift, Airflow, AWS
Familiarity with Agile methods (we use agile tools)
Experience with reporting tools like Tableau is a plus.
Team player, agile, highly accountable, curious, willing to learn, implement and teach
Ability to juggle multiple responsibilities and deliver to timelines
Experience in the consumer lending industry required

Bonus Points


Experience with open source tools such as Kafka is a plus
Experience in any JVM based language

Why Mosaic


As a customer focused and driven-to-win organization, there are many exciting reasons to join the Mosaic team. We provide competitive salaries, quality healthcare and an enjoyable, be-yourself office environment. We are deeply mission and vision driven, have unlimited vacation days, and support flexible schedules when needed. Mosaic has a dynamic, fast-paced, and entrepreneurial environment, which requires a professional, flexible, self-starter attitude. We believe in hiring the best, the brightest, and cultivating a culture of collaboration and appreciation.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

","Oakland, CA",Software Data Engineer,False
514,"Who is Blueprint?
Blueprint Technologies is a group of solution minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?
Blueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.

Why you want to be a part of Blueprint?
We are innovators. Motivators. Thought provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.

Blueprint is looking for Data Engineer to join us as we build cutting-edge technology solutions!

Qualifications and Skills Required:

5+ years of demonstrated data engineering experience
3+ years of experience with Big Data Technologies like Hadoop or Hive
Advanced knowledge and expertise with Data modelling skills, Advanced SQL with Oracle, MySQL, and Columnar Databases
3+ years' experience in custom ETL design, implementation and maintenance
Proficient designing and implementing data models and data integration
Experienced in all aspects of Power BI, including establishing gateways, use of embedding, DAX and M
Hands-on experience deploying the PBI service to mid-market and/or enterprise scale organizations
Experienced deploying Azure SQL Database, Azure Data Factory and well-acquainted with other Azure services including Azure Data Lake and Azure ML
Experience implementing REST API calls and authentication
Experienced working with agile project management methodologies

","Bellevue, WA",Data Engineer,False
515,"$70 an hourTemporary, ContractExp : 10 + yrsAll Visa AcceptedDuration : Long termExperience : 10 + yrsPosition: Full time /C2CAWS cloud/Aurora/ElasticSearch/Redshift/Athena.Should have expertise engineering(design,develop,ETL,Performance tuning) the data solution in the above said data platforms in AWS.Job Types: Full-time, Temporary, ContractSalary: $70.00 /hour","St. Louis, MO",AWS Data Engineer,False
516,"Mercy Ships is currently looking for a full-time Data Engineer in Lindale, TX. If you are interested in making a difference with an organization that impacts and changes lives around the world, please put your official application in at careers.mercyships.orgMercy Ships is an international faith-based organization based in Lindale, Texas that provides life changing medical procedures to people across the globe through our hospital ships and volunteers are ordinary people working together to achieve extraordinary things! .Summary DescriptionResponsible for supporting Mercy Ships departments in their data modeling, warehousing and analysis needs. Also assists in the execution of Mercy Ships' Master Data Management strategy. This includes: data analysis product support and training, knowledge worker support, data source identification and classification, data inventory maintenance, supporting departments in data quality efforts, and application of data governance policy, process and procedures.Essential Duties and Responsibilities (Include But Not Limited To)Assists and works with IS staff and management to develop models and processes for the management and leveraging of Mercy Ships data assetsSupports departments in the use of their data and development of insights by providing expert knowledge in data analytics and analytics productsWork with data owners to determine, develop and support data analysis goals and requirementsWork with data owners and consumers to develop and document the master data inventoryWork with the Solutions Development team in the design and development of data components in technical solutionsWork with the Enterprise Architect to identify data models and activities that advance the established and desired future state of Mercy Ships' enterprise architectureWork with the Information Security Director in providing input to and execution of data governance policy, procedure and standardsCollaborate with departments and other IS staff as needed in the development and execution of Mercy Ships' master data management strategyPrepare, administer and deliver data analysis trainingAdvise on issues pertaining to the effectiveness of data architecture, integration and data managementResearch and advise on emerging data management and analysis trends, products and methodologiesQualifications (Knowledge, Skills, Abilities, and Requirements)Strong analytical and organizational skillsStrong customer service skillsDetail-oriented and accurateTechnical skillsets: data architecture principles and methodologies; Structured Query Language (SQL); data analysis tools and techniques; data modeling tools and techniques; knowledge of relational database management systems (RDBMS), flat file data, object orientation and document database management systems; knowledge of data integration and exchange technologies and formats; familiarity with application architectures and interfacesExcellent listening, interpersonal, written, and oral communication skillsHighly self-motivated and directedAbility to effectively prioritize and execute tasks while under pressureExperience working in a team-oriented, collaborative environmentSupportive of Mercy Ships mission and vision, and committed to its core valuesUnderstand and apply servant leadership, work collaboratively with integrity and demonstrate accountabilityEducation and ExperienceBachelor's degree in the field of business administration, computer science, or information systems, or the equivalent combination of education and experienceExcellent understanding and at least 3 years hands-on experience in data or database management principles, practices and processesSuccessful completion of Mercy Ships Entry Training will be required within the first yearWHY MERCY SHIPS?Mercy Ships provides resources and events that promote the feeling of family and community.Our base located on 400 acres of scenic East Texas land offers on campus dining, a specialty coffee shop, indoor basketball and racquetball courts, weight room, walking and biking trails, a disk golf course, and an outdoor pool for you and your family.Paid time off, holiday benefits and health and life insuranceJob Type: Full-timeExperience:Customer Service: 3 years (Preferred)SQL: 3 years (Preferred)Education:Bachelor's (Preferred)","Lindale, TX",Data Engineer,False
517,"2DA Analytics is a early stage software company. We build analytics that are used to optimize how energy commodities are delivered to the end user. But it is more than that - our mission is to evolve how humans interact with data. We believe that analytics is about uncovering the story hidden in the data.

We are looking for a stellar Data Engineer to join the 2DA team. This is the role that is responsible for giving voice to the data so the world can hear it too. The ideal candidate will, in addition to having curiosity for learning, have history of being curious, pragmatic, and action-oriented, and being an highly skilled in the business or research domains you work in.

We’re big fans of hiring people who are not just great at what they do but also have a flair for how they do it. Critical to our culture is building and maintaining a team that works well together and knows how to communicate effectively - not just within their own team, but also across peripheral teams.

2DA Analytics are looking for someone who embodies the best parts of open-source culture: humility, open-mindedness, honesty, and respect for others. A developer who doesn’t try to single-handedly save the day but embraces input and collaboration as a means to find the best answer.

Your success in this role will be predicated on your ability to prioritize your work, be self-motivated and a self-starter, to speak up early and often, and to work well with others. You should be passionate about building great software, and about building a product that is slated to revolutionize the world of data analytics in commodities. We’re great at encouraging our people to learn different technologies, continue their professional growth, and try out new ways of doing things. We’re in it for the long-haul, and you should be too.

Our office is located in East Austin.
What you'll do:
Own and mature back-end data ingestion for ~20 data sources in our back-end data store
Maintain and evolve our Data Science & Engineering footprint in AWS
Add new customer accounts
Own maintenance and change requests
Deliver data to our front-end data store
What you have:
2+ years building and maintaining data pipelines
2+ years experience with SQL
2+ years of experience with Python
Working knowledge of AWS, and pinch-hitting in a DevOps role is a huge plus
A history of being an highly skilled in the business or research domains you work in
A history of being curious, pragmatic, and action-oriented
Experience in multiple roles (including internships) is a huge plus
How you work:
Strong problem solving and communication skills
Consults with colleagues and recommends solutions based on the best interests of customers and shareholders, even when the opinion is unpopular
Offers time and assistance to colleagues; is widely regarded and respected as someone to go to for help.
Fosters friendly and cooperative relationships with others; colleagues enjoy working with the employee
Shares knowledge and mentors staff; works to develop others’ knowledge as well as own
Gives and shares credit as appropriate
Contributes to recruiting and training efforts of others, including candidate referrals, job interviews and mentoring
Attends and participates in company and department level functions inside and outside of the office
Demonstrates an understanding of the urgent nature of our business and the need to proactively find and fix problems quickly and effectively takes ownership and follows through on decisions; doesn’t minimize or forget about problems
Determines and fixes root causes rather than just treating symptoms
Applies novel techniques to solve problems that are more ambiguous; challenges long-held beliefs when necessary
Quickly and effectively identifies alternative solutions and the pros/cons of each; confirms that a proposed solution has no unintended consequences
Takes steps to mitigate risk
What you'll get:
Fun, casual, fast-paced work environment filled with talented colleagues
Flexible paid time off
Flexible working options
Competitive salary & stock options","Austin, TX",Data Engineer,False
518,"InternshipCompany Name: Kroger General Office
Position Type: Intern
FLSA Status: Non-Exempt
Line of Business:
See what life is like at Kroger Technology
at https://www.kroger.com/livekt

Additional Technology Information:

Position Summary:
As an intern, you will work on innovative and challenging projects that will help drive the technical landscape of the grocery industry. You may work in a development, analytical, agile, infrastructure or digital environment to gain exposure to the different technology areas and continue developing the leadership and business skills needed to enhance your technology career. Demonstrate the company’s core values of respect, honesty, integrity, diversity, inclusion and safety.
Essential Job Functions:
Complete assigned projects and tasks related to specific technology initiatives
Partner with technology and business associates to assist with various projects to implement technical and corporate strategies
Maintain and follow operational procedures and processes
Participate in weekly/monthly department meetings
Assist in working on new technology platforms that are revolutionizing the retail industry
Engage with peers on committees that include philanthropy, branding and community events
Participate in intern programs that support technical and leadership development within the organization
Must be able to perform the essential functions of this position with or without reasonable accommodation
Minimum Position Qualifications/Education:
Pursuing an associates, bachelor’s or graduate degree at an accredited institution
Cumulative GPA of 3.0 or higher (official transcripts available upon request)
Strong written and oral communication skills
Ability to communicate and present information to all levels of the organization
Ability to work within a team environment
Desired Previous Job Experience/Education:
Pursuing a technical or business degree
Manage multiple projects with competing priorities
Involvement in leadership and community activities
Education Level: Other
Required Certifications/Licenses: None
Position Type: Intern
Shift(s): [[mfield4]]
States: Ohio
Keywords:

Jobs at Kroger: At Kroger, we hire people who have a passion for helping others and who want to build a relationship with our Customers. No matter what stage of your career, you can build your future at Kroger. We look for people who want more, aspire to be more and work hard to achieve their goals. Our focus on keeping the Customer first is what makes us successful. As the largest traditional grocery chain in the U.S. and one of the world's largest retailers, we employee nearly half a million Associates across 35 states. We offer many opportunities not only in our stores, but in Manufacturing, Logistics, Marketing, Finance, Human Resources, and many other fields.

Company Overview
Kroger Family of Companies employs nearly half a million associates who serve customers in 2,782 retail food stores under a variety of local banner names in 35 states. Our Family of Companies also operates 2,268 pharmacies, 274 fine jewelry stores, 1,489 supermarket fuel centers and 38 food production plants in the United States. Kroger is dedicated to our Purpose: to Feed the Human Spirit™ by serving America through food inspiration and uplift and creating #ZeroHungerZeroWaste communities by 2025. Careers with The Kroger Co. and our family of companies offer competitive wages, flexible schedules, benefits and room for advancement.","Blue Ash, OH 45242",Data Engineer Co-Op/Internship Spring 2019,False
519,"ContractJob SummaryRequired Experience:Strong data warehouse experience with various technologies* Strong knowledge of creating data warehouse in public cloud is required* Strong database/warehouse migration to public cloud.* Strong GCP Data background (BigTable, BigQuery, CloudSQL)* Preferred GCP Data Engineer certification* Have traditional database background with migrationMust HaveGCP Data (BigTable, BigQuery, CloudSQL)Job Type: ContractExperience:Google Cloud: 5 years (Required)","Irvine, CA",Google Cloud Data Engineer,False
520,$70 - $80 an hourContractShould have good experience on below skillsData WarehouseData ModelingData AnalyticsHiveBig-QuerySQLPython ProgrammingJob Type: ContractSalary: $70.00 to $80.00 /hourExperience:Python: 1 year (Preferred)SQL: 1 year (Preferred)Hive: 1 year (Preferred),"Atlanta, GA",Data Engineer,False
521,"Job Classification:
software engineers

Location: Iselin(New Jersey)

Skills:
BIG DATA

Description:

Phone & Skype!!

Sr. Big Data Engineer

Must have a minimum of 10 years in IT and 5 years in Big Data. Very Senior Position.

Senior Big Data Engineer/Architect, who will
Work under the direction of RCG Big Data ArchitectsHave hands-on experience with most of the skills listed belowHave the ability to take ownership and accountability for work assignedHave the ability to work independently but not hesitate to ask for help when neededBe proactive in recognizing and raising issues earlyHave a good client interface, with IT (mandatory) and business (preferably) audiencesHave excellent communication skills

Must-have Skills: proven hands-on experience with
Data Platforms
o Cloudera – setup, install, configure, use and administer all components of the ecosystem
o AWS – especially Kinesis, Glue, S3, EMR, EC2, DynamoDB, Redshift, Elasticsearch, Athena
Data Ingestion, Processing & Analytics
o Contemporary (Big Data-friendly) ETL/ELT tools like Talend, StreamSets or equivalent
o Scala/Python/Java on Spark
o Framework-based, metadata-driven implementations

Nice-to-have Skills
Data preparation: Trifacta, Talend Data Prep, or equivalentBusiness Intelligence: Big Data-friendly (e.g. Zoomdata) or traditional (e.g. MicroStrategy)Industry/domain knowledge: Financial Services, especially investment management","Iselin, NJ",Big Data,False
522,"About Us
VideoAmp’s mission is to bridge the gap between television and digital advertising with campaign measurement, planning and audience-targeting solutions that dramatically improve the performance and cost-efficiency of advertising investment. Our engineers, data scientists, designers and media strategists are inspired by tackling one of the most challenging problems in media and marketing — bridging the divide between TV and digital media.

Responsibilities
We're looking for a self-driven software engineer with experience working with the latest ETL frameworks who is eager to tackle new problems in a data driven environment. As a Data Engineer at VideoAmp, you will be working with a wide variety of datasets including TV viewership data and digital advertising data to help customers optimize their marketing spend.

Technology We Use
Python, Scala Spark, SQL, Airflow, Hive, HDFS, and other technologies to support an ETL pipeline, data warehouse, and reporting platform.
Required Skills
Experience working with Python or Scala
A solid and demonstrable understanding of ETL workflows, data warehousing, and big data principles
A solid understanding of NoSQL datastores
SQL fluency and an understanding of relational data models

Responsibilities
Designing and building out new data pipelines
Analyze, scrub, and integrate third party data
Building new, and scaling out existing ETL applications
Collaborating with Data Scientists and productionalizing various Data Science Models
Coordinating data models with other engineering teams
Perks
Top compensation
Comprehensive health benefits
Meaningful equity
401k
Unlimited vacation with a stipend for travel and accommodations of $2000/year
Unlimited in-office gym use with personal trainer
Childcare stipend
Plenty of snacks and beverages plus dinner every day
Corporate support for Hackathons, lunch and learns, and attending conferences
Personal and professional development","Santa Monica, CA",Data Engineer,False
523,"ContractUrgent - Data Engineer Sunnyvale, CA - 24 Months(In person interview - local candidates only)Job Description Job OverviewWe are looking for a savvy Data Engineer for our team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.ResponsibilitiesCreate and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies.Build data pipeline on premise and on Google Cloud Platform.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Product, Data and Design teams to assist with datarelated technical issues and support their data infrastructure needs.Work with data and analytics experts to strive for greater functionality in our data systems.QualificationsAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured data sets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with Google Cloud Platform esp. Google Pub-Sub, Big Query, Data Proc, Data Flow, Cloud Storage.Experience with IoT & Time series data. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, ScalaPlease send your resume to anu at intersourcesinc dot comJob Type: ContractEducation:Bachelor's (Required)","Fremont, CA",Data Engineer,False
524,"Data Engineer

Location: Culver City, CA

The Ipsos Science Center is a uniquely positioned group within Ipsos. We develop software tools and perform analyses both for clients and internal teams, respond to advanced analytic requests, and consult on a broad range of statistical and behavior science best practices. Working in a collaborative and supportive environment, we seek to expand what is possible in market research with data science.

We’re looking for a highly capable, data-literate person. We need someone who is a better statistician than most programmers and a better programmer than most statisticians. We are looking for someone with the communication skills to suggest analysis appropriate for a problem, develop a prototype, polish it if it works, stand up an API if it’s worth distributing, build visuals on top as needed, and work through the challenges along the way. We’re looking for a detail-oriented developer who has the flexibility, capability, and curiosity to collaboratively work across the process. In short, we’re looking for a full-stack engineer for the analytic stack.

As a Data Engineer, you will:

Enhance and maintain existing tools used by the larger organization as well as the local team.
Design and develop new tools for internal users and external clients.
Work closely with team members on ad-hoc projects.
Collaborate on researching, implementing, and testing new algorithms and approaches.


Required Skills
Requirements:

Proficiency in advanced R and/or Python programming, including the ability to develop packages/libraries and maintain existing ones.
Bachelor’s degree in Computer Science, Mathematics, Statistics, or related.
Comfortable working in a highly collaborative, consensus-oriented environment.
High-level familiarity with analytics and statistics, extracting and surfacing value from quantitative data.
Strong eye for detail and excellent time-management skills.


Pluses:

Professional or academic experience with analytics or machine learning, e.g. caret, scikit-learn.
Experience creating and deploying web applications, e.g. flask, web2py.
Experience designing and developing static and dynamic visualizations, e.g. d3.js, ggplot2
Working knowledge of SQL and experience with database design and administration.
Experience with Linux server and system administration.
Experience with collaboration tools (e.g. Atlassian suite) and version control systems (e.g. Git).
Experience integrating R and/or Python with each other and C\C++.
Large dataset manipulation. Experience in distributed storage and computing.
Advanced degree (M.S.), but by no means required.
Experience in the field of Market Research.



Required Experience","Culver City, CA",Data Engineer - Science Center,False
525,"Beeswax, named one of Business Insider's ""hottest pre-IPO startups"" is hiring top engineering talent for its growing New York office. Beeswax is a high scale, high availability digital advertising platform founded by executives from Google and funded by leading VCs including RRE and Foundry Group. We aim to offer the most extensible and transparent advertising system, servicing technology enabled clients and executing and processing billions of events every day.

The Beeswax engineering team is a top-notch group with backgrounds from Google, Amazon, Oracle and other premier technical teams. Because digital advertising operates at an extremely high scale (millions of transactions per second) and low latency, the opportunity to learn about web-scale distributed systems and hard scaling problems is a great advantage of our engineering culture and work.

We are looking for a Data Engineer to build clean pipelines and maintain data products that our customers rely on.

Our products are built on a variety of technologies including C++, Java, Python and LEMP stack (PHP, MySQL, nginx), and while our engineers typically specialize in one area, they need to coordinate and integrate with a wide variety of systems, including homegrown and AWS-native services.

This position will report into our Chief Data Scientist, Sergei Izrailev. Sergei brings over a decade of hands-on experience in data science, software engineering and database design utilizing the best in breed tools and technologies.

Responsibilities:

Code in a variety of languages, primarily Python, Java and/or C++
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Work cross functionally with Product, Ops and Engineering counterparts
Participation and collaboration from inception to deployment

Requirements:

MS in Computer Science, Math, related technical field or equivalent practical experience
4+ years of general software programming experience in Java, C/C++, Python and SQL
Large systems software design and development experience, with knowledge of Unix/Linux
Knowledge of database technology, schema design, and query optimization techniques
Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills

Preferred Qualifications:

PhD in Computer Science, Mathematics, or related technical field
Familiarity with open source cloud and application platforms, AWS development experience
Experience with big data technologies such as Spark, Hive, Presto and Impala.
Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza
Hands-on experience working in SOA and high throughput environments

About You:

You are passionate about learning, mentoring and building a world class team and culture, while constantly empowering others around you
You take a second to step back and look at the big picture before diving in head first
You care about the quality of the data flowing through your code as much as about the quality of the code
Not afraid to take risks, voice opinions or ideas that help build the next generation of data platforms all within a massive distributed system
You're the type to peel back the layers and use non-conventional means to solve the task at hand.

","New York, NY",Data Engineer,False
526,"The Schoolzilla Mission: Empower People to Use Data to Increase Student Success

We want to change millions of students’ lives by enabling people to use data to run great schools. Teachers and school leaders need lots of data to make good decisions for their students, but most of them can’t get that data in any kind of useful, actionable format. Schoolzilla's team of developers, data visualizers, seasoned educators and K-12 experts have done just that: we've made data easy to find, understand, and act on for school districts everywhere. We've already helped thousands of schools make better, faster decisions with our platform.

We are a Public Benefit Corporation, and have in our bylaws our social mission to enable people to use data to improve educational outcomes for students, especially students from underserved communities. Every day we live our values: Driven by Mission, Better Together, Teammates Matter, Equity At The Center, and Intellectual Humilarity.

How You Can Help

Do you want to lead the charge in crafting delightful data experiences that have real impact in schools and classrooms across the country? Do you thrive on scaling for the next million students?

The product engineering team is responsible for nearly all of the Mosaic product experience, from the user experience to the transformation of all the datasets used for analysis. As a Lead Data Engineer you'd be at the forefront of our product innovation.
Responsibilities
Design modern data architectures and end-to-end data transformation solutions
Build data structures, pipelines, and tools that enable robust analytics, insights, and personalization
Collaborate with cross-functional teams to understand how data is used within Schoolzilla and by our users
Define, execute and release new services and features
Help shape the internal engineering and product roadmaps and inform prioritization of data engineering projects
Identify, design, and implement internal data process improvements, including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability
Provide technical leadership and mentor fellow engineers
Skills & Characteristics
Experience with one or more major object-oriented programming languages (Python preferred)
Experience building enterprise-scale data pipelines that are both efficient and intuitive
Advanced SQL knowledge and proficiency with major relational databases, particularly MS SQL Server and Postgres
Mission-Driven: You feel a deep sense of ownership for your work, and a relentless desire to deliver better results. You are passionate about solving problems for our users.
A Lifelong Learner: You love learning new things. You're curious and ask good questions. You solicit feedback from others, accept it with grace, and act on it
Flexible: You are comfortable with technical ambiguity and rapid iteration
Bonus Points For
K-12 education experience
Familiarity with stream-processing technologies like Spark, Kafka, and Flink
Experience leading Agile development teams
Schoolzilla is an equal opportunity employer. We are committed to building a team and workplace that reflect the communities we serve. We especially encourage people who are underrepresented in the tech industry to apply, and welcome your application even if you do not meet every one of the above requirements.

Schoolzilla does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We have an office in Oakland, CA, and are also remote-friendly. Candidates must be legally eligible to work in the United States.",United States,Lead Data Engineer,False
527,"Job Description

The City of Philadelphia shares hundreds of datasets with the public and between departments. Open Data - opendataphilly.org. Metadata – metadata.phila.gov As a result, residents can see if there’s been an uptick in vehicle break-ins in their neighborhood. Civic associations can lookup landlords and see if they have violations before supporting their zoning variance. Voters can trust that campaign finance reports and city contracts are posted online for anyone to see.

Data Engineering at the City is an adventure: navigating a diverse technology portfolio, understanding what the data represents and its various caveats, removing sensitive information, and presenting it to the public in a way that’s easily understood. Because most of the City’s data can be attributed by location, experience with SQL Spatial and spatial Python libraries is a must.

We have begun building data pipelines using Python based, in-house open source software, and the Data Engineer will help expand or refactor this work. These pipelines connect various systems of record (business systems and semi-structured data), a centralized Oracle based RDBMS, and SaaS-based environments including ArcGIS Online and Carto.

Beyond being a key asset to enterprise integration and open data initiatives, the Data Engineer will help develop and support the City’s Address Information System – a cloud-based, address search API leveraging Databridge that enables developers to build applications that pull a myriad of the City’s information.

Essential Functions

Help City departments share data with other departments and with the public by:

 Working with business partners of varying technical ability to understand how their data is produced, stored, and updated
 Setting up automated extract, transform, and load (ETL) workflows
 Improving the “platform” we use to share data (reusable components, scheduling, logging, centralized storage, etc.) so that workflows are easier to write and maintain

Other duties as assigned.

Competencies, Knowledge, Skills and Abilities

Ability to
 Write clearly and concisely with organized structure
 Reason about and work with data from across a variety of potentially unfamiliar domains
 Write descriptive, technical policies and/or documentation of ETL workflows
 Communicate technical nuances in plain language with partners of varying technical background
 Apply creative problem-solving when considering how to best address technical challenges
 Advance several workflows at a time for different department datasets

Knowledge of
 Python, with demonstrated experience in spatial and non-spatial ETL
 SQL Spatial
 Databases, Oracle preferred. Write queries & views in RDBMS including Oracle and Postgres
 Ability to maintain clean and secure data environments
 Working in a hybrid environment (on-premises and Amazon Web Services)
 GitHub
 Spatial data formats (Esri SDE, GeoJSON)
 Minimal Linux server administration

Qualifications

At least 3 years experience in any of the following areas preferred
 Developing software
 Data integration
 Architecting and maintaining relational databases
 Moving a variety of data types (ie. CSV, fixed width, GeoDatabase, JSON) between a variety of systems (ie. FTP server, Relational Databases, APIs)
 Transforming and enriching data (ie. denormalizing, geocoding, reprojecting, anonymizing)
 Establishing ETL workflows, processes, and documentation for organizations

Additional Information

Please submit a resume and cover letter with your application.

Successful candidate must be a City resident within six months of hire.

The City of Philadelphia is an Equal Opportunity employer and does not permit discrimination based on race, ethnicity, color, sex, sexual orientation, gender identity, religion, national origin, ancestry, age,disability, marital status, source of income, familial status, genetic information or domestic or sexual violence victim status. If you believe you were discriminated against, call the Philadelphia Commission on Human Relations at 215-686-4670 or send an email to For more information, go to: Human Relations Website at: http://www.phila.gov/humanrelations","Philadelphia, PA 19107 (City Center East area)",Data Engineer,False
528,"Job Summary:
Encodia is an emerging, early stage biotech company developing the next generation of protein analysis tools. Encodia is seeking to recruit a data engineer with experience in designing and implementing pipelines for data ingestion, analysis and visualization. An ideal candidate would have experience with biological data (e.g., DNA/RNA sequencing), and with analyzing large data sets.
Job Responsibilities:
Design and implement data analysis and visualization workflows
Develop and manage compute pipelines in production environments

Collaborate across functions to identify and address analytical needs
Requirements
BS and/or MS in Computer Science, Bioinformatics, Computational Biology, or a related field
Experience in data visualization, analyzing large scale data, and familiarity with statistical analysis and machine learning
Self-motivated with the ability to work with and support many functions across the organization
Programming experience in Python as well as some strongly typed language (e.g., C/C++/Java)
Experience with command line Linux environments
Experience using, setting-up, and maintaining cloud-based computing environments (e.g., AWS/Azure)
Knowledge of git or other version control systems
Benefits
Encodia is an Equal Opportunity Employer that offers a competitive and comprehensive employee benefits package including medical plans, dental insurance, vision plans, LTD, paid vacation, and a stock plan.","San Diego, CA",Data Engineer,False
529,"We are seeking an enthusiastic, self-motivated Data Engineer to work collaboratively with our internal research team and our operations team to meet the Voloridge mission of delivering superior risk-adjusted returns using proprietary modeling techniques. Our Data Engineers work with a emphasis on rapid deployment of prototype processes for new datasets. At Voloridge we are passionate about expanding our knowledge and capabilities and find pleasure in creating better ways to deliver quality services both internally and to our investors. Our teams are comprised of energetic, hard working, highly analytical individuals that thrive on innovation and problem-solving at the nexus of big data and finance.

Top Reasons why you want to work for Voloridge Investment Management:
401k retirement plan, $1 for $1 match up to 4% of compensation
Regular in-office massages, weekly lunches, stocked kitchens with snacks, fruit and drinks
Work off the Intracoastal and 3 minutes from the beach
Work in an office chosen by South Florida Business Journal as one of the top 10 Coolest Offices in South Florida in May 2016
Profit Sharing Bonus
Summary of Job Functions
Understand the data-related needs of all internal parties, especially our Research team
Build datasets, continually strive to improve them, including increased automation, usability, transparency, documentation, and QA
Develop and maintain prototype ETL processes for new datasets, including initial data modeling, with an emphasis on rapid deployment balanced with stability and consistency
Obtain and document requirements for new datasets
Study new topics and gain domain knowledge related to data ingestion, storage and delivery
Have a consultant mindset, always striving to understand and fulfill the needs of our researchers and traders
Communicate and collaborate effectively with all internal associates, including research, management, development, and other operations personnel
Investigate and troubleshoot data anomalies
Represent the Strategy Data Group in internal forums for planning and collaboration
Assist with technical skill development and mentorship of junior personnel
Perform other duties and responsibilities as assigned
Minimum Requirements
Bachelor’s degree
7+ years of data engineering work experience in a field such as accounting, finance, business intelligence, or hard sciences
Expertise using Transact-SQL, and familiarity with other data technologies and tools
Experience with both transactional databases and data warehouses
Excellent oral and written communication skills
Must be able to demonstrate strong problem solving skills, and flexibility to consider new ideas and approaches
The ability to work daily, onsite in our Jupiter, FL office
Preferred Skills and Previous Experience
Experience using Visual Studio and SQL Server Management Studio to create/manage SSIS/ ETL packages, esp. with high volume data
Experience in performance tuning, server monitoring, and query optimization
Strong focus on data quality and attention to detail
Able to independently bring projects to successful completion
Experience working with trading / financial / investment / accounting data
Experience with master data management and data governance
Experience with data analysis tools such as Tableau, Excel
Programming experience, such as Python, Java, C#, VB.net, C, C++
Experience with leadership of small teams and/or projects
Demonstrated ability to work efficiently in a demanding, team-oriented and fast-paced environment
Compensation and Benefits
Highly competitive base salary
Profit sharing bonus
Health, dental, vision, life, and disability insurance
401K
Credit and Identity Monitoring Service
Additional Information
Voloridge Investment Management is an SEC registered investment advisor. A private investment company founded in 2009, our mission is to deliver superior risk-adjusted returns for qualified investors, using advanced proprietary modeling technology, conservative investment tactics and sophisticated risk management. Our market neutral equities strategy takes both long and short positions in the most actively traded equities, and is designed to capture alpha while limiting exposure to directional markets risks. Our futures strategy takes both long and short positions in the most actively traded global futures, and is also built to maximize alpha captured across all futures markets traded while capping exposure to any particular sector at a given time.
Voloridge Investment Management is an Equal Opportunity Employer. All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other legally protected characteristic or status.","Jupiter, FL",Data Engineer,False
530,"Zocdoc is the tech company at the beginning of a better healthcare experience. Every day, we are driven by our mission to give power to the patient, building products and services that simplify and streamline the overall healthcare journey for patients and providers, delivering the modern healthcare experience they expect and deserve. Our forward-thinking approach prioritizes collaboration, agility, and continuous learning in service of our long-term vision. This has helped us drive significant innovation in a complex, slow-moving industry, and our talented team is looking for impact-minded individuals to join us as we continue to reimagine the healthcare experience.

As a Senior Data Engineer, you'll help us build infrastructure for collecting, storing, processing, and analyzing huge sets of data in batch and streaming pipelines. You will contribute to the design and implementation of data flows and tools necessary to make key strategic decisions, and power machine learning and personalization within the product.

What you'll do:

Work closely with the Data Science and Business Intelligence teams to develop data models for research, reporting, and machine learning.
Build data tooling to enable data lake, data warehouse, and analytics workflows within the AWS cloud (S3, Redshift, DynamoDB, Spark, Kinesis, Kubernetes, etc.)
Contribute to our in-house ecosystem of developer data tools.
Collaborate with partners across the company to assess data needs and prioritize accordingly. Be proactive about driving data collection and storage best practices.
Drive adoption of data tools. Give tech talks and demos on the newest capabilities and enhancements to the system.
Consult on data security, design, and scalability to product engineering teams

What's required:

Curiosity and vision. You have a passion for technology and can think critically about performance, scalability, and reliability of software
5+ years engineering experience, 2+ years working with data in the cloud, ideally using AWS.
Well-versed in one or more of the following languages and functional programming in general: Scala, Java, Python, JavaScript
Expert in SQL and comfortable designing, writing and maintaining complex SQL based ETL.
Experience with building large-scale batch and real-time data pipelines; ETL design, implementation, and maintenance.
Experience with schema design and data modeling, and the analytical skills to QA data and identify gaps and inconsistencies.
Experience supporting Machine Learning or Business Intelligence teams and products
Computer Science or related degree preferred

","New York, NY 10012 (Little Italy area)",Senior Data Engineer,False
532,"Our engineers move extremely fast, while solving unique and challenging problems. Our team is small and nimble. We release everyday to ensure that engineers are able to iterate quickly, and make an impact immediately.

We’re looking for engineers to work on our massive semi-structured datasets. You'll develop software to process, transform and analyze the data to identify signals from billions of event we collect everyday. You'll provide insights that improve the experience of hundreds of millions of users, and 100,000+ stores in our marketplace.
Requirements:
3-5 years of industry experience
Bachelor's degree in Computer Science, Software Engineering or Statistics required
Passionate about Big Data
Experience developing data extraction and transformation pipelines
Expert knowledge in RDBMS, NoSQL and Data Warehousing
Experienced with map/reduce framework, such as Hadoop
Familiar with information retrieval softwares, such as Lucene/SOLR
Experience in data visualization a plus","San Francisco, CA",Big Data Engineer,False
533,"ContractJob Summary:

To support the business objectives by overseeing, installing and maintaining the servers (virtual/physical), storage systems, replication environment as well as peripherals in the Conshohocken Global Datacenter. This includes managing, configuring and maintaining the daily backup environment and managing outside storage. You will work closely with Network Engineering on the set-up of the firewalls, LAN/WAN and wireless networks. You will work with the Global Infrastructure team on on-premise and cloud solutions. This role is responsible for maintaining the integrity, structure, performance, capacity, and security of the ERP environment to ensure that business critical data is secured.

Responsibilities:
Maintain, update and upgrade the virtualization environments at the Conshohocken Datacenter, and locations in the US and South America; cooperate with Regional IT teams and with the EMEA Datacenter Engineer to keep the environment in both datacenters in line with each other; define/create a global standard for non-datacenter virtualization environments including a robust backup environment.
Collaborate with EMEA Datacenter Engineer and Microsoft Engineer on any Global on-premise system and cloud solutions.
Administer, configure and maintain the Global Infrastructure of the Global Barcoding systems and work with the Senior Network Engineer on the set-up of the Wireless and Wide Area Networks.
Assist Senior Manager of Global Infrastructure in selecting vendors and purchasing all equipment for the Conshohocken Datacenter; keep track of components lifecycles, assuring that the database (Asset register and purchase register) of this equipment is current and available.
Assist Senior Manager of Global Infrastructure in maintaining a high level of security.
General
Analyze malfunctions and problems on Global systems and initiate corrective actions. Communicate information and or corrective actions on Global issues with the Global counterparts to continuously improve quality of services to all users and/or communicate issues with Manager Global Datacenters and Datacenter engineers.
Be aware of development in information technology and promote this and systems for Quaker.
Keep information center library well documented.
Participate in activities and projects that are aimed at information processing and development. Identify, promote and adapt, after appropriate approval, new technology and systems for Quaker and support propagation of these new technology and systems globally.

Qualifications:
Bachelor’s or Master’s degree, preferably in Information Systems or Computer Science.
7+ years of relevant work experience in a data center or other critical environment.
Strong experience on virtualization using VMware (VCP certification is a plus) and/or Microsoft Hyper V.
Strong experience with Blade server technology, fiber switches and SAN storage technology, preferable IBM hardware and basic knowledge of hyper converged infrastructure systems.
Requires high knowledge of Microsoft Office 365, Microsoft Operating Systems, Microsoft Active Directory, Microsoft Server Applications
Basic knowledge of Microsoft SQL database environments, and at least 5 years’ experience in installing and maintaining these products.
Knowledge of Firewall, LAN and Wireless equipment is a plus, preferable Cisco.
Is a team player and accustomed to participating in large projects.
Is fluent in English (verbal and writing).
The position holder is characterized by e.g. following qualities: analytical, operational, tactical, result/customer oriented, team player, action oriented and creative.

Company Overview:

At Quaker Chemical, we are experts in the development, production, and application of process fluids, lubricants, and coatings for the steel, metalworking, and many other manufacturing industries. With approximately 2,000 Associates in more than 20 countries, we enable our customers to be more efficient - and ultimately more profitable. It's our obsession. And we achieve this through our intimate knowledge of the industries we serve and each one of the moving parts that comes with it. Quaker is a global publicly traded company with a unique collaborative culture that supports career growth for its associates and offers competitive compensation and benefit programs.","Conshohocken, PA 19428",Global Data Engineer,False
534,"About the role:
We are integrating multifaceted data streams such as real-time IoT data, purchase data at the fridge, engagement data from the mobile apps, production and logistics data so that we continuously improve and optimize our business and operations workflows. We are looking for a Data Engineer to join our tech team and this could be you.
In a typical week, you will...
Grow our existing cloud and data infrastructure, democratize access to data,
Champion the use of data and analytics at the company; be the pioneer of a data-driven culture.
Work with tech, marketing, sales, operations, finance and other business functions to scope, design and implement their data needs.
Help identify significant data sources and key variables for various business functions.
Lead the design and development of the data pipelines to productionize and ingest meaningful business data into a unified data model.
Develop machine learning and predictive analysis tools to identify new growth opportunities and personalize our services
Minimum qualifications:
Bachelor’s degree in Computer Science/Computer Engineering or relevant industry experience
Proficiency in multiple programming languages, particularly Python, R or Javascript, with minimum two years full time professional experience
You are proficient with SQL and are rarely satisfied with the current query performance
You have developed data ingestion, data warehouse and integration pipelines
You are comfortable with both batch and stream data processing.
You know what it takes to build scalable and performant data models.
You understand the differences between relational and columnar databases and when to use them.
You are familiar with web development.
You value writing tests and people who write tests.
You have worked with one of the major public clouds (preferably AWS).
Ideally, you have experience with data visualization.
Benefits:
Traditional Benefits - Health, Dental, Vision, Life, Short Term Disability, LTD
10 paid holidays
Generous PTO policy
Daily Fridge Credits
Monthly cell phone credit
Spontaneous company events and community service activities
Work with a phenomenal team of individuals that are constantly pushing the market boundaries to offer healthy food at the utmost convenience.
About Farmer’s Fridge:
At Farmer’s Fridge, we’re committed to making wholesome, delicious food simply accessible, so people can live a little happier. We aim to remove the roadblocks to eating well on the go. That means sourcing top-notch ingredients, handcrafting meals in our kitchen, packing & delivering fresh to our network of Fridges and ensuring the best possible customer experience. Unpurchased items are regularly donated to local food depositories, providing responsibly sourced nutrition to community members in need. If you’re passionate about our mission to change the food system for the better, our team just may be the right fit for you.

View our disclosures related to External Agencies and Applicants below.
https://www.farmersfridge.com/careerdisclosures","Chicago, IL",Data Engineer,False
535,"Ready to grow your career leveraging the latest DATA technologies?

Join a fast-paced and talented Agile Scrum team to unlock Data Capabilities for The Hartford. You will have an opportunity to participate in the entire software development lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies. We use the latest DATA technologies, software engineering practices, Agile delivery framework, and are passionate about technology and building well architected and innovative solutions that drive optimal business value generation.

This cutting edge and forward focused team presents the opportunity for collaboration, self-organization within the Scrum Team and visibility as we focus on continuous Business data delivery.

What’s in it for you?
Experience deeper understanding of Data analytics, Emerging technologies and Development practices
Collaboration with a high-performing, forward-focused team, Product Owner(s) and Business stakeholders engagement
Opportunity to expand your communication, analytical, interpersonal, and organization capabilities
Experience working in a fast paced environment – driving business outcomes in Agile ways of working
Enable and influence the timely and successful delivery of business data capabilities and/or technology objectives
Enhance your e ntrepreneurial mindset – network opportunity and influencing outcomes
Hone your development capabilities using various tools such as Hadoop, Informatica, B2B, PL/SQL, etc. to build data assets that enable business value generation
Appreciation and opportunity to learn and support rapid software construction and deployment using DevOps and Cloud based future technologies
Supporting environment that fosters can-do attitude and opportunity for growth and advancement based on consistent demonstrative performance
Optimize business value by leveraging your DATA experience and depth
Be part of a Scrum Team – driving work independently or collaboratively towards achieving business outcomes
Qualifications
What is The Hartford looking for?
Highly motivated candidates looking to grow their career with the Hartford
Forward looking and focused on continuous improvement
Team player and able to work with and through others
Takes initiatives in building their own technical and soft skills
Volunteers for extra assignments within the organization driving towards continuous improvements within our Data Delivery Life Cycle.
Ability to listen effectively, process information, ask appropriate questions for clarification and execute tasks accordingly
Detail oriented and good documentation skills
Bachelor degree with at least 1-3 years of applicable work experience
Desired educational experience include, but are not limited to: Computer Science, Engineering, IT, Management Information Systems, Data Analytics, Applied Mathematics, and Business
Desire candidates with prior Data Engineer competencies and prior experience with successful enablement of Data Delivery initiatives
Understanding of current and emerging IT products, services, processes and methodologies
Prefer experience with Big Data technologies and concepts on a Hadoop platform (e.g. Scoop, Hive, Pig, NoSQL, etc…) and willing/adapting to future technologies
Prefer working knowledge of ETL process across various tools and experience with SQL skills

What Else Can You Tell Me?
The Hartford is committed to the education and growth of our Information Technology Professionals. A number of IT Certifications are available to enhance your career and growth potential. IT Professionals at The Hartford may qualify for a stipend up to $1000 per year for additional certifications

Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age

** NO AGENCIES PLEASE **
Job Function
: Data Engineering
Primary Location
: United States-Connecticut-Hartford
Schedule
: Full-time
Job Level
: Individual Contributor
Education Level
: Bachelor's Degree (±16 years)
Job Type
: Standard
Shift
: Day Job
Employee Status
: Regular
Overtime Status
: Exempt
Travel
: No
Job Posting
: Aug 3, 2018, 2:06:19 PM
Remote Worker Option : No","Hartford, CT",Junior Data Engineer,False
536,"WHO WE ARE
Build the Business. Build the Brand.
At Havas Edge we influence people to act by combining multi-channel marketing and commerce plus the creative and technology that powers them. Our work results in profitable growth and lasting relationships between customers and our client's brands.
We are a full-service, direct response agency, headquartered in Carlsbad, CA with offices in Boston, MA; Los Angeles CA; and London, UK.

WHAT WE NEED
Responsible for developing and supporting data warehouse solutions built on the Microsoft SQL Server technology stack (RDBMS, SSIS, SSAS and SSRS). These solutions include ETL, data modeling, data quality assessment and data delivery. This resource will play a critical role in the development of data warehouse solutions utilized by internal and external clients.

WHAT YOU'LL DO
Create high performance data warehouse and ETL solutions using SSIS.
Performance tuning and optimization of SSIS, T-SQL and database structures.
Design and develop tables, views, triggers, indexes, constraints and stored procedures.
Monitor long running transactions and can optimize query executions using best practices and methodologies
Assist/support in the design process on an as needed basis
Design data models for enterprise-wide data integration incorporating:
Structured and unstructured data
Real time, on-demand, batch and varying timing schedules
Build data flow automation to retrieve, transform and load data
Manage data warehouse structure and file storage rules
Data management functions including:
Metadata management
Capacity planning
Create, alter, delete, grant permissions, indexing, updating
Query tuning
Storage, backup and recovery
Maintain best practices for handling, storing and using data
Tools:
SQL Server/SSIS
Unix/Linux
Java/R/Perl/Python
Web Analytics Platforms Google, Adobe, etc.
Cloud Technologies AWS, Azure, Google

WHO YOU ARE
Minimum Requirements:
BS Degree in one of the following subject areas; Computer Science, Business Administration, Information Technology or related field preferred
Minimum of 5+ years of experience developing large scale complex database solutions
Demonstrated experience with BI database design
Ability to track and resolve database related incidents as well as requests
Strong understanding of database structures, theories, principles, and practices
Strong understanding of ETL and data warehouse principles
Extensive experience writing T-SQL
Knowledge of reporting and query tools like SSRS and SSAS
Good interpersonal, written and oral communication skills
Technical documentation skills
Ability to present complex ideas in user-friendly language
Self-motivated and directed, with keen attention to detail
Able to prioritize and execute tasks in a high-pressure environment
Experience working in a team-oriented, collaborative environment
The ability to develop stored procedures, triggers, indexes, and views

Specific Skills Required:
Microsoft SQL Server 2014
Minimum 3-5 years T-SQL experience including the ability to tune queries
Minimum 3-5 years of ETL experience
Minimum 2-3 years of SSIS
Strong Data warehouse knowledge
Data modeling experience
Some DBA skills preferred, but not required
Experience with large data sets
Python, Java, C# or R experience a plus
Experience with REST API calls","Carlsbad, CA 92008",Data Engineer,False
537,"Nexon M is a mobile game publisher based in Emeryville, CA. Founded in 2013, Nexon focuses on free-to-play games for mobile and counts some of the world’s best developers as its partners, including Big Huge Games, maker of the top-grossing game “DomiNations”, and Pixelberry, maker of top-grossing game “Choices: Stories you Play”

We focus on free-to-play games for mobile can be traced to our roots as a subsidiary of NEXON Corp. As Korea’s largest game company, NEXON is known for having created the first free-to-play games in the early 2000′s and has multiple games which recently passed their 10-year anniversaries of live operation.

We're hiring a Data Engineer to join our Platform Engineering Team. Our Big Data platform processes over a billion of events per game to support our analysts and other functional groups to make better business decisions. This position develops and supports various real-time ETL processes to transform data from different input sources including our own game SDK, and effectively load into our data warehouse in a scalable manner.

Job Responsibilities:

Create, Document, and Maintain Various ETL processes
Write code to interface with 3rd party platform services
Assist with all parts of our internal analytics system (ETL, Redshift, Airflow)
Provide technical support and advice to the analytics and data visualization team
Other duties as assigned

Job Qualifications:
Work Experience


Experience with Big Data and analytics systems (Hadoop, Storm, Spark, etc…)
5+ Years doing back-end web services programming (Java, PHP)
3+ Years doing database work (mysql, Postgres / RedShift)
Experience using Amazon Web Services a plus
Experience with scalable systems in a load balanced environment a plus

Education, Professional Training, Technical Training or Certification


B.S. in Computer Science

Knowledge/Skills


Object Oriented Programming (Python / Java)
Database Technologies (Redshift / Postgres / Spark / Presto)
Amazon Web Services (S3, SQS, Kinesis, ECS, ECR, EMR)

This is an onsite, full-time position with Nexon M (nexonm.com). Our studio is located in Emeryville, CA. A casual, friendly, work environment, comprehensive benefits package, a competitive salary, and a great opportunity for career growth and development, are all part of what makes Nexon M a great place to work.

Nexon is an Equal Opportunity Employer

To all 3rd party recruiting agencies: Nexon does not accept agency resumes. Please do not forward resumes to our recruiters, employees or any other company location. Nexon is not responsible for any fees related to unsolicited resumes.","Emeryville, CA 94608",Data Engineer,False
538,"About StackPath
Founded in 2015, StackPath provides inherently secure cloud services so that developers can build online solutions and services that are safe—for their own operations and for their end users. More than 30,000 customers already use StackPath technology, ranging from Fortune 100 companies to early stage startups. We are a fast-growing team headquartered in Dallas, with offices in Orlando, Phoenix, Seattle, Tel Aviv, and Belgrade.
About the position
StackPath is hiring a Data Engineer enhance our Data Engineering team. Our Data Engineers empower StackPath and its customers by building data analysis platforms that ingest high volumes data streams from global sources, and provide insight and analytics to internal and external customers.
We are looking for passionate, intelligent, and motivated Data Engineers that want to be a part of a quickly growing team focused on collaboration and teamwork.
The Data Engineer:
Works in a data focused engineering team, on distributed data management platforms in a high ingest rate environment from batch and near-real-time streams.
Performs analysis, design, programming and testing for the data flow and ETL layers of data warehouses and operational data stores.
Designs and programs processes used for data storage and reporting, tests and debugs these systems prior to implementation, and troubleshoots and upgrades the system as needed.
Supports both internal and external customer facing systems. Occasionally the position will involve data warehouse administration (DBA), ensuring the successful operation of data load and analysis jobs.
Writes code, works not only in an ETL IDE or graphical administration tools.
Why you should apply:
You love working with data in a highly collaborative environment
You wake up each day hungry to solve problems
You want to make the world a better place one customer or issue at a time
You want to make a difference and have input
Help build an amazing company
You Must Have:
Experience in using PostgreSQL
Experience in scripting ETL workflows on Linux / Unix in Python and / or Go
Experience in building and maintaining data pipelines using Kakfa (or similiar)
Experience in writing analytic (OLAP) queries in SQL
Knowledge of common Linux shell tools for text processing: Awk, Sed, Grep
Familiarity with other relational database technologies such as ClickHouse or MySQL
Quantitative skills that will allow the successful use of data in operational and business processes
You Should Be Able To:
Handle data projects at a scale most companies don’t have
Solve problems not seen before
Quickly understand the complexities of the company’s products and markets
Work with a team of data engineers and business leaders in both local and remote offices
You Might Also Have:
PostgreSQL / MySQL database administration experience
Experience in:
Data analysis and visualization tools (Tableau, Grafana, R)
Distributed query engines (Citus, Presto, Apache Drill)
Distributed file systems (Ceph, HDFS)
Stream processing platforms (Kafka)
Container management platforms (Docker, Kubernetes)
Bachelor's degree in Computer Science, Math, Statistics or related quantitative experience
Other Details:
At StackPath, we take care of our employees and want you excited to get out of bed in the morning. We provide competitive salaries and a full slate of benefits. We invest in our people to ensure that they have everything needed to excel on the job.
Our goal is to make the internet a safer place. We need more help from people with the right passion and skills to help us get there.
This job description is not intended to be all-inclusive.","Dallas, TX 75201 (City Center District area)",Data Engineer,False
539,"Job Description
The ideal candidate is a motivated self-starter with strong business acumen to join the team and build analytics driving actionable insights to accelerate the growth of the business. The Principal Data Engineer will partner with Senior leaders, product managers, and other internal stakeholders within AWS Infrastructure and partner services.

This role requires an individual with excellent analytical abilities, deep knowledge of business intelligence solutions as well as a passion for problem-solving. Ideally, you are comfortable with ambiguity and accessing and working with data from multiple sources. You will analyze large amounts of data, discover and solve real world problems, and build metrics and business cases around key performance of this program. You will be responsible for understanding the health of the service and business, and drive necessary changes as needed. This is a perfect position for someone who loves data and knows how to work fast and smart.

Some of the key responsibilities for this position include:
Propose and implement business metrics for senior management reviewsWork with business intelligence and data engineers to design and develop data infrastructure strategy for the Quality and software development organization of AWS InfrastructureEnable effective decision making by driving partner teams to expose data pipelines and enabling the data team to aggregate data from multiple sourcesPerform deep-dives to find the root causes behind variances of key parameters over a given time-periodDevelop intelligent, insightful self-reporting toolsManage the data warehouse strategy

Key responsibilities:
Work closely with management, software engineering leaders, and business teams to plan catalog data improvements so as to maximize customer improvement impact.Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.Collaborate with software engineering teams to integrate experimental capabilities into large-scale, highly complex Amazon production systems.Report results in a manner which is both statistically rigorous and compellingly relevant.Assist in recruiting, mentoring, developing, and training other data engineers and data scientists within the organization

Basic Qualifications
BA/BS in Computer Science, Engineering, Mathematics or related field
Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.
10+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree
Advanced skills in business analytics as well as any data visualization tools like Tableau or similar BI tools (familiarity with Tableau and Quicksight preferred)
Advanced ability to draw insights from data and clearly communicate them (verbal/written) to the stakeholders and senior management as required.
Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.
An ability and interest in working in a fast-paced and rapidly-changing environment
Preferred Qualifications
MBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related field
Expert in writing and tuning SQL scripts
Experience working in very large data warehouse environments
10+ years of experience in a data engineer role with a technology company
Experience conducting large scale data analysis to support business decision making
5+ years of operations and/or multi-unit retail industry experience
Familiarity with AWS Redshift and AWS Glue
Experience with scripting languages (Python/R)","Seattle, WA","Principal Data Engineer, Infrastructure Automation",False
540,"Job Description
Have you ever wondered how Amazon shipped your order so fast? Wondered where it came from or how much it cost us? To help describe some of our challenges, we created a short video about Supply Chain Optimization at Amazon - http://bit.ly/amazon-scot

We in the Order Assignment team own and operate production systems that decide the most optimal strategy to ship a customer order, simulation systems that allow internal customers experiment with what-if scenarios and analytical systems to help understand results and derive intelligence from the same. When customers place orders, our systems use real time, large scale optimization techniques to optimally choose from where to ship and how to consolidate multiple orders so that customers get their shipments on time or faster with the lowest possible transportation costs. The team in Austin is focused improving the customer experience and on saving hundreds of millions of dollars using cutting edge science, machine learning, and scalable distributed software in the cloud that automates and optimizes shipments to customers under the uncertainty of demand, pricing and supply.

We are seeking an outstanding Data Engineer to join our team. Amazon has a culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. Your work will have an immediate influence on day-to-day decision making at Amazon.com.

As an Amazon Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. We maintain one of the largest data marts in Amazon as well as work on Business Intelligence reporting and dashboarding solutions that are used by thousands of users world-wide. Our team is responsible for mission critical analytical reports and metrics that are viewed at the highest levels in the organization. We are also working on newer tools that help users discover data using visualization and Big Data technologies. You should have deep expertise in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. You should be able to work with business customers in a fast paced environment understanding the business requirements and implementing reporting solutions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.
Basic Qualifications
Degree in Computer Science or related field, with 4+ years professional experience in database development
Proficient with processing data on relational databases like Redshift/Aurora
Experience handling large data sets using SQL and databases in a business environment
Familiar with ETL and DW processes
Excellent verbal and written communication
Strong troubleshooting and problem solving skills
Thrive in a fast-paced, innovative environment
Preferred Qualifications
Knowledge of AWS Technologies
Knowledge in using OLAP technologies and BI Analytics.
Oracle, Redshift, Linux, OBIEE experience
Experience with multiple database platforms
Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis","Austin, TX",Data Engineer - Supply Chain Optimization Technology,False
541,"You are a contemporary Data Engineer with data integration expertise and a passion for changing the data landscape. If you are interested in working for a data driven company on the edge of industry disruption, we'd love to talk with you. And if we ask you about bulk batch high volume data processing and your eyes light up and your brain goes into overdrive, we're talking to the right person.We are looking to hire a high performing Data Engineer with data integration (ETL) experience as part of an integral member of our Tech - Data Integration team. You will transform and translate data into a form that is ready for use to solve the tough challenges of our business. The Data Engineer will make data useful by moving it around, transforming it, distilling it, combining it with other data, addressing data quality issues and delivering it to a repository or consuming system in a performant manner. You will work with various forms of data (xml, json, text, csv) and data sources (APIs, server logs, external data, no-sql, etc). Ultimately, it will be you that manages Clear Capital’s complex data landscape in today's world.Here are a few exciting projects you get to work on:- MDM strategy and execution- Building new data domains- AWS cloud migration- Build out broader data integration service to support our data storesWhat you will doDesign, develop, automate, monitor and maintain Extract Transform Load (ETL) data movement applications using our preferred ETL tools and techniques.Performance tune ETL applications to manage high volume batch data transfer to and from internal and external system locationsTroubleshoot data issues, recommend, test and implement solutionsDevelop and document technical requirementsand solutionsParticipate in design and code reviewsTroubleshoot issues making recommendations and delivering on those recommendationsEngage in project planning and delivering to commitmentsInteract with Project Manager Lead and agile team to estimate development efforts and ensure accurate requirementsfulfillmentParticipate in daily stand-up meetings, planning meetings and review sessions (using Scrum / Agile methodology)Interact with cross-functional teams to ensure complete delivery of solutionsAssist with configuration of applications softwareSkills and experience you bringExtensive experience with delivery using ETL tools and techniques (ie, Ab Initio, Pentaho, Talend, SSIS)Programming skills in either Java, JavaScript, SQL, or PL/SQLUnderstanding of data warehouse and master data management approaches, ETL industry standards and best practicesExperience with Cloud migration and cloud-hosted ETL environmentsExpert SQL and database tuning skills for relational platforms (PostgreSQL, Oracle) is a plusFamiliarity with Revision Control - GitDemonstrated experience with Kimball ETL architecture techniquesDemonstrated working knowledge of Unix/Linux operating systems and related data integration tools such as Secure FTP serversExperience with AWS technologies and stack such as Lambda, EMR, S3, RDS, and EC2Experience with NoSQL databases (Cassandra) is a plusExperience with Apache Airflow is a plusWorking knowledge of an Operating System scripting language, ex. bash (using awk and sed), Python, and/or PerlExcellent communication, analytical, and development skills.Knowledge of current standards and practices in real estate information systems is preferredCLEAR CAPITAL 101Since its launch in 2001, Clear Capital continues to solidify its place as a leading provider of Real Estate valuation data and technology for banks, lenders and sizeable investors. Clear Capital—one of the leading real estate technology, data & analytics providers—places each customer’s financial future in their own hands by providing the most clear, transparent and trusted data to ensure the mortgage finance industry can make healthy and sound decisions.To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees or any other company location. Clear Capital is not responsible for any fees related to unsolicited resumes.CompensationSalary commensurate with experienceClear Capital is an equal opportunity employerJob Type: Full-time","Reno, NV",Data Engineer,False
542,"Job Description
The Data Engineer (DE) will be a key member of the technical team at Amazon Lending, contributing to a new and rapidly growing line of business for Amazon. This person will take the lead on optimizing our data architecture for the heavy analytics needed for this business.
The role presents a significant intellectual and technical challenge with enormous opportunity for business impact. Amazon is expanding into lending services and has launched with the small business segment. We believe we are in a unique position to serve our customers with exceptional value due to our deep understanding and insight into our base, coupled with our data-grounded analytic and customer-focused approach to building products.

We are looking for a talented and passionate data professional that can design a high-quality and scalable analytic infrastructure that can automate our decision-making processes. A successful candidate will be innovative, technically versatile and be able to interact with customers to gather requirements and deliver the right set of data to support business growth.

The key strategic objectives for this role include:
a) Innovating to deliver mission-critical near real-time data feeds to optimize the business.
b) Delivering a robust, flexible, and scalable data analytics environment to support Amazon Lending as it grows deeper in its existing lines of business and expands to new geographies, customer segments, and products.
c) Collaborate with technical teams, legal, finance, operations, and product managers to drive process improvement and track progress against goals.
Basic Qualifications
We are looking for Data Engineers who have a passion for supplying their clients with meaningful and trustworthy data. You know and love working with analytic tools, can write excellent SQL and Unix scripts, can partner with customers to answer key business questions, and you are an advocate for your customers. You are analytical and creative, and you don’t quit.

You should also have the following skills or experiences:
Bachelor's degree in CS or related technical field
5+ years experience in data warehousing
Strong data modeling and SQL skills
Excellent troubleshooting and problem solving skills
Passion for learning and using new technologies
Effective communication and strong collaboration skills
Preferred Qualifications
Experience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies
Proficient in Big Data processing (e.g. Hive, Scala)
Expertise in MPP and NoSQL persistent storage solutions
Experience integrating with 3rd Party data providers
Amazon is an equal opporunity employer.","Seattle, WA",Data Engineer II - Consumer Payments / Lending,False
543,"Make a difference. Help Gallup fulfill our mission of providing analytics and advice on everything that matters by creating the infrastructure to fuel analysis, insights and products.
As a data engineer, you will have the option to work from the comfort of your home office or from one of our world-class workplaces. Responsibilities include consulting with data scientists and client teams to develop and implement prototypes and products to advise on quantitative research across a wide range of organizational topics. In your role as an infrastructure builder, you will work with internal teams to: Design, construct, install, test and maintain highly scalable data management systems as well as integrate new data management technologies and software engineering tools into existing structures. You will also partner with client teams to develop data pipelines that support a range of enterprise and government projects, including supervised and unsupervised learning methods, time-series forecasting, and network analysis. You will also be responsible for helping consultants and data scientists produce high-quality, easily understood data products for clients.
Who we want:

Automation architects who can conceptualize manual processes, generate coding and systems to reduce human dependency.
Experienced coders who can identify the right tools for the job and who have an eye for the smallest details and a talent for flawless execution.
Strategic thinkers who can apply their expertise to make discoveries in complex data.
Problem solvers who enjoy working with others to design and implement robust solutions to challenging real-world problems.
Creative analysts who are familiar with a variety of coding languages, who enjoy participating in (and winning!) hackathons, and who have experience working across programming languages and systems to meet the needs of multiple stakeholders.
Mission-driven individuals who are motivated to create the best production, prototype and research infrastructures because of their desire to change the world.

Qualifications
What you need:

A bachelor’s degree from a computer science/engineering, computational social science or operations research program or related area required
Minimum of three years of corporate experience in designing and building production-level data pipelines required
Minimum of three years of experience in the fundamentals of machine learning and statistics, with an emphasis on the engineering aspects needed to implement these algorithms at scale (e.g., distributed computing) required; knowledge of an open-source statistical programming language (e.g., R, Julia, Stan) is a plus
Mastery of at least one open-source general-purpose programming language (e.g., Python, Go, Java, JavaScript, PHP, Perl) and knowledge of a second are key requirements
Must be authorized to work in the United States on a full-time basis

What we offer:

V irtual workplace opportunities are available
A strengths-based, engagement-focused and performance-oriented culture
World-class managers who support, position, empower and engage you
Ongoing learning and development opportunities
Mission-driven work that changes the lives of people around the world

Gallup is an equal opportunity/affirmative action employer that celebrates, supports and promotes diversity and inclusion. We will consider all qualified applicants without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation or gender identity, or any other legally protected basis, in accordance with applicable law.
Primary Location
 United States-Washington DC
Other Locations
 United States-New York City, United States-Chicago, United States-Omaha, United States-Atlanta, United States-Irvine","Washington, DC",Data Engineer,False
544,"We have an immediate opportunity with a large F500 client in the Quincy, MA area.

We are looking for Data Engineer to work with one of our major healthcare clients in Quincy, MA.

12 Months

Primary Skill : Hadoop

Secondary Skill: NO SQL

Roles and Responsibilities:
: Develops ETL (Extract / Transform / Load) processes and tools for real-time and offline analytic processing.

: Collaborates with data science team to gain a deep understanding of their needs.

: Applies knowledge of Hadoop architecture and experience designing & optimizing queries to develop large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.

: Uses strong programming skills in Python, Java, SQL or any of the major languages(Java/Scala) to build robust data pipelines and dynamic systems.

: Builds data marts and data models to support Data Science and other internal customers.

: Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.

: Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.

: Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.

For immediate consideration please contact:
Harini

UpStream Global Services.

Reply to: projects@upstreamgs.com

www.upstreamgs.com","Quincy, MA 02169",Data Engineer - Hadoop-Quincy,False
545,"Heartland Financial USA, Inc. is a growing dynamic organization with many locations offering uniquely different banking and financial solutions for businesses and personal clients. As a performance driven company, we strive to create a culture of excellence with high standards, and high values while providing outstanding growth and involvement opportunities for employees. Join a team that makes ""Great Things Happen!""™



Under general direction , the Data Engineer is the IS solution administrator responsible for understanding HTLF data warehouse type tools with a focus on developing reporting and business intelligence (BI) interfaces while understanding MS SQL database structures.
 

RESPONSIBILITIES:
1. Designs logical and physical databases or review description of changes to database design to understand how changes to be made affect physical database (how data is stored in terms of physical characteristics such as location, amount of space, and access method).

2. Builds databases for data storage or processing and develop strategies for warehouse implementation, data acquisition, and archive recovery.

3. Optimizes data storage receptacles, manage ETL processes, and clean and maintain databases by removing and deleting old data.

4. Establishes physical database parameters.

5. Responsible for fulfilling BI requests not outsourced or manageable by power users.

6. Codes database descriptions and specify identifiers of database to database management system or direct others in coding database descriptions.

7. Specifies user access level for each segment of one or more data items such as insert, replace, retrieve, or delete data.

8. Specifies which users can access databases and what data can be accessed by user.

9. Evaluates new data sources for adherence to the organization's quality standards and ease of integration.

10. Tests and corrects errors, and refines changes to database.

11. Identifies, researches, and resolves technical problems as well as answer user questions.

12. Confers with co-workers to determine impact of database changes on other systems and staff cost for making changes to database.

13. Works with project and business leaders to identify analytical requirements.

14. Manages aspects of the warehouses such as data sourcing migration, quality, design and implementation.

15. Scopes, plans and prioritizes multiple projects.

16. Completes annual E-Learning Plan and Bank Secrecy Act (BSA) training as assigned and keeps up-to-date knowledge of BSA as it relates to the job function.

17. Performs other duties as assigned.


REQUIRED SKILLS & EXPERIENCE:
1. Bachelor’s Degree in Information Technology; or equivalent years of experience.

2. 5-7 years related experience in an IS/IT department managing databases, warehouses and/or secondary repositories, data mining (using Joins, matching, etc.) within said receptacle(s) to extract pertinent records related to query requests, and effectively building, maintaining and trouble-shooting ETL procedures.

3. Experience with SQL, MS SQL Service and MS Access.

4. Familiar with field’s concepts, practices and procedures.

5. Familiar with Data Quality procedures and processes

6. Familiar with Metadata and Data Dictionary procedures and processes

7. Understands the Advanced Analytics concepts such as Clustering, Association, and/or Data Mining

8. Ability to document procedures and build work instructions.

9. High attention to detail.

10. High level of independence and responsibility for own actions.


Scheduled Weekly Hours:
40


Time Type:
Full time","Dubuque, IA",IT Data Engineer,False
546,"ContractWe are looking for a savvy Data Engineer for our team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.ResponsibilitiesCreate and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies.Build data pipeline on premise and on Google Cloud Platform.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Product, Data and Design teams to assist with datarelated technical issues and support their data infrastructure needs.Work with data and analytics experts to strive for greater functionality in our data systems.QualificationsAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing 'big data' data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with Google Cloud Platform esp. Google Pub-Sub, Big Query, Data Proc, Data Flow, Cloud Storage.Experience with IoT & Time series data.Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala,Job Type: Contract","Sunnyvale, CA",Data Engineer,False
547,"Company Overview :
REsurety is a mission-driven organization solving the challenge of resource intermittency for renewable energy. We work in partnership with the world’s leading energy and risk management providers to enable renewable energy consumers and producers to manage the fuel risk of the future: the weather. As a high-growth, profitable company that has already supported over 4,000 MW of clean energy transactions, we are a small team making a big impact! Our culture is open and collaborative. We expect excellence from our team members and reward it with high ownership and flexibility. If you’re a high-achiever with a passion for clean energy, then we look forward to receiving your application.

Position Overview :
As a Data Engineer, you will own and improve a world-class data pipeline that is scalable, reliable, flexible, and accurate to ensure that the REsurety team uses the best available data to characterize risk and support development of renewable energy worldwide.

Key Responsibilities :

Centralize many different data feeds
Work closely with the internal teams to understand their data needs
Maintain existing and design new relational database and model data sets
Build and integrate code interfaces for downstream users

Required Qualifications :

Experience with data centralization processes
Experience creating and maintaining relational and non-relational databases
Proficient in Python (or R)

Preferred Qualifications :

Bachelor's degree in Computer Science, Computer Engineering, or a related field
2+ years of related professional experience
Experience with Amazon Web Services: S3, EC2, RDS, EFS
Experience with Linux-based systems

Details:

Location: Boston, MA
Travel: Minimal

Benefits:

Unlimited Vacation Policy
Medical Insurance
Dental Insurance
Health Savings Account (HSA)
401(k)
Gym Membership Reimbursement
Blue Bikes Gold Membership

REsurety, Inc. is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation or any other characteristic protected by law.","Boston, MA 02109 (Central area)",Data Engineer,False
549,"InternshipThe Data Engineer position would support the FG-AM-60 Big Data product group in delivery Use Cases into production for both the Sales and Financial Services organization. The selected resource would work directly with the data scientists to harden code and implement the necessary security measures to move solutions into production. Without this position, BMW would need to hire external resources to productionalize the use ases at a much higher cost or the number of use cases that could be implemented would be significantly reduced should this position not be approved.

BMW Manufacturing Company is an EEO/affirmative action employer, all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, gender identity, national origin, disability or protected veteran status.
MUST ATTACH A COPY OF UNOFFICIAL TRANSCRIPT

Possess a minimum cumulative GPA of 3.0
Have full time status at an accredited four year college or university in the United States
Completed at least 30 credit hours
Able to complete 3 rotating terms for co-op positions
Have at least 1 remaining term in school after the completion of the co-op or internship
Transfer students must have a GPA from current university
Complete and pass a substance abuse test before the work term
Must have the legal right to work for BMW Manufacturing Co., LLC in the United States
BMW Manufacturing will not provide sponsorship now or in the future","Spartanburg, SC",Data Engineer Co-op (Spring 2019),False
550,"Big Data Engineer

As a (Big Data) Software Engineer you'll use your data engineering knowledge to provide scalable enterprise data integration processes and technical support for our data management systems. In this role, you will be responsible for understanding data requirements, security requirements, metadata, design, development, and testing activities to successfully implement and support the needs of the business.

You will grow a long-term career while continuously receiving hands-on training and mentorship from senior level architects and developers in our data services organization.

Responsibilities:

Development of Solutions. Duties include construction of basic and/or complex solutions based on functional and technical specifications. Creation of operational specifications and procedures. Definition, documentation, and execution of unit test cases. Support Quality Services and User Acceptance testing. Definition, creation and execution of implementation processes and procedures. Assist in task identification, planning, and tracking.
Passionate about distributed processing and very large-scale solutions.
Provide a high quality and reliable solution to solve business problems
Review and manage work assignments (includes support cases) for team in an Agile environment.
Provide technical representation and communicate with customers until issue is resolved.

Qualifications:

Bachelor of Science in Computer Science or equivalent years of relevant work experience
Strong knowledge of relational databases (SQL)
3 plus years of Java, Scala, Maven, Python, Spark, SparkSQL, SparkML NoSQL development experience
3 plus years of SQL programming
Strong experience working with Hadoop/Big Data technologies; i.e Hadoop, MapR, No-SQL Database capability
Strong analytical capabilities, data processing development background and multiple DBMS experience
Clear understanding of HDFS file types and when to use which
Clear understanding of the differences of the multiple data management systems available; i.e. HDFS, Cassandra, HIVE, HBase
Strong understanding of distributed systems and distributed computation
Strong knowledge of Linux scripting, BASH, KSH, Jenkins
Experience using tools such as Atlassian (Jira), Wiki, Visual Studio, TFS, Power Designer, Tortoise/SVN, is a plus
Strong analytical and communication skills

Perks:

Long-term growth opportunities
Casual and diverse working environment
Strong corporate commitment to this technology stack
BCD Travel has received numerous awards as a 'best place to work'
You will grow a long-term career while continuously receiving hands-on training and mentorship from senior level architects and developers in our data services organization.

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled",United States,Big Data Engineer II,False
551,"Data is the fuel of AI. We are looking for a strong Data Engineer to join our growing team of AI and computer vision experts.

The successful Data Engineer will be responsible for expanding and optimizing our image and video data and pipeline architecture, as well as optimizing data flow and collection for computer vision algorithm development and product teams.

This person will use their skills and prior experience working with datasets to build our data ingest and metadata management automation pipeline as part of our data infrastructure effort. He or she will deal with synchronization, cleaning, restructuring, filtering, and doing early analysis, followed by exploratory and automatic data analysis as our statistics build up.

The successful candidate is experienced with building image and video data pipelines and working with imaging data, and is someone who enjoys optimizing data systems and building them from the ground up.

The Data Engineer will support our algorithm developers, software developers, database architects, data analysts and data scientists on building world class datasets and will ensure optimal data delivery architecture is consistent throughout ongoing projects.

They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our image and video data architecture to support our next generation of products and data initiatives.
Key Responsibilities
Create and maintain optimal data pipeline architecture,
Assemble large, complex multimodal image and video data sets that meet functional / non-functional business requirements.
Maintain and analyze large quantities of inbound and outbound autonomous driving sensor data:
Facilitate the ingress and egress of sensor data from our acquisition drives to and from annotation partners
Review data for annotation
Interface with annotation partners, monitor and track quality and provide support
Build analytics tools that automatically parse annotated data for valuable statistics and other information
Dynamically generate datasets for training and testing of perception algorithms for autonomous driving
Infrastructure and database support, help automate processes
Qualifications and Requirements
BS or higher in Computer Science or related technical field
Scripting skills in Python, Shell, or other common scripting language
Experience in manipulating and analyzing real world image and video datasets
Working knowledge of Unix/Linux environments
Strong team player and communication skills
Strong project management and organizational skills.
 Desired Qualifications:
Knowledge of machine learning and/or computer vision concepts
Web based programming (XML,JSON, AJAX, HTML)
Database (e.g. MySQL, MongoDB)","San Jose, CA",Data Engineer,False
552,"As a Data Engineer for Business Intelligence at Compass, you will be responsible for helping to build the data-driven decision-making culture throughout the organization. You'll work as part of a rapidly growing team in a fast-paced environment. You will be responsible for managing large-scale business systems initiatives that impact multiple functions and teams across the organization. In this high impact role you will have an opportunity to work with emerging technologies, while driving business intelligence solutions end-to-end: business requirements, workflow instrumentation, data modeling, ETL, metadata, reporting, and dashboarding. You are someone who loves data, understands enterprise information systems, and has a strong business sense.

At Compass You Will:

Design, develop, and implement the infrastructure that elevates data-driven decision-making for our proprietary real estate technology
Work with the enterprise business systems that facilitate the end to end experience of real estate transactions
Work with company stakeholders and Product and Engineering teams to define analytics requirements
Deliver flexible and scalable solutions from end-to-end, harvesting process-level data and transforming it into normalized data marts from which operational and process metrics and analysis can be reliably generated

What We're Looking For:

Bachelor's degree in Computer Science, Information Systems, or related field
3+ years of SQL development experience
3+ years of Data modeling, ETL, and Data Warehousing experience
Familiarity with ETL tools such as Informatica, Pentaho, Talend, etc
Expertise in modern OO language (e.g. Java, C#, C++, Objective C)
3+ years or Python Scripting experience
Familiar with AWS as a Platform
Strong business communication skills
Experience with AWS technologies such as Redshift, RDS, EMR, etc.
Comfortable in a Linux environment
Capable of data processing and transference outside of ETL tools or databases (custom scripts to pull and load from APIs or Files)
Experience writing software requirements
Familiar with Enterprise Networking

At Compass, our mission is to help everyone find their place in the world. This means we continually celebrate the diverse community different individuals cultivate. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone's place.

Check out our Engineering blog ( https://medium.com/compass-true-north )!","New York, NY 10010 (Gramercy area)","Data Engineer, Business Intelligence",False
553,"About Stanza

Hundreds of millions of pageviews, natural language processing, real-time user analytics, header bidding… these are just a few of the challenges we face at Stanza. To the consumer, Stanza is a seamless calendar integration, allowing users to sync their favorite interests and events such as concerts, sporting events, movie releases and esports. With partners such as Warner Brothers, NBC Sports, CBS, NFL, NBA, NHL, NCAA, and many more, Stanza is becoming the default way to embed and share calendars on the web.

We are redefining the purpose of a calendar with one core value in mind: Your Time Matters. With hundreds of millions of synced events and global reach, our goal is to make sure that each and every user spends more time doing what they love. We are a fun, bright, diverse team that's passionate about solving problems in an elegant way using cutting edge technologies.

Stanza is backed by top tier venture investors and is headquartered in San Francisco.

What you will do


Work with key developers to design and build a scalable data infrastructure and analytics system
Build real-time analytics infrastructure (Kafka, Spark MLLib, Amazon Kinesis, etc)
Build automated ETL pipelines
Drive the collection of new data and the refinement of data sources
Identify and push efforts to improve data monitoring and automation

Who you are


Loves to code (JavaScript, Python, Java)
Driven to make a huge difference and have positive impact in the company
Hands-on distributed computing architect
6+ years experience with Hadoop, MapReduce, Pig, Hive, Redshift
AWS / cloud computing experience
Deep understanding with data stores such as MongoDB and SQL
Knowledge and experience performing advanced analysis in a statistical computing language (e.g. Python, R, SAS) is a plus
Function well in a fast-paced, open, and bold culture in rapidly-changing environment
Bachelors or Masters in Computer Science
Strong oral and written communication skills

What else


Exciting startup environment with competitive salary with equity
Commuter reimbursements
Catered lunches several times a week
Medical, vision and dental benefits

","San Francisco, CA",Data Engineer,False
554,"SOFTWARE ENGINEER - Big Data

Intertrust Technologies Corporation is a privately-owned Silicon Valley company that provides products and services around security, trusted computing, and data management. Our groundbreaking enterprise trusted data platform enables IoT data marketplaces in a diverse set of industries that include healthcare, renewable energy, media and targeted advertising.
One of Intertrust’s products is the Personagraph Data Analytics Platform™ intended for brand advertisers, agencies, media buyers, and publishers to segment audiences based on a mobile user’s interests, demographics, and intents. Personagraph’s data management platform makes it easy to find, understand and connect with audience segments, and supports audience data-driven demand for open RTB (real-time bidding) and private marketplace programs. The Personagraph team is based out of Intertrust headquarters in Sunnyvale, California with locations in India (Hyderabad, Bangalore, Mumbai and Indore).
We are seeking a Software Engineer/ Data Engineer to join our expanding team developing to support our data pipeline team developing scalable machine learning algorithms, data acquisition and data visualization.
Responsibilities:
Work closely with cross functional teams to design and develop a Big Data platform.
Develop real time analytics data pipeline processing billions of events.
Work with QA and DevOps teams to troubleshoot any issues that may arise during production.
Data acquisition, data gathering from different sources for example US census data, mobile application data, movies and music data, location data like Points of Interest.
Develop queries and data visualization tools using technologies such as D3.js, Druid or Presto, enabling data insights into user’s behavior.

Key Qualifications:
2+ years of experience in software development in Java or Python, and/or machine learning.
Strong Computer Science fundamentals.
Expertise in Big Data technologies with Spark, extra points for Hadoop.
Exposure to one of these technologies: ElasticSearch, Kafka, SQL, NoSQL.
Experience designing, building and maintaining large-scale, high-performance systems and frameworks
Experience with at least one major language (such as Java, Scala, Python).
Strong understanding of data modeling and storage with NoSQL and relational databases.
Bachelor’s Degree or higher in Computer Science, Computer Engineering, Data Science or similar field or equivalent years of experience.
Optional experience:
Experience with machine learning, data science and analytics.
REST APIs, AWS.
IoT (Internet of Things).
Functional programing experience or interest.","Sunnyvale, CA 94085",Data Engineer,False
555,"After several consecutive years of double-digit revenue growth, ktMINE is investing in new markets, content and software and is seeking an Unstructured Data Engineer who can contribute to this expansion. If you are a driven person seeking a fast-paced environment where every employee has a voice and collaborates daily with people who appreciate the contributions that our technology team makes to the company, ktMINE may be the ultimate environment for you.

The Unstructured Data Engineer will be responsible for aggregating multiple types of data from public sources. Sources include syndicated news, corporate press releases or data pulled from scraping or crawling tools.

This job requires a high level of creativity and ingenuity in order to be successful. The challenges in this position are endless and the ideal candidate should be able to take arbitrary requirements and turn them into actionable tasks.

Once data is collected and stored within our environment this candidate should be able to work with the engineering team for integration into our databases and platforms depending on the context of the data.

Required Skills:
Deep knowledge of data analysis using regular expressions, NLP, machine learning and entity extraction
Deep knowledge of HTTP and manipulation of web requests
Data transformation and categorization into object models that can be used within the application
Skilled at database design and normalization principles
Knowledge of SQL and full text index database systems
Experience in agile development practices including fast paced development iterations
Bachelor’s degree in Computer Science, Information Systems, Data Science or other
related experience
2+ years’ experience as a software developer or a related field

Ideal candidate will have:
Strong communication skills
An analytical and detail oriented approach to solving problems
Infer nuance in complicated data structures
Ability to persevere and pursue development objectives from start to finish
Relentlessly and proactively attack problems and business challenges
Drive for perfection","Chicago, IL",Unstructured Data Engineer,False
556,"Job Description

In this role, you will be responsible for a range of duties from basic data analytics, to implementing and delivering advanced data engineering, visualization solutions and high impact business projects. You will get chance to leverage your business acumen, programming skills, technical knowledge of big data and machine learning techniques.
The position will be based at Visa's headquarters in Foster City, California
Responsibilities
Work closely with technical leads and business analysts to understand and document business and technical requirements and constraints.
Work with business teams in understanding the overall client requirements and proposed Big Data Solution
Be hands-on in leading and/or performing development work during the production cycle.
Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming
Gather and process raw data at scale (including writing scripts, calling APIs, write SQL queries, etc.).
Design and develop data structures that support high performing and scalable analytic applications.
Work closely with Data science to integrate amazing innovations and algorithms into data lake systems.

Qualifications

We are looking for a strong analytics thinker and problem solver, with a blend of business and technical skills:
Technical skills:
High level of competence in Spark and Unix/Linux scripts
Extensive experience with SQL for extracting and aggregating data
Real world experience using Hadoop and the related query engines (Hive / Impala)/Hbase
Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs
Experience:
Advanced degree in Computer Science or Computer Engineering, plus relevant experience.
5 years of working experience in developing and doing integration projects for enterprise level solutions.
Strong experience with Big Data Technology, including Hadoop, MapReduce, Spark is needed
Strong experience in Spark Python/ Scala and Java is needed
Work on MPP, Big Data technologies like Hadoop, GreePlum, Data Governance implementations and support.
Produce technical specifications and design for development, solution development/migration and systems integration requirements.
Participate and lead internal development and external collaboration meetings.
Oversee testing of data acquisition processes and their implementation into production.
Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of Article 49 of the San Francisco Police Code.
All your information will be kept confidential according to EEO guidelines.","Foster City, CA",Big Data Engineer,False
557,"skip to Main Content
888-658-6462 info@foresightintelligence.com
Open Mobile Menu
Home
Products We Offer
Fleet Intelligence™
Telematics Devices
Intelligent Alert Triage Center
Foresight Intelligence Center
Financial Reporting System
About Us
Company
News
Careers
Support Portal
Contact Us
SQL in the Sun 2018

Data Engineer
Our Data Engineer performs a wide range of job duties utilizing technical know-how and an exceptional, pro-active customer service approach, ensuring high levels of customer satisfaction. Maintains a very hands-on focus for technology matters combined with an affinity for solving complex technical issues and delivering projects on time and within budget.

Foresight culture is that of spirited team players in an environment energized by innovation and continuous improvement. We truly believe it takes an entire team united behind something big. So together, we work hard, we have fun, we brainstorm, we love ideas and we give high-fives in the hallway. Foresight Intelligence employees are encouraged to take a high degree of ownership and improve continuously. We work in an environment where your voice matters and where your actions have a direct positive impact on the team and the customer.

REPORTS TO: Vice President of Operations

Job Duties and Responsibilities:

Identify, evaluate and recommend appropriate technologies and strategies for building products and delivering services, from a database perspective.
Assist in design and development of database systems.
Secure the database and ensure its integrity.
Establish the needs of users and monitor the user-access model; control access permissions and privileges.
Develop databases functions, scripts, stored procedures, etc. to collect, process and present data.
Monitor performance and manage parameters of the database and provide fast query responses to front-end users.
Synchronize the conceptual design with the actual database.
Enhance and refine the logical design of the database.
Ensure appropriate system storage requirements.
Update the database by installing and testing new versions of the DBMS.
Develop and manage backups and construct recovery plans.
Create physical and logical database models according to business needs or requirements.
Provide technical assistance to sort out database related issues, identify errors and then resolve in a proper way.
Prepare documentation related to the database.
Work with DBAs to manage the company databases effectively.
Performs other duties and responsibilities as required or assigned.
Minimum Requirements:

Bachelor’s degree in computer engineering or related field, or equivalent knowledge, skills and abilities in software engineering.
5+ years of technical experience in a database engineering role.
Excellent oral/written communication skills and interpersonal skills, with the ability to articulate ideas to both technical and non-technical audiences.
Expert with MS-SQL; familiarity with C# programming concepts.
Innovative, self-motivated and self-directed with keen attention to detail, exceptional service orientation
Ability to work directly with customers, understanding and fulfilling their needs in a competent manner.
Ability to discern user requirements and develop specifications.
Experience with Microsoft system administration and web server configuration.
Knowledge of internet protocols, database management systems, revision control systems, information security vulnerabilities and risk management.
Possessing a business understanding of the underlying data.
Ability to follow projects through to completion.
Ability to learn new technologies quickly, step outside your own comfort zone and handle unfamiliar challenges enthusiastically.
Enjoys work (overcoming obstacles is fun), loves to help people and can work well independently or in groups.
SALARY & BENEFITS:

Competitive salary commensurate with experience

Benefit Plan

Foresight Intelligence pays 100% Employee-Only Premiums – Health, Dental, Vision, Life, AD&D, Short Term Disability, Long Term Disability
Enrollment in Stock Appreciation Rights Plan
Health Savings Account with Employer Contribution
401(k) with Employer Matching
Paid Time Off
Employee Recognition
Weekly Team Lunch
Fully Stocked Break Room
Salary commensurate with experience | Foresight Intelligence is an equal opportunity employer | Applicants must have right to work in the US
Email cover letter & resume to careers@foresightintelligence.com
Foresight Intelligence strives to drive your organization into the future.
Schedule a private demo to find out exactly what our technology can do for you.
Free Consultation
7077 East Marilyn Road,
Building Six, Suite 150
Scottsdale, AZ 85254

888-658-6462
info@foresightintelligence.com
Site Map
About Us
Careers
Contact Us
Financial Reporting System
Fleet Intelligence
Fleet Intelligence Demo
Foresight Intelligence Center
Form – Free Trial
Intelligent Alert Triage Center
News
Telematics Devices
News
Foresight Intelligence® Featured on AEMP Podcast
Foresight Intelligence® Named Mixed Fleet Solution Provider for John Deere Worksight™
CalAmp Technology Adopted by Foresight Intelligence to Speed Time to Market of Foresight’s Fleet Intelligence Application For Construction Industry Mixed Fleets
Social Media
Twitter
Facebook
GooglePlus
LinkedIn
© Copyright Foresight Intelligence. All Rights Reserved.
Privacy Policy
Home
Products We Offer
Fleet Intelligence™
Telematics Devices
Intelligent Alert Triage Center
Foresight Intelligence Center
Financial Reporting System
About Us
Company
News
Careers
Support Portal
Contact Us
SQL in the Sun 2018

Back To Top","Scottsdale, AZ",Data Engineer,False
558,"Job Description
Amazon WorkSpaces is revolutionizing end user computing in the enterprise by providing Desktop as a Service from the AWS Cloud. Amazon WorkSpaces allows customers to easily provision cloud-based desktops that allow end-users to access the documents, applications and resources they need with the device of their choice, including laptops, iPad, Kindle Fire, Android tablets, and zero clients. With a few clicks in the AWS Management Console, customers can provision a high-quality cloud desktop experience for any number of users at a cost that is highly competitive with traditional desktops and half the cost of most virtual desktop infrastructure (VDI) solutions. This AWS offering has an immense green-field market to tap into, and is growing fast – this role is an opportunity to get on the ground floor.
This role will work closely with product management and engineering teams. You will help the BI team deliver financial and operational analyses, surfacing opportunities to reduce cost and innovate, manage projects, improve processes and help take the business team to the next level. As a member of this team, you will leverage strong data extraction skills to drive worldwide reporting deliverables, engineer ad hoc financial analysis, support business planning and implement ideas for process improvement. The individual must have the ability to communicate effectively across multiple technical and non-technical functions. Successful members of this team collaborate effectively to solve data problems, implement new reporting solutions, and deliver successfully against high operational standards.

Responsibilities include:
Sourcing, analyzing, and reporting quantitative data; perform ad hoc analysesAnalyzing trends, formulating projections, and evaluating savings initiativesSupport the development of continuously-evolving automated forecast models and methodologies, owning the quantitative analysis of the performanceProvide fact-based insights into variances and trendsPresentation of findings and recommendations to Senior Leaders


The ideal candidate will demonstrate the following abilities and characteristics:
Focus on customers and work backwardsBe highly analytical and detail orientedQuickly build credibility and trust with internal customersDemonstrate work ethic based on a strong desire to exceed expectationsAdvanced communication skills, both verbal and writtenSuccessful in an ambiguous environmentComfortable with tight deadlines and prioritize workload
Work on cross-functional teams
Basic Qualifications
BS/BA degree in a quantitative field (Mathematics, Engineering, Sciences, Operations Research)
3+ years of experience building business intelligence reports using one or more of the following tools: Tableau, MicroStrategy, and PowerPivot.4+ years of relevant work experience in analytics as a data scientist, data engineer, business intelligence engineer, or equivalentAdvanced knowledge of SQL and experience with efficient processing of large data sets is a mustFamiliarity with new advances in tools such as EMR and NoSQL technologies like DynamoDB.Track record for quickly learning new technologies.
Preferred Qualifications
MBA or advanced degree in a quantitative field preferredAbility to consistently meet deadlines, and provide well-vetted reporting and business recommendationsExperience communicating results to business leadersAdvanced problem-solving skills for difficult and complex issuesExperience with advanced statistical methods (e.g., matching, predictive modeling, cluster analysis, time series modeling, etc.) and data visualizationProficient in at least one scripting language (Perl, Python etc.)Experience working with Amazon Web Services (AWS) tools (i.e. S3, Redshift)","Bellevue, WA",Workspaces BI engineer,False
559,"Data Engineer in Cloud Platform

How would you like to collaborate on building out large data processing architectures? Do you thrive on variety and innovation in your daily work and would you enjoy close interaction with a world-class ""big data"" platform team? Are you passionate about making things work better, stronger, faster? If so, read on!

Bose is transforming the way audio devices interact with the Cloud, creating new experiences for connecting people to what they love the most. We are imagining new data-driven experiences, designing new services, building software architecture and infrastructure, and scaling our solutions to serve millions of users. Join us as a Co-op to help invent and build our cloud platform for the 21st century and power the next wave of innovation at Bose.

Principal Duties and Responsibilities
As a Co-op on the Cloud Platform Data Team you will collaborate on designing and developing major database components of our next generation cloud platform. Our data platform team takes on these challenges:
Builds highly scaled data abstractions and infrastructure like key:value and secret storage
Designs large graph infrastructure and query/search interfaces
Collects and processes data streams at scale (including stream-processing and batch ETL)
Works closely with engineering teams to help build and maintain systems that support advanced analytics
Quickly triages and analyzes complex performance and operational problems
Works with platform architects on software and system optimizations, helping to identify and remove potential performance bottlenecks
Selects and deploys best-of-breed open-source software and work with the larger community for support and improvements
Gains familarity with relevant technologies, plugs into user groups, understands trends and opportunities to ensure we are using the best techniques and tools








Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.","Framingham, MA 01701",Data Engineer in Cloud Platform -- Co-op,False
560,"Do you want to get a foot in the door at an online Advertising and Media company that has seen tremendous growth? Do you want to work in the world of online video; one of the fastest growing advertising formats in the world? Then Playwire is the place for you...Playwire Media is a full-service digital innovation partner that leverages online advertising and proprietary technologies to build publishing brands in the gaming and entertainment verticals.To keep pace with our explosive growth, we are currently seeking a Data engineer for our Deerfield Beach office.The ideal candidate has a passion for all things data, from input to output, and loves understanding and tweaking every aspect of a successful data pipeline. They should be adept at explaining data concepts both to software developers and non-technical stakeholders, helping them to understand core concepts and the pros and cons of different approaches. Candidates should like the responsibility and opportunity for ownership and vision within our dynamic, fast-paced (and fast-changing) environment; they must be ready to move quickly while also ""defending the fort"", making certain that we build our data systems in intelligent, scalable ways.Essential Functions: Work out of our Deerfield Beach office. Relocation assistance available.Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using database and AWS “big data” technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics for different areas of our business.Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.Qualifications: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing “big data” data pipelines, architectures and data sets.Experience performing analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of options for realtime analysis and presentation of data extracted from pipelines.Experience supporting and working with cross-functional teams in a dynamic environment.Nice To Haves: Working experience operating AWS-related data services, particularly Kinesis, Kinesis Firehose, Kinesis Analytics, Athena, RDS, and Redshift.Knowledge of BI toolsets.Experience with error logging and reporting.Knowledge of JavaScript and Ruby.Strong project management and organizational skills.Job Type: Full-timeExperience:building big data: 2 years (Preferred)SQL: 2 years (Preferred)Education:Bachelor's (Preferred)Work authorization:United States (Preferred)","Deerfield Beach, FL",Data Engineer,False
561,"Hart is a medical software company utilizing technology to bridge the gap between patients and providers. Our health and fitness platform encourages users to engage with their health in real-time. Health-conscious individuals can manage activities, medication, nutrition, sleep, track progress with a personal Health Score, and communicate directly with healthcare providers and employers.

Job Description:
As a Data Engineer at Hart you will work with the company's engineering team to develop an entirely new software system. This system will have the ability to automate segmented data sources and manipulate those data sources. These applications may use apache spark to render the data. Primary responsibilities include requirements analysis to gain an understanding of business needs to develop level of effort estimates, software design and development, unit testing, performing design and code reviews, and development of technical documentation. Additional responsibilities include performing system analysis, code modifications, and functional testing to troubleshoot application production issues.

You will be working with Python to build out Hart's data discovery services for use with all our applications that support the Hart ecosystem of applications and connected user devices. This role will join a team focused on building products on the cutting edge of usability, interaction, and design.

The Data Engineer, Python will work in a high-communication environment where collaboration with the Design and Backend Engineering teams is key to day-to-day success. Our ideal candidate is someone who's comfortable taking ownership of their work and effectively searching for creative solutions to the complex problems we encounter.

Requirements


Excellent programming skills in Python
Experience with Spark and Kafka - REQUIRED
Experience with Hadoop and MongoDB
Understanding of Distributed Process management
Scala, Java, Julia, and/or R experience is a plus

Employee Benefits Package


100% Medical, Dental and Vision coverage for you and your family
Unlimited Vacation Policy
Paid Paternity and Maternity Leave
Life Insurance
AD&D Insurance
Gym Membership

Perks


MacBook Pro, Thunderbolt Display, Magic Mouse and Keyboard
Stocked kitchen with coffee, drinks, and snacks
Daily catered team lunches provided by our chef

","Costa Mesa, CA","Data Engineer, Python",False
562,"Our company, National Retail Solutions (NRS), is a subsidiary of IDT, with many products including its flagship Boss Revolution products sold in tens of thousands of stores. NRS uses this relationship to expand the offerings that IDT provides to retailers.

The technology division is a small, high-powered development group with teams situated in the US and Israel that produces interactive, high-performance web applications that are both good looking and easy to use. We use the same technology stack for apps deployed on the Linux-based point of sale systems as we do for single page web apps deployed in the cloud. We currently have several openings for our development team.

For all positions, the successful applicant will be a self-starter, work independently or part of a small team when needed, developing software for our merchants, their customers and NRS internal operations, and contribute to overall architectural and design discussions. You will generally be responsible for the complete creation of a feature including design, implementation, deployment and support.
We are looking for:
Data Import/Export Development (Shell, SQL, PHP, Database, AWS, Linux)
Minimum of 5 years of programming experience
Degree in Computer Science
Fluency in SQL, including optimization of queries
Feel a t home with the Linux command line and system
Ability to create shell and/or PHP scripts for import and export of data
Comfortable using curl, S3 tools, and standard automated file transfer tools for data exchange
Experience with using a distributed source code control system such as Git (preferred).
We would prefer to see:
Experience with various AWS services and their management and operations including EC2, RDS, Kinesis, SNS, SQS, and Lambda functions
Experience with Real Time data feeds

Additional nice to have, though not required:
Ability to create PHP based web services (APIs) using Slim routing, PostgreSQL and Redis
Experience in creating scalable, fault-tolerant architectures in AWS
Experience with Node.js as a backend alternative
Linux server management ability
NRS operates a point-of-sale (PoS) terminal-based platform for independent retailers and bodega owners nationwide. Our media division provides the ability for advertisers to reach our retail partners’ consumers. The platform provides a robust portfolio of tools to help these retailers compete more effectively including rewards programs, consumer coupons, wholesaler discounts, and integration with Boss Revolution® communication and payment service products. Consumer package goods (CPG) suppliers are able to leverage the NRS platform to provision promotions, coupons and special offers to independent retailers and their predominantly urban customer bases nationwide. NRS is a subsidiary of IDT Corporation (NYSE: IDT).","Township of Brick, NJ 08723",Data Engineer,False
563,"Overview
Walsh is currently seeking a Data Engineer for the Power BI Team based in Chicago, IL.

We are seeking an experienced Power BI / SSAS (SQL Server Analysis Services) Engineer who can provide companywide strategic, analytical and technical support for BI activities. This is an opportunity to work in a fast-pace, team environment collaborating directly with all business units. This is a hands-on, results orientated role requires expertise in the Microsoft toolset for data and data analytics, including SQL Server, data models in SSAS, and Power BI dashboard creation.

Walsh is a rapidly growing, highly diversified construction company, and we constantly seek builders and business people to join our industry-leading team. Walsh employees are ""built to succeed"" - competitive entrepreneurs with strong character who are energized by working on a team to meet challenges and are willing to take risks after careful planning. There are many compelling reasons why exceptional people should consider a career with our company:
Challenging, complex projects
Creative and innovative problem solving environment
Supportive, communicative managers who reward your success
Opportunities for growth, training, and development
Flexibility to build what you want, where you want
Responsibilities
Collaborate with business leaders to analyze critical data and translate business requirements into technical, scalable specifications
Design, develop, test, and document new reports and dashboards.
Mentor others in using analytics tools
Evaluate and enhance existing reports and dashboards
Define and document business requirements for new metrics and reports
Ensure accuracy and integrity of data and reporting applications through detailed analysis, efficient coding, writing clear documentation processes, identifying and resolving problems as they arise
Review and write complex SQL queries and develop stored procedures and functions in SQL
Perform ongoing monitoring and refinement of reports and BI solutions
Ability to work effectively within competing deadlines with minimal guidance
Interact professionally and collaborate with a diverse group including executives, managers, and subject matter experts
Qualifications
4 or more years of quality experience using SQL Server, SSRS, SSAS, Azure, and SSIS
Strong knowledge of relational and multi-dimensional database architecture
Experience creating and maintaining documentation following standard creation and change control processes
Proficient oral and written communication skills
Ability to lead a meeting and present to small audiences
Experience integrating Power BI into web applications

Equal Opportunity Employer, Disability/Veteran
#LI-EK1","Chicago, IL",Data Engineer,False
564,"Spokeo is seeking a Data Engineer to join us in Pasadena, CA.

Spokeo is a people search engine that both enlightens and empowers our customers. With over 12 billion records and 18 million visitors per month, we reconnect friends, reunite families, prevent fraud, and more. Every day our nimble team takes on enormous challenges in data science that push the limits of the cloud and search architecture.

We are looking for a Data Engineer with an eye for building and optimizing big data systems to join our team. Working in an AWS, Spark, Hadoop ecosystem, this role will work closely with the engineering and analyst teams to direct the flow of data within the pipeline and ensure consistency of data delivery and utilization.

Responsibilities:

Design and build the infrastructure for data extraction, preparation, and loading of data from a variety of sources.
Build and manage existing analytic tools to provide deeper insight into the pipeline and capture key metrics.
Monitor technical performance and ensure that identified bugs are routed and resolved.
Mentor team members on working with highly scalable distributed systems and cluster architectures and maintain up-to-date knowledge of technological advances.
Create and maintain technical documentation.
Work with large, complex SQL/NoSQL databases
Create unit and stress test scripts/modules.
Write well-abstracted, reusable and efficient code.

Requirements:

Bachelor's degree in computer science, information technology or related field (willing to accept foreign education equivalent)
Hands-on scripting or programming
Experience working in big data ecosystem (e.g. Hadoop, Spark, Kafka) with complex SQL/NoSQL databases (Cassandra, DynamoDB)
Experience and understanding of ETL tools.
Prior experience working with highly-scalable, distributed systems and cluster architectures (e.g. AWS, Azure, Google Cloud etc.)
Prior experience working with large data sets ( > 10 billion).

Recruiters or staffing agencies: Spokeo is not obligated to compensate any external recruiter or search firm who presents a candidate or their resume or profile to a Spokeo employee without 1) a current, fully-executed agreement on file and 2) being assigned to the open position (as a search) via our applicant tracking solution.","Pasadena, CA 91101",Data Engineer,False
565,"We are a former real estate startup, acquired by CBRE in 2017. We are uniquely positioned within the organization to access global data, tools, resources, and leaders throughout the industry. We have the stability of secure backing and the freedom to explore and experiment to develop leading-edge products. For more information about our team, please visit http://nyc.cbrebuild.com

We are looking for the first data scientist/engineer/analyst to help us understand and leverage all the data sources and products we have. This person will have a great deal of autonomy and will be expected to come to the table with their ideas and suggestions for making our products better with data.

Responsibilities:
Analyzing proprietary commercial real estate data on availabilities, lease rates, and market statistics to identify trends and opportunities
Refining predictive models for space estimation and time-forecasting for capacity planning
Empowering user research and improving customer engagement through development and analysis of user metrics
Architecting data models and developing ETL functions to move data between RDMBS and NoSQL stores
Leveraging machine learning and data mining to take multiple streams of data and create novel customer experiences
Automating manual processes to generate data like lease availabilities and floor plan layouts
Working closely with our heads of Product and Engineering as well as team leads across the company to address a wide variety of business problems
Identifying new approaches and tackling ambiguous problems with creativity and improvisation


Requirements:
Smart, productive and tolerant of sarcasm
Self-motivated and curious, crafty and driven
Interested in building a friendly, collaborative, and transparent work environment
Programming experience in Python, Julia, Torch (Lua), or other programming languages/frameworks
Bachelor's degree in a related field or equivalent work experience
Minimum of 3 - 5 years of relevant experience required

Why work at CBRE Build?
Great team - Diverse. Smart. Nice. We like to always keep learning.
High impact - We’re a small team with a global reach in a trillion dollar industry.
Growth and development - We invest in employee development. Time to explore. Machine shop with a laser cutter and 3d printer. Funding for education, conferences, etc.
Benefits - Unlimited vacation. Flexible work hours. Work life balance. Health, vision, dental for employees and their dependents. 401k plan with a 3% match.","New York, NY",Sr. Data Engineer,False
566,"Oath, a subsidiary of Verizon, is a values-led company committed to building brands people love. We reach over one billion people around the world with a dynamic house of 50+ media and technology brands. A global leader in digital and mobile, Oath is shaping the future of media.



The Oath Analytics team is searching for a qualified, highly motivated Data Engineer. This opportunity will require the individual to creatively approach problems from both an analytical and automation perspective in order to deliver scalable and innovative data, reporting and dashboard solutions in a fast paced, team oriented environment. Oath is a data intensive organization, and this position requires the ability to transform large volumes of data into information for operational and financial decision making. Driven by your passion for data and utilizing various programming methods, you will work with both internal and external partners to define and craft innovative reporting solutions in support of our leading global digital advertising products.

Required Skills:
• Passionate about data and automation
• Intermediate knowledge of relational databases and SQL
• Basic knowledge of Python
• Experience with UNIX based systems
• Experience with ETL processes
• Experience with MicroStrategy, building dashboards and adhoc solutions
• Strong analytical and problem solving skills
• Strong communication skills; both written and verbal
• Strong time management and prioritization skills
• Familiarity with Hue/Hive/PIG/Spark/Hadoop, Tableau/Data Visualization or Web Development (Javascript, HTML, CSS, PHP) a plus

Preferred Experience:
• Bachelor’s degree in Computer Science, Mathematics, Statistics, Engineering or Economics preferred
• Prior experience in a related analytics or developer role, digital advertising or media experience a plus


Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","New York, NY",Oath Analytics Data Engineer,False
567,"Who We Are
GalaxE.Solutions
Every day, our solutions affect people throughout the world. From Fortune 100 companies to start-ups, GalaxE develops and implements strategic projects that are critical to the success of customers’ businesses and the lives of tens of millions of people.

For over twenty-five years, we have grown and evolved into a multi-national firm that employs over 2000 team members worldwide. But we’re not done evolving. It took collaboration and innovation to get here, and it takes collaboration and innovation to get where we (and our customers) want to be tomorrow.

What does this mean for our employees? They have the security of an established company, with the benefits of working for a company where great minds, hard work, leadership and innovation are highly regarded and rewarded.

As Thomas Edison said, “There’s a way to do it better – find it”. We want our employees to find it.

We’re looking for creative people, with an entrepreneurial spirit, looking to work on awesome projects! Sound like you? Come work with us! Find out for yourself what it means to be part of the GalaxE team.

It’s not always easy, but important work never is. #WeAreGalaxE

Equal Opportunity Employer/Veterans/Disabled
What You Will Do
Full lifecycle application development
Design, code and debug software
Perform software analysis, risk analysis, reliability analysis
Participate in software modeling and simulation
Integrate new software solutions with existing systems
Extract and reverse engineer existing code
Perform regular status reviews of problems/issues
Participate in the development or refinement of proactive services and/or data repositories
Query database to provide data extracts
Skills and Experience You Will Need
Required:
5+ years of development experience in at least one of these languages: Java, Python, or C#.
Experience with RESTful API design and implementation.
Experience in Workflow and BPMN development using Activiti, Camunda, etc.
Experience with Data migration, transformation, and scripting.
Proficient understanding of code versioning tools, such as Git.
Ability to work cross-functionally with engineering and non-engineering teams.
Passionate about engineering quality, testing, automation, and documentation of code and systems to ensure easy maintenance over a long period.
Experience working on high-volume server software.
Understanding and implementation of security and data protection.
User authentication and authorization between multiple systems, servers, and environments.
Experience with NoSQL databases such as DynamoDB
Experience with data frameworks such as Hadoop, Hive, Pig, and Spark
Experience in building high-performant, scalable backend services in the cloud (especially AWS)
Desired:
Experience with MuleSoft application development
Passionate about application scalability, availability, reliability, and security.
Exposure to Test Driven Development, Behavioral Driven Development frameworks and libraries
Experience with Collibra, administration, management, working with APIs
Experience with Anypoint APIs","Detroit, MI",Data Engineer,False
568,"Contract7+ years of IT experience and 3+ years of big data.Work across multiple client projects that require developing and implementing big data analytics solutions.Strong Understanding in at least one of Python, Scala, Java or C++1-3 years of working experience in Apache Hadoop, Sqoop, Avro, Flume, Oozie, Zookeeper, Yarn.Hands on experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for programing common tasksManage client deliverables, communicate successfully with internal and external teamsExcellent communication skills to be able to effectively translate to business stake holdersWith this position you will be on Up2date Technology Solutions, LLC Payrolland will be working at my client's site as posted on the job description.Job Type: ContractSalary: $50.00 to $55.00 /yearExperience:BigData: 3 years (Preferred)","Plano, TX",Big Data Engineer,False
569,"Job Description
Amazon’s eCommerce Platform (eCP) organization is responsible for the core components that drive the Amazon website and customer experience. Serving millions of customer page views and orders per day, eCP builds for scale.

As an organization within eCP, the Big Data Technologies (BDT) group is no exception. We collect petabytes of data from thousands of data sources inside and outside Amazon including the Amazon catalog system, inventory system, customer order system, page views on the website and Alexa systems. We also support Amazon subsidiaries such as IMDB and Audible. We provide interfaces for our internal customers to access and query the data hundreds of thousands of times per day, using Amazon Web Service’s (AWS) Redshift, Hive, and Spark. We build scalable solutions that grow with the Amazon business.

BDT is growing, and the data processing landscape is shifting. Our data is consumed by thousands of teams across Amazon including Research Scientists, Machine Learning Specialists, Business Analysts and Data Engineers. Amazon.com is seeking an outstanding Data Engineer to join the Big Data Technologies Business Intelligence team. The Business Intelligence team delivers business intelligence to over 1000 internal customers, and a diverse community of external customers. Amazon.com has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the Amazon.com Business Intelligence team your work will have an immediate influence on day-to-day decision making at Amazon.com.

As an Amazon.com Data Engineer III you will be working in one of the world's largest and most complex data warehouse environments. You should be skilled in the architecture of DW solutions for the Enterprise using multiple platforms (RDBMS, Columnar, Cloud). You should have extensive experience in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

As a Data Engineer III on Amazon.com’s Big Data Technologies team, you will develop new data engineering patterns that leverage a new cloud architecture, and will extend or migrate our existing data pipelines to this architecture as needed. You will also be assisting with integrating the Redshift platform as our primary processing platform to create the curated Amazon.com data model for the enterprise to leverage. You will be part of a team building the next generation data warehouse platform and to drive the adoption of new technologies and new practices in existing implementations. You will be responsible for designing and implementing the complex ETL pipelines in data warehouse platform and other BI solutions to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making at Amazon.com.

Interfacing with business customers, gathering requirements and developing new datasets in data warehouseBuilding and migrating the complex ETL pipelines from Oracle system to Redshift and Elastic Map Reduce to make the system grow elasticallyOptimizing the performance of business-critical queries and dealing with ETL job related issuesTuning application and query performance using Unix profiling tools and SQLIdentifying the data quality issues to address them immediately to provide great user experienceExtracting and combining data from various heterogeneous data sourcesDesigning, implementing and supporting a platform that can provide ad-hoc access to large datasetsModelling data and metadata to support ad-hoc and pre-built reportingWorking with customers to fulfill their data requirement using DW tables & maintain metadata for all DW Tables.
Basic Qualifications
A desire to work in a collaborative, intellectually curious environment.Degree in Computer Science, Engineering, Mathematics, or a related field and 4-5+ years industry experienceDemonstrated ability in data modeling, ETL development, and data warehousing.Data Warehousing Experience with Oracle, Redshift, Teradata, etc.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sourcesQuery performance tuning skills using Unix profiling tools and SQLExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering spaceLinux/UNIX including to process large data sets.Experience with AWS","Seattle, WA",Data Engineer - Big Data Technologies,False
570,"Healthcare Data EngineerCompany OverviewImagine if we could accelerate healthcare discovery leading to more effective treatment and cures? Here at iSpecimen, we are working to get researchers the specimens they need from the patients they want to do just that. Headquartered in Lexington, MA, iSpecimen is the marketplace for human biospecimen collections. The privately held company has developed the iSpecimen Marketplace, an online platform connecting healthcare organizations that have access to patients and specimens with the scientists who need them.Healthcare Data Engineer Position OverviewWe are looking for a highly-skilled, self-motived Healthcare Data Engineer with strong communication and technical skills to join our collaborative environment. This individual will be responsible for working closely with our team to ensure our data processes are efficient, effective, and well run. The right individual will have a passion for data, quality, improvement, data pipelines, and reporting with a solid understanding of healthcare data vocabularies.Responsibilities of Healthcare Data EngineerWork with the team to understand our suppliers’ data and how we can add valueDefine, design, implement, improve, and support ETL solutions using the myriad healthcare and behavior measures and controlled vocabularies used to drive marketplaceLead the data onboarding and transformation processes for various source dataContinuously improve control processes, ensuring data integrity, quality, standardization, and security throughout the ETL processDevelop test methods, measurement tools, baseline reference data and measures, and analyses to support our reporting and visualization technologies and applicationsFunction as a healthcare data subject matter expert to support the design, development, testing, implementation, documentation, and support of our solutionsProvide complete documentation and communication of all processes, methods, and resultsSupport production solutions and the ongoing updating and maintenance of our reference data sourcesParticipate in our data strategy design and deliveryQualifications of Healthcare Data EngineerBS/BA degree in healthcare informatics, information systems, computer science, or related information technology degree preferred. Advanced degree in one of these areas preferred5+ years of experience in healthcare informatics and hands-on experience handling diverse healthcare data sets especially lab and transforming them into consistent, high quality data sets3+ years of experience exploring and analyzing data and/or scripting ETL processes using SQL1+ years of experience using a scripting language (Python, Bash, Perl, etc.) to automate processesExtensive knowledge of existing healthcare databases, data sources, and industry standard measures and code sets (e.g. ICD9, CPT/HCPCS, DRG, NDC, GPI, LOINC) Knowledge of current standards (e.g. HL7). Clinical lab and/or biorepository experience a plusProficient using SQL with solid familiarity and experience with database applicationsSome database management experience with the ability to explore and manage database internals (e.g. storage/memory, query performance, users, permissions, etc.)Experience integrating tools and/or workload automation software to streamline processesExperience with data analysis/visualization applications (e.g. Domo, Tableau) to generate reportsAdvanced analytic, reporting, and problem-solving skills around healthcare dataExperience with a cloud computing platform (e.g. AWS) a plusExcellent written and oral communication skills with a proven ability to present solutions with both technical and non-technical argumentsIntensely curious, not settling for accepted or superficial answers, always asking why and digging deeperBenefitsiSpecimen offers a generous suite of comprehensive benefits including ten paid holidays per year, paid time off (PTO) for vacation, sick, or personal use, health, dental, and vision insurance, health savings accounts, and a 401(K) retirement plan.Job Type: Full-timeExperience:healthcare informatics: 3 years (Required)data analysis: 1 year (Preferred)Education:Bachelor's (Required)Work authorization:United States (Required)","Lexington, MA 02420",Healthcare Data Engineer,False
571,"$45 - $50 an hourContractLocation: FL6+ monthsLeads team of developers, responsible for the analysis, design, development, testing, and documentation of information management systems. Drives defining and maintaining development standards and development best practices, and will participate in and contribute to technical feasibility studies, business cases, proposals, and 3rd party assessmentsKey Duties and ResponsibilitiesActs as principle contributor in obtaining business and technical requirements, GAP analysis, technical design, development and documentation of information systems using data transformation techniques to move the data between systems.Prepares detailed functional specifications, design models, and system work flows from which software applications will be developed and implemented.Provides leadership and facilitation in the definition and development of the technical solution concept, including vision, scope, and system architecture.As part of production support, supports, troubleshoots, and maintains production systems as required, optimizing performance, resolving problems, and providing timely follow-up on identified issues.Provides business support on data comparison as requested by the business.Defines and maintains WellCare Information Systems (IS) development standards and best practices.Supports, troubleshoots, and maintains production and ad-hoc reports as required, optimizing performance, resolving problems, and providing timely follow-up on identified issues.Performs other duties as assigned.EducationState the minimum required for the jobEducation Level Education Details Required/PreferredA Bachelor's Degree in Information Technology or related field Requiredor equivalent work experience Additional 2 years of relevant work experience may be substituted in lieu of degree RequiredWork ExperienceState the minimum required for the jobExperience Level Experience Details Required/Preferred5+ years of experience in Data Engineer or similar technology related role RequiredOther Internal candidate will require min 4 years of experience in data engineer or similar technology role Required2+ years of experience in Healthcare PreferredSkillsState the minimum required for the jobSkill Sets Other Skills ProficiencyDemonstrated interpersonal/verbal communication skills AdvancedDemonstrated written communication skills AdvancedAbility to work as part of a team IntermediateAbility to work in a matrixed environment IntermediateDemonstrated project management skills BeginnerTechnologyList technical skills associated with the jobTechnology Other Technology Proficiency Required/PreferredOther Structured Programming Language (e.g., Java, Python, etc.) ( Based on the organization under EIM that they report to, this position needs one or more skills as ""Required"") Intermediate RequiredOther Database Design, Construction, Tuning and Query ( Greenplum, Hadoop) ( Based on the organization under EIM that they report to, this position needs one or more skills as ""Required"") Intermediate RequiredOther Data Cleansing and Transformation(Informatica PowerCenter, BDE, Data Quality, etc.) ( Based on the organization under EIM that they report to, this position needs one or more skills as ""Required"") Intermediate RequiredOther Business Intelligence tools (e.g., Tableau, Cognos, SAS, etc.) Advanced RequiredOther Solution Design and Architecture Intermediate RequiredOther Master Data Management Intermediate PreferredLevel of Supervision Received:A statement which describes the level of independence for this position.Functions independently within broad scope of established departmental policies/practices; generally refers specific problems to supervisor only where clarification of departmental operating policies/procedures may be required.Problem Complexity:A statement which describes how clearly a problem is defined when presented, how much additional effort is required to understand the nature of the problem and the typical timescales for resolution.Provides resolution to a diverse range of recognizable complex problems. Analysis is required to identify root cause. Uses judgment within defined boundaries to develop solutions.Job Type: ContractSalary: $45.00 to $50.00 /hourExperience:Tableau: 5 years (Preferred)","Chicago, IL",Tableau Developer/Data Analyst,False
572,"Description:
JOIN US AS A LEAD DATA ENGINEER – DIGITAL DATA SOLUTIONS
The Data Science and Engineering team at Target is a hyper-growing, dynamic and collaborative team. Data Engineers work closely with Data Scientists to create valuable insights using voluminous data collected from internal and external systems on a large scale. Business operations are empowered with these insights to achieve Target’s strategic initiatives while providing world-class shopping experiences for our guests.
About this Opportunity
As a Lead Data Engineer, you’ll have the opportunity to create software solutions using Agile practices and DevOps principles. Responsibilities will include designing, programming, debugging and supporting high quality, distributed, and large-scale software solutions on the latest Big Data tech stack. Designging and building data pipelines using large data sets on game changing services/products in the innovative Enterprise Data Lab.

We are looking for a highly motivated engineering professional who is a team player and can engage with both technical and business team members while mentoring/developing Sr Data Engineers on the team.
Key Responsibilities:
Develop software systems using test driven development employing CI/CD practices
Partner with other engineers and team members to develop software that meets business needs
Follow Agile methodology for software development and technical documentation
Innovate constantly and stay current with latest technologies while staying focused on solving problems at hand
Requirements
BS degree in Computer Science or relevant experience
5+ years’ experience in developing software applications
3+ years’ experience working on Big Data technologies like Hadoop, Spark, Scala and Hive
Extensive understanding of application/software development and design
Proficiency in at-least one of the following languages: Java, Scala, Python
Worked on building and supporting Web Services. Experience with REST APIs’ and services preferred
Experience/knowledge of streaming solutions: Kafka, Apex
Qualifications:","Sunnyvale, CA",Lead Data Engineer,False
574,"Supremus is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Supremus will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Duties & Responsibilities:

Supremus is seeking Big Data Engineers with technical experience in optimizing of management and deriving insights from large, non-structured, non-relational data.

In this role you will:

Apply semantic correlation, ontology structured data, and text analytics techniques and systems to analyze non-structured data and identify critical insights.
Apply Big Data technologies, such as Hadoop or Cassandra, with NoSQL data management and related programming languages, such as Jaql, HBase, Pig, or Hive.
Participate in all aspects of the software life cycle, including analysis, design, development, unit testing, production deployment and support.
Formulate approaches and gather data to solve business problems, develop conclusions and present solutions through formal deliverables.
Create Big Data accelerators to help deploy scalable solutions fast.
You will be successful in this role if you enjoy problem solving and utilizing consulting skills. Team leadership experience is preferred.

In this dynamic role, you will have the opportunity to interact directly with clients. As such, travel to client sites may be required, up to 4 days per week.

Qualified candidates are encouraged to please send resumes to careers@supremusglobal.com","Philadelphia, PA",Big Data Engineer,False
575,"Overview
Linden Lab is hiring a Data Engineer to service our next generation VR and virtual experience platform, Sansar. The Data Engineering team supports everything from business analytics to real-time fraud mitigation, and this role will contribute by developing real-time and batch ETL, modeling data, and helping build out our real-time data pipeline. Come join a friendly, seasoned team and a great company as we change the world.
What you’ll do
Build ETL code to populate our Google BigQuery data warehouse with Apache Airflow scheduled batch updates from our Sansar Virtual Reality platform
Develop real-time ETL apps using Google DataFlow (Java or Python) to provide critical insights into the business
Maintain, improve, troubleshoot, and evaluate real-time data processing systems such as PubSub, Kafka, and Stackdriver
Work closely with our Data Architect, Product Managers, and Analysts to design and model new tables to meet constantly-evolving analytics needs
Liaise with our systems engineers, Google support, and our consulting partners to quickly assess the impact of production system changes to existing data warehouse processes
Other duties may be assigned
What you need
Extensive Real Time Data Engineering experience - we are not looking for a Data Analyst or Scientist.
3 to 5 years of active coding using Python or Java.
Strong Analytics Modeling experience.
Very strong SQL and relational database experience.
Familiarity with non-relational document stores.
Fluency in Linux; some system admin experience will be useful.
Experience with message queues like Kafka, Pubsub, Kinesis etc. and working with real-time systems. Prior experience with Google DataFlow and/or Spark is a big plus.
Experience working with large-scale data warehouse platforms such as BigQuery, Redshift, Snowflake, Teradata, etc..
Desire to work in a collaborative, entrepreneurial environment on really interesting problems.
Strong attention to detail and the ability to ensure that warehouse data is complete and accurate.
Bachelor’s Degree in a computer/database related field or equivalent professional experience.
What you’ll learn
Advanced real-time processing concepts
Advanced data modeling
Google Compute Platform tools and methods
Pioneer our use of graph databases","San Francisco, CA",Data Engineer,False
576,"Key member of high-performing team providing fact base for sales transformation and sales acceleration activities, for Cisco’s entire sales force.

Collaborate with rest of Sales Analytics team (data scientists, analytics managers and data foundation project managers) to drive company growth, operational efficiency and profitability.

Build relationships with database architects and data stewards across Cisco to discover new data sources and influence corporate decisions for greater functionality in our data systems.

Greatly enhance team productivity: scale manual processes, accelerate data visualizations and reduce data manipulation through in-memory data enrichment and metrics calculations.

Prototype data foundation improvements by working with sales stakeholders and peer analytics organizations to identify data infrastructure needs. Provide innovation pipeline to data foundation project managers to add to the company data warehouse.

Assist data scientists in building and optimizing sales analytics predictive models. Models under development include forecast models, product recommendation models, classification models and natural language processing.


Qualifications for Data Engineer

Fast, creative problem-solver to support projects in response to the real-time nature of sales leadership decision-making.

Continuous improvement mindset to optimize data architecture for repeatability and scalability, greatly enhancing team productivity.

Strategic visionary for Cisco’s data architecture to support next generation sales data initiatives.

Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases. SAP HANA experience is a plus.

A successful history of manipulating, processing and extracting value from large disconnected datasets. Experience collaborating cross-functionally to build and optimize ‘big data’ sets, for both structured and unstructured data.

5+ years of experience in a Data Engineer role, with a degree in Computer Science, Computer Engineering, Information Systems or another quantitative field.","San Jose, CA",Data Engineer - Sales strategy,False
577,"About Us:
Swift Navigation ( https://www.swiftnav.com/ ), Inc. was founded in 2012 to make GPS positioning technology more accurate and affordable. Its GPS and GNSS positioning products are available a fraction of the price of the competition and deliver 100 times better accuracy than the GPS in a cell phone. Swift Navigation's technology benefits a multitude of industries and applications—including autonomous vehicles, drones, precision agriculture, robotics, surveying and space. With its innovation and technology honored by Inc.'s 2016 ( http://www.inc.com/profile/swift-navigation ) and Forbes 2017 ( http://www.forbes.com/profile/swift-navigation/ ) 30 Under 30 lists, Swift Navigation is enabling a world where fields farm themselves, drones fly safely and autonomous transportation can take you home. Swift Navigation provides an end-to-end GNSS solution with a line of Piksi® Multi and Duro® receivers and Skylark™ Cloud Corrections Service. Learn more online at swiftnav.com.

About the Job:
Swift Navigation is looking for a Business Data Engineer to help support our Business, Marketing, Operations and Product teams. This person will manage the entire business intelligence ecosystem at Swift—from gathering the raw data to the finished reporting product. Additionally, you will be responsible for administration and architecture of cloud, CRM systems and operational process automation.

The ideal candidate will be an independent ""jack-of-all-trades"" with a proactive approach. Incorporation of scalability/automation should be the primary concern of the finished product.

Requirements:

Previous experience in an Analyst and/or Business Intelligence Developer role, preferably in a start-up creating systems and processes from scratch.
Experience with Python; you can interface APIs and ETL.
MySQL/PostgreSQL; you can administer, query, tune and develop schemas on both server and RDS.
Administrative knowledge of AWS architecture and its constellation of services, security and command-line (boto3); you can architect a serverless pipeline.
Building dashboards and notification alerts.
Marketing/Sales automation, tracking and analytics; you can manage email lists and track a user through the system.
Salesforce marketing campaign and CRM administration and development.
Excellent analytical and communication skills.
Track record of excellence in both academic and professional settings.
Quantitative Undergraduate Degree (e.g., Engineering, Analytics, Mathematics, Economics, Business, Computer Science).

Strong Candidates will also have:

Zapier, Slack API, Netsuite ERP/SOAP API, Google API.
KPI metric development and a strong business sense, with the ability to interpret data.
Linux, Github, Docker, CI/CD tools.
GDPR.
Security best practices, especially with AWS.
GIS data.
Node.js, Haskell, C, C++

Perks:

Open vacation policy, competitive salary, stock options, employer covered health insurance, 401(k), commuter benefits.
Fully stocked kitchen, weekly catered lunches and tech-talks, free gym membership.
Dynamic engineering organization—technological innovation is at the core of our business.
Growth and learning opportunities from a startup environment include working closely with an international team of scientists, engineers, platform architects, programmers and professionals.

Swift Navigation is a diverse and inclusive team. We are an equal opportunity employer. We welcome applicants from all backgrounds to apply regardless of race, ethnicity, religion, gender, sexual orientation, age, disability status, or other defining characteristics.","San Francisco, CA",Business Data Engineer,False
578,"The Digitalization team at C.H. Robinson is transforming the logistics industry. Our team is comprised of people who are passionate about delivering world class products. We are looking for a Data Engineer to join the Navisphere Vision team at C.H. Robinson. In our agile environment, we value personal initiative and team collaboration. If you are motivated, ambitious, and like to be challenged, we would like to talk with you!
Successful applicants are self-starters who are well organized, have outstanding communication skills, are solution oriented and thrive in a collaborative environment. In this position you will work with cross-functional teams to define, document, and deliver business initiatives that leverage technology to grow our competitive advantage. Your main goal will be to understand the supply chain challenges our business has, analyze internal and external data, and work with our data science team to create and implement solutions for the rest of the Vision team.
Responsibilities:
Build and maintain enterprise grade data pipelines to facilitate and advance predictive analytics projects and the product as a whole
Partner with team architects, technical leads, and data scientists to translate requirements into system designs
Provide expertise surrounding appropriate sources of data to solve novel problems and best practices to access this data
Understand the data landscape both internally and externally
Data analysis to identify opportunities for efficiency and savings in the supply chain and to work with data science team members to provide guidance on how best to use the data
Collaborate with engineers and data scientists during design and development
Proactively identify opportunities to improve and extend the quality of the product
Participate in daily stand-ups and periodic sprint demos, retrospectives, grooming and planning meetings.
Address issues and mitigate risks, communicate status to IT leaders and other IT teams impacted by the project.
Supply chain and engineering knowledge and thought leadership to stay current with best practices, tools, and offerings
Qualifications:

Required Qualifications:
Advanced relational database knowledge for data analysis and verification
Experience working with unstructured datasets and large, loosely connected datasets
Demonstrated expertise of data warehouse and pipeline design practices
Powerful analytical abilities
Demonstrated ability to write enterprise grade applications in a modern programming language (e.g. Python or R)
Familiarity with big data tools (e.g. Hadoop, Spark, etc.)
Working knowledge of Linux command shell
Understanding of machine learning and predictive analytics concepts
Ability to use strong decision making and problem-solving skills, logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions, or approaches to problems.
Knack for anticipating both technical and functional impacts and requirements.
Excellent written and verbal communication skills
Self-starter and able to work with minimal direction
Preferred Qualifications:
3+ years of experience building data pipelines, including processing large structured or unstructured data sets
Extensive experience working with Big Data
Working knowledge of a variety of databases
Experience in information technology and enterprise level business analysis for custom software solutions
Knowledge of supply chain management systems
Knowledge of Agile Principles
Required Education:
Undergraduate degree or similar work experience
Benefits

We offer a competitive compensation package and excellent benefits including medical, dental, and vision insurance, prescription drug coverage, paid holidays and vacation, disability insurance, life insurance, 401K with company match, profit sharing, Employee Stock Purchase Plan, and the opportunity to prosper in a Fortune 500 company.

About C.H. Robinson

Become a part of our team of over 500 talented IT Professionals. Work in collaborative, Agile development environment. Find continuing challenges and work with committed leaders. Stay with us – we’re large enough to build global solutions, but small enough to make real impacts as individuals.

C.H. Robinson—accelerating careers with immense opportunities and professional growth within the global supply chain industry. Start here. Accelerate here.

Every individual working at C.H. Robinson is integral to the success of our customers and our company. C.H. Robinson is a Fortune 500, global company that values teamwork, initiative, accountability, and integrity from its employees. We work globally and innovate daily to enhance and execute supply chains that move goods around the world. The fast pace of the logistics industry translates into a high-energy and collaborative workplace environment. We are empowered to make decisions, help our customers grow, and accelerate our careers.

No matter the product being shipped or from which corner of the globe, C.H. Robinson can help make it happen—quickly, securely, and reliably. Through personal connections and solid relationships, our employees use their in-depth knowledge, robust tools, and global network to help customers reach their goals quickly. Whether shipping by plane, rail, ship, or truck, C.H. Robinson has the knowledge, flexibility, and dedication to deliver the goods that make our world go ‘round.

Join the 12,000 employees worldwide who are accelerating their careers at C.H. Robinson.

Equal Opportunity Employer

C.H. Robinson - Affirmative Action Employer/EOE/M/F/Disabled/Veteran
#LI-MM1","Eden Prairie, MN 55347",Data Engineer (Data Science Focus),False
579,"Job Description
Love food? We do! The AmazonFresh and Prime Now operations finance team is seeking an experienced and innovative Data Engineer to build tools that support Operations teams in AmazonFresh and Prime Now. We are an analytics team responsible for building tools, analysis, and reporting to support internal leaders within fulfillment, last mile, and supply chain operations. This is a unique opportunity for someone interested in Amazon’s start-up consumables-focused environment. AmazonFresh and Prime Now experiment, fail fast, learn, and scale rapidly.

Ultra-fast delivery delights Amazon customers by delivering what they want quickly: medication for a sick kid, lunch at work when you forgot, food and drinks for a party, last minute gifts, dinner from a local restaurant, and so many more uses.

The business model of ultra-fast delivery is attractive, and offers our Engineering team the opportunity to work on any number of complex technical problems. Our team designs, builds and owns our end-to-end services from the ground up and works on large scale back-end systems to support the entirety of our order and inventory pipelines.

We are seeking Data Engineer. In this role you will:

You help build the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems
You manage AWS resources.
You collaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
You help drive the architecture and technology choices that enable a world-class user experience
You develop expertise in a broad range of Amazon’s data resources and know when, how, and which to use and which not to use
You encourage the organization to adopt next-generation data architecture strategies, proposing both data flows and storage solutions
You are comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve
You create extensible designs and easy to maintain solutions with the long term vision in mind
You have an understanding and empathy for business objectives, and continually align your work with those objectives and seek to deliver business value. You listen effectively.
You are comfortable presenting your findings to large groups

We have a very flat team structure, and offer a unique opportunity for technical leaders who want to work closely with the business in defining, designing, building and operating products that are in the early stage of fast expansion.
Basic Qualifications
BS in Computer Science, Math, Physics, or Engineering
6+ years relevant work experience in software development or related data-driven field
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Demonstrated experience in relational database concepts with an expert knowledge of SQL
Demonstrated ability in data modeling, ETL development, and Data warehousing
Preferred Qualifications
Experience working with AWS Big Data Technologies
Experience working with Open Source Big Data tools
Proven track record of delivering a big data solution
Experience developing tools for data engineers and machine learning
Experience working with both Batch and Real Time data processing systems","Seattle, WA","Data Engineer, Global Specialty Fulfillment",False
580,"Platform Science, is a connected Vehicle Platform for Transportation logistics, currently focused on asset based trucking side of the market, with top tier fleets.We have created a SaaS based IoT platform for core telematics applications, productivity, compliance, connected devices down to connectivity with physical locations like distribution centers and ports. Currently we offer out of the box solutions in addition to a development platform for the Fleets and also 3rd party developers.
Based out of San Diego our team is composed of product development experts, leading software/hardware technologist and transportation industry experts. We have embraced San Diego as a ground truth test ground for our [very large] test devices and as the natural playground we love. Our diverse team embraces creative thinking as we pride ourselves as collaborative force for innovation in IoT.
Requirements
3+ years Python experience
3+ years C/C++
Linux/Unix Administration (familiarity with embedded device Linux variants such as OpenWRT or Yocto with BusyBox a plus)
At least 1 year of real world experience with Vehicle CAN bus protocols (OBD-II and J1939)
Experience with software distribution via package management solutions
Understanding of basic Networking
Experience working with Web Services.
Good technical foundation with ability to pick up new skills and adapt quickly
Desire to learn new technologies while supporting existing
Some nice to haves; Embedded experience, working with Hardware, AWS, Java, SOA, Mobile Dev Experience, Vehicle Simulator Experience (Vector CANalyzer, AU Simulators, Intrepid NeoVi/Vehicle Spy)
Benefits
Work in a fast-paced development environment where you will see the results of your work immediately
Competitive Base Salary
Work with an intelligent team and solve meaningful problems
Exposure and experience working within a broad technology stack
Cell phone reimbursement
Health benefits: Medical/Dental/Vision
On-Site Gym","San Diego, CA",Vehicle Data Engineer,False
581,"What Strategic Intelligence/ Analyt contributes to Cardinal Health

Strategic Intelligence/Analytics is responsible for strategic data/information, value at stake analysis and other business analytics in support of strategic planning and execution

Qualifications
Bachelor's degree (preferably computer engineering, MIS, Data and Analytics, or related job experience)
Proficient with SQL and familiar with a variety of databases
Proficient with SQL Server Integration Services (SSIS) or similar ETL tools
Experience analyzing data and identifying trends and patterns
Experience supporting and working with cross-functional teams in a dynamic environment
Strong communication & analytical skills
Minimum 3 – 5 years job related experience
Project planning experience
Accountabilities in role
Applies comprehensive knowledge and a thorough understanding of database concepts, principles, and technical capabilities to perform varied tasks and projects
Develops data solutions to a wide range of difficult problems. Solutions are innovative, seek opportunities for automation of tasks, and are consistent with organization objectives
Identifies, designs, and implements internal process improvements by automating manual processes and optimizing data delivery for greater scalability
Subject Matter Expert for data solutions with deep understanding of raw data sources and how the data is leveraged by downstream applications
Demonstrates a passion for technology with a curiosity for business and strategy
Acts as a mentor to less experienced colleagues
Continues ongoing skills development to keep with evolving technologies
What is expected of you and others at this level
Applies comprehensive knowledge and a thorough understanding of database concepts, principles, and technical capabilities to perform varied tasks and projects
Develops data solutions to a wide range of difficult problems. Solutions are innovative, seek opportunities for automation of tasks, and are consistent with organization objectives
Identifies, designs, and implements internal process improvements by automating manual processes and optimizing data delivery for greater scalability
Subject Matter Expert for data solutions with deep understanding of raw data sources and how the data is leveraged by downstream applications
Demonstrates a passion for technology with a curiosity for business and strategy
Acts as a mentor to less experienced colleagues
Continues ongoing skills development to keep with evolving technologies
Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status.","Dublin, OH 43017",Data Engineer,False
582,"As a Data Engineer, you’ll work with our backend development teams to build and real time reporting & analytic capabilities for the restaurants and hospitality industry. You’ll work with a number of cutting edge technologies to wrangle various data sources into industrial strength pipelines that feed the full portfolio of services we offer to the industry.
Who you are:
You are an individual who thrives in a team atmosphere. You believe in agile development and continuous delivery. You’re a self-motivated and intellectually curious engineer with superb problem solving and analytical skills. You have a willingness to learn, adapt, teach, and pitch in wherever possible to help your customers.
We’re looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark - You’ve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.
Roles and responsibilities:
Work with a team of developers to provide top tier features to a wide audience of restaurant & hospitality customers
Build and maintain the ETL for HotSchedules using open source software
Provide architectural guidance on integrating with multiple systems to move data between layers of the software stack
Develop parallelized processes to analyze Big Data, produce forecasts and insights for the restaurant industry at cost and scale
Write quality, maintainable code with extensive test coverage in a fast-paced professional software engineering environment
Manage tasks within an Agile framework, clearing tasks and managing JIRA workflows
Manage long and short term deliverables with Product Management according to a product roadmap
Architectural design and implementation for both internal and external consumption
Requirements:
6+ years experience in software development
4+ years experience with Python and Scala or Java
Experience with multiple large scale distributed systems or data platforms, including Spark, Flink, Kafka, Dataflow, BigQuery, BigTable, Dataproc etc
2+ years experience working on AWS with an emphasis on their EMR, S3, and Kinesis
Experience working with terabyte level, real-time datasets
Dedication and practical experience with Microservice architecture
Strong algorithm & data structure knowledge
Comfortable with On-call coverage / rotations
Excellent communication skills and the ability to work well in a team
Strong team, customer focus, ownership, urgency and drive","San Francisco, CA",Data Engineer,False
583,"Primary Job Functions
Support and building our core analytics product
Building and maintaining client data integrations and scalable ETL processes
Ensuring data integrity and completeness
Develop and support internal reporting systems
Job Requirements
3+ years relevant work experience, (self-taught hackers encouraged to apply, just know your code! also medical experience a plus)
3+ years relevant work experience
Strong skills in both a statically typed and a scripting language (we mainly use Java/Python, but the ability and willingness to learn will be important.)
Strong SQL skills including tuning and optimization
Experience with relational database design (esp. PostgreSQL)
Passion for developing efficient, testable and well-documented code.
Development of ETL/Integration processes including data integrity
Pluses
Interest or experience in machine learning and predictive analytics.
Experience with Medical Billing Systems and Ontologies
Knowledge of reporting systems and software e.g. ( Apache POI, Jasper )
Experience web development in Java (GWT/Spring) or Python (Django)
Experience with Amazon web services: EC2, S3 or equivalent cloud computing approaches
Send your resume and a cover letter to jobs@acustream.com and please indicate the position for which you are applying in the subject line.","Lafayette, CO 80026",DATA ENGINEER,False
584,"Our mission, at Civitas Learning, is to partner with forward-thinking colleges and universities, harnessing the power of insight and action analytics to help a million more students learn well and finish strong. Data and predictive models are at our core to achieve this mission. Are you amazing at SQL? Curious about Big Data and predictive models to deliver insights to leading institutions and students? We are looking for smart and dedicated data engineers to join our data engineering team. As a Data Engineer you will serve as a key, data-driven implementation engineer and interact with customers on a daily basis during their deployment.

The Data Engineer role at Civitas Learning is critical to onboarding colleges and universities onto our cloud platform and making the process smoother, more flexible and faster. We want to work with people who are passionate about our mission and have a desire to be part of a rapidly expanding high performance team. We work hard, but also like to have fun! If this sounds like you, keep reading.

Responsibilities


Collaborate directly with external customers to understand their student success goals, specify and design data solutions, and commission products into production.
Develop ETL transformations to map customer data systems into our canonical data model.
Build and operationalize data science models on AWS.
Collaborate with teams across Civitas to drive innovation and best practices into our data and data science platform.
Up to 20% travel visiting customer institutions for technical discovery and/or UAT/QA of data mappings.
Develop data pipelines to integrate institutions onto our data platform using Amazon Web Services (AWS).
Design, implement, and maintain database models.

Qualifications


Bachelor's degree plus 2 years experience designing, developing, testing, and/or implementing complex ETL solutions using enterprise ETL tools.
Expertise in writing complex database SQL queries with a focus on Postgres and Redshift.
Strong understanding of ETL best practices including experience with ETL tools, and command line scripting.
Proficient developing code with command line build tools.
Expertise in working with technical and business teams to extract and document data integration/exchange requirements.
Ability to work independently as well as in a cross-functional team environment, collaborating with others and sharing tools, skills, and knowledge.
Solid problem-solving and analysis skills that demonstrate resourcefulness and attention to detail.
Strong organizational skills and ability to meet deadlines, prioritize workload, and manage time effectively.
Strong customer service orientation.
Ability to express complex technical concepts effectively, both verbally and in writing.
Ability to handle multiple projects and deadlines with minimal supervision.

Nice to haves


Some experience with the following or their equivalents:
Python/Java/Ruby or other programming language
Amazon Web Services
Github
Predictive models
Higher Education Student Information Systems or Learning Management Systems (Ellucian Banner, Peoplesoft, Blackboard LMS, etc.)
JIRA

About Civitas Learning:
Civitas Learning offers medical, dental, and vision insurance as well as a 401k plan. We also have a generous flexible, paid time off policy. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.

Civitas Learning partners with universities and colleges dedicated to helping more students learn well and finish strong. We provide tools and services for educators that bring together and make the most of their diverse and disconnected data streams; personalize information and support for their students; and deepen understanding of the impact of their student-success initiatives. Through our work together, our partners are empowering leaders, advisors, faculty, & students—and measurably improving enrollment, persistence, and graduation outcomes.

Today, Civitas Learning is a strategic partner to more than 350 colleges and universities, serving nearly 8 million students. Together with our growing community of partners, Civitas Learning is making the most of the world’s learning data to graduate a million more students per year by 2025.","Austin, TX",Data Engineer,False
585,"Castlight is looking for seasoned data engineers who have a penchant for problem solving with a passion for innovation to join our Provider Directory data engineering team. You must be comfortable dealing with large sets of imperfect data and designing components to process it. You will be working in the context of a complex set of systems within a multi-tier enterprise architecture. As our data volume explodes, you will apply your deep knowledge of performance, scalability, and optimization to make appropriate recommendations, as well as to help materialize them.
To succeed in this role, you'll need to be a versatile engineer. You must have an innate desire to build things the right way. You should be pain-averse - if some process or system isn't as streamlined as it could be, you want to fix it! You want to accelerate your career growth by taking on a role that will give you deep knowledge of a complex system, acting as the gatekeeper of the results. Finally, you should want to communicate and collaborate with a team of smart people just like you.
Responsibilities:
Participate in development efforts incorporating new data sources, as well as in support of infrastructure improvements
Enjoy working in a collaborative engineering team, writing code and the test cases to go with it
Participate in design discussions, sizing exercises, and code reviews
Qualifications
BS or MS in Engineering or Computer Science, or related technical field
2+ years of experience in production software development, preferably in a SaaS environment
Some experience handling complex data sources and large scale batch data infrastructure
Proficiency in writing SQL queries, performance tuning, and data modeling
Proficiency in Python, or another object-oriented programming language such as Ruby or Java
Experience working with very large data warehouses
Knowledge of PostgreSQL and Greenplum a plus","San Francisco, CA 94105 (Financial District area)",Data Engineer,False
586,"At Change we are a unique blend of engineers, activists, marketers, designers and scientists with a common goal: to give the voiceless a voice and a chance to be heard in today’s digital jungle; to allow everyone to connect with likeminded community and help people change the world for the better. People, the members, the activists, and our employees, are at the center of our mission and everything we do. Actually, employees at Change are a lot like the service itself: bright, brave, and innovative. Collaboration is the foundation of our workforce, and we're looking for smart individuals who are self-motivated and passionate to join us. Be a part of the team that creates a brighter future for everyone. Discover your future at Change!

Change has a great opportunity for a data engineer with several years of diverse experience who will help deliver Change to millions of people around the world. We are looking for someone who will own, lead, and execute projects. You should have outstanding analytical and programming skills with a deep understanding and proven track record of building robust, scalable, data processing pipelines. As a member of our highly motivated team, you should be dedicated to excellence and have a strong sense of personal responsibility. We hold ourselves to high standards and take pride in our work. We are looking for someone who is not afraid to get their hands dirty in data and be an integral part of the machine learning team's expansion.

What you’ll do: Work with data scientists and other data engineers to productize analytics and data models, developing and maintaining new ETL flows for new applications driven by model-based statistics and signals. Push the envelope with scalable data processing and model deployment solutions leveraging technologies such as Spark, Kafka, Kinesis, Airflow, DynamoDB, TensorFlow, TensorBoard etc. Build and support internal A/B testing and model evaluation platforms. Tool our systems for observability, including logging, metrics monitoring, and dashboarding . Have fun and make friends.
Experience we're looking for:
Development experience of which 2 years are focused on data engineering working with big data technologies (Hadoop: i.e. MapReduce, HDFS/Hive, Spark)
Team player with excellent communication and interpersonal skills
Experience developing high quality software in Python, Scala or Java
Proficient with data preprocessing, data transformation, and integration of data from multiple data sources (ETL processes)
Experience with one of the following distributed relational databases: PostgreSQL, MySQL
Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server side software for deployment
Experience designing and configuring hosted and cloud-based data and machine learning infrastructure
Experience unit testing with frameworks and a dedication to thorough testing to create high quality software, i.e. JUnit
Experience with productionizing feature engineering for machine/deep learning algorithms and exposure to machine learning algorithms and/or statistical modeling methods.
Experience with API design/development (i.e. RPC, REST, JSON, XML, SOAP)

Bonus Skills:
Experience with recommender, or search/ranking systems.
Experience with Kafka and Yarn or Mesos
Experience with AWS services (Athena, Glue, Redshift, Kinesis) or Google cloud services (BigQuery, BigTable)
BA/BS or above in Computer Science or a related field
Experience with NoSQL databases and key-value stores, such as Cassandra, Redis

This is a full-time opportunity. The position is located in San Francisco, CA, U.S.A.

Change.org is committed to being a diverse and inclusive workplace. We encourage applicants of different backgrounds, cultures, genders, experiences, abilities and perspectives to apply.

All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, sexual orientation, gender, gender identity, age, physical disability, or length of time spent unemployed.
Apply for this job","San Francisco, CA 94103 (South Of Market area)",Data Engineer,False
587,"Description: Lead Data Engineer – Next Gen Analytics
Join the EDABI team, where your goal is to unlock the value of the vast treasure of data available – make it accessible through natural language interfaces and build novel Big Data solutions to create insights – your customer is business analysts/users.

In this role, you will be at the forefront of creating high performance, real time, streaming as well as Data at Rest analytics platforms. You will derive satisfaction from deploying non-trivial scale solutions to solve business problems and create insights that were either downright impossible until now or took several days of effort.

Use your skills, experience and talents to be a part of groundbreaking thinking and visionary goals. You will be required to:
Understand how to build scalable, real time, streaming based, Big Data systemsHave developed and been a key influential member in a fully delivered data product
Lead the architecture and design of several modules related to the backend of a search system, a real time relevance engine, a system that computes several complex functions on the data on the fly, etc.Be a hands-on developer and lead by example as a programmerProvide guidance and contribute to coding standardsProvide leadership in sprints, CI/CD and the DevOps efforts

RequirementsM.S. in computer science or related areas7+ years of experience developing production grade softwareProficient in Linux or related Unix systemsExpert in one or more of C, C++, Java and Python. Exposure to modern programming languages such as Rust and Go a huge plus.
Experience building and deploying large scale distributed systems
Excellent written and verbal communication skills

Qualifications:","Sunnyvale, CA",Lead Data Engineer - Next Gen Analytics,False
588,"We are looking for a Data Engineer to join our operations team either at our Albuquerque headquarters or remotely within one of the states in our region: Arizona, Colorado, Nevada, New Mexico, and Texas. Accion is embarking on a three-year project to overhaul our technology infrastructure and implement a state-of-the-art lending platform. It will be the only end-to-end lending platform in our industry, and is fully owned and operated by Accion, allowing it to be customized to meet our (and our customers') needs. This will have a transformational effect on Accion, our clients, and the sector of lenders supporting underserved entrepreneurs. As our Data Engineer, you'll be an integral part of this transformation, helping us create a data-centered culture we need to make data-informed decisions.

What you'll be doing:

You'll work closely with our technology operations and engineering teams to understand business needs and design/maintain scalable data models.
You'll make data-backed recommendations to the executive team to help inform business decisions and answer complex questions.
You'll own the design, build, maintenance, quality, and expansion of a data warehouse, and support and scale the pipeline that relays data from Accion's lending platform back to the warehouse.
Over time, you'll act as a product manager, determining project timelines, goals, and deliverables for updates to our lending platform.

Skills and Experience:

Must have a passion for Accion's mission and a strong commitment to Accion's culture of exceptional customer service, excellence and accountability;
Knowledge of ETL processes and applications;
Experience in the financial services industry and familiarity with the lending process strongly preferred;
Advanced skills in SQL/Java/Ruby preferred;
Bachelor's Degree in Computer Science, Engineering, Applied Mathematics, or related quantitative discipline plus four years' relevant experience preferred.

Accion offers an excellent total compensation package, including competitive base salary, the opportunity for exciting incentive pay, health and dental coverage, retirement benefits, and generous paid time off.

About Accion:
A nonprofit leader in the high-impact fields of community development and microfinance, Accion is dedicated to helping entrepreneurs realize their dreams and fuel increased economic opportunity and mobility through business ownership. Since 1994, Accion has infused more than $123 million in the growth and success of more than 8,000 small businesses across Arizona, Colorado, Nevada, New Mexico, and Texas.

Accion is a member of the Accion U.S. Network, the largest and only nationwide nonprofit micro- and small business lending network in the United States. Since 1991, the members of the Accion U.S. Network have collectively made more than 57,000 loans totaling over $500 million. Globally, Accion is a pioneer in microfinance, reaching millions of individuals through its international network of partners. Learn more at www.us.accion.org ( http://www.us.accion.org/ ).","Denver, CO",Data Engineer,False
589,"ContractJob SummaryApplication Developer-Big Data ToolsJava/ J2EE/ Map reduce/ Hive/ Spark/ Sprint bootWork Location: Phoenix, AZ. no T&L, no remoteJob Type: Contract","Phoenix, AZ",Data Engineer - Big Data,False
590,"Atlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that powers crucial business decisions throughout the organization. We are looking for an open minded, structured thinker who is passionate about building systems at scale. You'll be the genius who understands data at Atlassian, knows where to find it, and manages the process to make that data useful for Analytics. You love thinking about the ways the business can consume this data and then figuring out how to build it.

On a typical day you will be building the data models and ETL processes to provide this data for business use. You've got industry experience working with large datasets. You are interested in reporting platforms and data visualization. As the data domain expert, you will be partnering with our technology teams, analytical teams, and data scientists across various initiatives.
You'll own a problem end-to-end, so those skills will come in handy not just to collect, extract, and clean the data, but also to understand the systems that generated it, and automate your analyses and reporting. On an on-going basis, you'll be responsible for improving the data by adding new sources, coding business rules, and producing new metrics that support the business. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.

More about you

As a data engineer, you may have experience spanning traditional DW and ETL architectures. But for this role it is important to have industry experience working with big data ecosystems like Spark/Hadoop and Redshift. You've probably been in the industry as an engineer for 2+ years and have developed a passion for the data that drives businesses.
On your first day, we'll expect you to have:
Deep understanding of big data challenges and eco-system
Experience with solution building and architecting with public cloud offerings such as Amazon Web Services, Redshift, S3, EMR/Spark, Presto/Athena
Experience with Spark and Hive
Expertise in SQL, SQL tuning, schema design, Python and ETL processes
Expertise in data pipeline with such workflow tools as Airflow, Oozie or Luigi
Solid understanding experience in building RESTful APIs and microservices, e.g. with Flask
Experience in test automation and ensuring data quality across multiple datasets used for analytical purposes
Experience with Lambda Architecture or other Big Data architectural best practices
A graduate degree in Computer Science or similar discipline
Commit code to open source projects
Experience with test automation and continuous delivery
It's great, but not required, if you have:
Experience with Tableau
Experience with Machine Learning
Have worked with Data Scientists
More about our team

Atlassian is over a decade old, but our team is much younger. We'll have to blaze new trails to enable important growth decisions, so we're constantly growing, learning, and trying to do things differently. You'll be joining a team that is crazy smart and very direct. We ask hard questions and challenge each other to constantly improve our work. We are self-driven but team oriented. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

More about our benefits

Our offices are open, highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: ample time off to relax and recharge, flexible working options, five paid volunteer days a year to support your favorite cause, plenty of food and beverages, ergonomic workstations with sit/stand desks, unique ShipIt days, a company paid trip after five years, generous employer-paid insurance coverage (medical, dental, and vision) for you and your family, 401k matching and more.

More about Atlassian

Software is changing the world, and we’re at the center of it all. With a customer list that reads like a who's who in tech and a highly disruptive business model, we’re advancing the art of team collaboration with products like Jira, Confluence, Bitbucket, Trello, and now Stride. Driven by honest values, an amazing culture, and consistent revenue growth, we’re out to unleash the potential of every team. From Amsterdam and Austin to Sydney and San Francisco, we’re looking for people who are powered by passion and eager to do the best work of their lives in a highly autonomous yet collaborative, no B.S. environment.

Additional Information

We believe that the unique contributions of all Atlassians is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.

All your information will be kept confidential according to EEO guidelines.

Atlassian, Inc., will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of SFPC Art.49.","San Francisco, CA",Data Engineer,False
591,"If working on a high performance, scalable Data solution on one of the largest Big Data Systems in the world that supports 70+ Billion transactions a day, sound exciting and challenging enough for you, then read on! Rubicon Project is looking for passionate engineers to work on our industry-leading Reporting Application which will offer world-class user experience for our customers.on a global scale.

This is a great opportunity for a Jr-Mid Level Engineer who is looking to gain experience on a larger scale and be involved with the Data and UI/API side of reporting.

What you will be doing:

The main focus of this position is on the Backend data processing and API. Definitely a great learning opportunity to be a full-stack developer!
You'll be working within a small but rapidly growing team to build the next generation Reporting and Analytics platform. As we grow, you grow along with the team and company together to earn technical experience.

What we look for:

BS degree (or higher) in Computer Science or related major.
2+ years working experience in MR, Spark, and Big Data processing platform in high volume and low latency environment.
Good Scala, Java, SQL skills are required.
Proficiency in working and developing on Linux.
Proficiency in using relational databases and writing SQL.
Proficiency in Python
Knowledge and experience of developing on Druid is a huge plus.
Knowledge and experience in building RESTful Web-services.
Familiarity with Concurrency, and multi-threaded programming.
Familiarity with automated testing (TDD, Mocking, Unit/Functional/Integration)
Familiarity with development continuous integration tools like Maven, Git, Jenkins, etc.
Great interpersonal, written and verbal communication skills; including the ability to debate technical trade-offs, and explain technical concepts to business users.

What's in it for you:

Take time for yourself: Take what you need -- our vacation days are unlimited and we close down the week of 4th of July and last 10 days of the year that is paid for as well.
Stay healthy: Choose from a variety of medical, dental and vision plans to cover you and your loved ones at a very low cost
Enjoy your stay: We want to make it easy for you. Each Rubicon Project office enjoys a variety of benefits like daily catered lunches, a fully stocked kitchen with healthy snacks and free vending machines.

","Seattle, WA 98104 (First Hill area)",Data Engineer - Scala/Java,False
592,"Data Engineer
Who we are
lululemon is a yoga-inspired technical apparel company up to big things. The practice and philosophy of yoga informs our overall purpose to elevate the world through the power of practice. We are proud to be a growing global company with locations all around the world, from Vancouver to Shanghai, and places in between. We owe our success to our innovative product, our emphasis on our stores, our commitment to our people, and the incredible connections we get to make in every community we are in.
About this team and this role

The Data Engineer will be part of the engineering team building real time enterprise data streams ingesting data from multiple heterogeneous data sources and provisioning data as raw data streams and Canonical business events. Essentially provisioning data once at enterprise level for any enterprise level data related use cases. Deploying and maintaining stream processing technologies to provision data to real time Operational data stores, raw data stores, providing frameworks on how to transform raw data using big data technologies and helping data scientist in machine learning deliverables.

Ingest data from heterogeneous data sources and publish them as enterprise business events.
Responsible for building and consuming from API’s.
Building Scalable Data Pipelines for generating training datasets for machine learning deliverables.
Mentor Junior resources and drive end to end design, implementation and delivery of engineering components.
Deploying, maintaining and building apps on distributed stream processing engines such as Storm, Flink, Nifi, Spark etc.
Building data transformation layers, ETL frameworks using big data technologies such as Hive, Spark, Presto etc.
Experience working with container management technologies such as Docker, Kubernetes.
Building and maintaining solutions on highly available environments.
Working knowledge of CI/CD.
Working knowledge of building data integrity checks as part of delivery of applications.
Build code that is performant as well as secure.
Collaborate with cross-functional teams – business stakeholders, engineers, program management, project management, etc. - to produce the best solutions possible.
Anticipate system/application challenges and proposes solutions for the same.
Contribute to story sizing and work estimates for implementation, validation, delivery and documentation.
Review user stories to ensure a quality user experience, well-defined acceptance criteria and thorough test coverage.
Participate in design and code review to ensure quality and testability of feature code.


Qualifications

BS in Computer Science or Related
5+ years of data engineering experience
Strong Java coding experience.
Experience with DevOps, Real Time Analytics and Real Time Messaging.
Experience working with Kafka, Pulsar, or Bookkeeper is required.
Working experience of scripting, data science and analytics (SQL, Python, PowerShell, JavaScript, or R)
2+ Years of performance tuning and optimization, bottleneck problems analysis, and technical troubleshooting in a, sometimes, ambiguous environment.

Desired Qualifications:

MS Degree in Computer Science or related technical degree completed
Experience with Real Time Analytics and Real Time Messaging.
Should be a Java expert.
Experience with open source technologies, Spring is preferred
Working experience with Microservices is desirable
Experience working with large volume data; retail experience strongly desired.
DevOps experience and use of Docker, Chef, Puppet, Jenkins, Kubernetes, Istio or Ansible.
Experience supporting Machine Learning, Recommendation Engines and or Search Personalization is preferred
Acknowledges the presence of choice in every moment and takes personal responsibility for their life.
Possesses an entrepreneurial spirit and continuously innovates to achieve great results.
Communicates with honesty and kindness, and creates the space for others to do the same.
Leads with courage, knowing the possibility of greatness is bigger than the fear of failure.
Fosters connection by putting people first and building trusting relationships.
Integrates fun and joy as a way of being and working, aka doesn’t take themselves too seriously.

NOTE: Only those applicants under consideration will be contacted. Please accept our utmost appreciation for your interest. lululemon is an Equal Employment Opportunity employer. Employment decisions are based on merit and business needs, and not on race, color, creed, age, sex, gender, sexual orientation, national origin, religion, marital status, medical condition, physical or mental disability, military service, pregnancy, childbirth and related medical conditions or any other classification protected by federal, state or provincial and local laws and ordinances. Reasonable accommodation is available for qualified individuals with disabilities, upon request. This Equal Employment Opportunity policy applies to all practices relating to recruitment and hiring, compensation, benefits, discipline, transfer, termination and all other terms and conditions of employment. While management is primarily responsible for seeing that lululemon equal employment opportunity policies are implemented, you share in the responsibility for assuring that, by your personal actions, the policies are effective.
#LI-MR1

Job: Information Technology
Organization: Store Support Center
Schedule:: Full-time
Unposting Date: Ongoing","Seattle, WA",Data Engineer,False
593,"A pioneer in K– 12 education since 2000, Amplify is leading the way in next-generation curriculum and assessment. Our captivating core and supplemental programs in ELA, math, and science engage all students in rigorous learning and inspire them to think deeply, creatively, and for themselves. Our formative assessment products turn data into practical instructional support to help all students build a strong foundation in early reading and math. All of our programs provide teachers with powerful tools that help them understand and respond to the needs of every student. Today, Amplify serves more than three million students in all 50 states. For more information, visit amplify.com .


As an engineer at Amplify, you will join a talented team tackling the toughest problems in education with the best ideas in technology – including user experience, APIs and services, data analysis, and deployment pipelines. You’ll play an active role in imagining and improving product design and the classroom experience.

We hire engineers “for the slope, not the intercept” – we’re looking for intellectual ability, flexibility and ability to learn, and commitment to work together in tight-knit teams.

What You’ll Do
Our data team builds, augments, and maintains the infrastructure that empowers teams across Amplify and our customers to make sense of and tell stories with their data. We believe strongly in teaching our teammates to serve themselves, within a safe, reliable, and agile environment. You’ll be building data systems, but also the sharing-and-learning culture so that every team uses these tools to improve their own lives, and those of our students and teachers.

Impress the toughest customers around – seventh graders – by:
helping teams create fun, compelling apps by leveraging millions of data points
Make life better for passionate, overworked teachers by:
helping teachers understand their students by building reusable data pipelines
Make life better for passionate, overworked Marketing and Sales teams by:
using REST APIs for sourcing/sending data to SAAS like Salesforce, Hubspot
Help school administrators build great schools by:
respecting privacy and ensuring security while offering useful insights by making smart choices in tech stack, database design, and encryption
helping school principals understand how teachers are teaching and how students are learning by architecting data warehouse schemas and SQL transforms with just the right CTEs, window functions, and pivots
analyzing performance and squashing tricky bugs using tools like AWS Redshift, Matillion, Python, SQL, AWS CloudWatch, AWS SNS
Learn every day by:
immersing oneself in agile rituals and leveraging our infrastructure
leading collaboration, pull request-ing, and mentoring on a cross-functional team
participating in cross-team share-outs, brownbags, and workshop series
becoming an expert in the data models and standards within Amplify and the educational industry in order to deliver quality and consistent solutions

Example Projects You Might Work On
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines

You Must Have
BS/MS in Computer Science, Data Science, or equivalent
2+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Vault)
Strong communication skills in writing, conversation, and maybe silly gifs

Extra Credit For
Experience with tools we use every day:
Storage: AWS Storage Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet, Postgres
ETL/BI: Matillion, Looker
Experience with tools we don’t use, but should, and the wisdom to know when to recommend them
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Experience in education or ed-tech



Amplify is an Equal Opportunity Employer of Minorities, Females, Protected Veterans and Individuals with Disabilities.

This position may be funded, in whole or in part, through American Recovery & Reinvestment Act funds.

Amplify Education, Inc. is an E-Verify participant.","Brooklyn, NY",Data Engineer,False
594,"Overview
The Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children’s Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement etc .
Requirements
Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of two (2) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentTwo (2) years of experience in a data engineering roleData pipeline development experience with industry standard data integration toolsAdvanced competency in SQL with ability to perform query optimization in large scale database platformsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience wrting test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.

Required Credentials:
N/A

Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard development","Seattle, WA",Data Engineer,False
595,"Data Engineer (JetBlue Travel Products)
The JetBlue Travel Products (JBTP) Data Engineer reports directly to the GM of Data Engineering and is responsible for developing and supporting the data management and analytics platforms including data infrastructure in order to support the evolution of travel products. The Data Engineer is focused on the design and development of data ingestion, processing and storage pipelines as well as the transformation of raw data into business insights that enable JetBlue Travel Products to become a digital disruptor. This role collaborates heavily with their matrix partners in JetBlue’s IT, JetBlue Tech Ventures and external business partners.
The data engineer can change priorities and focus to meet business demands, excels when working on complex projects, is motivated to deliver results, and exhibits the JetBlue values of Safety, Caring, Integrity, Passion , and Passion.
Essential Responsibilities:
Design, develop and manage data management products from conception to retirementEnsure that the data engineering team delivers with consistency, high quality and predictabilityCreate and promote a work environment focused on engineering excellence to attract, develop and encourage a culture of technical innovationPartner with product management and marketing teams and help develop the product vision and roadmapCreate the JBTP data infrastructure and analytics environments, understand the data and provide support for key business decisionsParticipate in the DevOps practice as it pertains to data engineeringHelp establish frameworks, design and integration patterns as well as guide the software development performed by business partnersHelp manage a collection of external technology products used to deliver business productsPartner with JetBlue Tech Ventures to identify, monitor, learn, experiment and share information about emerging technology that is relevant to JetBlue Travel Products.Other duties as assigned
Minimum Experience and Qualifications:
Bachelor’s Degree in Computer Science or related technical field or equivalent practical experience with demonstrated capability to perform job responsibilities through four (4) previous years of combined experience and educationThree (3) years’ experience in an engineering roleGood understanding of software engineering environments and standardsDeep understanding of Internet technologies, protocols and methodologies for delivering web-based productsExperience managing Service Level AgreementsExperience interacting daily with people at different levels within the organization, including developing and maintaining ongoing relationshipsMust pass a ten (10) year background check and pre-employment drug testLegally eligible to work in the country in which the position is located
Preferred Experience and Qualifications:
Experience maintaining a public profile and building relationships throughout the organizationFive (5) years’ experience in technology rolesExperience in writing software in one or more languages such as Java, Python, Go and/or JavaScriptExperience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as Amazon Web Services, Azure and Google Cloud Platform.
Crewmember Expectations:
Regular attendance and punctualityPotential need to work flexible hours and be available to respond on short-noticeWell-groomed and able to maintain a professional appearanceWhen working or traveling on JetBlue flights, and if time permits, all capable crewmembers are asked to assist with light cleaning of the aircraftOrganizational fit for the JetBlue culture, that is, exhibit the JetBlue values of Safety, Caring, Integrity, Passion, and Fun
Equipment:
Computer and other office equipment
Physical Effort:
Generally not required, or up to 10 pounds occasionally, 0 pounds frequently. (Sedentary)


Disclaimer: The above statements are intended to describe the general nature and level of work being performed by the crewmember(s) assigned to this position. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of individuals in this position. may be subject to change as the needs of the organization change.

JetBlue Airways Corp. is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, marital status, veteran status, sexual orientation, gender identity or expression, disability status, pregnancy, genetic information, citizenship status or any other characteristic protected by law. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.


EEO is the Law

EEO is the Law GINA Supplement","Fort Lauderdale, FL",Data Engineer (JetBlue Travel Products),False
596,"Custora exists to help our customers improve the relationships they have with their own customers. We do this by ingesting data about every interaction a company has with each of their customers and then making predictions using that data about how those customers will behave in the future. Our customers then use these predictions to tailor their communications.

Data engineers at Custora work on the pipelines that sit at the core of this architecture. We work to make these pipelines faster, more fault tolerant and to expand their scope.
The volume of data is large: we're working with 7 of the top 20 largest retailers in the world (+ many more not in the top 20), and are ingesting data from them both in a regular batch and in near-real time.

We've carefully selected the types of data to ingest to favor high signal data, so we care deeply about maintaining the correctness and completeness of the data being ingested as our models (and therefore the output of our product).

Your work directly impacts both the predictions we are able to make, and the day to day performance our customers experience when using our product.

Getting more specific, you will:

Design and build complex data pipelines on the Spark platform, ingesting both batch and real time datasets
Work with our data science team to deploy predictive models at scale
Build tools to continuously validate incoming data and proactively identify and communicate data anomalies before they manifest into problems.
We’re a small team, so you’ll be working on (and be able to meaningfully contribute to) high impact projects from your first day.
Sure, but what’s it really like?

Inspired by Basecamp, we work in ~8 week product cycles. First, we work together (engineering + product) to identify the projects we think will have the biggest impact on our company goals. Here’s an example of a recent project we conceptualized and delivered over one of these cycles:

Migrate self-managed Spark cluster to EMR
To lower overall cost, and to be able to easily scale to handle bigger datasets and processing volumes, we recently switched from a self managed Spark cluster to Amazon’s Elastic Mapreduce service.

Our challenge was to move terabytes of data used by our clients while having no downtime. Some initial concerns were the performance on the EMR cluster and the migration process itself because some clients ingest a combination of live data and batch based data. The scale of our data sent meant that we had to switch several clients at a time to EMR.

After ensuring that Hive backed by S3 was performant enough, we built tools to move vast amounts of data in parallel, to redirect requests to the correct cluster as clients were being moved, and to validate the data after migration. Along the way, we also had to reshape the data (in terms of partition size) to ensure efficiency of copying and loading into the Hive database.

The Stack:

While we make use of a wide variety of tools, our primary web stack is ES6/React and Ruby on Rails deployed on AWS. We make extensive use of R for statistical analysis, and our primary data stores are Hive, MySQL, and Redis.

What it’s like to work here:

On Monday we eat and meet as a team to chat projects and progress.
We’re 50 genuinely nice people; 25 men and 25 women. We work together and experiment with how to do things.
We move quickly. You build something and the next day it comes to life. You see and feel an immediate impact with the collective efforts of the team.
We’re building a company and a team we love. We’re in it for the long run.
Read more about what makes us, us here.
Find out more here or here.
Custora is an equal opportunity employer. We value diversity. We don’t discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, marital status, veteran status, disability status, or socioeconomic status.

Qualifications:

5 or more years of experience as a software engineer.
Degree in Computer Science or a deep competency achieved via other means.
Familiarity with Ruby on Rails, AWS, and SQL-based databases.
High standards for code quality and maintainability.
Nice to Have's:

Experience with R, Scala, Spark, and/or Chef.
Consistent record of delivering significant features or building out platforms and services.
Experience working in e-commerce.
The perks

We’re a flexible work environment
Competitive salary and meaningful equity
Health, dental and vision insurance (100% covered)
Free lunch every day, plus free water — hot and cold!
Unlimited vacation: take as much time as you need (we recommend at least 3 weeks)
Monthly unlimited MetroCard
401k","New York, NY",Data Engineer,False
597,"About Hive

Hive is a full-stack deep learning platform helping to bring companies into the AI era. We take complex visual challenges and build custom machine learning models to solve them. For AI to work, companies need large volumes of high quality training data. We generate this data through Hive Data, our proprietary data labeling platform with over 500,000 globally distributed workers, generating millions of high quality pieces of data per day. We then use this training data to build machine learning models for verticals such as Media, Autonomous Driving, Security, and Retail. Today, we work with some of the largest companies in the world to redefine how they think about unstructured visual data. Together, we build solutions that incorporate AI into their businesses to completely transform industries.

We are fortunate that investors like Peter Thiel (Founders Fund), General Catalyst, 8VC, and others see Hive’s potential to be groundbreaking in AI business solutions. We have over 100 rock stars globally in our San Francisco and Delhi offices. Please reach out if you are interested in joining the AI revolution!

Data Engineer Role

In order to execute our vision, we need to grow our team of best-in-class data engineers. We are looking for developers who conduct impeccable data practices and implement high quality data infrastructures. We value hard workers who are comfortable improvising solutions to a stream of big data challenges while building a system that stands the test of time. Our ideal candidate has experience building data infrastructure from the ground up, contributes innovative ideas and ingenious implementations to the team, and is capable of planning out scalable, maintainable data pipelines.

As a data engineer, you would at first work primarily on our Hive Media product, taking real-time data from hundreds of television streams and turning them into a combination of real-time and scheduled outputs, especially our signature ads feed. Your work would improve the quality of our results while reducing computational cost and latency. Expect truly novel challenges.
Responsibilities
Writing scheduled Spark pipelines that perform sophisticated query plans on the entirety of our datasets
Writing real-time pipelines that execute complex operations on incoming data
Synchronizing large amounts of data between unstructured and structured formats on various data sources
Creating testing and alerting for data pipelines
Building out our data infrastructure and managing dependencies between data pipelines
Determining and implementing metrics that provide visibility into our data quality
Requirements
You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics
You have 1-2 years of industry experience as a data engineer
You have hands-on experience doing ETL and have written data pipelines in either Spark or MapReduce
You have a sound understanding of SQL or CQL
You have worked with data lakes such as S3 or HDFS
You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons
You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos/cron, Marathon, Jenkins
You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C)
You have great communication skills and ability to work with others
You are a strong team player, with a do-whatever-it-takes attitude
What We Offer You

We are a group of young and ambitious individuals passionate about creating a revolutionary machine learning company. At Hive, you will have a significant career development opportunity and a chance to contribute to one of the fastest growing AI startups in San Francisco. The work you will do here will have a noticeable and direct impact on the development of Hive.

Our benefits include competitive pay, equity, health / vision / dental insurance, catered lunch and dinner, and a corporate gym membership.

Thank you for your interest in Hive.","San Francisco, CA",Data Engineer,False
598,"Looking for talented, passionate and results-oriented individuals to join our team building data foundations and tools to craft the future of commerce and Apple Pay. Collaborating with the head of Data Engineering for IS&S Payments & Commerce Analytics, you will create scalable, extensible, highly-available and high performance data pipelines that will help create insights for measuring performance and driving strategy. You will collaborate with various data analysts, instrumentation authorities and engineering teams to identify requirements that will derive the creation of data pipelines. You will work closely with the application server engineering team to understand the architecture and internal APIs involved in upcoming and ongoing projects related to Apple Pay.
Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way; we believe analytics is a team sport, but we strive for independent decision-making and taking smart risks. Our team collaborates deeply with partners across product and design, engineering, and business teams: our mission is to drive innovation by providing the business and data scientist partners best-in-class systems and tools to make decisions that improve the customer experience of using our services. This will include using large and complex data sources, helping derive actionable insights, delivering dynamic and intuitive decision tools, and bringing our data to life via amazing visualizations.
You are a self-motived teammate, skilled in a broad set of Big Data processing techniques with the ability to adapt and learn quickly, provide results with limited direction, and choose the best possible data processing solution is a requirement.

Key Qualifications
5+ years of professional experience with Big Data systems, data pipelines and data processing
Practical hands-on experience with technologies like Apache Hadoop, Apache Pig, Apache Hive and Apache Sqoop
Ability to understand server API Specs, identify the corresponding server events, extract data from the events and define & derive actionable data pipelines
Understanding on various distributed file formats such as Apache AVRO, Apache Parquet, data structures and common methods in data transformation
EXPERTISE IN PYTHON SCRIPTING
You have expertise in Unix Shell scripting and Dependency driven job schedulers
Expertise in Oracle Database and ANSI SQL
Proficiency in Core JAVA
You have knowledge on Scala
Familiarity with Apache Oozie , Apache Spark and PySpark
Familiarity with Data visualization tools such as Tableau
Familiarity with rule based multi-stage data correlation on large data sets is a plus
You have excellent time management skills with the ability to run work to tight deadlines and handle the pressure of executive requests and product launches
Description
- Translate business requirements by analysts into data and engineering specifications
- Build new scalable data sets based on engineering specifications from the available raw data and derive business metrics out of it
- Identify and implement Data Validation rules and alerts based on data publishing specifications for data integrity and anomaly detection
- Identify server API calls that needs to be tapped for data analytics and reporting and align the server events for execution in already established data pipelines
- Analyze complex data sets, identify and formulate correlational rules between heterogeneous data sources for effective analytics and reporting
- Process, clean and validate the integrity of data used for analysis
- Develop Python and Shell Scripts for data ingestion from external data sources for business insights
- Work hand in hand with the DevOps team and develop monitoring and alerting scripts on various data pipelines and jobs

Education
Minimum of bachelor’s degree, preferably in Computer Science, Information Technology or EE, or relevant industry experience is preferred","Santa Clara Valley, CA",Big Data Engineer - Apple Pay Analytics,False
599,"ContractJob SummaryJob Description: Candidate should have over all of 5+ yrs. of total IT experience ,Responsibilities and Dutiesin which candidate should have 2-3 yrs. of Spark + Scala combination on big data environmentRequired Experience, Skills and Qualificationsshould have good oral and written communication, should have good confidence level and should be customer facingJob Type: ContractExperience:Spark and scala: 2 years (Preferred)","Sunnyvale, CA",Big Data Engineer,False
600,"With our portfolio of global Power Brands such as Oreo and belVita biscuits, Cadbury Dairy Milk and Milka chocolate and Trident gum, we're the world's #1 in biscuits and candy, and #2 in chocolate and gum. We're Mondelēz International, a snacking powerhouse with operations in more than 80 countries, with approximately 90,000 employees globally and our brands are marketed in around 165 countries.
Our purpose and vision is to create more MOMENTS OF JOY by building the BEST SNACKING COMPANY IN THE WORLD.
Purpose of Role
Building infrastructure and architecture to support data mining from complex R&D data to deliver new insights to enable rapid, high quality innovation across our chocolate, biscuit, gum & candy categories. Structuring solutions and systems from a wide range of data sources and types.
Main Responsibilities

Works with engineers and scientists to define complex technical challenges to be solved
Defines system needs to maximize data as an asset to the R&D function including combining data / connecting data from multiple sources
Uses appropriate software tools and programming methods to harness data. Improving data quality, reliability and efficiency to enable high performance solutions.
Builds, tests and maintains architectures including planning and costing of new solutions.
Communicates systems needs and engages with stakeholders to enable expansion beyond the existing infrastructures
Write and builds best practice documentation and actively strives to change traditional ways of working within R&D
Develops simplified software tools for others to use to add and interpret data, and understand results
Researches and keeps up to date with developments in data engineering techniques
Maintains and develops data mining resources including hardware and software updates, working with IT department to maintain security and back-up systems
Bachelor degree or higher in a STEM subject / computer science
5 years experience of Data Engineering in a large complex business with multiple systems such as SAP, LIMS, etc. and experience setting up, testing and maintaining new systems
Experience of a wide variety of languages and tools (e.g. script languages) to marry systems together
Ability to simplify complex problems and communicate to wide audience
Global experience and additional language preferred
Mondelēz Global LLC is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected Veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Applicants who require accommodation to participate in the job application process may contact 847-943-5460 for assistance.
Applicants must complete all required steps in the application process, including providing a Resume/CV, in order to be considered for this position.","East Hanover, NJ 07936",Associate Principal Engineer- Data Engineer,False
601,"CARVANA….

At Carvana, we sell cars, but we're not salespeople. Since 2013, we've been making it our mission to change the way people buy cars. We saw a huge problem with how much it can suck to buy a car the traditional way, so we committed ourselves to tackling one of the largest, yet-to-be-disrupted markets in the world – the $1T per year U.S. car market (yes, that's $Trillion with a ""T"").

With the ability to search thousands of vehicles from our expansive inventory, to high-resolution 360° photographs of our vehicles' interior and exterior, to real-time financing and the ability to complete contracts without visiting the back room of a dealership, we provide a seamless, online car buying experience for consumers that can be completed from their desktop or mobile device. All our vehicles are inspected and reconditioned based on our 150-point certification checklist and come with a 7-day return policy. We also operate our own logistics network to deliver cars to customers as soon as the next day, as well as offer customer pick-up at our state-of-the-art Car Vending Machine locations (yes, you read that right). By putting customer satisfaction at the core of our business, we've built a no-pressure, no-haggle online car buying experience that our customers time and money.

For more information on Carvana and our mission, sneak a peek at our company introduction video. ( https://youtu.be/c5dpBRp0-_w )

JOB DESCRIPTION

We are looking for a Data Engineer to support our data science and predictive modeling team. The successful candidate will work with our data science and business intelligence teams to seek out, consume and productionalize new data, both structured and unstructured. Additionally, you will be responsible for designing and maintaining the predictive modeling data pipeline from data acquisition and facilitation of model building to production scoring and model maintenance. The candidate will thrive in a fast-paced, challenging environment and be comfortable managing multiple disparate projects and using myriad tools such as Spark, MS SQL, Python, R, etc. as necessary to get the job done. You will have an insatiable curiosity and drive to learn and implement new technologies, programming languages and database systems to help ensure you and our Data Scientists are maximizing business impact.

As a Data Engineer in Data Science you can expect to…


Work with data scientists to support model building, scoring, monitoring, and reporting.
Identify, collect, store, process and analyze data using various storage engines (MS SQL Server, Spark, etc.)
Design how data is stored, consumed, integrated, and managed
Be focused on choosing optimal solutions to use for those purposes, and then implement, maintain, and administer them
Design large relational data sets from unstructured data
Create data flows to automate the use of algorithms created by our data scientists
Plan, design, and optimize data processes and structures for throughput and query performance
Ensure the accuracy and integrity of the data sets before they are presented to end users
Create ETLs to integrate data between different systems and formats using tools like python, SQL Server Data Tools, etc.
Design processes that contain sensitive data in a responsible manner (using certificates, hashing, AD permissions), ensuring that necessary security practices are followed
Have the ability to read beyond the initial specs of a project to determine if there is additional functionality that should be added
Use basic statistical and visualization techniques to analyze the resulting data sets of your processes
Learn and stay abreast of new technologies that can improve the efficacy of the analytics and data science teams

REQUIREMENTS


Quantitative undergraduate degree (such as math, economics, statistics, engineering, etc.) with a strong academic record, graduate degree preferred
Minimum 3 years of professional experience in analytics, engineering, or data science
You are well-versed in SQL and Python, or a similar language that will easily transfer to Python such as Java or C++
Experience with a cloud platform such as AWS, Azure, or similar
Experience with CI/CD pipelines such as Jenkins, Azure DevOps (VSTS), or similar
You hold (or have the equivalent experience to) a degree in a technical or quantitative field, such as Computer Science, Information Systems, Engineering, Statistics, Applied Mathematics, etc.
You have extensive knowledge of ETL processing – data manipulation, database structure, and data management.
You follow software engineering best practices using tools such as unit testing and git
You are a self-starter with the ability to lead and build trust quickly
You have strong diagnostic and analytical skills, along with the ability to breakdown complex, cross-functional business problems
Have a deep love for data, an analytical brain, and some serious technical aptitude
You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to grow through challenging work and new technologies

NICE TO HAVES:

Exposure to a big data platform, such as Hadoop and AWS
Experience with a data visualization tool
Knowledge of a statistics package such as R, SAS, or pandas in Python
Experience deploying Docker containers to production

What you can expect in return….


A full-time, salaried position
Medical (employee medical fully paid by Carvana), Dental, and Vision benefits
A 401K with company match
All the snacks and drinks your heart desires (plus iced coffee on tap!)
Access to opportunities to expand your skill set and share your knowledge with others across the organization

Legal stuff…

Carvana is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.

This role is not eligible for visa sponsorship.","Tempe, AZ",Data Science Engineer,False
602,"iSpot.tv is a fast growing venture backed start-up with rapidly ramping revenues. We have a product that is changing how brands, agencies, and networks measure and assess the impact of TV advertising campaigns. Our software uses proprietary audio and video analysis to monitor and extract TV commercials, movie trailers and other promotional content from TV. At the same time, our software analyzes consumer interactions with TV ads on the digital screen across search, video and social. Interactions are matched with specific ads and attributed back to airings on TV.
We are looking for an experienced Data Engineer to join our growing team of analytics experts. This position will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
We are looking for someone that is highly motivated and knows how to get things done quickly and efficiently.
Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Collaborate with data scientist and engineering team members to create data tools that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications:
3+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience with Python and AWS. (Redshift, MySql, Java, and Spark are nice to have)
Experience in problem-solving and finding issues
Strong entrepreneurial skills that bring customer-focused ideas to the projects
Knowledge of best practices of software development including coding standards, code reviews, source control management, build processes, testing, and operations.
A natural collaborator with a proven ability to work across teams to get things done.","Bellevue, WA 98008 (Crossroads area)",Data Engineer,False
603,"Big Data Developer:

Cognizant has immediate openings for Big Data. If you meet our background requirements and skills, and looking for an opportunity to be rewarded for your skills and expertise, is the ideal opportunity for you!

Job Description:

Hadoop, Python, Kafka â€¢ Apply expertise and research designs with subject matter experts to devise requirements, design specifications, and usage criteria for Big Data solutions.
 Responsible for leading the design and maintaining solutions conformant to enterprise standards, architecture, and technologies.
Provide oversight, direction and mentoring to ensure adherence to business direction, architectural strategies, and enterprise technology standards.
 Collaborate with developers and business users for overall design oversight on Big Data solutions
 Maintain expertise and proficiency with Big Data technologies using business intelligence best practices.
 Documented experience in a business intelligence or analytic development role on a variety of large scale projects (5 years minimum)
 Expertise in Hadoop and related technologies - Apache Hadoop, Kafka, Python, Apache Spark (including pyspark), Spark streaming, Scala, MapReduce, Hive, HBase


Technical Skills

SNoPrimary SkillProficiency Level *Rqrd./Dsrd.
1Informatica PowerCenterPL1Required
Proficiency Legends

Proficiency LevelGeneric Reference
PL1The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels.
PL2The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects.
PL3The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects.
PL4The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Oct 05 2018
Cognizant US Corporation is an Equal Opportunity Employer Minority/Female/Disability/Veteran. If you require accessibility assistance applying for open positions in the US, please send an email with your request to CareersNA2@cognizant.com

About Cognizant
Cognizant is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant, a member of the NASDAQ-100, is ranked 195 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us on Twitter: USJobsCognizant.

Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.","Bridgewater, NJ",Big Data Developer/Data Engineer,False
604,"Square Root is built on understanding our customers' data more deeply than they do and our data engineers are instrumental in that. In this role, you'll take heterogeneous, unstructured data, mine it for insights, and apply it to business problems in innovative ways. While you don't need tons of experience, you'll need to be sharp and possess a genuine interest in using data to solve business problems. You'll be working side by side with experienced cloud architects and data engineers. We want someone that will grow with us and is comfortable with the idea of programming, algorithmic thinking, and automated testing.
Sound like your kind of challenge? Dig in to learn more!
The Gig
You'll design, develop, and improve our ETL Engine to allow for rapid customer implementations, minimal customer investment, data quality and consistency, flexibility, durability, and availability.
As part of our implementation process, you'll work directly with customers to integrate their business data. You'll work closely with different teams at Square Root to deliver enhancements, ensure operational stability, and define + refine product features.
You'll help expand the scalability of our system and maintain our customer relationships as we adapt to more clients and larger data volumes.
We're all about driving action from data and that includes putting our own enterprise data at the fingertips of all Radicals to empower our own team.
About You
You have a bachelor's degree in Computer Science, Math, Physics or related field.
You've demonstrated your ability as a leader and technologist. We currently work with Python, SQL, Cloud Computing, Linux, and Kafka.
You're able to articulate data insights to non-technical customers.
You should be eager to learn, able to adapt and perfect your work, + willing to seek out help and put it to good use.
Our Radical Culture
Our culture is at the core of everything we do. We have fun, but we work hard. So, as we grow we're not only looking to hire the best and brightest, but we're also looking for people that share our values. This is the code we live by:
Think big. Do bigger. We think big ideas are meant to be pursued. So, we think critically and have a bias for action and impact. We continually iterate to deliver smarter direction and better software.
Be customer-inspired. We aim to over-deliver and delight by doing what's best, not just what's expected. That's why we make it a priority to understand how our customers operate and the challenges that they face.
Partner. We build strong relationships by having a partnership mindset – internally and externally. We go above and beyond to help our customers and one another succeed.
Thrive. We set our internal bar high so that we can bring enthusiasm, speed, and innovation. We revere personal drive, growth, and balance. We celebrate successes, and, sometimes, we just celebrate.
The Good Stuff
Founded in Austin, Texas, we've been bootstrapped and profitable since our start in 2006. Along the way we've added a slew of benefits + perks inspired by our team of Radicals. Here's a sample:
Flexibility: no dress code, office hours, or vacation policy
Competitive benefits and compensation
Cozy campus of five 1920's craftsman homes in downtown
Radicals get $3,000 a year to learn anything (seriously!)
Team lunches, stocked kitchen + bar, coffee-snob compliant coffee machines
Square Root is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected Veteran status or any other characteristic protected by law.","Austin, TX",Data Engineer,False
605,"About Us

Thrive Causemetics was born out of a friendship. Makeup artist and product developer, Karissa Bodnar, lost her dear friend Kristy to cancer at just 24 years old. Kristy’s compassionate and vivacious spirit inspired Karissa to establish Thrive Causemetics, a beauty brand and philosophy that goes beyond skin deep by empowering women.

Thrive Causemetics is Beauty with a Purpose : for every product purchased one is donated to help a woman thrive. We believe changing the world starts with a single ingredient, and that’s why we create vegan, 100% cruelty-free formulas containing proven ingredients without the use of parabens and sulfates. All of our high-performance cosmetics are developed with our customer in mind. We control every step of the product development process.

We are a company that believes everyone is responsible for doing anything and everything to contribute to our team's success. This is why we thrive on a collaborative and cross-functional workplace. There is no such thing as ""that's not my job"" at Thrive Causemetics. We offer a culture rewarding our driven, hard-working team with special perks and benefits. Together, we prioritize strong work ethic while maintaining a positive, exciting environment where people are passionate about what they do.

The Role

The data analyst role at Thrive Causemetics is a hybrid role combining the skill sets of a data analyst, business intelligence analyst, and a data engineer. You will work cross functionally with all departments, helping out with data pipelines, dashboard creation, inventory forecasting and other ad hoc data requests as needed.

Responsibilities

Creatively solving problems in diverse business areas
Analyze customer data to inform our marketing strategy for both ecommerce and brand marketing
Create dashboards containing a mix of data visualizations and tables, providing real time insights for all departments
QA data to ensure accuracy prior to piping to the data warehouse
Extract data from 3rd party tools using built in API’s, cleaning the data and piping it into our data warehouse
Provide insights into marketing ad performance to improve our ad efficiency
Ad hoc data requests and queries as needed
Daily / weekly / monthly KPI reporting for Marketing, Operations and Ecom
Analyze current business processes regarding data and find ways to automate or streamline

Qualifications

Bachelor’s degree (or higher) in Mathematics, Statistics, Economics, Computer Science or related field
2+ years of SQL experience (Redshift or Vertica experience preferred)
1-2 experience with Python for data analytics (pandas, sklearn, matplotlib, seaborn)
Experience with Jupyter Notebooks
Experience working with data in JSON format
Experience connecting to a diverse set of APIs with varying authentication processes a plus
Experiencing working with eComm and Growth Marketing is a plus
Periscope experience a plus
AWS & S3 experience a plus

","Los Angeles, CA",Data Analyst,False
606,"If you are looking to join a great company that is committed to delivering high-level client and employee satisfaction then Dev Technology is the company for you! We are a growing software firm with a stellar history of supporting mission critical applications that protect and serve American Citizens.

Dev Technology Group is looking for an experienced Data Engineer. This position will have the following responsiblities:
Support data analysis to enable data-driven decisions by designing, building, and maintaining data structures and data processing systems
Ability to collaborate with stakeholders and draw and communicate insights from data
Create conceptual and logical models and support data integration
Required Skills and Experience:
Minimum of three (3) years of experience in data analysis and software development
Experience in Java EE/JavaScript programming languages
SQL knowledge and experience working with relational databases, query authoring (SQL) as well as familiarity with a variety of databases
Strong verbal/written communication and data presentation skills
Preferred Skills and Experience:
Experience in big data tools such as Hadoop
Experience data modeling/data visualization tools
Experience with AWS technologies
Experience with AWS CLI and Cloud Formation templates
Experience working with Agile environment
Experience working with DevOps or DevSecOps
Ability to complete application development tasks with Java-based technologies, preferred
Experience with deployment to a government supported cloud environment
Education and Certifications: Bachelor's degree in Computer Science, Information Systems, Engineering, Business or related field

Clearance: DHS Public Trust or the ability to obtain

We offer competitive compensation along with excellent benefits in a team-oriented environment. Learn more about us at www.DevTechnology.com.

Dev Technology Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, gender identity, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws. Dev Technology complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.","Reston, VA 20190",Data Engineer,False
607,"ContractJob SummaryRequired Technical Skills:Proficiency in SQL and PL/SQLDemonstrated experience in decomposing complex problems and provide solutionsExpertise in identifying data relationships, data patterns in the data sourced from different subject areasDemonstrated ability to produce visualizations and detailed analysis documentsStrong analytics and algorithm development skillsDemonstrated experience in management of data in SQL and/or NoSQL databases (Postgres, Oracle, Teradata, Hadoop)Excellent performance tuning and debugging skills to work on large data setsPreferred Qualifications:Experience with large-scale parallel computing in distributed environmentsExperience in analysis of large spatial and non-spatial datasets.Experience with a multitude of databases in general (Oracle, Postgres, Teradata, and Sqlite.)Knowledge of statistics and experience using statistical packages for analyzing datasets (R, Excel, SPSS, SAS, Python etc)Knowledge/Expertise in AI/MLExposure to data visualization tool Tableau or any similar toolsStrong programming language skill in .NET, C#, SQL, Javascript, etc.Great technical and problem solving skillsKnowledgeable in various software design patterns and be able to apply appropriate design patterns to solve business/technical problems.Proficient in RDBMS such as Oracle, SQL Server, etc .NET 3/.5/4.0 with WCF, WWF, WPF, and Silverlight. SQL Server 2008 2012 or 2016.Proficient in JAVA / J2EE , Oracle/JAVA , REST APIKnowledge of RPA /BOT tools (Automation Anywhere, Kore.AI)Job Type: Contract","Irving, TX",Data Engineer,False
608,"$40,000 - $45,000 a yearContractAbout the JobImpact Research is looking for a detail oriented data engineer to support our company for a 3-month contract period with the option to extend. This position may be remote or onsite at our main facility in Columbia, Maryland. In this role, you will be responsible for understanding the structure of source data (typically flat txt or csv) files, transforming as needed and uploading data into a PostgreSQL database. You will be expected to validate final datasets using SQL and document all assumptions and final organization of the data.About the CompanyImpact Research is a small business located in Columbia, Maryland that specializes in Transportation Data Analysis. We are a highly motivated company working on impactful projects in the auto safety and railroad safety industry.ResponsibilitiesSelecting and integrating any tools and frameworks required to provide requested capabilitiesImplementing, documenting and maintaining ETL processMonitoring ETL and data performance and any necessary infrastructure changesBasic QualificationsEducation: Bachelor’s degree in Computer Science, Software Engineering, a related field or equivalent experienceMinimum 2 years’ experience working with relational databases such as PostgreSQL or MysqlMinimum 2 years’ experience working in Linux/Unix environmentDemonstrated ability to program in Java, Bash scripting, C or Python to accomplish project goalsAbility to communicate with technical and non-technical team membersDesired Qualifications: Proficient understanding of distributed computing principlesMinimum 2 years’ experience working with ETL techniques and frameworks such as Apache Flume, Sqoop, Twister or NifiExperience with integration of data from multiple data sourcesExperience with NoSQL databases such as Hbase, MongoDB or CassandraWorking knowledge of big data query tools such as Pig, Hive or ImpalaExperience working within the AWS eco-system a plusJob Type: ContractSalary: $40,000.00 to $45,000.00 /yearExperience:ETL techniques and frameworks: 2 years (Preferred)Education:Bachelor's (Preferred)","Columbia, MD",Data Engineer,False
609,"Location: US-CA-SAN-FRANCISCO
Sponsorship Available: Yes
Relocation Assistance Available: No

Position Description:

The staff data engineer is part of the global Data Science and Analytics team. The team’s project portfolio includes data analytics for manufacturing, business, marketing, and tire performance. Associates come from a broad range of backgrounds and collaborate to develop new ways to analyze and model complex processes using the data available throughout the company and elsewhere. Due to the continuously expanding area of applications of team capabilities, there are ample opportunities for team members to assume leadership roles within current projects, lead the development of new exciting capabilities as well as explore career opportunities across many Goodyear divisions.
Principal Responsibilities:
Develop an understanding of the business, technology, manufacturing processes, and data science related topics such as data systems (data sources, data gathering, storing) and business analytics (business reporting procedures & KPIs).
Serve as a specialist in data engineering to support large scale Data Science initiatives as well as lead Data Science projects in some instances.
Grow the technical competencies associated with data engineering within the Data Science team through the personal development of new skills and integration of new technology into the team.

Required Experience and Education:
Bachelor Degree in Computer Science (or related field)
Masters or PhD in Computer Science (or related field) desired
3+ years’ experience
Application of big data technology
Database design
Designing, developing and maintaining data flows
Supporting data science and machine learning

Personal Skills/Attributes/and Qualifications:
Big data technology (e.g. HDInsight/EMR, Hive/Pig, HDFS/Spark, S3/blob, Kafka/Storm/Kinesis)
Database design (e.g. SQL/NoSQL, knowledge of database normalization, graph database)
Data flows tools (e.g. Python, R, Java/Scala, C++, git, CI/CD, CloudWatch, Apache Airflow, REST API)
Data science tools (e.g. knowledge of machine learning algorithms, distributed computing, GPU computing, model serving)
Demonstrated leadership skills, good communication, technical writing skills and ability to work in a team environment

Goodyear is one of the world’s largest tire companies. It employs about 64,000 people and manufactures its products in 48 facilities in 22 countries around the world. Its two Innovation Centers in Akron, Ohio and Colmar-Berg, Luxembourg strive to develop state-of-the-art products and services that set the technology and performance standard for the industry. For more information about Goodyear and its products, go to www.goodyear.com/corporate.
Goodyear is an Equal Employment Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regards to that individual's race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender identity, age, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
If you need reasonable accommodation to complete the online application, or any other part of the employment process, please contact the Goodyear Candidate Care Line at 330.796.4500.
Click here for more information about Equal Employment Opportunity laws, and here for additional supplementary information.","San Francisco, CA",Staff Data Engineer,False
610,"Sun Trading is looking for a talented Market Data Engineer to join our London office. Our work environment is open, fast-paced, and highly professional. The Market Data team is responsible for building and maintaining time series databases as well as analytics infrastructure. This infrastructure is used for quantitative research, back testing, parameter calibration, risk data modeling, and compliance reporting.
Responsibilities
Assist in developing and maintaining the Market Data team's KDB+ infrastructure. This includes troubleshooting production issues and automating processes to make the team more efficient.
Work directly with the trading teams, risk managers and compliance officers to identify and fulfill their market data needs.
Produce reports using market data, internally generated data and other data sources. Analyze and compare new data sources and make recommendations.
Follow company-wide software development policies including documenting code and code changes.
Build validation to ensure all data adheres to strict quality specifications.
Required Qualifications & Skills
Bachelor’s degree or higher in Computer Science, Computer Engineering or equivalent.
Very strong KDB/q skills.
Exceptional troubleshooting and problem solving skill
A strong work ethic, excellent communication skills and the ability to collaborate closely both within Market Data and with our customers outside the team.
Additional useful qualifications
Working knowledge of building high frequency applications and real-time data processing.
Experience in Python and/or shell scripting.
Working knowledge of C++ development.
Understanding of exchange protocols, standards and connectivity.
Experience in the financial services or trading industry.
About Us
Sun Trading International Limited (UK) is a leading proprietary trading firm registered with the Financial Conduct Authority. We look for certain qualities in our industry-leading traders, technology professionals and operational/financial professionals. Our most successful applicants are typically both creative and analytical. They’re intellectual risk takers who are disciplined in defining and adhering to our processes and procedures. Taking what you do – not yourself – seriously also is essential.
We know that growth creates opportunities to learn new skills and expand what we’re capable of achieving, both collectively and individually. Our firm’s future is always top-of-mind, so continuously developing our up-and-coming leaders is a priority. Sun Trading is a meritocracy, so exceptional accomplishment and increased responsibilities typically go hand-in-hand.
Each day, our team confronts mind-bending challenges, contributes value-added skills and ideas, and has the chance to reap the rewards of hard work. In addition to highly competitive compensation, excellent benefits and continuous education/professional development training, we also offer the opportunity to be a part of something bigger than yourself.
What we offer
Excellent remuneration package including annual bonus.
Employee benefits that include exceptional insurance and fitness membership reimbursement.
Free in-house lunch program.
Encouragement to attend industry related conferences, training and continuing education.
Become part of a uniquely diversified, vastly experienced and globally educated team.
Working on and access to top tier technology and related hardware and software.
Company-sponsored social functions.","London, OH",Market Data Engineer - London,False
612,Ability to perform ETL operations on both On-Premises and Cloud based technologies.Setting up instances for batch and real time ETL operations.Integration and testing of data of models in development and production environments.Medium proficiency and experience in implementing AIML codes in distributed node environmentJob Type: Full-time,"San Francisco, CA",Data Engineer,False
613,"Overview





Stansberry Research is looking for a talented Data Engineer to join their Business Intelligence Team. This is a role that will collaborate with the technology team in building a world class BI/DW solution and will have a direct impact in building a culture of data driven decision-making where information is timely, accurate and actionable.

The Data Engineer will be responsible for integration and transformation of data sources into dimensional models for intuitive consumption. This includes designing DW/BI solutions for performance, efficiency, scalability and security. You will have an opportunity to use the latest technology platforms in Snowflake, Looker, Tableau, Talend and AWS (RDS, Redshift and S3).
Responsibilities
Work closely with the business and technology teams to create scalable data models for reporting and analytical consumption.
Identify and understand source data systems and become an expert of data sources and flows.
Design, build and maintain ETL/ELT processes.
Work with business and technology teams to extract data components related to customers, sales, marketing, and accounting for intuitive consumption in the data warehouse.
Optimize and tune DB schemas for faster performance.
Explore and recommend technologies and techniques to improve DW/BI functions.
Qualifications
BS/BA in Computer Science, Information Systems or related field, or equivalent experience.
3+ years experience in database concepts with hands-on experience with MySQL, Postgres, Oracle, or MSSQL.
Experience in columnar store database concepts with hands-on experience with Snowflake, AWS Redshift or similar.
Experience working with ETL/ELT tools (Informatica, IBM Websphere Information integration, SAP Data Integrator or SSIS) and technologies.
Experience working with BI tools (Tableau, Looker, Business Objects, Cognos, Microstrategy, SSRS, and QlikView).
Experience with scripting languages such as Python, Powershell, and/or bash.
If interested, please submit a resume and cover letter to the link provided.

About Stansberry Research
Stansberry Research is a subscription-based publisher of financial information and software, serving millions of investors around the world. Their business is guided by two simple principles:
They strive to give their customers the information they'd want if their roles were reversed.
They only publish analysts whose advice and strategies they'd want their own families to read and to follow.
They believe in offering a range of opinions. Experienced analysts, with their own unique investment strategies and philosophies, lead their franchise brands. As a result, they do not promote a single, unified view of the markets, but instead they publish a mosaic of opinions, recommendations, and strategies. This multi-franchise approach gives their work far greater breadth, creating more diverse opportunities for their subscribers. Their franchises are linked, however, by a continuous commitment to risk management and a contrarian approach to identifying investment opportunities. Across all of their franchises, they focus on investments that are unloved, ignored, or unknown. It is in these situations where having an informed perspective gives their subscribers the best risk-to-reward opportunities. Learn more about Stansberry Research at http://stansberryresearch.com","Baltimore, MD",Data Engineer,False
614,"Data EngineerJoin our seasoned team of data engineering professionals to take your career to the next level. As a Data Engineer, you will face the most complex and up-to-date challenges by helping our clients address their complex data management, reporting and analytical challenges using Big Data Technologies (Hadoop, Columnar/ NoSQL DBs, Python) and Cloud Services (AWS, Azure, GCP)Minimum Requirements1+ years of Experience in working with global clients in data management and business intelligence using Python and Hadoop for data managementExperience in writing efficient data management code to support high performance/ parallelization requirements in complex environments such as: distributed and high performance environments, cloud and enterprise solutionsExperience in working closely with clientsBachelor’s Degree in technology disciplinePreferred RequirementsStrong Linux and Windows administration skillsCertifications in cloud platform (AWS/ GCP/ Azure)Strong Python programming and object oriented programming skillsExperience in Hadoop based data managementMaster's level educationSkills & CapabilitiesWriting complex and high performance data pipelinesAbility to automate end-to-end production data pipelines using automation technologies such as Airflow, AWS EMR, Hadoop, AWS Lambda etcUser, data and security administration including integration with enterprise directories and data encryptionStrong understanding of cloud server environmentsStrong understanding of SQL (2+ years), programming languages and other scripting languages (e.g., Python, Perl)Strong communication and project management skillsAbility to work effectively in a global teamJob DutiesCreate high performance data pipelines to support complex data integration workflowsDevelop automation programs, shell scripts and other utilities to support overall goal of end to end automationProduces a weekly status report documenting project health and progressSupport process flow analysis and ETL process redesignDocument and gain approval for customer requirements definitionParticipate in completion and implementation solution documentationParticipate in user acceptance testing efforts as neededParticipate in training design, documentation and delivery efforts in concert with other project team membersParticipate in internal projects as requiredJob Type: Full-timeJob Type: Full-timeExperience:Software Development: 1 year (Required)SQL: 1 year (Required)PySpark: 1 year (Required)","Research Triangle Park, NC",Data Engineer,False
615,"Applecart deploys proprietary technology to run smarter advertising campaigns. We work with some of the nation’s most prominent corporations, non-profit organizations, and political candidates to activate and communicate with key target audiences. Our core offering, the Social Graph, leverages publicly-available data to map real-world relationships between individuals at a national scale. We got our start in politics, where we have tested and refined our methods on countless campaigns, giving our clients a proven technological edge. Now, we’re branching out beyond political campaigns to tackle new advertising challenges, using relational data to provide decisive advantages for our clients.

Applecart’s political work has been featured by The Colbert Report, CNN, The Washington Post, The Associated Press, USA Today, The Huffington Post, among other prominent news outlets.

As a Senior Data Engineer on our Data Acquisition team, you will be responsible for designing, building and maintaining scalable systems that acquire, enrich and validate the data that feeds our social graph. At Applecart, we live and die on the coverage and quality of our data - your work will directly affect our clients in the form of election outcomes, increasing political and non-profit fundraising yields and optimizing advertising spends and risk assessments.

Responsibilities:

Work with large quantities of structured and unstructured data of various integrity in both streaming and batch environments.
Handle the architecture and implementation of various data stores and schemas across data lakes, document stores, cache layers and data warehouses.
Lead the design of containerized, auto-scaling microservices for data extraction, enrichment and validation.
Lead the design and implementation of testing, monitoring and instrumentation across streaming and batch ETLs, RESTful APIs, infrastructure and frontend interfaces.
Collaborate on technical estimates, interface design, and architectural decisions for cross-team data initiatives.
Mentor team members in best practices as they relate to object-oriented design, testing, optimization, code reviews, etc.


Basic Qualifications:

Strong grasp of SQL, Python, and Spark or Hadoop.
Experience designing object-oriented software across all ends of the stack (APIs, pipelines, front-end interfaces, Python clients).
Experience in both batch (using Airflow, Luigi) and streaming (using Pulsar, Kafka Streaming, or AWS Kinesis) environments.
Experience with unit, integration and functional testing in a data-intensive environment.
Experience with data architecture - fundamental understanding of document stores, relational databases, and data lakes as well as strong schema design.
6+ years as a software engineer including past ownership over a product or experience leading a team.
Bachelor’s Degree or higher in Computer Science, Electrical Engineering or related field.


Prefered Qualifications:

Intermediate to strong understanding of Java, Javascript and/or Scala.
Experience with containerization technologies like Kubernetes or Docker.
Experience with infrastructure tools such as Terraform, Chef, Ansible or Puppet.
Experience with search technologies such as ElasticSearch, Solr or analogous tools.
Experience with web scraping or OCR.
Startup, advertising or fintech experience is a plus.
Interest in professional growth and mentorship of engineers.
Interest in adtech, politics, predictive analytics and/or graph technologies.","New York, NY","Senior Data Engineer, Data Acquisition",False
616,"Job Description
The ideal candidate is a motivated self-starter with strong business acumen to join the team and build analytics driving actionable insights to accelerate the growth of the business. The Data Engineer will partner with Senior leaders, product managers, and other internal stakeholders to build metrics.

This role requires an individual with excellent analytical abilities, deep knowledge of business intelligence solutions as well as a passion for problem-solving. Ideally, you are comfortable with ambiguity and accessing and working with data from multiple sources. You will analyze large amounts of data, discover and solve real world problems, and build metrics and business cases around key performance of this program. You will be responsible for understanding the health of the service and business, and drive necessary changes as needed. This is a perfect position for someone who loves data and knows how to work fast and smart.

Some of the key responsibilities for this position include:
Propose and implement business metrics for senior management reviewsWork with business intelligence and data engineers to design and develop data infrastructure to support business growthEnable effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable formatPerform deep-dives to find the root causes behind variances of key parameters over a given time-periodDevelop intelligent, insightful self-reporting toolsManage the data warehouse
Key responsibilities:
Work closely with management, software engineering leaders, and business teams to plan catalog data improvements so as to maximize customer improvement impact.Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.Collaborate with software engineering teams to integrate experimental capabilities into large-scale, highly complex Amazon production systems.Report results in a manner which is both statistically rigorous and compellingly relevant.Assist in recruiting, mentoring, developing, and training other Research Scientists as well as other technical roles when needed.
Basic Qualifications
B.S. degree in mathematics, statistics, computer science or a similar quantitative field
3+ years work experience in relevant field
Experience in using SQL to analyze data in a database or data warehouse and be able to use a major programming (e.g. Java/C) and/or a scripting language (Perl, Unix shell, Python) to process data for modeling
Experience working with a wide range of predictive and decision models and data mining techniques, as well as tools for developing such models
Preferred Qualifications
Experience in data modeling, ETL development, and Data warehousing.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with AWS services including S3, Redshift, EMR and RDS.
Data Warehousing Experience with Oracle, Redshift, etc
Experience with software coding practices is a strong plus.
Experience using Linux/UNIX to process large data sets
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.","Cupertino, CA","Data Engineer, Infrastructure Automation",False
617,"Providence is calling a Data Engineer – PSJH to Providence Health & Services in either of the following locations: Renton, WA, Seattle, WA, Portland, OR, Beaverton, OR or Anaheim, CA.
We are seeking a Data Engineer – PSJH who designs and builds modern data-centric software applications to support clinical and operational processes across all parts of the health system. These applications leverage cloud computing, big data, mobile, data science, and modern software development methodologies and frameworks. Builds the data pipelines, enrichment processes, provisioning layers, APIs and user interfaces to meet the requirements of key initiatives. Enjoys a fast pace and has a focus on regular delivery. Seeks simple solutions to complex problems through the use of modern and emerging methods and tools. Emphasizes sharing and enables collaboration with meticulous source control and documentation. Works closely with the Product, Platform, and Architecture teams to deliver on joint efforts.

In this position you will have the following responsibilities:
Design, build and deliver quantitative applications that improve operations and generate value
Participate in DevOps, Agile, and continuous integration frameworks
Stay abreast of emerging technologies, open source projects, and best practices in the field
Data warehousing, big data, enterprise search, business intelligence, analytics, modern and mobile applications
Build processes that are fault-tolerant, self-healing, reliable, resilient and secure
Work effectively and in real-time with other developers, product managers, and customers to deliver on collective goals
Actively participate in code reviews, support the overall code base, and support the establishment of standard processes and frameworks
Take an open and transparent approach to the work by sharing code and expertise, by consulting peers for problem-solving, and by being a mentor to your peers
Required qualifications for this position include:
Bachelor’s Degree in in computer science, engineering, mathematics, MIS or similar field.
3 years in technology roles.
Demonstrated analytical skills
Demonstrated problem solving skills
Possesses strong technical aptitude.
Cloud computing, Linux, Hadoop, MapReduce, Spark, Hbase, Kudu and NoSQL platforms in general; Apache Solr and Lucene
Java, Scala, C#, Python, shell scripting and/or similar languages
Relational database platforms, database design, and SQL
APIs, JSON, REST and other relevant W3C open standards
Modern application development frameworks
Familiarity with commercial or open source ETL tools
Preferred qualifications for this position include:
Master’s Degree","Anaheim, CA",Data Engineer PSJH,False
618,"About Earnin:
Our financial system is fundamentally unfair for people who live paycheck to paycheck. People shouldn't have to wait weeks to get the pay they've already earned. This wait forces them to spend more than $100 billion a year on short-term credit products, late fees and overdraft fees.

Earnin helps people overcome this unfairness by giving them real-time access to their money as soon as they have worked -- without fees or interest.

Our team is bound together by a desire to level the financial playing field. Our investors, Andreessen Horowitz, Ribbit Capital, Felicis Ventures, Matrix Partners and March Capital believe we will change the world.

You can help make a difference. Join us.

About the Team:
We are a data driven mobile financial tech company and we're looking for a Data Engineer to join us and help us build out our data infrastructure to aid in our mission of enabling people to gain access to their paycheck on demand.

Data engineers are an important function to interact with every team within Earnin and you will be interfacing heavily with our analytics, engineering, and data science teams to help them advance our product utilizing machine learning intelligence.

As a Data Engineer you will:

Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our business
Help us build a world class data lake/data warehouse, by building data pipelines
Design and develop new systems and tools to enable folks to consume and understand data faster
Use your expert coding skills across a number of languages from Python, Java, C++, Go etc.
Work across multiple teams in high visibility roles and own the solution end-to-end
Design, build and launch new data extraction, transformation and loading processes in production
Work with data infrastructure to triage infra issues and drive to resolution.

Some skills we consider critical to being a Data Engineer:

BS or MS degree in Computer Science or a related technical field
Advanced knowledge of Java. Familiarity of Python
Familiarity with Hadoop stack, Spark, AWS Glue, AWS Athena etc
Diverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.)
Deep familiarity with schemas, metadata catalogs etc.
Ability to manage and communicate data warehouse plans to internal clients
Strong communication skills, including the ability to identify and communicate data driven insight

","Palo Alto, CA",Data Engineer,False
619,"ContractData EngineerMarietta, GAContractMDI Group is seeking a Data Engineer for a project in Marietta, GA. In this role, you will be responsible for extracting, transforming and loading data from an ERP Data Source into a data repository for operational reporting. We seek your experience in pulling this data from various data sources and loading this data into another database / data source. This role is key to contributing to business intelligence solution delivery and development (programming) of data-driven reports, day to day operations of reports, BI dashboards and other BI tools.Required Skills & Experience: 5 years of experience in Data EngineeringExperience developing solutions to extract data from various data sources for loading into a data source for operational reportingReach out to MDI Group today to learn more about this excellent opportunity.Contact:Kaci Raileykrailey -at- mdigroup -dot- comkrailey @ mdigroup .com***In person interview required in Marietta, GA. No Skype.***Job Types: Full-time, ContractJob Type: Contract","Marietta, GA",Data Engineer,False
620,"Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions – underpinned by the world’s largest delivery network – Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With approximately 425,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.
Are you willing to come and shape the future of technology? At the Liquid Studios, we are part of Accenture’s innovation architecture, working on helping companies understand how to apply bleeding edge technologies like AI, IoT and Blockchain to their business.

The studio typically works on short projects (our engagements usually last less than 12 weeks) based out of the Boston studio. Our engagements use the latest of Dev-ops, agile and serverless to make sure that we deliver the value in a short period of time.
Job Description

Data Engineers will create the foundations used to power the data-driven applications that are changing how businesses operate, including real-time data feeds for IoT devices, distributed ledgers using blockchain, or large data sets used for training machine learning algorithms.
Design, implement and deploy custom applications using real-time data streams and/or big data platforms
Collect, create, and structure data sets from disparate sources to be able to leverage them for machine learning applications
Build data pipelines to ingest, transform, and analyze data in artificial intelligence and analytics systems
Design data architectures that address specific client needs, using combinations of relational databases, No-SQL databases, and unstructured file stores in both cloud and on-premise settings
Develop solutions and iterate rapidly
Ensure code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.

Basic Qualifications
Minimum 2 years of hands-on technical experience implementing or supporting big data or real-time analytics solutions
High level of competence in SQL, Python and/or scripting languagesBachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years
Ability to travel about 10% of the time

Preferred Qualifications

• Full life cycle development experience
Experience with delivering Big Data Solutions in the cloud with AWS, Azure or Google CloudAbility to configure and support API and OpenSource integrationsExperience administering Hadoop or other Data Science and Analytics platforms using the technologies aboveExperience working with DevOpsDesigning ingestion, low latency, visualization clusters to sustain data loads
Experience developing solutions utilizing any of the following:
o Apache Kafka or AWS Kinesis based streaming services
o Relational and Non-relations Databases(No-SQL)
o Amazon S3
o Scala, Spark
o Jenkins, Chef, Puppet

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.


Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.


Accenture is a federal contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.


Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.


Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.


Accenture is committed to providing veteran employment opportunities to our service men and women.","Boston, MA",Data Engineer - Liquid Studios Boston,False
621,"Company Overview:


Marketing Evolution has been recognized by Forrester as the industry leader in marketing optimization, due to our big-data, person-centric, decision-making SaaS platform. Marketing Evolution has announced $20.6M in new 2018 funding from top-tier global private equity and venture capital firm. This funding will used to help us continue to lead product innovation focused on customer obsessed marketers. Our award winning platform, the ROI Brain, helps companies fully unlock the power of customer relationships and use insights to make the overall business more effective, efficient, and profitable. Our customers – including many Fortune 500 brands – choose Marketing Evolution because they recognize the value of moving beyond the aggregated and backward-looking limitations of marketing mix and attribution models. Due to very fast growth, we are hiring at all levels; but know that our preference is always to promote from within.
Position Overview
Marketing Evolution provides marketing ROI insights to Fortune 500 companies by aggregating and analyzing vast amount of both public and proprietary data feeds. As a Data Engineer, you will be responsible for building and supporting ETL pipelines to make data consumable by our predictive analytics process.
Responsibilities
Design and implement new ETL pipelines based on client business logic.
Optimize and scale existing ETL pipelines.
Develop tools to support ETL platform.
Perform ad-hoc queries to support client teams.
Technologies we use
Python, Spark, AWS, Airflow, Bash, Docker, SQL, Scala, Hive.
Skills and Experience

Required:
Strong communication skills relating technical work to non-technical people.
Bachelor’s degree or higher in a relevant technical field, or comparable work experience.
Competence in Python and SQL.
Working knowledge of Unix systems, such as Fedora, CentOS, Redhat, etc.
Understanding of data quality Best Practices.
Preferred:
Experience working with big data technologies such as Hadoop, Hive, Pig, Spark, Presto, etc.
Experience working in a cloud-based infrastructure, such as Amazon AWS or Cloudera.
Competence in at least one OOP language, such as Java, Scala, C++, etc.
Experience working with NoSQL database technologies such as MongoDB, Cassandra, etc.
Metrics for Success
All employees have annual career growth goals based on their position and core skills/development areas; you’ll receive project-based performance reviews quarterly, and career reviews bi-annually.
Customer satisfaction and the ability to drive large efficiency gains are the most important measures of success.
Marketing Evolution Cultural Values
We seek candidates who are excited to work at a company with the following cultural values:
Disrupt the norm: Cherish the opportunity to find better ways to do things, regardless of how disruptive or initially painful it might be.
Dissent then execute: Raise issues and fixes in the planning stage, but when it’s time to execute, execute diligently and without reservation.
Test and learn: When developing new processes or features, link it to a hypothesis, and evaluate that hypothesis without bias.
Nothing is impossible: Imagine what’s possible and then have the discipline to execute it. One without the other is not valuable.
Defect is treasure: Every mistake is an opportunity for the organization to learn and make improvements.
Share success: Given how hard it is to promote change in our industry, celebrate each success along the way.","New York, NY 10168 (Murray Hill area)","Data Engineer, ETL",False
622,"InternshipWe are seeking a motivated Biotech Data Engineer Intern. This position will be located in Chesterfield, Missouri, reporting to our Data Analyst and Reporting Lead and our Data Engineering Lead.
The successful candidate will join a team of developers in the Biotechnology and Chemistry organization that focuses on development and integration of next generation IT platforms and solutions to deliver automated outcomes across Biotechnology and Chemistry. This internship will center around improving laboratory analytics and infrastructure.

This internship will be offered from May-August of 2019.
Responsibilities May Include:
Analysis of laboratory software architecture and Databases for novel, new data integration/messaging capabilities
Development of cloud based ETL processes aggregating data, applying decision framework, and automating outcomes throughout the organization.
Development of a standalone tool to facilitate lab information management

Required Skills/Experience:
Currently enrolled at a university within the U.S. pursuing an undergraduate or graduate degree in Computer Science OR majoring in Life Sciences or Engineering with a minor in Computer Science.
3.0 GPA on a 4.0 scale.
Must be returning to school to continue program of study upon completion of internship
Experience with common programming languages (for example Java, Javascript, C++, NET/C#)
Experienced with Relational and/or Graph Databases
Desired Skills/Experience:

Experience with cloud computing platforms such as AWS, Google Cloud or Microsoft Azure.
Experience with Git and Docker
Bayer successfully completed the acquisition of Monsanto in June 2018, bringing together Monsanto’s leadership in seeds and plant traits with Bayer’s leadership in chemical and biological crop protection. By joining forces, we will create even more extensive career opportunities for talent around the world. We’re a global team working to shape agriculture through breakthrough innovation that will benefit farmers, consumers, and our planet.

While we are now Bayer, we will continue to hire using separate career sites until we can integrate our career platforms. We invite you to explore the career opportunities available at the combined company by visiting advancingtogether.com/careers.","St. Louis, MO",Biotech Data Engineer Intern,False
623,"Hi,I would like to share an excellent opening Full time “Big Data Engineer” do go through the details and kindly send me the updated resume.Location : Charlotte , NCClient : CognizantType of hire : full timeJob Description :Big data engineers, Hive, Spark, PythonJob Type: Full-timeExperience:Spark: 1 year (Preferred)Python: 1 year (Preferred)Hive: 1 year (Preferred)","Charlotte, NC",Big Data Engineer,False
624,"ContractJob SummarySkills Needed: Expert Level- Spark, Azure, Data Engineeringo Additional Skills: ETL Processes, Data Pipelines, SQLOverall 10 + years of experience as a data engineerCommunication Level- 10/10 and highly collaborativeAnalytical mindset to solving a problemAlmost a ""consultant"" like mind set of going into a project ready to rock, and getting things done for the duration of the contractPrevious leadership experience is a huge plus!Thanks & Regards,KANTHIBusiness Development ManagerMXPRACTICE , Alpharetta, GA 30004Contact Number : +1770-406-0486Job Type: Contract","Seattle, WA",Data analyst,False
625,"Tradesy is a peer-to-peer marketplace for women’s designer fashion that’s pioneering a sustainable future for commerce. Backed by top tier venture capital firms, our mission is to make the resale of consumer goods as simple, safe, and stylish as retail, at scale. We have millions of passionate members, a product that people love, and an office with an ocean view in sunny Santa Monica, California.

Tradesy is seeking an exceptionally talented Data Engineer who works closely with other engineers, data scientists, architects and product owners to develop reliable, efficient, and scalable systems.
The ideal candidate enjoys being heavily involved in the planning and design of our data pipelines, storage, warehouses, and applications as we migrate and re-platform our systems from AWS to Google Cloud.
You Will:
Build and automate reliable data pipelines using batch and streaming technologies
Design and test ETL systems using the latest technologies
Collaborate with other software engineers and data scientists to develop data driven applications
Become a data warehousing expert (if you aren’t already one!) and work with others to gain key insights
You Have:
Excellent communication and collaboration skills
Coding proficiency in at least one of Java, Python, C++, Go, Scala
Strong computer science skills with a focus on algorithms and data structures
Experience querying data using SQL and/or NoSQL
Familiar with MapReduce and other big data concepts
Experience with batch processing technologies (ie. Hadoop, Spark, Apache Beam, Flink)
Pluses:
Understand trade offs among data formats such as CSV, JSON, Avro, Parquet
Experience with stream processing technologies such as Apache Beam, Spark Streaming, Flink, Kafka Streams
ETL experience on AWS using EMR, Firehose, Lambda
ETL experience on Google Cloud using Dataproc, Cloud Functions, Dataflow
Experience using a data warehouse such as Redshift or BigQuery
Familiar with messaging systems such as Kinesis, Kafka, PubSub
Familiar with automation tools such as Apache Airflow, Luigi, AWS Data Pipeline
Compensation:
Competitive base salary plus meaningful equity
Comprehensive benefits (Medical, Dental, Vision, 401k)
Flexible Paid Time Off
Additional Perks:
Daily catered lunches
Dog friendly office
Collaborative, fun team","Santa Monica, CA 90401",Data Engineer,False
626,"Strength Through Diversity

Ground breaking science. Advancing medicine. Healing made personal.
Clinical Innovation - Req #2279732
Data Engineer
Roles & Responsibilities:
Are you ready to discover the limitless possibilities of big data analytics and machine learning on improving the patient’s care? Are you interested in learning and developing machine learning data product

The Data Engineer will work on learning and creating batch and streaming machine learning pipeline for accelerating translational research and improving clinical care. Day to day responsibilities include:

Create machine learning pipeline using big-data technology stacks
Work on Python, Apache Spark, Kafka, MongoDB, and Machine Learning
Additional responsibilities include developing prototypes and proof of concepts for the selected use cases, and implementing complex machine learning pipeline
Requirements:
Education: Master/PhD degree in Engineering (e.g. computer science , Biomedical Informatics, Physics, Statistics)

Experience:

Good knowledge with at least 1 programming languages among
Scala/Python/Java/C/C++.
Must be flexible and fast to pick up new languages.
Basic Knowledge Big Data Technology Stacks like Apache Spark, Apache
Kafka, NoSQL, and Machine Learning.
Preferred Qualification:

Hands-on experiences on Apache Spark, Kafka, MongoDB, Apache Nifi and other big data technology stacks and streaming tools.
Familiarity with and the ability to leverage a wide variety of open source technologies and tools.
Knowledge of cloud architecture and implementation on Azure or AWS is a big plus.
Strength Through Diversity

The Mount Sinai Health System believes that diversity is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize medicine together.

We work hard to acquire and retain the best people, and to create a welcoming, nurturing work environment where you can develop professionally. We share the belief that all employees, regardless of job title or expertise, can make an impact on quality patient care.

Explore more about this opportunity and how you can help us write a new chapter in our story!

Who We Are

Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve.

Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospital campuses, including Mount Sinai Beth Israel, Mount Sinai Beth Israel Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai.

The Mount Sinai Health System is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.

EOE Minorities/Women/Disabled/Veterans","New York, NY",Data Engineer - Clinical Innovation,False
627,"Are you passionate about diving into data, unraveling the complexities, and building powerful business intelligence solutions?
Yelp’s Business Intelligence and Data Integration team works with groups throughout the organization and data from an array of diverse sources. The team creates business intelligence solutions that enable business leaders to make quicker, more informed decisions.
You might be a great fit for this role if you are adept at data warehousing concepts including data profiling and analysis, data extraction and transformation, and dimensional modeling. You should also be very comfortable working with multiple teams and various levels of management to explore opportunities, elicit requirements, and collaborate on reporting and analytics projects.
What You Will Do:
Partner with business partners to understand data needs, how business processes are accomplished in various data systems, business rules and definitions, and current data and reporting challenges.
Analyze source system data and design and implement ETL pipelines, relational and dimensional data models, and other business intelligence infrastructure to address needs.
Create and distribute business and technical documentation and train analysts, engineers and business partners.
Become a subject matter expert on Yelp’s data and share knowledge with analysts, engineers, and business partners.
Explore emerging trends and new technologies to continue to improve Yelp’s business intelligence capabilities.
We Are Looking For:
Degree in Computer Science or related field.
3+ years professional experience building ETL jobs, performing dimensional modeling, and using SQL/scripting languages in a BI context.
Excellent working experience creating and optimizing complex SQL queries.
Excellent working experience writing ETL jobs (SnapLogic preferred) and using scripting languages (Python preferred).
Working experience with REST, SOAP, and other web API technology.
Good understanding of financial management and human capital management systems like Oracle EBS and Workday, and CRM systems like Salesforce.
Knowledge of open source technologies like Kafka, Hadoop, Hive, Presto, and Spark is a plus.
Experience with Redshift database administration is a plus.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.","San Francisco, CA 94105 (Financial District area)",Business Intelligence - Data Engineer,False
628,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Lead Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of streaming data architectures and technologies for real-time and low-latency data processing
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Understanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftsmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels

There's no typical day or engagement for our Senior Data Engineers. Here's what you'll do:


Be the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

A few important things to know:

Travel is required. Projects are almost exclusively on customer site, so ThoughtWorkers need to be flexible and up for extensive travel. Most of our consultants travel every week and fly home for weekends. We do our best to take people's personal situations into account, but we know it's not for everyone.
Residing near one of our North America offices in Chicago, Dallas, New York, Atlanta or San Francisco is preferable although not necessary.

Not quite ready to apply? Or maybe this isn't the right role for you?

That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (tick 'contact me about recruitment opportunities' to hear about jobs in the future).

#LI-NA","New York, NY 10016 (Gramercy area)",Lead Data Engineer,False
629,"Renew Financial is the nation’s leader in affordable financing products for renewable energy and energy efficiency projects that will help move America toward a clean energy model. We originated the Property Assessed Clean Energy (PACE) model, an innovative, low-cost financing solution that helps homeowners make improvements that dramatically reduce their energy and water use - and repay on their property taxes.
As a Data Engineer with a focus on data visualizations, you will join our Business Intelligence team dedicated to empowering Renew Financial to make data driven decisions. Positioned in the Information Technology group, you will work across various business units (Risk, Sales, FinOps, Operations, Product, Government Relations etc..) to create BI reports, dashboards, and improve existing reporting processes. Solid communication skills and building interdepartmental relationships are critical to success in this role. Your work here will directly contribute to a shared vision of self-service data analytics.
Ideal Candidate:
Passion for using data visualization tools to turn complex data sets into actionable insights
Proven success in requirements gathering, scoping projects, and delivery timely results
SQL is your second language: you write SQL in your sleep and have a keen eye for performance tuning
Strong data analysis skills and KPI development
Passionate about promoting data literacy and training the business on new tools and technology
Tool Agnostic: you care about using the right tool for the right job
Job Responsibilities:
 Develop both operational and analytical reports
 Translate business users’ needs into BI implementations
 Design, build, and deploy Tableau dashboards in the form of ongoing and ad-hoc reporting
 Educate business users on Tableau usage and best practices
 Investigate data discrepancies, working with data engineers and developers to identify root cause
 Identify opportunities to bring data to life in new/creative ways
Required skills and knowledge:
2+ years of experience using data visualization tools to develop dashboards and reports
Proficient in SQL
Bachelor's degree in Computer Science or related field (STEM) degree
Strong data analysis
Proficient in Tableau
Familiarity with Postgres, AWS, Github
Knowledge of star schema data models
Nice to have:
Scripting language such as ruby, python, R","Alpharetta, GA",BI Data Engineer,False
630,"RESOLVIT
Bringing Solutions That Make Business Better
We are seeking an Azure Data Engineer to be part of a creative, forward-thinking team. Our success at deploying skilled, highly knowledgeable experts has landed us on the Inc. 5000 list of America’s fastest-growing companies four times – and we’re just getting started.
As the Azure Data Engineer, you will develop sustainable data driven solutions with new data technologies to meet the needs of our organization and business customers. You will build robust end-to-end systems with an eye on the long term maintenance and support of the application. You will also leverage reusable code modules to solve problems across the team and organization. Additionally, you will:
Handle multiple functions and roles for the projects and Agile teams
Work with established standards across the team and organization
Understand complex multi-tier, multi-platform systems
Contribute to building a framework of a significant complexity
Work with internal team of data engineers (both full-time associates and/or third party resources)
What You’ll Need to be Successful:
At least 5 years of coding OR data warehousing OR unstructured data environments experience
At least 2 years of experience with Azure cloud technologies
At least 2 years of experience with big data technologies (Cassandra, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, Zookeeper, or similar)
At least 2 years of experience with Agile engineering practices
Bachelor’s degree
Strong problem solving and conceptual thinking abilities
Excellent communication, interpersonal, and leadership skills
Desire to be part of cutting edge, high profile projects
A love for learning new technologies and mentoring junior analysts to raise the bar on your team
Passionate about intuitive and engaging user interfaces
We currently have more than 100 open career opportunities across the country, so be sure to mention the appropriate Job Code with any correspondence!
About Resolvit:
Resolvit is an international technology consulting firm with industry-leading customers in the financial services, high tech, manufacturing, retail, life sciences, and government sectors. Through its partnerships, Resolvit delivers highly impactful, innovative solutions across five core areas: Infrastructure Modernization, Application Development Services, Enterprise Data Management & Analytics, Knowledge & Content Management, and Strategic Staffing.","Philadelphia, PA",Azure Data Engineer,False
631,"Who is Blueprint?

Blueprint Technologies is a group of solution-minded thinkers changing the face of consulting in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?

Blueprint provides strategic and professional consulting services as well as technical delivery for companies of all sizes and market segments. We bridge the gap between the segments for a complete solution for our clients, allowing us to be a partner from idea to implementation.

Why you want to be a part of Blueprint?

Working at Blueprint will change the way you think about the typical consulting job. We want you to be accountable and make decisions that positively impact the client. Our people are our solutions, we work together and collaborate to make sure our offerings and execution are always superior. We also have a great benefits package offering medical, dental, vision, 401k, PTO, and company events throughout the year.

Blueprint is looking for Big Data Engineers to join us as we build cutting-edge technology solutions!

Note: we are not able to sponsor visa candidates at this time. Must be able to work on a W2 basis.

Qualifications:

Experience with Spark with knowledge of how Spark and other big data tools work (3+ years)
Experience with Hadoop (2+ years)
Experience with Scala (3+ years) and Java skills
Experience with GitHub and/or other versioning tools
Advanced experience with Software development, in an open source environment (5+ years)
Ability to quickly learn new technologies, application domains, and adapt to changes.
Ability to develop simple, elegant solutions to complex problems.
Ability to handle multiple competing priorities in a fast-paced environment.

Preferred Qualifications:

Experience with Python/R and/or functional languages like Clojure/Haskell (3+ years)
Experience with cloud technologies (preferably Azure)
Experience with Change Data Capture/Data Lineage preferred
Experience with agile software development methodology and continuous delivery models
Experience with team building and mentoring
Bachelor's or Master's degree in Computer Science, Computer Engineering or related discipline preferred

","Bellevue, WA",Big Data Engineer -Spark,False
632,"Our team is growing! Here is your opportunity to come and join an exciting engineering team responsible for building next-generation health sensors and features. The Human Interface Devices team is looking for talented and passionate engineers with expertise in data pipelines and infrastructure. This is an integral role where you will help design, develop, and support high quality, scalable data platforms and applications for analysis of machine, user, and sensor data.

Key Qualifications
Strong software development skills, with proficiency in relevant languages such as Python, Java
Extensive experience with Spark and/or Hadoop MapReduce
Exposure to web frameworks such as Django, Flask
Familiarity with cloud-based infrastructure such as AWS and Docker
Practical experience with SQL and NoSQL databases (MySQL, Postgres, MongoDB, Cassandra, HBase, etc)
Creative and collaborative
Description
As a Senior Data Engineer in this central role you will own data pipelines, work with the data engineering team to develop general use tooling, and collaborate with the algorithm and QA teams to design and validate the pipelines and tooling. Your work will directly impact the development of features across multiple Apple hardware platforms.
ADDITIONAL RESPONSIBILITIES INCLUDE:
Scaling our existing data pipelines
Parallelization of data processing tools and frameworks and platform virtualization.
Supporting data collection and curation and handling large datasets.
Working closely with data scientists and algorithm engineers

Education
Bachelor or Master degree in computer science, 3+ years of programming experience, 1+ year distributed computing with Spark
Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.","Santa Clara Valley, CA",Sr. Health Sensing Data Engineer,False
634,"Req ID: 32803

At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.

NTT DATA Services currently seeks a Principal Big Data Engineer to join our team in Durham, North Carolina (US-NC), United States (US).

Primary Responsibility
As a Principal Data Engineer, you must be an expert with Big Data technologies like Hadoop, Hive, Hbase, Spark, and various AWS technologies.
You will architect and drive the build out of next generation data platform.
As a hands-on engineer, you will influence all architecture decisions.
You will build reusable code, with the ability to scale with very large data volumes.
Everything you build will need to scale and perform.
As a Principal member on the team you will also mentor junior/senior engineers on the team.
Define and lead the frameworks for compliance with data management standards for emerging technologies (streaming platforms, cloud integration, etc.)
You will architect and drive the build out of next generation data platform.
General Responsibilities
Build and lead high-performing, agile teams focused on Data Engineering.
Be part of a core team leading migration to new data technologies for unstructured, streaming and high volume data.
Overall accountability for ensuring teams adopt established data management frameworks to prevent data lakes from becoming data swamps.
Provide senior level technical consulting to application development teams during application design and development for highly complex or critical projects
As a hands-on engineer, you will influence all architecture decisions.
As a Principal member on the team you will also mentor junior/senior engineers on the team
Job Functions:
Provide technical leadership to build and implement data and big data solutions
Articulate pros and cons of various technologies, platforms and tools.
Demonstrated work experience with distributed, scalable Big Data programming model and technologies such as Hadoop, Hive, Pig, etc.
Demonstrated hands-on experience with at-least one of the major Hadoop distributions (Preferrably Cloudera).
Deep technical Expertise in Hadoop eco system components.
Experience designing, developing and implementing online-machine learning libraries.
Minimum Experience:

10+ years of Data Engineering experience
8+ years’ experience in dimensional data modeling, ETL development, and Data Warehousing
5+ years of experience in Hadoop MapReduce, Hive, Pig, Spark, and Yarn
4 years of business experience leading analyses and initiatives with track record of business impact
Cloud Dev and Migration-AWS-Analytics/DW/Redshift : 1+ year

Travel: Need locals
Degree: Bachelors in Computer Science or equivalent work experience

Desirable Skills:
Hadoop/Cloud Developer Certification
Experience deploying applications in a cloud environment; ability to architect, design, deploy and manage cloud based Hadoop clusters
Exposure to cloud infrastructure

This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.

About NTT DATA Services

NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.

NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.

NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.","Durham, NC",Principal Big Data Engineer,False
635,"Job Description
Amazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, Fire phone, and Amazon Echo. What will you help us create?

Work hard. Have fun. Make history.

The Role:
We are seeking a talented, self-directed Data Engineer to design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for our internal customers. Implement data structures using best practices in data modeling and ETL/ELT processes. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.

The ideal candidate relishes working with large volumes of data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates outsized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoys working in a fast-paced team. The ideal candidate needs to possess exceptional technical expertise in large scale data warehouse and BI systems with hands-on knowledge on SQL, Distributed/MPP data storage, and AWS services (S3, Redshift, EMR, RDS).

Specifically, the Data Engineer will:
Design, implement, and support a platform providing ad hoc access to large datasets
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL
Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, and Redshift
Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark
Build and deliver high quality datasets to support business analysis and customer reporting needs
Interface with business customers, gathering requirements and delivering complete data structures
Basic Qualifications
Bachelor's degree in Computer Science, Computer Engineering, Business Administration, Mathematics or a related field
3+ years of industry experience as a Data Engineer or related specialty (e.g., Business Intelligence Engineer, Data Scientist)
Experience in data modeling, ETL development, and Data warehousing.
Data Warehousing Experience with Oracle, Redshift, Teradata, etc.
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Experience in continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Preferred Qualifications
Experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Strong ability to interact, communicate, present and influence within multiple levels of the organization
Track record of manipulating, processing, and extracting value from large datasets
Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions
Master's degree
Amazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation","Sunnyvale, CA",Data Engineer,False
636,"Who We Are: Aunalytics is a data science software and analytics service company with a mission to harness the power of data and use it to fuel the economic engine of growing companies, communities, and people. We offer an end-to-end cloud analytics platform that was designed to manage and automate data ingestion, compute resources, data set creation, advanced data science, and accelerated data interaction—allowing dramatically increased speed to analytical insights for businesses. Aunalytics also provides analytics solutions for specific business objectives, with a focus on digital, consumer, enterprise, and IoT applications across a wide variety of industries. We focus on providing indisputable value to our clients by acting as their trusted data and analytics partner, and by creating powerful tools to help companies realize the full potential in their data.Position Overview: As a Big Data Engineer, you will be responsible for developing, maintaining, testing, and evaluating big data solutions utilized within Aunalytics. You will be a member of the software team, but work closely with our infrastructure team.Essential Duties & Responsibilities: Implement, support, and maintain big data tools and frameworksAssist in the development of deployment automation and operational support strategiesDeliver near-real time and non-near-real-time data and applications to a team of analysts and data scientists who create insights and analytics applications for our stakeholdersRequired Skills: Understanding of modern data structures and reporting toolsExperience designing/integrating heterogeneous software systemsExperience building Microservices using Javascript (Node.js, Express.js) or PythonResourceful in getting things done, self-starter, and productive working independently or collaboratively – ours is a fast-pace entrepreneurial environment with performance expectations and deadlinesAbility to learn quickly and contribute ideas that make the team, processes, and solutions betterAbility to communicate your ideas (verbal and written) so that team members and clients can understand themAbility to defend your professional decisions and organize proof that your ideas and processes are correctShare our values: growth, relationships, integrity, and true gritPreferred Skills: B.S. or equivalent in a computational discipline: computer science, applied mathematics, engineering, or related field (preferred)Three or more years of hands-on experience with Big Data Systems such as: Hadoop, Apache Spark, Apache Drill, Apache Alluxio, and DockerExperience with large distributed data and systemsExperience developing and managing data warehousesWhat's in it for You?Opportunity to work in the booming field of data science, alongside some of the brightest minds in the industryOpportunity to work with cutting-edge technology in a casual, fun environmentOpportunity to be a part of a local company committed to making a difference in our communityChance to work with a rapidly expanding tech companyFlexible schedule and paid time offFree snacks and an unlimited supply of coffeeSocial events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc.Competitive salary and benefits package including health, vision, dental, and life insuranceJob Type: Full-timeWork authorization:United States (Required)","South Bend, IN",Sr. Big Data Engineer,False
637,"ATGer; noun; Rockstar of ATG – self-motivated, entrepreneurial driver of change. Not afraid to challenge processes and get stuff done (GSD). Collaborate, take risks, and problem solve. Oh, and we really like office dogs and breakfast burritos.

Perks; noun; all the good stuff and more. Fully stocked kitchens. Competitive paid time off and medical benefits. Outdoor “walk and talk” meetings where you will solve global, complex business problems – all while eating a breakfast burrito
Advanced Technology Group (ATG) is a leader in Quote-to-Cash advisory and implementation services to both mid-market and large enterprise brands seeking increased agility in the “everything-as-a-service” economy. Through our entrepreneurial spirit, results focused culture, and innovative delivery model, ATG brings contemporary solutions to the way clients transform and manage their customer and revenue technology platforms. Leveraging decades of deep domain expertise in billing and hundreds of Quote-to-Cash implementations of the leading cloud and on-premise software, ATG helps clients realize their desired outcome faster.

What does a Data Engineer do for ATG?
The ATG Data Engineer’s primary role is to strategize, implement, solve, and execute on data related solutions to support ATG initiatives on a variety of platforms. Common activities may include, creating data strategies, data migration/conversions, data mapping, and data profiling. The role will interact with the latest enterprise SaaS platforms within the monetization ecosystem. Individuals in the role will leverage their business, analytical, and technical skills to provide expert guidance on data strategies to our customers that are critical in solution implementations.

What are some of the responsibilities of a Data Engineer?
Participate in discovery sessions with project teams to understand data requirements needed to support solution implementations.Work collaboratively with a project and client team in an agile environment to support our cloud initiativesLeverage system and process analysis skills to produce data strategies that create a seamless and uninterrupted experience for our clients.Collaborate with clients to generate data mapping documents between target and source systems.Leverage industry leading tools to extract, transform and load data into source systems in an automated fashion to support data migrations.Perform data profiling tasks to collect statistics, trends, impacts, and summaries that lead to a structured and successful implementation.Provide key insights based on data analytics to aid the client in drawing conclusions as to the business value and impact of their data and best approaches for migrating and transforming data to support new system architectures.Work with project management to provide regular status updates to internal and external stakeholders.

What are the minimum requirements to be a Data Engineer?Bachelor’s degree and 1-5 years’ experience in a data related position that includes the use of common database tool, ETL products, and interacting with data within relational databases.

What other skills, knowledge and qualifications are needed to be a successful Data Engineer?

Demonstrated knowledge of ETL (Extract-Transform-Load) concepts
In-depth experience with SQL and PL/SQL
Proficiency in object-oriented programming (Java, C++, C#) and/or scripting (Python, Perl, PHP)
Experience working in cloud transformation projects within CRM, CPQ, and/or Finance systems
Ability to effectively manage time and prioritize tasks
Ability to work with ambiguity
Capability to take on tasks with little direction, and produce high-value deliverables in a fast-paced environment
Excellent verbal and written communication skills. Ability to work with technical and non-technical users, other departments, and clients
Experience working in an Agile (Scrum) environment desired
Organization skills - ability to work in a highly dynamic environment with shifting priorities
***The specific locations for the Data Engineer are Missoula, Montana and Kansas City, Missouri**
The Advanced Technology Group (ATG) is a privately held professional management and consulting firm founded in 2000 and headquartered in Overland Park, Kansas. Our niche service offering centers on customer care and billing/CRM expertise supporting our communications industry (telecom/cable/conferencing) based clients. Additionally, ATG’s management consulting practice focuses on working with our targeted clients from a program delivery perspective in providing business solutions, through strategy, process, and program/project implementation assisting our clients meet their business critical program/project initiatives.","Schenectady, NY 12345",Data Engineer,False
638,"Date: May 22, 2018
Global Artificial Intelligence Accelerator

Ericsson Overview:

Ericsson is world’s leading provider of communications technology and services. Our offerings include services, consulting, software and infrastructure within Information and Communications Technology.
Using innovation to empower people, business and society, Ericsson is working towards the Networked Society: a world connected in real time that will open up opportunities to create freedom, transform society and drive solutions to some of our planet’s greatest challenges.

We are truly a global company, operating across borders in over 180 countries, offering a diverse, performance-driven culture and an innovative and engaging environment. As an Ericsson employee, you will have freedom to think big and the support to turn ideas into achievements. Continuous learning and growth opportunities allow you to acquire the knowledge and skills necessary to progress and reach your career goals. We invite you to join our team.

Machine Intelligence at Ericsson

It will be practically impossible for human brains to understand how to run and optimize a 5G network, Machine Learning (ML) and other Artificial Intelligence (AI) technologies- will be vital for us to handle that complexity. We are setting up a Global AI Accelerator in the US, Sweden and India, with 300 experts, to fast-track our strategy execution
Machine Intelligence (MI), the combination of Machine Learning and other Artificial Intelligence technologies is what Ericsson uses to drive thought leadership to automate and transform Ericsson offerings and operations. MI is a key competence to enable new and emerging business. This includes development of models, frameworks and infrastructure that we use to power our 5G networks and services. We engage in both academic and industry collaborations to drive the digitalization of Ericsson and the Industry. Our global group develop state of the art solutions that simplify and automate processes in our products and services and create new value through data insights.
Ericsson is hiring Junior Data Engineers to significantly expand its Global AI Accelerator team.

Job Description:
Ericsson is looking for savvy Junior Data Engineers to join our growing team of MI experts. As a team member, you will be evolving and optimizing our data and data pipeline architecture, as well as, optimizing data flow and collection for cross functional teams. You are an expert data pipeline builder and data wrangler who enjoys optimizing data systems and evolving them. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data and models devOps (dataOps) architecture is consistent throughout ongoing projects. You are self-directed and comfortable supporting the dataOps needs of multiple teams, systems and products. You will also be responsible for integrating them with the architecture used across the company. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s dataOps architecture to support our existing and next generation of MI-driven products and solutions initiatives.

Your Roles and Responsibilities
Assist with the creation and maintaining of optimal data and model dataOps pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies from AWS, Azure and others.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across national boundaries through multiple data centers and strategic customers/partners.
Create tool-chains for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Support dataOps competence build-up in Ericsson Businesses and Customer Serving Units

What we would like to see
BS, MS or PhD degree in Computer Science, Informatics, Information Systems or another related field.
0-2 years experience using the following software/tools:
Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with Data and Model pipeline and workflow management tools: Azkaban, Luigi, Airflow, Dataiku, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of other databases/date-sources.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and seek opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Have built processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Project management and interpersonal skills.
Experience supporting and working with cross-functional teams in a dynamic environmentLI-BF1
DISCLAIMER: The above statements are intended to describe the general nature and level of work being performed by employees assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of employees assigned to this position. Therefore employees assigned may be required to perform additional job tasks required by the manager.

We are proud to be an EEO/AA employer M/F/Disabled/Veterans. We maintain a drug-free workplace and perform pre-employment substance abuse testing.

Ericsson provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetics information. Ericsson complies with applicable country, state and all local laws governing nondiscrimination in employment in every location across the world in which the company has facilities. In addition, Ericsson supports the UN Guiding Principles for Business and Human Rights and the United Nations Global Compact.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, training and development.

Ericsson expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetic information.

Ericsson will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by Ericsson or (c) consistent with Ericsson’s legal duty to furnish information.

Employee Polygraph Protection Act Notice - Employers are generally prohibited from requiring or requesting any employee or job applicant to take a lie detector test, and from discharging, disciplining, or discriminating against an employee or prospective employee for refusing to take a test or for exercising other rights under the Act. For more information, visit https://www.dol.gov/whd/regs/compliance/posters/eppac.pdf.

Ericsson is an equal opportunity employer and is committed to providing reasonable accommodation for qualified disabled individuals during the application and hiring process. Ericsson will make modifications or adjustments to the job application or interview process that will enable a qualified applicant to be considered for a position. If you require an accommodation due to a disability, please contact Ericsson at hr.direct.dallas@ericsson.com or (866) 374-2272 (US) or (877) 338-9966 (Canada) for further assistance.

Primary country and city: United States (US) || || Santa Clara || R&D","Santa Clara, CA",Junior Data Engineer- ML and AI,False
639,"If you have experience with big data solutions, you have a wealth of opportunities to grow your career. How about an environment with extraordinarily talented peers? How about a company offering a range of both big data products and professional services to expand your skills? How about an organization working on cutting edge projects that many of the biggest companies simply can't execute on their own? Are you looking to join a startup where you can directly contribute to the establishment of one the next great high tech companies? Then Kogentix, now hiring Junior-to-Mid-Level Big Data Engineers, may just be a perfect fit.

Responsibilities
Develops distributed applications to solve large scale processing problems, utilizing various languages like Java, Scala , Shell etc.
Implements, troubleshoots, and optimizes solutions based on modern big data technologies like Hadoop, Spark, Elastic Search, Storm, Kafka, etc. in both an on premise and cloud deployment model
Implements data architecture, including data ingress in batch and real time from a broad variety of external systems; data transformations to prepare data for analytics processing, and data egress for availability of analytics results to visualization systems, applications, or external data stores
Supports documentation, change control, and QA processes consistent with enterprise requirements
Establishes strong teamwork with client technical resources, and effectively communicates project status, technical issue options and resolution, and operational requirements to client stakeholders
Qualifications
Overall 2-3 years experience with minimum of 1 year on Hadoop or a closely related technology
Very strong server-side Java experience, especially in an open source, data-intensive, distributed environments
Expert in the Hadoop Framework & java programming (i.e. Spark, MapReduce, Pig, Hive, Kafka, Storm, etc.) including performance tuning
Implemented complex projects dealing with the considerable data size (TB/ PB) and with high complexity
Good understanding of algorithms, data structure, and performance optimization techniques
Experience with agile development methodologies like Scrum
Self motivated, and has the ability to drive technical discussions.
Organized, detail oriented, able to work both independently and in a team
Excellent problem solver, analytical thinker and quick learner.
Strong verbal and written communication skills
Broad understanding of most of the following, with depth of expertise and experience in at least 1:
o Hadoop security (Kerberos, Ranger, Knox)
o Amazon EMR and related technologies (e.g. DynamoDB, Kinesis, S3)
o Data mining, statistical modeling techniques and quantitative analyses
o Data Architecture, Master Data Management and Governance
o Kafka
o Search capabilities such as Elastic Search
o NoSQL DB such as Cassandra and MongoDB
Certifications a plus: Amazon, Cloudera, Spark
Masters / Bachelor of Computer science with focus on distributed computing",United States,Big Data Engineer,False
640,"Description
Team Daugherty is hiring a Data Engineer to join us in Minneapolis. The ideal candidate for this position is a problem solver with the ability to utilize insights, creativity and perspective to drive business success for our clients.

As a Data Engineer you will have the opportunity to:

Contribute to the creation and maintenance of optimal data pipeline architectures.
Collaborate and work closely with team to build data platforms.
Maintain and manage Hadoop clusters in development and production environments.
Assemble large, complex data sets that meet functional/non-functional business requirements.
Work with team members and functional leads to understand existing data requirements and validation rules to support moving existing data warehouse workloads into a distributed data platform.
Create custom software components (e.g. specialized UDFs) and analytics applications.
Employ a variety of languages and tools to marry systems together.
Recommend ways to improve data reliability, efficiency and quality.
Implement & automate high-performance algorithms, prototypes and predictive models.
We are looking for someone with:

1+ years of experience in a similar role.
Proven experience working with AWS technologies such as Redshift, RDS, S3, EMR, ADP, Hive, Kinesis, SNS/SQS and QuickSight.
Familiarity with Python, R, sh/bash and JVM-based languages including Scala and Java.
Familiarity with Hadoop family languages including Pig and Hive.
Familiarity with high performance data libraries including Spark, NumPy and TensorFlow.
Proven ability to pick up new languages and technologies quickly.
Intermediate level of SQL programming and query performance tuning techniques for data integration and consumption using design for optimum performance against large data assets within an OLTP, OLAP and MPP architecture.
Knowledge of cloud and distributed systems principles, including load balancing, networks, scaling, and in-memory versus disk.
Experience building data pipelines to connect analytics stacks, client data visualization tools and external data sources.
Exposure to stream-processing and messaging, such as Storm, Spark-Streaming, Kafka and MQ.
Understanding of DevOps and CI/CD toolset, such as Jenkins, GitLab CI, Buildbot, Drone and Bamboo.
Some experience with programming Languages, such as Scala, Java, R and Python.
We offer members of Team Daugherty:

Excellent health, dental and vision insurance.
Revenue sharing and a 401(k) retirement savings plan.
Life, disability and long-term care insurance.
Little to no travel.
Robust career development and training.
Do you think you’re a good fit for Team Daugherty? Apply now and find out why working here satisfies the smart, the talented and the curious!","Minneapolis, MN",Data Engineer - MSP,False
641,"About Freestar:
Freestar engineers cutting-edge monetization solutions for websites. By combining industry-leading technology, data, and massive scale, we enable busy site owners to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. Publishers then have more time to do what they do best: create content.

Job Description & Responsibilities:
As a member of the Freestar Data Engineering team, the Data Engineer is responsible for manipulating and transforming data to meet business needs, for ad-hoc and permanent reporting, and for productionalizing processes in a microservices architecture using python and/or java. The Data Engineer will write both code that fetches data from myriad sources (RESTful and SOAP APIs, file systems, ftp, etc.) and that publishes data to API endpoints; as well as ETL processes, data cleaning, and the like.

At Freestar our technology stack is built on Google Cloud. The Data Engineer should have experience working with cloud technologies and should have experience with Big Data storage and transformation technologies including large-scale relational databases such as Google BigQuery or Amazon Redshift; Google Cloud Storage or Amazon s3; and some exposure to streaming/batch processing technologies.

Reporting to our VP of Data Engineering, the Data Engineer is responsible for the following:


Ingesting datasets to meet business needs; writing and productionalizing ETL processes
Coding solutions to the business' data needs, including delivery of data through reports or endpoints as appropriate
Testing and vetting outputs and working with stakeholders to ensure that data being produced is accurate and fully meets business needs
Responding to ad-hoc reporting requests
Working with front-end and other engineers to integrate data products into dashboards and user interfaces as needed

Data Engineer Requirements:

2+ years python or java with a focus on data manipulation and transformation
2+ years SQL
At least one year writing and deploying code in a production environment
At least one year working with cloud technologies
Experience working with big data (datasets on the order of hundreds of millions to billions of records)
Bachelor's degree in a relevant field (CS, Engineering, Math, Economics, etc) or equivalent experience.
Experience building statistical models or similar algorithms is a plus.

We'd love it if you had experience with:

Spring Boot
Microservice Architecture
Kubernetes / Docker
Apache Beam
Apache Kafka
Google Cloud Platform / Amazon Web Services

What you can expect in return:

Full-Time, Salaried Position
Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something BIG

Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.

This role is not eligible for visa sponsorship","New York, NY",Data Engineer,False
642,"We are looking for a versatile Data Engineer to join our growing team. This hire will be responsible for a variety of expansions and optimizations across the company’s entire data architectural stack.

The ideal candidate should be familiar with modern data concepts including: data lakes; cloud data concepts; ETL and data wrangling, and search engine speed data access. The position will also support the software and data science teams on designing and building data initiatives in fast paced environment as well as driving strong standards across the Data Engineering group. Individuals must have the ability to be self-directed at times, and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing and contributing to the design/development of our company’s future data architecture. Ideally this hire will act as a mentor for other Data Engineers.

Experience:
Total 8+ years of database experience, with 4+ years in Cloud/Big Data and/or NoSQL Databases as well as maintain relational structuresApplicant should be ambitious, and agile enough to move with the ever-changing landscape of Data Engineering, including up-to-date AWS cloud services

Preferred Skillset:
Strong ability to navigate AWS cloud services including: S3; RDS; EC2, and some exposure to AWS data services
A thorough understanding of the ELK stack (Elasticsearch, Kibana and Logstash) including optimizations at the cluster, index and document levels
Intermediate Linux administration
Hands on experience developing applications that run on Spark (preferably PySpark and/or vanilla Python)
Hands on experience designing and developing non-trivial ETL processes
Intermediate NoSQL skills (preferably MongoDB)
Intermediate SQL skills (MySQL, Postgres, etc.)
Experience using Kafka (both consuming and producing content) and/or other pub/sub enterprise tools (SQS)

Bonus:
Java and/or C# development experience a plusDocker container creation and/or enterprise managementApache NiFi

Preferred Qualifications:
Bachelors in Computer Science 8+ years of database experience, with 4+ years in Cloud/Big Data and/or NoSQL Databases","Boston, MA",Data Engineer,False
643,"Our client is a mission-minded SaaS company headquartered in Austin that is building a new product to improve the way they deliver their solution to clients. This role focuses on the design, development, and deployment of our client's SaaS Platforms' Enterprise Datawarehouse & underlying Data Transformations. Applying a strong technical and data lifecycle understanding, this Engineer will partner with other Developers, Data Engineers, Data Scientists, and the DevOps Teams to create Data Science Platforms.
Responsibilities
* Designing and developing schemas, attributes, facts, advanced report design, data aggregation, and advanced report services development. * Using tools including Web Intelligence, Report Services, Pentaho Spoon, and Report Services. * Providing well-documented, efficient and effective solutions to meet business needs. * Institutionalizing a Data Management Culture to support the company's scale initiatives. * Mapping data elements and attributes into the Enterprise Data Model. * Developing solutions with team members to implement defined and documented data requirements. * Learning new technologies and sharing your knowledge with the team. * Advocating for new features, new technologies and processes. * Prototyping ideas for discussion/demonstration. * Providing guidance on data security compliance initiatives. * Participating in code reviews.
Required Skills
* Experience with Agile development methodologies. * Working knowledge of Microsoft SQL Server, MySql, and/or PostgreSQL. * 2-3 years' experience in reporting/analytic role with a focus on data transformation, ETL, and design. * Deep SQL Development Skills. * Knowledge of dimensional database design. * Experience with Pentaho Spoon, Microsoft SQL Server Integration Services (SSIS), or other Enterprise Data Transformation Tools * Working Knowledge of Python and other automation technology frameworks is a plus. * Working Knowledge of Java is a bonus. * Experience with SAP Business Objects is a plus. * Experience with Sparx Systems Enterprise Architect or other Data Modeling Tools is desired. * Experience with Big Data Technologies including Amazon MapReduce, Hadoop, Cassandra, SAP Hana, Microsoft HDInsight, Amazon RedShift, Amazon Glue, and similar technology stacks.","Austin, TX",Data Engineer,False
644,"First Republic is an ultra-high-touch bank that provides extraordinary client service. We believe that one-on-one interactions build lasting relationships. We move quickly to serve our clients’ needs so that their financial transactions are handled with ease and efficiency. Client trust and security are paramount in our line of business. Ultimately, our goal is unsurpassed client satisfaction which will lead to personal referrals – our number one source of new business. We recognize that our competitive advantage starts with our people and our culture. At First Republic, we work hard and move quickly as a very coordinated team. If you are looking for an opportunity to grow and contribute in a fun, fast-paced environment, First Republic is the place for you. We have exceptional people focused on providing extraordinary service.

The Data Scientist/Data Engineer will play a crucial role in our Enterprise Data and Client Insights (EDCI) group. In this role you will help build the data infrastructure that will allow the team to conduct, analyze, and consult on business decisions. The Data Scientist/Data Engineer must be comfortable with a fast paced environment where you may need to shift responsibilities depending on project/team needs. You will be exposed to a number of departments and executives in this role giving insight into various verticals in the bank. We’re looking for someone who is willing to roll up their sleeves to help build the foundation so that you can create the reports and run analysis from a Data Science perspective.
Responsibilities
Experience with analytics; Investment Management/Banking experience is a plus
Experience with Python, Java, etc.
SQL and experience with relational databases
Experience building real-time data pipelines and DW's
Designing for the present and future
Design and implement data modeling
Develop solutions for real world, enterprise wide problems that impact the business and customer
Interact with various business groups and a variety of people across the bank to identify opportunities
Partner with leadership and stakeholders to develop and execute various reporting packages and ad hoc requests
Experience in Machine Learning and/or Artificial Intelligence
Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes
Qualifications
Programming experience in one or more of the following: Java, Python and/or C, C++.
Writing high-performance, reliable and maintainable code
Good knowledge of database structures, theories, principles, and practices
Relational SQL, distributed SQL and NoSQL databases including, but not limited to, MSSQL, PostgreSQL, MySQL, MemSQL, CrateDB, MongoDB, Cassandra, etc.
Fluent in writing shell scripts
Knowledge and experience in statistical and data mining techniques
Experience creating and using machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.","San Francisco, CA",Data Scientist / Data Engineer,False
645,"$2,500 a monthDescription
Upsight is the world’s largest enterprise-grade user lifecycle management platform. Our mission is to transform the world’s data into valuable action. Upsight customers leverage our consolidated solution stack to learn about their users, make informed decisions and take immediate action to positively impact KPIs, increase LTV and add value to their businesses. We handle more than 500 billion data points and deliver over 1.4 billion targeted web and mobile communications every month.
Born of the 2014 merger that married Playhaven’s marketing automation solution with Kontagent’s omnichannel analytics, Upsight was built from the ground up to handle the challenges inherent in managing the world’s largest software portfolios. The 2016 acquisition of Fuse Powered and their best-in-class ad optimization technology secured our place as the industry’s only one stop shop for enterprise technologists. Upsight’s battle-tested infrastructure has scaled to facilitate some of the largest digital product launches in history, and our commitment to perfecting the industry’s only full-service solution has afforded us the privilege of working with companies like Electronic Arts, Warner Brothers, Activision and more.
Above all, Upsight values relationships. Our refusal to adopt a “one-size-fits-all” approach has empowered us and our customers to build resilient partnerships that drive growth. It’s a philosophy we aim to carry with us into the future as we look forward to leading our industry in the discovery and development of practical technologies for the digital enterprise market.
About the role
Upsight is looking for a Sr. Data Platform Engineer, who will work on a team of Data Platform Engineers, to scale and expand on Upsight’s Data Platform, and runs at optimal performance.
What you’ll do
Work in an Agile development environment
Write rock-solid code for a variety of data processing systems
Work with our support team to debug customer issues
Work hand-in-hand with our DevOps team to horizontally scale Upsight’s infrastructure to handle tens of billions of events per day
Deepen your knowledge of Big Data Infrastructure, tuning production systems running at Petabyte scale
About you
Bachelor’s degree in Computer Science, related field or equivalent experience
5 years of software engineering experience
Excellent understanding of distributed systems and associated algorithms
Skills for Success
Rock solid understanding of the Hadoop ecosystem (YARN, HDFS, ZooKeeper, etc)
Experience with both batch and streaming data processing systems (Storm, Kafka)
Experience with big-data SQL engines (e.g. Hive, MR, Tez, Spark)
Deep debugging and problem solving skills
Attention to detail and focus on maintaining data quality
Ability to place components into the context of a broader architecture and identify any issues at a systems level
Ability to understanding scaling characteristics of a system, and address any deficiencies before shipping code
Solid understanding of Linux and associated metrics and tunables
Expert knowledge in Java and the JVM, Scala experience a bonus
What You’ll Get
An opportunity to make immediate impact
$2,500 flexible benefit program to be used towards a combination of vacation, fitness, mobile devices, and education
Snacks on snacks on snacks
Catered lunches 3x per week
Upsight welcomes and encourages applications from people with disabilities. Accommodations are available on request for candidates taking part in all aspects of the selection process.","Portland, OR",Sr. Data Engineer,False
646,"The Data Engineer is responsible for utilizing current and future data ingestion and data storage technologies to build, manipulate, enhance, and utilize data to meet the needs of the global end user analytical community. The position will implement the tools necessary to meet the data warehousing, data lake, and big data requirements of the organization. The Data Engineer will be responsible for designing, implementing, and maintaining the enterprise data warehouse in alignment with the data and analytics vision and strategy. The Data Engineer will collaborate with the architect team, along with the technical team, to implement creative and innovative solutions to solve complex data opportunities. This role will build and maintain manual and automated data workflows/pipelines. This position will also be involved in automating the analysis, processing, and testing of our big data and IoT storage platforms.



Role & Responsibility


Technology Research
Research, select, and pilot technologies for use, including:
Insightful, data-driven solutions that support a variety of business needs
Data ingestion technologies to meet or exceed the needs of the analytics users
Support keeping the technologies up to date by understanding the benefits of updated versions and/or componentized replacement products
Application Development
Design, build, and deliver innovative technical solutions to the analytical community and other project teams:
Integrate transactional systems with the enterprise data warehouse
Create data structures to accommodate source data and allow efficient data consumption
Create data ingestion processes to collect, manipulate if necessary, and load data into tables available for end users
Develop data lake and big data storage capabilities
Application Support
Design, Build, and Maintain data storage facilities including data warehousing, data lakes, and big data:
Maintain the enterprise data warehouse
Enhance data structures to accommodate source data and allow efficient data consumption
Maintain data lake and big data storage
Enhance data lake to align with the needs of the analytical community
Analytics Strategy Support
Implement data requirements to align with the strategy for business intelligence and advanced analytics:
Understand big data technologies and the tools to implement
Keep abreast of new BI and related technology and trends and assess when and if new technology is needed
Implement the architectural vision, initiatives and roadmap for effectively leveraging data



Job Requirement


Bachelor's Degree in Information Technology or related field. Master’s Degree in Information Technology preferred.
Minimum of 8 years’ experience, including:
4+ years of industry experience with application support and/or development, or equivalent,
4+ years of industry experience with building or maintaining traditional data storage and ETL/ELT technologies,
3+ years of industry experience working with newer data storage and data ingestion technologies and platforms including use of open source tools.
Solid knowledge of technology relevant for the domain(s).
Proven experience in IT solution design (data model design, integration design, service/component design).
Proven experience identifying and managing technical risks.
Familiar with operational support models and project management methodologies.
Able to translate business problems into IT solutions.
Analytical mindset and problem solver.
Excellent written and verbal communication skills in English.
Highly computer literate.
Willing to work with distributed and multi-cultural teams.
Travel: 10-24%.




The world-renowned brands that make up Doosan Bobcat produce industry-leading compact and heavy construction equipment, attachments, air compressors, lighting systems, generators and articulated dump trucks. Doosan Bobcat is a part of the Doosan Group, which employs 43,000 people in 38 countries worldwide. With people at the core of who we are, we believe the growth of our people will lead to the continued growth and success of our world-class company. Our team of dedicated employees is the backbone that allows us to provide construction equipment solutions that help our customers build the world of tomorrow. Wherever you find us, you’ll hear the sounds of progress, see the results of our people and feel the rhythm of transformation in everything we do.
Doosan is committed to a diverse workforce and is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, age, race, color, religion, creed, citizenship status, national origin, disability, marital status, sexual orientation, gender identity, protected veteran status, or any other status or characteristic protected by law. Individuals with disabilities who require a reasonable accommodation in the application process or who need assistance accessing the information on this website should call 701-476-4263.","West Fargo, ND 58078",Data Engineer,False
647,"Who is Blueprint?

Blueprint Technologies is a group of solution-minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?

Blueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.

Why you want to be a part of Blueprint?

We are innovators. Motivators. Thought Provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.

Blueprint is looking for Big Data Engineers to join us as we build cutting-edge technology solutions!

Basic Qualifications:

Bachelor's or Master's degree in Computer Science, Computer Engineering or related discipline.
Advanced Experience with Software development, in an open source environment (5+ years)
Experience with Spark and/or Scala (3+ years)
Experience with Hadoop (2-4 years)
Experience with GitHub and/or other versioning tools
Ability to quickly learn new technologies, application domains, and adapt to changes.
Excellent critical thinking and problem-solving skills.
Ability to develop simple, elegant solutions to complex problems.
Excellent verbal and written communication skills, team player.
Ability to handle multiple competing priorities in a fast-paced environment.

Preferred Qualifications:

Experience with a distributed cloud (preferably Azure)
Experience with agile software development methodology and continuous delivery models
Experience with team building and mentoring

FLSA - Job Classification: Exempt, Full-Time Position.

Department: Big Data","Bellevue, WA",Big Data Engineer,False
648,"Job Description
At Annalect, we work with clients to develop data driven marketing strategies, powered by a connected system of technology, tools, consultants and digital activation. Drawing from the vast resources of our parent company Omnicom, we are positioned as the data and analytics experts for dozens of Fortune 500 companies.
Our job is one of empowerment. Specifically, empowering our clients to improve their business through advances in technology and data. As the ad tech ecosystem continues to become more complex, we are counted on to provide expertise on digital marketing technologies, incorporate strategy and manage tracking of all digital marketing efforts. But as our client roster continues to grow, we’re going to need more help.

Position Overview
We are looking for a Data Engineer to join our team of engineers. Primary work involves managing large ad tech related data sets and making them available for data scientists, marketing analysts, and data products via automated workflows. Work also includes building command-line and web-based tools to support teams using big data, doing large scale data analysis projects, and building data products.

The ideal candidate would identify him/her self as:
An automation specialist who conceptualizes and builds automate systems for workflows.
A cloud-based infrastructure builder who integrates the use of cloud services into workflows.
A detail oriented developer who enjoys complex challenges and who anticipates and designs for external failures.


Experience required:
Building workflows to support data environments and data products
Python, Bash, and SQL scripting
Linux system administration and working at the Linux command prompt.
Distributed data processing engines such as Spark, Hive, Presto, and Drill.
AWS services such as Batch, ECS, EC2, SQS, S3, Elasticache, EMR, Glue, Redshift, and RDS.
Schedulers such as Airflow, Luigi, and Cloudwatch Events
Docker
Git or other version control systems.","Chicago, IL",Data Engineer,False
649,"About Blue River

Blue River Technology serves the agricultural industry by designing and building advanced farm machines that utilize computer vision and machine learning to enable farmers to understand and manage every plant. These machines help farmers to improve profitability, protects the environment by reducing pesticide use, and captures valuable plant-by-plant data. Blue River is a pioneer in the agricultural robotics space and has developed the See & Spray precision sprayer, which applies pesticide only where needed, and can reduce pesticide use 90%. John Deere & Company, with over 180 years of experience in designing, manufacturing, and distributing innovative products to farmers, acquired Blue River Technology in the fall of 2017 as an independently-run subsidiary. In partnership with John Deere, Blue River has expanded rapidly and together both companies see many opportunities to apply advanced computer vision, machine learning, and robotics technologies to other areas in agriculture beyond spraying.

Blue River is based in Sunnyvale, CA and has over 80 team members with diverse experience including: computer vision, machine learning, systems software, autonomous vehicles, and precision agriculture. Our working environment is fast paced and highly collaborative, and employees are excited to use their talents to improve food production and protect the environment.

Position Description:
We are seeking a Data Engineer to design and maintain Blue River's cloud Data Lake platform. This person will collaborate closely with engineering, product, and support to define local and remotely hosted infrastructure needs. They will create and maintain data pipeline to store and manipulate data acquired at remote sites with highly limited connectivity.

Requirements:

Experience with large data sets
Have deployed complex system on AWS / GCP
Java, Scala, Python/Javascript, NoSQL, SQL, RDBMS
Track record of developing Web API's
Self motivated, Ability to work independently
Experienced in data mining and visualization of large data sets
Deep understanding of Map Reduce framework and related big data ecosystem components like Hive, Spark SQL, DynmoDB/HBase etc.

Preferred Qualifications:

At least 4+ years of experience with Big Data & Analytics solutions – Hadoop, Kafka, MapReduce, Hive, DynmoDB/HBase,
Solr or Elastic Search,Spark, AWS EMR, Knowledge of experience with NoSQL database a plus
Experience with NoSQL such as Elasticsearch and DynmoDB/HBase preferred
Experience with NoSQL database schema design.
Working experience of ETL and data visualization is plus
Understand the different formats parquet, protobuf
Experience developing with web/app containers such as Tomcat/Jetty etc
Practical expertise in performance tuning and optimization, bottleneck problems analysis
Experience deploying systems to a cloud provider like AWS or Google Compute Engine

","Sunnyvale, CA 94085",Data Engineer,False
650,"Data Engineer-269025
Description

We are pioneers. We were the first to break the sound barrier and design the first functional jetpack. We were aboard NASA’s first lunar mission and brought advanced tiltrotor systems to market. Today, we are defining the future of on-demand mobility. At Bell, we are proud to be an iconic company with superb talent, rapidly creating novel and coveted vertical lift experiences.
The Data Engineer is a member of Bell’s team of analytics experts. This position is responsible for optimizing our data pipeline architecture, as well as data flow and collection for cross functional teams on our V-247 aircraft program. This role will be working with data from disparate aircraft systems data and aircraft maintenance data focusing on maintainability and operations.
The ideal candidate enjoys optimizing data systems and building them from the ground up.

The Data Engineer will support our software developers and database specialists on data initiatives and will ensure optimal data delivery through ongoing projects. This position supports the data needs of multiple teams, systems and products. This position is based at Bell’s headquarters facility in Ft. Worth, TX.
Position Responsibilities

Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and potentially AWS big data technologies.
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that includes simulator and flight data, flight systems data that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for team members that assist them in building and optimizing our products and services.
Build analytics tools that utilize the data pipeline to provide insights into customer acquisition, operational efficiency and other key business performance metrics.
Qualifications

Education Requirements:

Bachelor’s Degree in Computer Science, Information Technology, Statistics, Informatics, Information Systems - or another related quantitative field - is required.
Master's Degree in one of the above disciplines is preferred.
Position Requirements:

At least 10 years of experience in Data Engineering and in optimizing data pipelines, architectures and data sets is required.
Experience with build processes supporting ETL data extraction, transformation, loading, data structures, metadata, dependency and workload management
A successful history processing and extracting value from large disconnected datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with relational, NoSQL, and graph databases
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in application of best practices in software development including source code control, object-oriented development, and programming principles.
Preferred Education, Skills and Experience

Experience with military aviation maintenance databases
Knowledge of aircraft systems and aircraft maintenance procedures focusing on maintainability and operations.
Experience with MSSQL, Oracle, Postgres, MySQL, and neo4j


Don’t miss the chance to join a diverse, inclusive environment where you feel a sense of belonging. As a member of our global workforce, you will collaborate with dedicated, enthusiastic teams where unique experiences, backgrounds and ideas combined with a strong passion for our products take us above and beyond flight.

EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

This position requires use of information which is subject to the International Traffic in Arms Regulations (ITAR) and/or the Export Administration Regulations (EAR)., Non-U.S. persons selected must meet eligibility requirements for access to export-restricted information. , The ITAR/EAR defines a U.S. person as a U.S. Citizen, U.S. Permanent Resident (i.e. 'Green Card Holder'), Political Asylee, or Refugee.

Pay Transparency Policy Statement
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise, have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.
Job Field: Information Technology
Primary Location: US-Texas-Fort Worth
Recruiting Company: Bell Flight
Schedule: Full-time
Job Level: Individual Contributor
Shift: First Shift
Travel: Yes, 5 % of the Time
Job Posting: 09/19/2018, 11:23:15 AM
Textron (and its subsidiaries) participates in E-Verify. We will provide the U.S. Social Security Administration (SSA) and, if necessary, the U.S. Department of Homeland Security (DHS) with information from each new employer’s Form I-9 to confirm work authorization.","Fort Worth, TX",Data Engineer,False
651,"$75 - $80 an hourContractJob SummaryRole: Big Data Engineering LeadLocation: Philadelphia, PADuration: 1 yearVisa Status: H1B, GC, Citizen, H4-EAD, TNPassport Number: Mandatory Experience: 10+Java/Scala/Spark, AWS experienceOverall Big Data architect who has experience working on AWSUsing Big Data technology and customer’s business requirements, design and document a comprehensive technical architecture / solution.In depth knowledge of Big Data ecosystem (design, implementation)Experience working in a client deliver role in an on off-shore model.Detailed knowledge of RDBMS data modeling and SQ, Data warehouse, BI and ETL toolsJob Type: ContractSalary: $75.00 to $80.00 /hour","Philadelphia, PA",Big Data Engineer 10 years of experience Philadelphia PA,False
653,"The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform and load) tools and methods, analytics, reporting, and documentation.

Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures and code sets as well as applicable data privacy practices and legal requirements.

Required Skills and Experience:
Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.
Knowledge of data warehousing best practices, concepts and processes.
Strong analytical and problem-solving skills, with demonstrated change management experience.
Demonstrated ability to set and meet project timelines and deliverables.
Effective interpersonal and communications skills with the ability to interact with various levels of personnel.
Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.
Experienced in Microsoft Office and Microsoft Operating Systems.
Must be able to work in CCMCN's main office and travel to all required meetings
Fluency in written and spoken English.

Additional Preferred Skills:
Strong business and technical writing abilities.
Knowledge of healthcare data standards such as HL7, CCD, CCR and claims data.
Knowledge of standard healthcare code sets like LOINC, ICD9/10, CPT4 and SNOMED.
Knowledge of IHI triple aim, clinical quality improvement, and primary care operations/workflow.
Ability to stay current on healthcare reporting requirements such as UDS, PQRI, NQF, Meaningful Use and Patient Centered Medical Home.
Ability to attend conferences and workshops for further education to expand and improve management skills.

The above statements are intended to describe the general nature of work being performed by personnel assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of personnel so classified.

CCMCN is an equal opportunity employer offering flexible benefits, a casual work environment, and a competitive salary (DOE).

TO APPLY:
If interested in this position, please send cover letter, resume and salary requirements by email.
PLEASE INCLUDE THE JOB TITLE IN THE SUBJECT OF THE EMAIL.

Please send all application information to:
Savannah Bermudez
Re: Data Engineer
Email: jobs@ccmcn.com

No phone calls please.","Denver, CO 80210 (University area)",​Data Engineer,False
654,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile and desktop computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done — just by asking.
The vision for the Siri Data organization is to improve Siri by using data as the voice of our customers. Join us, and impact hundreds of millions of customers across a plethora of Apple of devices!
The Siri Metrics Platform team is responsible for building a highly available and scalable system to ingest, process, and analyze billions of events generated daily from devices and our services. This system, along with the data visualization tooling we’re building, will allow our engineering teams to perform quick exploration with the overarching goal of improving the quality of Siri.
As a Data Engineer on the Siri Metrics Platform team, you will have significant responsibility and influence in improving Siri by using data as the voice of our users. You will develop large scale data pipelines and analytical solutions, interface with our quality initiative leaders to ensure the system is meeting the needs, and iterate as well as innovate based on observations and requirements gathering.
Successful candidates will have strong engineering and communication skills, as well as a belief that data informed processes lead to better products.

Key Qualifications
5+ years of industry experience working with large-scale and high-throughput systems.
Expert knowledge of one or more object-oriented programming languages (Scala, Java, C++).
Ability to leverage several scripting languages (Python, Ruby, Bash, etc.).
Thorough understanding of the Hadoop ecosystem (HBase, HDFS, Hive, MapReduce), Spark, Solr, Kafka.

Description
The Siri Metrics Platform team is in a unique position to align our quality initiatives to a singular platform. This will allow all performance, reliability, accuracy, and usability metrics to utilize the same methodology for dashboard/reporting creation and curation. This platform will also empower our developers to do ad-hoc queries into their problem space in order to troubleshoot or validate the effectiveness of their code from a quality perspective.
A successful candidate will have experience in large-volume data ingestion, processing, and analysis in near real-time.
Design, implement, and manage scalable data models and pipelines leveraged by all Siri teams.
Build analytical solutions to enable data analysts to perform accurate and consistent analysis efficiently.
Develop and contribute back to open source projects supporting the platform.
Ensure the platform can handle all types of robust data exploration in real-time.
Partner with all Siri teams and build features to enhance data analysis.

Education
B.S. degree in Computer Science or 5+ years of programming experience or equivalent.","Santa Clara Valley, CA","Siri - Data Engineer, Data organization",False
655,"Who we are:
Calico is a research and development company whose mission is to harness advanced technologies to increase our understanding of the biology that controls lifespan, and to devise interventions that enable people to lead longer and healthier lives. Executing on this mission will require an unprecedented level of interdisciplinary effort and a long-term focus for which funding is already in place.

Position Description:
Great software engineering and data science are increasingly crucial to biology. We are in the midst of an explosion in the quantity and quality of biological and medical data that are transformative to our understanding of biology and disease. But the tools to store, process, and analyze these data are often primitive, and in some cases don't yet exist. Calico is seeking an exceptional data engineer to join our computing group and be a part of changing that story.

In this role, you will work closely with computational and research scientists to define strategies and implement systems for modeling, collecting, storing, and accessing diverse scientific data and metadata. Collaborating with other scientists and engineers, you will design, build, and maintain databases and data warehouses that underpin our scientific endeavors and accelerate our ability to ask new, sophisticated questions spanning multiple organisms, data modalities, and timescales. You will not only build tools to support existing scientific workflows, but also help set the vision for future data generation and collection efforts.

If you are passionate about data, passionate about biology, and passionate about their intersection—this is the job for you.

What you'll do:

Work with computational and research scientists to understand common analysis use cases and data access needs.
Design strategies for data storage and integration across different data sources (both internal and external) for multiple use cases.
Implement, document, and maintain processing pipelines, databases, and data warehouse infrastructure.
Work closely with full-stack engineers to develop APIs and GUIs for accessing and visualizing scientific data.
Set data engineering vision and drive both independent and collaborative software development projects end-to-end.
Contribute to a range of projects, from one-off solutions to long-term, complex systems.
Build out core infrastructure, tooling, and software development processes.

Position requirements:

5+ years working with contemporary ETL tools and frameworks.
3+ years building Python-based backend systems.
Fluent knowledge of SQL.
Experience implementing RESTful APIs, GraphQL, and other programmatic interfaces to complex multidimensional data.
Experience deploying high-performance data backends in the cloud with Amazon Web Services, Heroku, Google Cloud Platform, or a similar service.
Firm grasp on software testing and test-driven development.
Demonstrated success in owning projects end-to-end, including working with non-technical stakeholders to define requirements and seek feedback.

Nice to have:

Worked with machine learning tools and infrastructure, e.g. TensorFlow and PyTorch.
Built back-ends for high-dimensional graph or network data.
Worked in biology or life sciences, and have familiarity with databases and data types used by computational biologists.
Built software with technologies like ElasticSearch, GraphQL, and Google Cloud Platform.

Some projects you may contribute to:

Data warehouse—a system to extract, transform, and load public and private datasets into a single repository, then making these data available for analysis visually with either off-the-shelf or custom-built GUIs.
Exploratory data visualization & analysis tools—apps to help scientists explore and understand diverse, complex, and multidimensional data.
Data platform—a modern, React (front-end) and Python (back-end) application that our scientists use to manage and process experimental data.

Automation—software to ingest and transform data from custom high-throughput instrumentation.","South San Francisco, CA",Data Engineer,False
656,"Evidation Health is a new kind of health and measurement company that provides the technology and guidance to understand how everyday behavior and health interact. The volumes of behavior data generated from wearables and smartphones has opened up new ways to analyze individuals’ behavior and health in real time. With a virtual pool of 2 million research participants, Evidation Health undertakes research for innovative biopharma and health care companies to transform how diseases are identified, treated, and monitored.

We are seeking a Staff Data Engineer to join our Enterprise Data Engineering team. As a Senior Data Engineer you will be working on our Data Platform, coding in Python and React and using cutting-edge big data technologies such as Apache Spark, Airflow, Hadoop, and Jupyter to build out data pipelines, data lakes, full-stack applications, and analytics tools that scale on the cloud to empower data scientists to gather insights from billions of data points to improve health outcomes.

RESPONSIBILITIES
Design scalable, high performance solutions that continuously handle terabytes of data and enable data scientists to analyze data at scale.
Understand overall performance, and design around improving data performance using different technologies, algorithm efficiencies and infrastructure architecture.
Work with customers to understand their needs. Develop and deliver products, that meet the customer’s needs, in a timely manner.
Implement best practices and provide feedback to team members through peer reviews.
QUALIFICATIONS
Technical Qualifications:
8+ years experience in data engineering or full-stack engineering with strong database skills
Proficiency with at least one interpreted, object oriented programming language (Python, Ruby, Java, etc.)
Proficiency building data pipelines with big data processing technologies (Spark, Hadoop, Kafka, etc.)
Experience with SQL databases (e.g., Postgres, MySQL, Redshift) and Distributed Datastores (e.g., S3)
Distributing computing experience on the cloud (AWS, Google)
Professional Competencies:
Strong written and verbal communication
Work effectively in a collaborative and cross-functional team environment
Take personal responsibility for driving initiatives forward and continually improving the product
Quality-focus, ensuring proper test coverage for all the code you write
BENEFITS
Health, dental, and vision benefits for you and competitive coverage for your family
Relocation support
Equity
Flexible work hours
Open vacation policy - take time when you need it
Support for remote work when needed
Relaxed work environment
Your choice of computing equipment and gear
Lots of opportunities for growth
Opportunity to work on fascinating challenges that improve people’s lives


Evidation Health values diversity and is committed to equal opportunity for all persons without regard to race, color, creed, religion, marital status, age, national origin or ancestry, political activity or affiliation, physical or mental disability, medical condition including genetic characteristics, marital status, sexual orientation, gender identity, sex or gender.","San Francisco, CA",Staff Data Engineer,False
657,"NAVICAN Genomics, Inc. delivers precision cancer care. All patients deserve access to precision cancer care - the right therapy at the right time to ensure better outcomes and reduced healthcare costs. Today many people don’t get the precision therapy they need, and with the exceptional team of professionals we are building, we believe we can do better. Based on proven processes developed through years of experience and clinical best practices of Intermountain Healthcare, NAVICAN was created to transform advances in precision medicine into precision cancer care for patients everywhere that advanced therapies are available.

Expert prioritized therapy recommendations and experienced treatment navigators will simplify the path to drug and clinical trial access. At the same time, we’ll help patients and their caregivers navigate often frustrating administrative processes. We’re driven by an impatience to make more precision therapy options available to more cancer patients. If you have a passion to help people, a desire to grow, and you excel at what you do, join us at NAVICAN and make a positive impact on the world.

DATA ENGINEER
NAVICAN is seeking a highly motivated Data Engineerin San Diego to join our team and help in building out the data platform that powers our data driven products. As part of the IT team you’ll work closely with several departments providing expertise on data management.
The Data Engineerwill play a key role in NAVICAN’s data management strategy and assist in curating and cataloging multiple sources of data used throughout NAVICAN for use in internal and external products. In addition, they will contribute in improving the data management processes and take on other duties as needed.

PRIMARY RESPONSIBILITIES AND DUTIES

Architect, implement, and maintain NAVICAN’s data platform
Build robust and scalable data integration pipelines for ETL and reporting using appropriate technologies
Identify gaps and improvements to internal data management processes including automation, sanitization, and optimizing data delivery to customers
Collaborate with internal customers in gathering requirements to develop reporting solutions and evolve the data platform to meet their needs
Develop and deliver high quality data sets for analysis to Bioinformatics, Data Science, and Business Analytics
Assist in tool evaluation, selection, and integration with various other frameworks
Collaborate with DevOps in establishing best practices around data security and access


COMPENTENCIES AND ESSENTIAL SKILLS
Knowledge of relational database concepts and the SQL querying language
Knowledge of common flat file data formats such as JSON, XML, and CSV
Knowledge of health-related data formats including HL7, FHIR, EDI, etc.
Knowledge of data concepts such as Enterprise Data Warehouses, Data Lakes, and ETL
Knowledge of HIPAA requirements & data management best practices related to the storage of health information
Excellent interpersonal, verbal, and written communication skills
Self-driven with the ability to work independently and within a team
A strong sense of ownership and urgency
Demonstrates NAVICAN’s values by acting with integrity, respect, and trust

EDUCATION AND EXPERIENCE
Bachelor’s degree in Computer Science, Information Technology, or a related field
3 - 5 years of relevant experience in one of the following areas: data engineering, data analytics, business intelligence, or business analytics
Experience building and optimizing data pipelines, architecture, and data sets
Experience with a relation database such as PostgreSQL, MySQL, or MSSQL
Experience with an object oriented or functional language such as Python, Java, R, etc.
Experience with a cloud platform, such as AWS or Azure, and its respective data processing technologies
Preferred experience: NGS (Next Generation Sequencing) and working with laboratory related agencies
Preferred experience: Middleware platforms integrating multiple systems including EMR, LIS, and LIMS


NAVICAN is an EOE/AA employer and offers competitive salary and benefits package.","San Diego, CA",Data Engineer,False
658,"Job Description

CapTech Data Engineers collaborate between business operations and IT services by offering expertise in evaluating, understanding, and leveraging client data to drive fact-based decisions. BI consultants help our clients effectively leverage reporting and data tools to build strong, robust and dynamic systems that will serve the client’s long-term strategic goals. Our BI consultants enable our clients to make more informed and data-driven decisions to gain competitive advantages, increase sales, improve sales, improve core business competencies and satisfy today’s escalating customer expectations.
Specific responsibilities for the Data Engineer include:
Design, develop, document, and test Business Intelligence solutions using industry standard tools.
Create, own, and present documentation/designs to fellow team members and clients.
Facilitate requirements gathering sessions with business and technical stakeholders to distill data and reporting requirements from business requests.
Coordinate design and development efforts with client stakeholders to ensure the solution delivered meets the business need and is consistent with approved architectural standards.
Performance tuning to ensure a responsive solution.

Qualifications

Specific qualifications for the Data Engineer include:
At least 5 years experience developing business intelligence solutions using Tableau.
Experience with tools, such as SSRS, Cognos, Microstrategy, QlikView, Spotfire, etc. is a plus.
Familiarity with at least two different database platforms, such as Teradata, Oracle, MS SQL server or other platforms; Teradata and MS SQL strongly preferred - Extremely strong SQL skills.
Dimensional data modeling skills.
Hands-on experience with ETL development a plus.
Excellent interpersonal, team management, facilitation and communication skills; must be able to communicate effectively at all levels of the client’s organization.
Additional Information

We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.
At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career

At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.

Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements).

CapTech is an equal opportunity employer.

CapTech is a Drug-Free work place.

Candidates must have the ability to work at CapTech’s client locations.

All positions include the possibility of travel.
CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly.","Charlotte, NC","Data Engineer, Analytics - Tableau",False
659,"ContractPosition Summary

Very Strong engineering skills. Should have an analytical approach and have good programming skills
Provide business insights, while leveraging internal tools and systems, databases and industry data
Minimum of 5+ years’ experience. Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter
Ability to document requirements, data lineage, subject matter in both business and technical terminology
Guide and learn from other team members
Demonstrated ability to transform business requirements to code, specific analytical reports and tools
This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team

Requirements:
Must Have


Strong analytical background
Self-starter
Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights

Technical Requirements

Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata)
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes
Real time data ingestion (Kafka)

Nice to Have

Development experience with Java, Scala, Flume, Python
Cassandra
Automic scheduler
R/R studio, SAS experience a plus
Presto
Hbase
SAP Hana
Tableau or similar reporting/dash boarding tool
Modeling and Data Science background
Retail industry background

Education

BS degree in specific technical fields like computer science, math, statistics preferred

Benefits:
This opportunity is open to both full-time and contract employees.","Sunnyvale, CA 94085",Data Engineer,False
660,"Responsibilities:
Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Datalitical Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance

Experience & Skills:
Extensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet) Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in scripting language (Python, Ruby, Perl, Bash) Experience with workflow management tools (Airflow, Oozie, Azkaban, UC4) Comfortable working directly with data analytics to bridge business requirements with data engineering

Bonus:
MPP database experience (Redshift, Vertica, Teradata) Experience with building tools to support self-service pipeline Experience with one of the messaging system (Kafka, SQS, Kinesis) and different data serialization (json, protobuf, avro)

Our corporate office supports and offers a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) savings plan with company match.

Apply: hr@datalitical.com","Dallas, TX",Data Engineer,False
661,"What if you could shape the future of work and be part of the team that creates the digital workforce of tomorrow, by means of Robotic Process Automation?
In the beginning of the 20th century, Henry Ford had a vision of creating assembly lines and facilitating mass production.
100 years later, UiPath has a grand vision of liberating the human workforce from tedious, boring, repetitive tasks, by means of software robots, artificial intelligence and machine learning.

Here's what you would be doing at UiPath:

As a Data Engineer, you will be responsible for building and maintaining the infrastructure for big data and AI lifecycle. You will also be responsible for maintaining the algorithms developed by our data scientists and using a myriad of tools for production readiness of AI models. You will be working on a cross-functional team of Product Managers, devops engineers, machine learning engineers, and software engineers for high impact shipping. Being a part of hypergrowth startup, you are not afraid of getting your hands dirty and are expected to be a jack of all trades for all steps in the ML lifecycle right from tagging, featurization, training and benchmarking to experimentation, monitoring and analytics.

Role & Responsibilities:
You will work with our team of experts in machine learning and software engineering to do the following:

Design and build large scale data ingestion, storage and processing platform.
Build and maintain platform to for the complete machine learning lifecycle.
Qualification & Educational Requirements:

Post Graduate / Graduate in computer science or a related field.
Overall 4+ years of experience in IT industry with 1+ years working on products built to handle big data in both cloud and on-prem settings.
Experience of working on both SQL & No-SQL databases.
Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.
Experience with big data technologies like Hadoop, Spark & Kafka.
Experience in Python, data processing and parsing.

Preferred Skills:

Experience of building platforms in cloud using AWS, Azure or GCP.
Experience in offline batch processing and/or online real-time stream processing systems.
Knowledge of Machine Learning and Interested in working across our entire data science stack including model building, data pipelining, and performance/scale analysis.
Benefits :

We are offering the possibility to work from home or flexible working hours in a nice office plus free daily premium catering. Healthcare plan.
Competitive salary, a Stock Options Plan and the unique opportunity of working with us to develop state-of-the-art robotics technology are just a few of the pluses.
We must have caught your attention by now if you've read so far, so we must connect.","Seattle, WA",Data Engineer,False
662,"Description
Big Data Engineer

Location: New York, New York

About Symphony Talent:

At Symphony Talent, we connect an employer’s brand with best-fit talent through our integrated cloud-based suite of solutions. Symphony Talent’s leadership, staff and investors are dedicated to paving the future for forward-thinking organizations focused on sourcing, nurturing and hiring the best-fit talent for great brands.

At Symphony Talent, we respect relationships with one another just as we respect our relationships with our clients, partners and supporters. Integrity is ingrained in our core values and we are committed to an open and honest work culture. Our teams of creative thinkers are inspired to make a unique impact in this often complex market landscape. We are committed to building an organization fueled by the likes and minds of those passionate for this industry.

Symphony Talent is using data to revolutionize and disrupt the online recruitment marketing space. Our Data Management Platform (DMP) is now ingesting thousands of GB's of data from our online marketing sources:


Display Advertising

Social Advertising

Email marketing

Online Job Postings

Career Web Sites


Amongst many other things, Symphony Talent will be using this data to produce the following:

Multi source attribution analytics

Predictive Analytics

Client facing Analytics - via our SaaS portal

Integration with external vendors and clients (Google, Facebook, Twitter, Indeed etc)
About the Role:

We are looking for a ""generalist"" engineer, that will primarily be responsible for collecting, storing, processing, and analyzing huge sets of data. You will also be responsible for integrating the resulting output of the data processing to our internal Analytics Team, our customer facing SaaS analytics platform as well as 3rd party vendors and clients. Over the course of time, you will be the go-to person for all things data related.
Requirements
Responsibilities

Implementing complex, robust ETL pipelines (python, bash, AWS, Hadoop, Redshift)

Writing/Using API's for internal/external data integration and ingestion (Python, java, Node.js)

Building monitoring services

Writing, testing and debugging online tracking tags (Javascript)

Contributing to our client facing SaaS analytics platform (JasperReports Server)

Contributing to our continuous development framework (Github, Jenkins, AWS)

Skills and Qualifications

3+ years of, working with large complex sets of data

Highly proficient in SQL (This is a mandatory requirement)

Competent, working experience with at least 1 of the following languages:

Python

Nodes.js

Javascript

Java


The entire platform runs on AWS (specifically AWS Linux). While it’s not mandatory to have hands on AWS experience, having solid working knowledge and experience of the *nix OS is mandatory (Shell scripts).

The following hands on experience will be highly desirable:

Hadoop (Hive, Spark, UDF's)

Managing infrastructure in AWS

Building and scaling Machine Learning frameworks

BI tool (JasperReports Server, Tableau, Qlik etc)



Symphony Talent Perks Include:

Competitive compensation

Great benefits package, including a 401(k) plan and unlimited PTO

Open, collaborative culture and flexible work hours

Fun, conveniently located office in Midtown


If this sounds like an exciting next step for your career, we’d love to hear from you!

Only candidates with proper permits to work in the United States can be considered. Symphony Talent is an equal opportunity employer M/F/Disability/Veterans and committed to a drug-free workplace.","New York, NY 10001 (Chelsea area)",Big Data Engineer,False
663,"$142,000 - $181,000 a year (Indeed Est.) Data Scientist / Data Engineer (San Francisco, CA)
Location: San Francisco, California, United States
Full-time
The New York Times describes Thunder as ""an ad engine to put Mad Men out of business."" We're changing how digital ads are created and distributed by automating much of what people thought couldn't be done by computer. Our technology retrieves all relevant content about an advertiser across the web to intelligently design a beautiful set of ads for desktop, tablet, and mobile devices all in under a minute.

THE JOB

Thunder is looking for a talented Data Scientist with a track record working with Big Data and Distributed Systems to manage a cutting-edge infrastructure used by the world’s largest digital advertisers. We’re using Big Data in groundbreaking ways to uncover customer insights, personalize customer experiences and fix digital advertising. You will contribute as a key member of the Product Engineering team where you will be driving product and engineering innovation to better leverage Thunder's growing personal graph. We are looking for a self-starter who thrives with ambiguity and loves solving challenging problems.

RESPONSIBILITIES

Design and develop Big Data and real-time analytics solutions using industry standard technologies
Collaborate with internal business and product teams to identify product features that can be powered by advanced data analytics
Use various machine learning and statistical techniques to analyze data, build models and identify requirements for operationalizing those models into production services
Work with external customers on challenging data analysis problems


QUALIFICATIONS

Ideal candidates will have hands-on, operational experience building and operating large-scale data analytics services and thrive working in a fast-paced startup environment.

5 -7 years of hands-on experience with using advanced statistics techniques and machine learning to build operational production services
Strong understanding of machine learning, recommendation systems, predictive analytics, and multivariate analysis
Strong computer science fundamentals including data structures, algorithms, distributed systems and common design patterns
Strong database and data engineering experience with hands-on experience building services that leverage a variety of database systems including SQL, Redshift Spectrum, Druid, Hadoop, Hive, HBase, Spark, Kafka, AWS Kinesis, MongoDB
B.S. or M.S. in Computer Science, Computer Engineering, Mathematics, Statistics, Applied Mathematics or related experience","San Francisco, CA",Data Scientist / Data Engineer,False
664,"Job Description

Machine learning, big data; near real-time scoring environment. If these areas resonate with you, then join us to work on extremely motivating challenge at Amazon Web Services (AWS) marketing. We build and run custom machine learning models to solve challenging business problems at scale. If you are a strong Data Engineer, self-starter and learner who is passionate about working with massive amounts of data to build state-of-art system on AWS platform, then this is the right opportunity for you. You will work with a team of highly skilled engineers and scientists, to build the next generation ML platform using AWS services. As part of your job, you will deal with large amounts of training data, partner with Scientists and help rapidly prototype new models that meet stringent performance requirements, perform offline and online testing, and push these models to production.

As part of this role, you will be required to:
Analyze and extract relevant information from large amounts of historical data to help automate and optimize key features and ML processes
Establish scalable, efficient, automated processes for large scale data analyses, model development, validation and implementation
Work closely with scientists and engineers to create and deploy new features
Work closely with stakeholders to solve various business problems
About you
You are fascinated by the power of large scale systems and using machine learning algorithms to optimize decision making. And you're looking for a career where you'll be able to build, to deliver, and to impress. You look at problems holistically, and thrive on the intricate complexity of designing feedback loops and ecosystems. You want to work on projects where you are implementing solutions to real problems that require creative solutions and deep understanding of the problem space. You will partner with scientists and engineers to challenge yourself and others to constantly come up with better solutions. You'll be given an opportunity to own and drive initiatives - from customer facing features, system innovation, all the way down to the datasets that the back-end services consume.
Basic Qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
5+ years of experience in scripting languages like Python etc.
Demonstrated strength in data modeling, ETL development, and Data warehousing. Data Warehousing
Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.
Experience in working and delivering end-to-end projects independently.
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative field
Experience with building high-performance, highly-available and scalable distributed systems.
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership for best practices on data engineering
Masters in computer science, mathematics, statistics, economics, or other quantitative field
Experience working with large volumes of real-world noisy data
A willingness to dive deep, experiment rapidly and get things done.","Seattle, WA","Sr. Data Engineer, Machine Learning",False
665,"What we do

Uptake helps industrial companies digitally transform with open, purpose-built software that delivers outcomes that matter. Built on a foundation of data science and machine learning, our vision is to create a world that always works — one where the machines and equipment we depend on daily don't break, and industrial companies are once again the creators of economic growth and opportunity.

What you'll do

As a Data Engineer on the Data Science team, you'll work with Uptake's data scientists and product team to design and build data infrastructure in support of Uptake's Data Science. The tools you create will have lasting impact on model development and deployment, performance and outcomes reporting, as well as data monitoring. The ideal candidate has strong analytic and technical abilities, as well as the ability to be flexible and adaptive to rapidly evolving needs of the team.

Responsibilities


Design and implement data warehouses, real-time ETL, and batch processing of data to support modeling and reporting needs
Work with data ingestion teams to develop data expertise and resolve upstream issues relating to data quality
Define best practices and design for the management of data
Partner with Data Scientists to build and maintain internal data processing and visualization tools
Translate requests into replicable analytic reports using varying applications
Create tools to serve data such as APIs and packages

Qualifications


Required:
Bachelor's degree in computer science, information technology/information systems, or a field related to a computational science or 2+ years experience working as a data engineer
Ability to write efficient SQL queries
Experience managing data ETL processes and making data available through service applications and databases.
1+ years experience with NoSQL databases (Cassandra or Elasticsearch preferred)
3+ years experience with programming languages (Python, Java, R, and/or Scala preferred)
Familiarity with a variety of data processing technologies (e.g. Spark, Kafka, Hadoop)
Excellent communication skills, including a knack for clear documentation
Experience with or knowledge of REST APIs and making data available through microservices.
Experience using version control (Git, Mercurial, SVN, etc.) for collaborative code development.
Preferred:
MS or PhD in Computer Science or other technical field
Ability to architect data solutions
Some knowledge of machine learning and data science processes
Experience supporting data science and analytical efforts is preferred
Some experience with frontend web-development
Experience defining and implementing APIs
Experience working with Docker

Why Work Here

We build and deliver, then explore to build more. Curiosity and flexibility enable everything we do, and we get stronger as we make each new industry smarter. As a team, we bring our diverse backgrounds, beliefs and experiences to solve problems no one has yet to solve, at a speed no one has yet to experience. We support and challenge one another to bring out a new best in each of us, and we might have a little fun along the way.","Chicago, IL",Data Engineer,False
666,"Join a team recognized for leadership, innovation and diversity
Join a company that is transforming from a traditional industrial company to a contemporary digital industrial business, harnessing the power of cloud, big data, analytics, Internet of Things, and design thinking. You will lead change that brings value to our customers, partners, and shareholders through the creation of innovative software and data-driven products and services. You will Be responsible for the development and maintenance of the database infrastructure needed for successful analytic processing. You will ensure that there is optimal use of technology to meet business requirements.
Join a high-performing team and distinguished talent pipeline
Work within Honeywell to identify opportunities for new growth and efficiency based on data analysis
Build strong relationships with leadership to effectively deliver contemporary data analytics solutions and contribute directly to business success
25 Data modeling

25 Enterprise reporting solutions

25 Technical analytic work

25 Cross team collaboration



YOU MUST HAVE
Bachelor's degree in Computer Science, Engineering, Applied Mathematics or related field
WE VALUE
Some experience with SQL, .Net Applications or other programming languages
Experience with SSIS and general administrative data experience
Experience with the techniques of CQI/TQM and statistical process control methodology
Experience with visualization software (Tableau, Spot fire, Qlikview, d3.js)
Understanding of best-in-class model and data configuration and development processes
Experience with Agile Scrum methodology
Strong understanding of relational databases
Conveys specific, observable, and/or measurable expectations for each assignment, and verifies understanding and agreement on deliverables and timeframes
Consistently makes timely decisions even in the face of complexity, balancing systematic analysis with decisiveness
Due to US export control laws, must be a US citizen, permanent resident or have protected status. Exempt How Honeywell is Connecting the World
INCLUDES
1st Shift
Continued Professional Development

ADDITIONAL INFORMATION
Job ID: req162256
Category: Information Technology
Location: 9680 Old Bailes Rd, Fort Mill, SC 29707-7539 USA
Honeywell is an equal opportunity employer. Qualified applicants will be considered without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.","Fort Mill, SC 29707",BI & DW Data Engineer,False
667,"USC/GCH/L2EAD/H4EAD ONLY
NO C2C
CANDIDATES WILL WORK ON OUR W2
Role: Data Engineer
Location: HOUSTON Texas (US-TX)
Salary: $144K
Client: NTT DATA
Primary functions: Participating in projects as a Data Lead / Data Engineer responsible for performing many different data related tasks to make data available for the designated business need.
These tasks could include the following components:
Database and Interface design, Creating/Editing Data Models (physical & logical), Data Profiling, Data Mapping, data quality management.
Required Advanced Level of Technical Competencies:
Data modeling (physical and logical), ER/Studio diagrams, data dictionary, data map, normalize/de-normalize, agile dataData manipulation tools such as: SSMS, SSIS, TOAD, Access, Excel
- Advanced database handling languages (SQL and variants) - Experience in Data Architecture specifically in a Data Lake environment
– i.e. Data Lake Zones, Data Modeling, etc. Preferred (Beginner to Expert)
Technical Competencies: - Practical skill sets in BI and Data visualization tools (Tableau)
- Knowledge of programming languages and some ability to write code (e.g., python, VBA, XML, R) - API Development and Mulesoft
- SAP Business Object Data Services (BODS) ER/Studio Modeling Preferred candidate should: - Strong communication and interpersonal skills with strong English proficiency
Demonstrate critical thinking, analytical skills, and employ judgment to offer thoughtful, concise input toward resolutions of problemsBe able to translate data requirements into business processes and reverse engineer business processes into data requirementsComprehension of DevOps and Agile development and application to data centric architecture and solutionsMinimum Bachelor’s Degree in Engineering Technology, Computer Science, or a related field with equivalent experience","Houston, TX",Data Engineer,False
668,"Minimum Qualifications: Bachelor’s degree in engineering, computer science, or related discipline3-6+ years of experience as a software or data engineerSoftware development experience in JavaExperience building large scale data warehouses and ETL data processing pipelinesExperience with big data technologies (Hadoop, Hive, Spark, Presto, etc.)Have successfully supported an enterprise-scale web application in the cloudRDBMS experience (SQL, schema design and optimization)Excellent communication skills, both written and verbalNice to have: AWS experienceExperience with Python, R, and ScalaExperience with reporting and analytic toolsExperience with NLP, Machine Learning, Deep LearningExperience with healthcare data and workflows (e.g. HL7, FHIR)Experience with columnar data stores (Parquet, ORC, Redshift)NoSQL experience (DynamoDB, Cassandra, HBase, MongoDB)Knowledge of infrastructure automation (CloudFormation, Terraform)Experience with streaming data technologies (Kafka, Kinesis, Storm, Spark Streaming)Job Type: Full-time","Belmont, CA",Sr. Data Engineer,False
669,"Summary: The Data Engineer is a part of an IT team creating data warehouse solutions to support business strategies, priorities and growth. The Data Engineer is responsible for the design, architecture, creation and implementation of Data and Analytics Environment including data lakes, operational data stores data warehouse and data marts. This individual is also responsible for selecting, customizing and implementing data translation, data mining, and machine learning modules while ensuring a smooth integration with existing data. This role also ensures business continuity requirements are met for all data and participates in disaster recovery planning, testing and execution. This position contributes in the planning, forecasting and management of the IT budget. This individual consistently promotes and emulates our company values and our 5P’s.Key Responsibilities & Essential Functions: Works with all functional teams to set up, maintain and support the data warehouse, and all data warehouse related tasks (includes SSIS, SSAS, MDM and Data Marts)Responsible for monitoring data warehouses, setting up data warehouse, and backup and recovery processesAids the technical team in setting database standards, launching and implementing new data warehouse capabilitiesWorks effectively with infrastructure and developers on performance tuning and optimization, query optimization, index tuning, caching, buffer tuning, etc.Helps define and improve best practices for data and data warehouse activities and initiativesChampions change and innovation, stimulating creativity and innovation in othersParticipates in business process re-design, including identifying current state, future state and gapsDrives the data warehouse technology direction and delivery through his/her understanding of the technical environment (possibilities as well as limits)Participates in the IT Steering Committee meetings to ensure all data needs are understood for approved projectsParticipates on the Business Data Governance team to ensure consistent business rules, data quality and efficient data managementImplements and maintains data governance framework and processes to manage data assetsAssists in defining and implementing development and integration standardsServes as a subject matter expert for all data source integrations, translations and data loadsConsults with internal customers and partners to identify data sources, data owners and data flowsEnsures detailed documentation, such as data models, data hierarchies and data flows are reviewed and kept up to dateEnsures that data flows are built to meet business service level agreementsParticipates in the evaluation and recommends solutions to meet data needs; helps evaluate data sources that are targeted for possible integration into the systems or environment, including additional tools and utilities needs to facilitateCritical Success Factors: Hands-on experience and demonstrated expertise in data warehouse technologyFamiliarity and experience with integrating data warehouse with vendor and other applicationsDemonstrated experience troubleshooting and resolving problems with MS SQL Server and other database enginesAble to define and champion processes, identify and avoid potential data errors to ensure data integrityPerforms capacity planning and properly identifies required hardware, software, data warehouse configuration/architecture needed to support application needsDesigns and implements core services, processes and technologies to provide reliability, high availability, performance and scalability for creation, access, caching and lifecycle of data objects and entitiesUtilizes toolsets to diagnose and resolve production issues for data warehouse based project rollouts, or for post rollout supportSelf-motivated and capable of working with infrastructure, applications, and business areas to align systems solutions with business requirementsAble to manage the reporting needs of the business at the same time ensuring the data warehouse is kept responsiveDemonstrated experience working in a large scale, dispersed, networking and communications environment and maintaining continuity of operationsAids in the setup, design and maintenance of the data warehouseDemonstrates technical skills and is trained/certified in MS SQL Server, SSIS, SSAS, and MDMWorks with team to enhance processes, review standards and support current and future platforms and applicationsParticipates in project definition and design activitiesHelps conduct analysis and implement recommendations affecting data warehouse technical initiatives and helps prepare reviews and evaluate system documentation, specifications, test plans and proceduresParticipates in end-to-end data warehouse and system testing and integrationAssists functional teams in setting up, maintaining and supporting data warehouses and all data and data warehouse related tasksHelps monitor data warehouses, set up database clusters, and backup and recovery processesDesigns, builds and supports data cubes, data marts and ETLConducts performance tuning and optimization, query optimization, index tuning, caching, buffer tuning, etc.Helps define and improve best practices for data and data warehouse activities and initiativesDevelops and implements an enterprise data architecture strategyFacilitates strategic planning and development of architectural roadmapsPartners with business on implementation and enhancement projectsDevelops and oversees data management standards, data dictionary and meta data standards and incorporating industry standards for data managementUnderstands industry direction, best practices and technologies to discuss options with other team membersProvides logical and physical modeling and design servicesPartners with end-users to developing monitoring and performance standardsDrives resolution of implementation and technical issues related to assigned projectsJob Requirements: Bachelor’s degree in Business Information Systems, Computer Science or related discipline5+ years of related project/industry experienceDemonstrated competency in full life cycle of Data Warehouse development and support4 - 7 years professional experience in MS SQL ServerExperience working with functional teams on data model and schema design, performance optimizations and data intensive tasks and architecture components3+ years of experience with data warehouse administration, support, optimizations and monitoring3+ years of design and development of extremely high volume, high availability applications and systems.2+ years of experience writing ETL jobs, working with data cubes/data martsStrong experience developing and deploying triggers, SQL jobs, data imports, migrations, transformations, etc.Strong understanding of Microsoft, VMware operating system, storage solutions, networking, security and web serversExpert level skills in data modeling and design of logical and physical schemasAbility to communicate effectively and work cooperatively with development and infrastructure engineersDeep understanding of performance, unit and load testing instrumentation and analysis strategiesBroad knowledge of common applications and technologies in Internet computing, including web servers, app servers, database servers, load balancers, etc.Experience mentoring internal business partners on data management.Hands-on experience and demonstrated expertise in database technology specifically MS SQL Server 2012 and higherFamiliarity and experience with integrating the data warehouse environment with vendor solutions and other applicationsAble to help troubleshoot and resolve problems with data warehouse implementations and projectsAssists with identifying and avoiding potential data errors to ensure data integrityIs familiar with hardware, software, database configuration/architecture needed to support application needsHelps design and implement data warehouse services, processes and technologies to provide reliability, high availability, performance and scalability for creation, access, caching and lifecycle of data objects and entitiesHelps utilize toolsets to diagnose and resolve production issues for project rollouts, or for post rollout supportJob Requirements (Preferred): Distribution industry knowledgeExperience administering disaster recovery plansAppropriate MS SQL Server certificationJob Type: Full-timeExperience:data warehouse administration, support, & optimizations: 5 years (Preferred)writing ETL jobs, working with data cubes/data marts: 2 years (Preferred)deploying triggers, SQL jobs, data imports, & migrations: 1 year (Preferred)Education:Bachelor's (Preferred)","Cottage Grove, MN",Data Engineer,False
670,"Overview
Cures Start Here. At Fred Hutchinson Cancer Research Center, home to three Nobel laureates, interdisciplinary teams of world-renowned scientists seek new and innovative ways to prevent, diagnose and treat cancer, HIV/AIDS and other life-threatening diseases. Fred Hutch’s pioneering work in bone marrow transplantation led to the development of immunotherapy, which harnesses the power of the immune system to treat cancer. An independent, nonprofit research institute based in Seattle, Fred Hutch houses the nation’s first cancer prevention research program, as well as the clinical coordinating center of the Women’s Health Initiative and the international headquarters of the HIV Vaccine Trials Network. Careers Start Here.

Working with minimal supervision, the Data Analyst III will utilize a preapproved business intelligence tool or direct SQL development to create reporting artifacts including but not limited to: simple reports, complex reports, dashboards, cubes, data visualizations and data mining constructs in support of Fred Hutchinson Cancer research Center and the various consortium partner’s organizational goals.

The Data Engineer III reports to the Manager of Business Intelligence, Data and Analytics.
Responsibilities
Gathering and documenting requirements from the user community and then translating those requirements into an analytics solution that meets the customer’s needs
Produce high-quality output reports, analysis, development consistent with team and industry best practices
Participate in the design and development of dimensional data warehouse, onboarding and access of data sources
Analyze data to develop and implement standardized reporting formats procedures and processes
Possess in-depth knowledge of preapproved business intelligence tool/Tableau set functionality and options
Possess knowledge on a wide range of possible reporting solutions and have sufficient discernment skills to select the most appropriate solution for each set of requirements
Have an in-depth understanding of the SQL programming language
Will serve as a liaison with the business community around the usage and deployment of reporting solutions and tools
Conduct one-on-one or small group training sessions in order to advance the adoption of analytical solutions
Will participate/lead code review sessions and will be expected to make updates as directed by the primary reviewer
Will have an understanding of reporting within both a relational and a dimensional framework
Respond to ad hoc requests for operational needs related to BI tools, platform, reports and other artifacts
Participate in change control, problem management, and communication process
Utilizing approved BI tools translate written requirement documents into appropriate reporting artifacts
Develop, adopt and evangelize reporting guidelines and standards
Assist in preparing work estimates for assigned projects
Meet required deadlines as outlined by project team and business partners
Prepare documentation for deployment purposes and respond to any implementation issues as needed to ensure quality deliverable has been met
Assist the data quality team in tracking down/debugging potential data quality issues.
Qualifications
Minimum qualifications:
BS in Computer Science, Engineering, Math or equivalent experience
Minimum of 7 years professional database reporting development experience with business intelligence tools, Tableau preferred (MS Reporting Services, Cognos, Business Objects, Tableau Software, MS PowerBI etc.)
Minimum of 7 years SQL development experience
Minimum of 3 years technical leadership experience
Experience working with both relational and dimensional models
Experience developing a variety of different reporting artifacts (parameterized reports, cubes, dashboards, etc.)
Sharp analytical abilities, proven design skills and problem solving skills
Enjoys working in a team environment and has a proven record of positive contributions to the work of the team
Coordination and working knowledge of project management methodologies (Agile/Scrum), time estimates and allocation
Preferred qualifications:

Experience with Microsoft SQL Server
Experience gathering and documenting requirements
Some past experience with analytical, data management & reporting tools
Expertise and knowledge of gathering and documenting analytic/reporting requirements
Expertise in architecting and developing access to data sources for scale
Previous experience in delivering to end users reports and/or extracts to support research and/or clinical care
Our Commitment to Diversity
We are committed to cultivating a workplace in which diverse perspectives and experiences are welcomed and respected. We are proud to be an Equal Opportunity and VEVRAA Employer. We do not discriminate on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital or veteran status, sexual orientation, gender identity, political ideology, or membership in any other legally protected class. We are an Affirmative Action employer. We encourage individuals with diverse backgrounds to apply and desire priority referrals of protected veterans. If due to a disability you need assistance/and or a reasonable accommodation during the application or recruiting process, please send a request to our Employee Services Center at escmail@fredhutch.org or by calling 206-667-4700.","Seattle, WA 98109 (Westlake area)",Data Engineer III,False
671,"Job Description
Sponsored Products is Amazon's native advertising solution (run a search in Amazon and try to spot our sponsored native ads alongside with the organic results). Sponsored Products uses automatic and keyword targeting capabilities to give merchants control over product merchandising on Amazon by boosting the visibility of products when customers shop. Merchants can access the platform via self service capabilities and only pay when shoppers click on the ads. Even though this product started in 2012, it is growing rapidly and it has the potential of eventually becoming as large as other established CPC advertising publishers. Learn more about Sponsored Products here http://sp.amazon.com.
Our team of high caliber software developers, statisticians and product managers use rigorous quantitative approaches to ensure that we target the right ad to the right customer at the right moment; managing tradeoffs between monetization, advertiser ROI and customer relevance. In order to accomplish this we leverage the wealth of Amazon’s information to build a wide range of models, set up experiments that ensure that we are thriving to reach global optimums and leverage Amazon’s technological infrastructure to display the right ads in real time.

As a Data Engineer with our team, you will be working in a large, extremely complex, and dynamic data warehousing environment. We are looking for someone with the uncanny ability to build efficient, flexible, and scalable data lake/warehouse and analytic solutions enabling identifying risks and opportunities. The candidate should be able to use new technologies (AWS stack (redshift, EMR/spark) and be able to implement solutions using these technologies to empower internal customers and scale the existing platform. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing reporting applications. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive growth.

To succeed in this newly created role you will work closely with BI , product and business leaders in Seattle. You will have the opportunity to be mentored by top data engineers, and you will work directly with BI peers

Key responsibilities:
Design, implement, and support data warehouse infrastructure using AWS (Redshift, datalake) technologies.
Create ETLs to take data from various operational systems and create a unified dimensional or star schema data model for analytics and reporting
Use business intelligence and visualization software (e.g., OBIEE, Tableau, Micro Strategy, etc.) to empower non-technical, internal customers to drive their own analytics and reporting
Develop a deep understanding of our vast data sources and know exactly how, when, and which data to use to solve particular business problems.
Monitor and maintain database security and database software
Support the development of performance dashboards that encompass key metrics to be reviewed with senior leadership
Work with Product Managers, and a growing global team to help in analyzing data and derive new insights that drive Amazon's Sponsored Product success.
Manage numerous requests concurrently and strategically, prioritizing when necessary
Basic Qualifications

5+ years of relevant experience with ETL, data modeling, and business intelligence architectures
Experience with Big Data solutions: Hadoop, Pig/Hive or EMR/Spark
Experience in relational database concepts with a solid knowledge of star schema, Oracle, SQL, PL/SQL, SQL Tuning, OLAP, Big Data technologies
Experience building self-service reporting solutions using business intelligence software (e.g., OBIEE, Tableau Server, Micro Strategy etc.).
Exceptional troubleshooting and problem-solving abilities.
Excellent written and verbal communications skills.
Demonstrated ability to work effectively across various internal organizations.
Preferred Qualifications
Experience with setting up infrastructure on AWS
Experience with scripting languages like Python or Unix shell scripts.
Experience with applied data science
Experience with web technology to develop dashboards.
Demonstrated experience in dealing with Senior Management on addressing their reporting and metrics requirements.
#sspajobs #sspasv

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation","Seattle, WA",Business Intelligence Engineer - BIE,False
672,"Data Engineer

Locations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC

At Deloitte Digital – Marketing & Experience Services (XSP), we are combining data, programmatic decisioning, and real-time delivery to build Customer Data Platforms (CDPs), complimented by a suite of client-specific managed services.

We’re growing fast and need brilliant Data Engineers like you to fuel our continuing innovation and help us scale. Along the way, you’ll find exceptional growth opportunities limited only by your hunger for learning and ability to apply best practice methodology in our start-up-like environment.
Work you’ll do

As a Data Engineer, you’ll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
You’ll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.

As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.

Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparce data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.

The team

Advertising, Marketing & Commerce

Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:
Cross-channel customer engagement strategy, design and development
(web, mobile, social, physical)
eCommerce strategy, implementation and operations
Marketing Content and digital asset management solutions
Marketing Technology and Advertising Technology solutions
Marketing analytics implementation and operations
Advertising campaign ideation, development and execution
Acquisition and engagement campaign ideation, development and execution
Agile based, design-thinking, user-centric, empirical projects that accelerate results

Qualifications

Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.

Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data

Technologies of Interest:
Languages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud – AWS, Azure, Google

How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Deloitte’s culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.

Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.

As used in this posting, “Deloitte” means Deloitte Consulting LLP, a subsidiary of Deloitte LLP. Please see www.deloitte.com/us/about for a detailed description of the legal structure of Deloitte LLP and its subsidiaries. Certain services may not be available to attest clients under the rules and regulations of public accounting.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.","Greensboro, NC",Data Engineer,False
673,"As a Data Engineer at Simon-Kucher, you will be responsible for large data management processes during client projects. You will develop and automate robust processes to extract, transform, and load large, scattered, and unstructured data sets into clean and powerful analysis cubes, which form a fundamental basis of our business recommendations. In addition, you will identify, tackle and implement internal data process and technology improvements. We're looking for
Data Engineer (m/f) – Germany
for our office in Bonn or Cologne.

Your tasks:
Generate large, multi-dimensional analysis cubes based on client data
Validate data and ensure completeness, correctness, and relevance for meaningful analyses
Develop and automate analytical tools based on the generated cubes to provide marketing, sales and pricing insights
Advise and support clients on how to model, implement and automate robust ETL processes
Work with internal and external business and technology experts across the world
Detect, solve, and implement internal process and technology improvements, as well as create, optimize, and maintain new and existing data transfer channels
Your profile:
Proven capabilities to build and maintain large and complex data sets
Experience in structuring and automating ETL processes based on scattered and unstructured data sources
Experience with relational data management software and languages (esp. SQL)
Extensive knowledge of analytical/BI software and languages (e.g. Tableau, Python, R, SAS)
Familiarity with big data technologies (e.g. NoSQL databases, distributed file systems, or stream processing technologies) are a strong plus
Sharp analytical and problem-solving mindset
Pro-active, reliable attitude and enthusiasm for working in teams
Fluency in English; additional language abilities are a major advantage
Master’s or bachelor’s degree (business, economics, computer science, or engineering preferred)
What we offer:
Possibility to work in a new, fast-growing team, comprised of young, dynamic individuals
Possibility to shape and implement new technologies within our organization and those of our clients
Frequently changing project assignments; no year-long IT implementation projects
A large variety of projects centered around sales, marketing, and pricing across different industries
Highly international role and challenges
Corporate culture led by our entrepreneurial spirit, openness, and honesty
Simon-Kucher & Partners
Simon-Kucher & Partners is a global consulting firm specializing in TopLine Power®, which encompasses strategy, marketing, pricing, and sales. Our practice is built on evidence-based, practical strategies for profit improvement via the top line. Simon-Kucher & Partners is regarded as the world's leading pricing advisor and thought leader.

Your personal contact:
Dorothea Hayer
Willy-Brandt-Allee 13
53113 Bonn
Tel. +49 228 9843 351
E-Mail dorothea.hayer@simon-kucher.com",Oregon,Data Engineer (m/f) – Germany,False
674,"$120,000 - $145,000 a yearAs a Data Engineer working for one of the world's most well-known news organizations, you will be working with newer technologies like Spark, GCP/Google Cloud, etc. Sound interesting? Keep reading...

What is the Job?

You will be an individual contributor, moving large amounts of internal data into our data warehouse. This will not be an architecture role.

We are using Google Cloud Platform after going through a number of different data storage solutions. You should be someone looking to work with newer technologies and the cloud.

We want to know what are visitors are interested in, what they do on the site, and what they like to do with our content. Therefore, you will be helping to organize and store data from a variety of sources so we can put together an understanding of a single view of our visitors.

What Skills Do We Need?


Our biggest tools are Python and Spark, so you should know these technologies.
A cloud technology background (AWS is okay, GCP preferred).
A good background working in ETL

Who Are We?

We are a world-renowned multimedia news, information and entertainment company located in Midtown Manhattan near Penn Station. We have a large portfolio of web properties and print outlets, reaching tens of millions of people worldwide daily.

Compensation


$120,000 - $145,000
Generous Paid Vacation and PTO
Full Benefits

Whats In It For You?

Our office is located conveniently in Midtown Manhattan. Our headquarters is a state-of-the-art, naturally lit, ecologically friendly workspace featuring a cafeteria, coffee bar and access to all the major modes of mass transportation.","New York, NY 10018 (Clinton area)",Data Engineer (Media),False
675,"Founded in 2001, Qlarion is a dynamic and rapidly growing consulting firm focused on providing Analytics and Data Management solutions to Public Sector and related organizations. We are looking for smart, well-rounded technologists that are passionate about helping our long-term clients use Business Intelligence and Analytics to achieve their missions.Qlarion offers a “life friendly” work environment that is both challenging and rewarding. While our consultants develop cutting edge solutions for our clients, we also encourage them to explore innovative ideas and technologies that complement our company’s vision. Qlarion provides career mentoring as well as defined technical and functional career paths that allow employees to become more empowered and prepares them for leadership roles. Each consultant has an annual dedicated training budget and opportunities for ongoing training and certifications to ensure they remain on the leading edge.Qlarion provides an extensive package of large company benefits including healthcare, vision, dental, 401k matching, flextime, and PTO among others.Qlarion is seeking an experienced, well-rounded Data Engineer for the Washington D.C. area. The desired candidates must meet the requirements below and will be compensated based on their qualifications.Bachelor’s Degree in Computer Science, Engineering or related field3+ years of experience configuring and supporting advanced analytical techniques in a multi-node Linux computing environment or multi-node cloud computing environments (i.e., High-Performance Computing environment).3+ years of experience in writing shell scripts (e.g., bash) to support configurations of advanced analytical toolkits on multimode environments.3+ years of experience supporting and tuning python code in a multi-node computing environment. Experience in R or Java preferred.Experience utilizing Git-based code repositories.Experience configuring natural language processing (NLP) toolkits.Experience configuring machine learning toolkits including but not limited to TensorFlow, keras, and scikit-learn.Experience using and/or managing an Anaconda Python environment.Experience in writing queries and transformations with SQLExperience with the Hadoop ecosystem to support high-performance computing environments (preferred).Experience working in an Agile or Scrum-based environment (preferred).Strong analytical skills and problem solving skillsMust possess excellent written and oral communication skillsResults-driven with the ability to take initiatives, handle multiple tasks and shifting priorities and meet deadlinesMust have a strong ability and desire to assimilate and apply knowledge as well as to spread acquired knowledge and experience to other team membersTo learn more about Qlarion visit our website at www.qlarion.comQlarion will not accept resumes that are more than 5 pages long.Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.Job Type: Full-timeExperience:supporting: 3 years (Required)","Washington, DC",Data Engineer,False
676,"$100,000 - $120,000 a yearContractA large Health start up that specializes in streamlining health insurance claims and finding solutions to get patients money sooner just refactored their entire tech stack to make it more scalable.
With that they are going to need skilled Data Engineers to make sure the terabytes of data being collected is pipelined into the proper channels for the data scientists and nontechnical-folk alike to be able to utilize it. So, in addition to being skilled technically, creating a system that can easily be accessed by anyone.
Check the required skills and if this is something that sounds exciting, send me your resume and let’s talk!
Required Skills & Experience
Around 3 years of Python experience
Spark for processing large repositories of data
Comfortable with AWS Lambdas and server less architecture
Understanding of web development (Django or Flask)
Data ingestion
Batch file processing
Desired Skills & Experience
Healthcare experience is a big plus
Experience helping companies scale existing structures is a plus
Automation is a plus
What You Will Be Doing
Tech Breakdown
50% data ingestion
50% data pipelining
Daily Responsibilities
80% Hands On
20% Team Collaboration
The Offer
Competitive Salary: Up to $120K/year, DOE
You will receive the following benefits:
Medical Insurance & Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit


Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.
This position does not offer sponsorship.
Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) across 10 major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients","Chicago, IL",Data Engineer (Downtown),False
677,"Lime is a technology company that is changing how people get from point A to B via our fleet of shared Lime-S electric scooters and Lime-E e-assist bikes. We're empowering our communities with new mobility options that are clean, affordable, and a ton of fun!

As a Data Engineer at Lime, you will own the infrastructure that processes every single data point we collect. We're building an on-demand service that stretches into the physical world, so you will be presented with a vast array of exciting, open-ended data challenges. You will lay the groundwork for how our entire organization runs data pipelines, trains statistical models, and turns analytical reports into concrete actions. Your work will directly drive many of our key business decisions. You'll be working alongside a group of engineers, designers, PMs, and operators from top schools and with experience building systems and apps at companies like Facebook, Twitter, Uber, and Square.

Responsibilities
Ensure system uptime for all of our data infrastructure (i.e. our central data warehouse, ETL systems, and realtime processing)
Democratize data within the organization by formulating and internally distributing best practices for building data pipelines
Jump into at least one engineering/product unit to build data pipelines and produce actionable insights for that team
Embrace your role as a founding member of our data infrastructure team
Requirements
You have a degree in Computer Science from a top engineering school, or equivalent technical background
You possess 2+ years experience in the industry, via internships or full-time positions
You have experience working with big data technology like Redshift, Snowflake, Hadoop, or Hive
You understand how to harness realtime data streams using technologies like AWS Kinesis, Kafka, Storm, or Spark
What We Offer
Opportunity to revolutionize transportation in your hometown with the leader in urban mobility solutions
A position that offers a variety of career and resume building experiences with the fastest growing startup of 2018
You will have the chance to scale with a rapidly growing organization, with tons of opportunity for growth
Play a role in the transformation of urban mobility and sustainability
Work with a team of fun and motivated people
Competitive salary and benefits
We here at Lime strive to build a workforce comprised of individuals with diverse backgrounds, abilities, minds, and identities that will help us to grow, not only as a company, but also as individuals. Lime is an Equal Opportunity Employer.","San Francisco, CA","Software Engineer, Data",False
678,"Note: This position is open to full-time/W-2 candidates only (no CTC).ResponsibilitiesFull lifecycle application developmentDesign, code and debug softwarePerform software analysis, risk analysis, reliability analysisParticipate in software modeling and simulationIntegrate new software solutions with existing systemsExtract and reverse engineer existing codePerform regular status reviews of problems/issuesParticipate in the development or refinement of proactive services and/or data repositoriesQuery database to provide data extractsQualificationsRequired:5+ years of development experience in at least one of these languages: Java, Python, or C#.Experience with RESTful API design and implementation.Experience in Workflow and BPMN development using Activiti, Camunda, etc.Experience with Data migration, transformation, and scripting.Proficient understanding of code versioning tools, such as Git.Ability to work cross-functionally with engineering and non-engineering teams.Passionate about engineering quality, testing, automation, and documentation of code and systems to ensure easy maintenance over a long period.Experience working on high-volume server software.Understanding and implementation of security and data protection.User authentication and authorization between multiple systems, servers, and environments.Experience with NoSQL databases such as DynamoDBExperience with data frameworks such as Hadoop, Hive, Pig, and SparkDesired:Experience with MuleSoft application developmentExperience in building high-performant, scalable backend services in the cloud (especially AWS)Passionate about application scalability, availability, reliability, and security.Exposure to Test Driven Development, Behavioral Driven Development frameworks and librariesExperience with Collibra, administration, management, working with APIsExperience with Anypoint APIsAbout GalaxE.SolutionsGalaxE.Solutions is a global IT services firm headquartered in Somerset, New Jersey with over 2,000 team members worldwide. Our Downtown Detroit office overlooks the beautiful Campus Martius Park and is rapidly expanding to serve some of our largest and fastest-growing clients within multiple industries, including financial services, gaming, energy, healthcare, sports, and entertainment.Be a part of an energetic and fast-moving team that delivers incredible, innovative software systems. GalaxE’s Software Engineers take pride in Software Craftsmanship and a polyglot engineering approach to solving tough challenges. GalaxE’s application development teams are small (6-10 team members), agile, and lean-focused with a passion that goes beyond the workplace. Learn more at galaxe.com.Equal Opportunity Employer/Veterans/DisabledID: 2018-8224Job Type: Full-timeExperience:hands-on Java, Python, or C# development: 5 years (Required)Work authorization:United States (Required)","Detroit, MI 48226 (Downtown area)",Sr. Big Data Engineer (Full-Time),False
679,"SQL Server DBA (Data Engineer)

The Data Engineer will use smart technology to empower stakeholders enterprise-wide to make the best decisions based on timely, accurate and actionable information. Must enjoy working with SQL Server and Big Data Technologies. The Data Engineer will have the chance to work with the best data and massively parallel processing databases to speed analyses, with broad exposure to the workings of TeamHealth and critical thinking to tackle complex challenges. When other teams need to know about providers, productivity, performance, or other data insights, they count on our Data & Analytics team.

ESSENTIAL DUTIES AND RESPONSIBILITIES:

Part of a progressive, technology-focused team that is dedicated to identifying and delivering solutions to our customers and employees with smart technology and teamwork;
Embody our company values along with Agile and lean software development principles;
Proactively engage in both internal and external learning opportunities;
Responsibility for the full development life cycle of the solution, including detailed design, code development, code reviews, unit testing, build/test support, data quality monitors, deployment activities and post-deployment support;
Build great leadership skills by managing and optimizing the EDA team ETLs along with analytic operations: job-stream definition and management, parameters, scheduling, monitoring, communication and alerting;
Juggle multiple project timelines simultaneously and estimate development duration and effort for all projects;
Communicate openly and candidly with your Team Leader and other teammates verbally and written;
Viewed as a mentor by others in and outside the team for technical solutions;
Hands on with Big Data management activities including acquisition, ingestion, transformation, and data quality.

QUALIFICATIONS / EXPERIENCE:

Bachelor’s degree preferred;
At least 8 years experience of all 4 of the following BI disciplines: Data Integration, Modeling, Analytics or Reporting;
10 years of SQL experience with the ability to write complex SQL statements while mentoring optimal code hygiene practices;
Closely involved in learning and teaching industry trends and practices along with your past experiences;
A record of accomplishment of designing, deploying and evolving ETL solutions to meet business requirements via creating SQL and SSIS best practices for multiple platforms;
Proven analytical, troubleshooting and problem-solving skills and ability to communicate well, even to non-technical users;
Built rock solid relational databases solutions along with designing, implementing, testing, migrating, deploying and managing ETL solutions: Core functional ETL processing (extract transform, and load);
Built solutions by leveraging advanced testing techniques and implemented proper audits, alerts, and notifications that proactively measures failures before they happen along with mentoring others to do the same;
Strong desire to teach and mentor other developers along with proven ability to work autonomously or in a team setting ;
Microsoft BI Stack, SSIS, SSAS, Netezza, and MicroStrategy is required;
Agile and Big Data experience is preferred.

PHYSICAL / ENVIRONMENTAL DEMANDS:

Job performed in a well-lighted, modern office setting;
Occasional lifting/carrying (10 pounds or less);
Occasional standing/bending/stooping/reaching;
Moderate stress;
Prolonged sitting; and
Prolonged work at a computer/PC.

","Knoxville, TN",Data Engineer,False
680,"We invite you to explore a future with us at PRA Group, a diverse and growing company that has a tangible impact on the global economy.

Position Summary:


Proactively contributes in designing, developing and evolving high quality, usable and extensive Data Engineer solutions. Under supervision, perform ad-hoc and routine tasking.

Working closely with other PRA Group Data Engineer, Software Engineers, and reporting to the Manager, Data Engineering, the Data Engineer will engage internal customers at all levels, develop ad-hoc and continuous enterprise level reports, and contribute to new and existing analytical studies. As a member of the enterprise Data and Analytics team, this position will document key protocols, procedures, and system infrastructure as part of an ongoing data stewardship/governance effort of the Data and Analytics team.
Key Responsibilities (other duties may be assigned):


Employing various tools (SQL, shell, R, Python, etc.) to complete data engineering requests.
Develop, modify, and implement enterprise level reports
Assist in the documentation of new and existing data stores, to assist in the administration and maintenance of the enterprise data governance program.
Assist in provide expertise on statistical and mathematical concepts used by the broader analytics team.
Assist with advocating, evangelizing, and building data driven solutions to help PRA Group meet our yearly net income goals.

Supervisory Responsibilities:
None


Professional Experience/Qualifications:
Bachelor’s degree in Math, Statistics, Computer Science, Economics, or related quantitative disciplines
1+ year(s) experience in the following:
SQL
GitHub
Unix/Linux
Strong written and oral communication skills
Ability to effectively juggle multiple development, administrative, and support projects simultaneously.

Preferred
Database:
SQL Server
SSMS command line tools (BCP, SQL Command, etc.)
Oracle 11g, 12c
Sqlplus*
Business Objects
Open Source tools (including, but not limited to):
R
Python
Data Warehouse dimensional modeling concepts (star, snowflake schema)
Transactional Entity-Relationship modeling (ERM)
Dashboard report development

Work Environment:
The noise level in the work environment is usually moderate as the employee works in a collaborative office environment, in an individual work station, using telephone and computer. Employee may be required to occasionally work evenings and weekends. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.



Disclaimer:
The above information on this description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job.


All qualified applicants will receive consideration for employment regardless of age, race, color, sex, gender, religion, national origin, physical or mental disability, citizenship, or any other classes recognized by state or local law or any other characteristic protected under applicable federal, state or local law. We are a drug free workplace.","Norfolk, VA",Data Engineer I,False
681,"At Fitbit, our mission is to help people lead healthier, more active lives by empowering them with data, inspiration and guidance to reach their goals.

We started our journey in 2007—as a team of two with one big idea. Since then, we've grown to over 1,500 employees, sold over 60mm devices, and built a health and fitness community across the globe. In fact, the Fitbit Community has taken enough steps to walk from the Sun to Pluto! Offering award-winning products, a top-rated mobile app and an easy-to-use online dashboard, Fitbit provides personalized experiences that help our users reach their goals. With a reenergized focus on innovative devices, interactive experiences, and enterprise health we are transforming the way consumers and businesses see health & fitness.

From your first steps as a Fitbitter, you will be at the forefront of developing new products. Our culture combines the spirit of startup with the perks of being public. We offer a competitive benefits package and amazing perks like unlimited snacks, Friday happy hours, onsite workout classes, and a strong focus on a healthy work-life balance. As part of our team, you'll have the opportunity to grow your career, contribute your ideas to life-changing products and services, and—above all—have fun doing it.

Fitbit's HQ campus is located in the heart of San Francisco with office locations in Boston, San Diego and around the world. Think you've found your fit?

What will you do?

As a member of the Data Warehouse team at Fitbit, your primary responsibility will be development, maintenance and operational stability of ETL pipelines which feed the Fitbit data warehouse. This is an excellent opportunity for a recent graduate (or experienced software engineer) looking to get into the field of Data Engineering. Day-to-day activities include:


Development of scalable, resilient data pipelines in the cloud
Upgrading legacy pipelines to leverage latest technologies
On-call duties to ensure data pipelines are meeting our SLAs

Our ideal Data Engineer for the Data Warehouse team will have experience in most (but preferably all) of these skills and an interest in learning them all:


Development with Spark/Scala and Python
ETL best practices
Database concepts (Relational and NoSQL), including a working knowledge of SQL
Cloud environments (AWS, Google)
Linux environments

In addition, an individual joining our team in any capacity must:


Take personal responsibility for driving initiatives forward
Work effectively with stakeholders around the globe
Display top notch written and verbal communication skills

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","Boston, MA 02210 (South Boston area)","Data Engineer, Data Warehouse",False
682,"Required Skills and Experience:

Expert-level Java development experience, including common backend frameworks.
Hands-on development with key AWS technologies, including AWS Batch, AWS Step Functions, and AWS Lambda.
AWS Certified Developer or other AWS certification.
Experience with highly-scalable and distributed databases and the key issues affecting their performance and reliability.
Experience using high-throughput, message queueing systems such as AWS SQS.
Familiarity with devops technologies, including Docker and common configuration management tools
An ability to periodically deploy systems to cloud environments such as AWS.
Mastery of key development tools such as GIT, and familiarity with collaboration tools such as Jira and Confluence or similar tools.
Overall, 5+ years of industry experience developing backend components with a recent focus on big data systems.
Bachelors degree or equivalent required, Masters degree preferred

Job Responsibilities:

Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.
Analyze and support requirements from machine learning specialists, focusing on the most important topics in the rapidly evolving automotive industry.
Move architecture and implementation through the automotive development pipeline, from research to deployment in millions of cars on the road.
Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.

","San Jose, CA",Big Data Engineer,False
683,"The communications research group at Syneos Health is looking for a data engineer with experience working with healthcare or media data sets. You will build and manage databases and establish workflow pipelines that lead to API endpoints in support of data science tasks. You will also manage code libraries, automate database updates, and build and validate new methodologies for data set QA.

About us


Syneos Health Communications is a network of PR, communications, branding, and advertising companies that help bring new medical products to market.
We are expanding the group that helps the company’s clients - biopharma companies - make better marketing decisions through advanced uses of data. You will be instrumental in identifying the right approaches, tools, and methods, as well as opportunities to put them to use.

Types of Projects
Among the bigger problems you will help solving is optimizing and plugging large historical data lakes into disjoint public and commercial data sets to build models for:
Identifying factors that influence patients’ adherence to a treatment regimen
Ranking a list of physicians by the driving distance between their offices
Creating the shortest route for a traveling salesman that will result in the largest number of conversions

It is meaningful and challenging work in a team of supportive and bright colleagues. A lot of things will have to be invented and built from scratch. You will not be bored.
Responsibilities
Get things done:
Establish ETL for both structured/unstructured data sources from internal/external sources
Manage and create performance/error/analytics systems and processes for QA of all data sets
Create dashboards and API data access tools for both technical and business users
Manage and grow our network of data and research partners by finding and evaluating new suppliers and offerings

Make us better:
Introduce best practices for database design, processing, and workflows
Extend our capabilities by helping to build efficient and scalable frameworks
Share your knowledge through training others, evaluating new tech, and building our documentation library
Job Requirements
We are looking for someone who has done similar work elsewhere. You will need to be good at:

Data management
Integrating data sources and schema design
Querying, troubleshooting, and designing SQL and NoSQL databases
Working directly with a variety of stakeholders to evaluate project needs
Working with common cloud data repositories (AWS) and versioning systems (Git)
Building processing pipelines between remote data lakes and local data warehouse

Analysis and optimization
Making use of data visualization strategies for data QA to develop internal and end user dashboards (JS libraries and Tableau or similar)
Identifying and maximizing data delivery methods specific to various end-user types

What it will take to succeed here

Ship: Executing the day-to-day tasks; delivering projects on time and within parameters

Background and training. Degree in information/library/computer sciences, operations research, physics, engineering field and/or relavent work experience

Skills: 2-3 years with SQL, NoSQL, Python (or similar systems), Spark

Independent problem-solving and grit. Willingness to own one’s work, and confidence to push best practices.

Obsessive accuracy when it comes to numbers.

Drive: Identifying and pursuing the most impactful opportunities. Requires an entrepreneurial, self-directed attitude, creativity, and familiarity with the business context (healthcare marketing).

Grow: Being able to solve problems of increasing complexity. Requires awareness of gaps in one’s own skills and knowledge, and a motivation to fill them; lots of self-directed learning. You will have access to whatever online tutorials, textbooks, and reference materials you need.","Boston, MA","Data Engineer, Health",False
684,"Description:
Circinus seeks an experienced Data Engineer to manage process and protocol in a state of the art data analytics center in Abu Dhabi UAE. The Data Engineer will develop, implement, manage and improve data administration for a rapidly growing intelligence and analytics company with worldwide presence. This is a full-time position with flexible hours in Abu Dhabi. Occasional travel between UAE and the US may be required. This position includes company-paid housing and a comprehensive benefits package

The Data Engineer will work with a team of talented IT professionals, data scientist and data analysts to ensure the security and integrity of data and unhindered, encumbered or delayed access to data by the analyst team, as well as continuous access to data during continuity of operations (COOP) events. The Data Engineer is a direct report to the Director of the analytics center in the UAE. Because this is a small team, the Data Engineer will be cross-trained and be expected to assist in performance of a range of other supporting functions related to IT including enterprise cyber and anomaly detection.

The Data Engineer's responsibilities include:

Work with the Circinus analytics architecture team to design, implement, enhance and optimize data ingest and ETL
Create, design and maintain reusable datasets for analysis by data scientists and analysts
Assess new data sources to better understand availability and quality of data
Provide governance and best practices of data structures, data integrity, and querying
Interpret business needs from requests, and rapidly implement effective technical solutions
Write SQL queries to answer questions from the data science team
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.)
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs
Automate and improve creation/maintenance of reports and dashboards.

Requirements:

Programming skills in Javascript, Python, SQL/NoSQL and DB2 in both a Windows and Linux operating environment.
Database design and maintenance, including designing new tables and relationships, proper database normalization, and tuning using database indices
ETL (Extract, Transform, Load) development, including merging and normalizing related data sources
Data visualization skills using Javascript and other tools/languages
Technical skills in MVC frameworks, Web API design, natural language processing (NLP)
Must possess the ability to communicate clearly, concisely, and with technical accuracy in both oral and written modes
Must be able to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to detail
Must be able to work in a collaborative, team environment
Bachelors Degree in Engineering, Mathematics, Operations Research, or Computer Science, or related technical field
Current driver’s license and US Passport

","Arlington, VA 22209 (Radnor-Ft Myer Heights area)","Data Engineer - Abu Dhabi, UAE",False
685,"At Skillshare, we’re building a platform to connect curious, lifelong learners everywhere. Our teachers and students are constantly interacting with the platform (and with each other!), and the data we capture plays a huge role in driving business decisions and building an informed product. As Skillshare’s first Data Engineer, you’ll build and own the home for this important data and create robust systems to support analytics and modeling needs.

We’ll look to your strategic expertise, reliable execution, and sound judgment to lead our data warehousing efforts and instrumentation to ensure our team has consistent, reliable access to the inputs they need. This role is highly collaborative – you’ll work closely with engineers, data scientists, product leaders, and data stakeholders across the company.
What you'll do:
Design, build, and own our data warehouse (and the associated ETL pipeline)
Ensure that we have robust instrumentation and logging by implementing key pieces as well as providing leadership to the team to disseminate best practices
Collaborate with engineers to optimize queries that power our core application
What you'll need to be successful:
Hands-on experience building and maintaining data warehouses from scratch
Experienced with relational databases and queueing systems (we use MySQL, Redshift, FluentD, AWS Kinesis, Elasticsearch, etc.)
Passion for data and industry knowledge – you can make informed recommendations about the best tools for our needs, and lead the implementation
Strong communication skills – you’re a natural collaborator and can report out to stakeholders of all levels
Ability to balance strategy and execution
Why you want this job:
Impact: harnessing and accessing the right data is at the heart of our growth, and this role plays a key role in enabling us to do so.
Our mission: We are building a learning ecosystem for the new economy and changing millions of lives for the better.
Our team: We have a passionate, smart team that is a lot of fun to work with.
Your life: We take pride in our flexibility. Need flexible hours, or work a day or two remotely? No problem. We trust you to do what you need to do.
About Skillshare

Skillshare is an online learning community whose mission is to connect curious, lifelong learners everywhere – and, in so doing, build a more creative, more generous, and more prosperous world. Today, our community has grown to over 5 million members who come to Skillshare to learn creative and entrepreneurial skills, network with peers, and even teach a class themselves. We are backed by Union Square Ventures, Spark Capital, Amasia, Spero Ventures, and Burda Principal Investments.

We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Apply for this job","New York, NY",Senior Data Engineer,False
686,"ContractWe are committed to bringing passion and customer focus to the business.


Job Summary

OMNIGON is looking for a Data Engineer to build, maintain, monitor, and improve a real time scalable, fault tolerant, data processing pipeline.


The Data Engineer will support the building of a Redshift-based data mart, implementing ETL processes and integrating with various marketing platforms (SAS, Salesforce) and other systems.


This is a remote contract position located in the United States.



Job Requirements

4+ years of data engineering or related experience
Strong Java and/or Scala experience
Experience with AWS services including S3, Redshift, EMR, Lambda, and RDS
Experience with stream processing using Spark Streaming/Storm/Beam/Flink
Experience with messaging systems, such as Kafka or Kinesis
BS in Computer Science, or a related field
Excellent communication skills

Nice to Know
Experience with NoSQL databases such as MongoDB, Cassandra, or DynamoDB
Experience with Elasticsearch
Experience with Machine Learning using Mahout/Deeplearning4j/Spark ML

Responsibilities
Implementing ETL processes
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies
Collaborate with team members to help shape requirements

About OMNIGON

OMNIGON, an Infront Sports & Media Company, is a team of digital strategists, artists and technologists working exclusively in the areas of consumer loyalty, audience growth and digital content delivery. Since its founding in 2008, OMNIGON has established itself as a market leader, focused on helping clients achieve returns on the strategic, creative and technical investments they've made. OMNIGON, headquartered in New York and with teams in Los Angeles, London, Toronto, Kiev and St. Petersburg, works with celebrated, global brands including AS Roma, PGA TOUR, NASCAR, CONCACAF, the United States Golf Association (USGA), International Champions Cup, StubHub, Under Armour, Legends, FOX Sports, the German Football Association (DFB) and countless others.


Are you ready to contribute to transforming and enhancing every aspect of the sports industry? Unite your passion for sports with a rewarding career by joining our ONE team of talented, highly-motivated and hard-working individuals. Please submit your online application now.

Infront is an equal opportunities employer.","Los Angeles, CA",Data Engineer,False
687,"Description
JOB SUMMARY

The job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets. Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed. The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Works in a team to leverage experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks. Ability to work in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Junior level staff. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica.

ESSENTIAL RESPONSIBILITIES
Work closely with IT, architect and engineer solutions that provide views for the Enterprise Data Hub or other analytic ecosystems. This includes working with the appropriate teams, building out the design, and providing upkeep for the solution. Create high performance Big Data (and traditional) systems to be used with analytic applications.
Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT)
Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
Responsible for delivery of assigned projects, this may include providing guidance and Junior contributors within team. Will attend meetings with customers as needed.
Assist with the establishment of standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work.
Other duties as assigned.
EDUCATION

Required
Bachelor's Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems
Substitutions
None
Preferred
Master's Degree Management Information Systems, Healthcare Informatics or Computer Engineering
EXPERIENCE

Required
3 - 5 years in Analytics
1 - 3 years in IT Application - Hadoop
Preferred (any of the following)
3 - 5 years in Data Warehousing
3 - 5 years in the Healthcare Industry
1 - 3 years in Database Administration
LICENSES AND CERTIFICATIONS

Required
None
Preferred
None
SKILLS
SQL
Data Warehousing
Problem-Solving
Communication Skills
Analytical Skills

Language (Other than English)
None

Travel Required
0% - 25%

PHYSICAL, MENTAL DEMANDS and WORKING CONDITIONS

Position Type
Office-Based

Teaches / trains others regularly
Occasionally

Travel regularly from the office to various work sites or from site-to-site
Rarely

Works primarily out-of-the office selling products/services (sales employees)
Never

Physical work site required
Yes

Lifting: up to 10 pounds
Constantly

Lifting: 10 to 25 pounds
Rarely

Lifting: 25 to 50 pounds
Rarely

Disclaimer: The job description has been designed to indicate the general nature and essential duties and responsibilities of work performed by employees within this job title. It may not contain a comprehensive inventory of all duties, responsibilities, and qualifications required of employees to do this job.

Compliance Requirement: This job adheres to the ethical and legal standards and behavioral expectations as set forth in the code of business conduct and company policies.","Pittsburgh, PA 15222 (Strip District area)",Big Data Engineer,False
688,"A leading quantitative investment company focused on computer-driven trading in global financial markets is searching for a talented quantitative data engineer to join the team. The business is a team of researchers, engineers, and financial industry professionals using sophisticated statistical models to analyze data, identify predictive signals, and generate superior investment returns in a collaborative environment. Their investment teams each focus on their independent strategies while utilizing the firm’s proprietary, state-of-the-art technology and data platform to optimize their alpha research. They are passionate about implementing scientific and mathematical methods to explore, isolate, and solve problems in the global financial markets. They believe that career fulfillment and enterprise success converge when smart, hard-working and intellectually curious people come together with a shared goal of innovation and the pursuit of excellence.Job Summary: The Quantitative Data Engineer will join a small, high performing team focused on trading global markets in a completely automated fashion. This role is an exciting opportunity for a strong technologist looking to get exposure to several aspects of quantitative trading with wide ranging responsibilities across data, research infrastructure, real-time trading, and operations. The team believes in rewarding people on merit and excellence, not necessarily on experience. Work is fast-paced, decision making is efficient, and changes are quickly implemented.Responsibilities: Design and maintain data pipelines that fuel research and production tradingBuild machine learning / data-oriented toolsDevelop and support applications for all aspects of an automated trading system - order handling, risk, operations reportingAnalyze trading data to optimize and improve trade execution performanceDay to day maintenance and monitoring of complex trading algorithms in a real-time environmentCommunicate with internal groups and external counter parties to solve trading issuesDynamically adapt to evolving business needsCandidate Attributes: Keen interest in quantitative trading and metrics driven decision makingDetail oriented and a passion for spotting trends in dataDesire to tackle challenging problems under tough deadlinesOther Requirements: Strong programming skills in Python, C++, KDB+ (good to have)Familiarity with vector programing and functional languagesBasic knowledge of statistics, machine learningComfortable on the linux command linePractical knowledge of operating system and networking basics2 - 5 years of work experience in a fast-paced environmentTakes pride in well documented systems and can communicate their work to the rest of the teamJob Type: Full-timeEducation:Bachelor's (Required)","New York, NY",Quantitative Data Engineer,False
689,"ContractData EngineerDuration: 2+ YearsLocation: Rockville, MD and/or Baltimore, MDClearance:  Ability to obtain a Public TrustSome local travelThe Sr. Data Engineer's mission will be to guide this platform’s development so that it becomes the engine of a healthcare-wide transition to value-based care by enabling effective data sharing, analytics, performance transparency, and value-based payment.Responsibilities: Expected to provide hands on software development support for a large data warehouse project, hosted in a cloud environment.Provide architectural guidance and oversight to projects within a team.Be a mentor to junior and mid-level engineers.Enforce high-quality coding standards and practices via reviews and by demonstrating this in their own work.Be able to help others break down large team goals into specific and manageable tasks.Be involved and supportive of agile sprint model of development, helping to enforce the practice and the discipline.Able to work efficiently and pro-actively across engineering teams to enable us to deliver on our goals of loosely coupled adaptable, scalable solutions.Have a good understanding of where their project fits into the larger goals for engineering and adapts their work so that the priorities of the systems they are creating match those of the organization.Qualifications: Demonstrated expertise with Java, Scala, and PythonExperience software development in a team environmentExperience with one or more software version control systems (e.g. Git, Subversion)Experience software development on a team using Agile methodologyFamiliarity with DevOps tools and techniques (e.g continuous integration, Jenkins, Puppet, etc)Experience with cloud infrastructure (e.g. Amazon Web Services)Experience with big data processing frameworks (e.g. Spark)Preferred experience with big data tools (e.g. Databricks)Very strong SQL skills, experience with Parquet filesExceptional communication and listening skills, with the ability to break down problems and generate shared understanding across technical and non-technical audiencesExperience in enterprise software or healthcare-related domains.Preferred knowledge of Medicaid & Medicare and Databricks - notebooksEDUCATION: Bachelor's Degree in Computer Science, Statistics, Applied Math or related fieldJob Type: Contract","Rockville, MD",Data Engineer,False
690,"About Capgemini

With almost 200,000 people in over 40 countries, Capgemini is one of the world's foremost providers of consulting, technology and outsourcing services. The Group reported 2017 global revenues of USD 15.78 billion. Together with its clients, Capgemini creates and delivers business and technology solutions that fit their needs and drive the results they want. A deeply multicultural organization, Capgemini has developed its own way of working, the Collaborative Business ExperienceTM, and draws on Rightshore®, its worldwide delivery model. Learn more about us at http://www.capgemini.com. Rightshore ® is a trademark belonging to Capgemini Rightshore® is a trademark belonging to Capgemini
 Capgemini America Inc is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.



Job Responsibilities

Job Title: Data Engineer
Location: Charlotte, NC
Duties & Responsibilities:
The resources must have domain technical experience in delivering data
engineering solutions using data lake technology and experience working with
our IT support team.
Experience with the following: Hadoop (CDH), relational databases and SQL,
ETL development, spark, data validation and testing (Data Warehousing,
ETL/ELT to the Data Lake, Using the Data Lake for data analysis (Hadoop tools
Hive, Impala, Pig, Sqoop, Hue, Kafka, etc., Python, R, java, Docker, Dakota).
Knowledge of Cloud platform implementation (Azure or Amazon). Knowledge of
data visualization tools is also a plus (AXA US uses Tableau on multiple
platforms along with Python visualization in the Data Lake using Pandas and
bokeh packages)
Experience with collaborative development workflows (e.g., Microsoft VSTS)
Relevant technical skills include applied mathematics, statistics, calculus,
quantitative or statistical methods or techniques, data mining, informatics,
machine learning, data science, programming, computational algorithms,
databases, artificial intelligence, natural language processing, bayesian
inference, Markov logic, java, software engineering and/or systems design
analysis.
Excellent written, verbal and interpersonal skills, a must as there will be
significant collaboration with the business and IT",North Carolina,Data Engineer,False
691,"Position Title: Data EngineerExperience: Three to five years experience with data warehousingStatus: Full Time EmployeeJob Location: Arlington, VirginiaBusiness Hours: 9:00-5:30 M-FStart Date: ImmediatelySalary:  Commensurate with ExperienceTravel: 0% - 5%Please include cover letter explaining pertinent data engineering experience. Resumes without cover letters will not be taken into consideration.Company OverviewMorten Beyer & Agnew (mba), established in 1992, is an international consulting firm specializing in transaction advisory, valuation, and strategic analysis within the commercial aviation industry. mba provides support to investment banks, aircraft owners, operators, investors, lessors and governments on their respective involvement with commercial aviation. More information can be found at www.mba.aero.Position DescriptionYou will be part of a world class Digital Services team in the Market Intelligence group supporting data services and other product offerings of Morten Beyer & Agnew, and supporting a data warehouse process used by a global community of aviation professionals and the various groups as stakeholders internally. The team is responsible for ensuring the health of the data warehousing efforts internally, supporting reliability across the various systems, high performance and high availability for the clientbase.The database engineer will report to the Managing Director on topics such as the development and organization of the databases, assessment and implementation of new technologies, and providing the IT group with a long-term perspective on the relationship of database technology to the business opportunities facing the company, to help achieve a state-of-the-art environment that meets current and future objectives. This will include providing technical support and administration of all database installations, designing and scheduling.Qualifications & Responsibilities – Applicants should be able to address all the following criteria. RequirementsBachelor's degree in Computer Science, Information Systems or Engineering3 - 5 years of hands-on database administration, support and performance tuning experiencePrior SDLC experience, or experience in a start-up or Agile or Dev Ops environment is a plus.Advanced working knowledge of SQL and PL/SQLExperience with Pentaho, Talend or any other ETL tool preferredExperience with Python, Bash or any scripting language  Preferred*Knowledge of software development and user interface web applicationsAn ability to understand front-end users requirements and a problem-solving attitudeExperience with FreeBSD, LINUX, Dtrace, Go, ZFSPackage building and upgrades in a production environment ( MySQL / PostgreSQL 9.x )Experience with gitUnderstanding of virtualization and clustered environmentsInterest in Aviation IndustryResponsibilitiesDatabase administration - install, configure, upgrade and migrate existing databases (listed below)Solid knowledge of physical and logical database designSet up highly available architectureAutomate various DBA tasksOptimize performance, and monitor the data warehouse for integrity and solve outstanding issuesReview processes and identify areas of improvementTroubleshoot and optimize queries and performance bottlenecksDevelopment and maintenance of the database stored procedures, views and functions in PostgreSQL for hosted web applications * Maintaining and updating ETL process with various methods/ tools. Ie, Bash/Python,PentahoWorking closely with Development Team to partition requirement needs based on backend/frontend change requirements.Analysis and visualization of data using KPIsDevelop technical and training manualsBenefitsHealth Insurance, Dental Insurance, Long-Term Disability Insurance, Life Insurance, Flexible Spending Account (FSA), Paid Parking, Paid-Time-Off (PTO), 401K participation with company matching contribution.Job Type: Full-timeExperience:Database Administration: 3 years (Required)PostgreSQL: 1 year (Preferred)Scripting: 1 year (Preferred)SQL: 1 year (Required)Shell Scripting: 1 year (Preferred)","Arlington, VA 22201 (Lyon Village area)",Data Engineer,False
692,"Description
Perform data analytics extraction. Support the fundamental structure of applications and relational databases. Understands solutions that enables one to investigate, analyze and provide comprehensive resolution for service requests by using applicable monitoring and troubleshooting tools and techniques. Provides ongoing systems and user support of one or more computer applications. Consults in system design, build, configuration and implementation within the appropriate tools used by the solution. Utilizes technical expertise to configure the system and solve client issues. Provide advanced design and maintenance of programs using Visual Basic/Cerner Command Language/SQL. Creates scripts, reports, and data files. Supports Financial applications, including Accounts Payable, Materials Management, EDI, and General Ledger.
Education: Associate degree in Information Systems or related discipline. Experience: Ten years as a Systems Analyst and Programmer. Visual Basic/ CCL/SQL programming skills and relational database knowledge. Experience: Health care experience.
Experience:
Ten years as a Systems Analyst and Programmer. Ten years advanced programming in Visual Basic/CCL/SQL skills and relational database required. Health care experience.

Schedule: Full-time
Shift: Day shift, Mon-Fri 8:00 a.m.-4:30 p.m.
Education:
License or Experience: Experience is preferred
Location: CarolinaEast Medical Center
Department: Information Systems","New Bern, NC",Systems Data Engineer,False
693,"We are seeking a Senior Data Engineer/Scientist to assist in improving our optimization algorithms. In the same week, you could work on user-facing interfaces and reports with front-end developers, write code to import, process and QC terabytes of new data, and work with analysts and statisticians to ensure the validity of our processes. A strong understanding of data structures algorithms and software design is a must.

Required Skills

Bachelor’s degree in Computer Science or a related field (or 4 additional years of relevant work experience) • A strong understanding of data structures, algorithms, and effective software design • Significant development experience with a major modern language (e.g. Java, Scala, Python, Ruby, C/C++, etc.) • Experience with a distributed/parallel computing engine such as Apache Spark or Hadoop • Experience wrangling terabytes of big, complicated, imperfect data • Significant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMSes and data warehouses • Comfort with version control systems (e.g. Git, SVN) • Excellent verbal and written communication skills; must work well in an agile, collaborative team environment
Master’s in Computer Science or a related field • Experience with big datatechnologies such as Cassandra, Kafka, HBase • Basic understanding of statistics and experience with statistical packages such as R, Matlab, SPSS, etc. • Practical experience with supervised machine learning techniques • Experience with AWS products • Experience writing unit and functional tests
We are unable to sponsor a visa at this time - must be US Citizen no 3rd party candidates
Best,

Miranda Krayca
Senior Recruiter
Comcentric Inc.
303-993-6873","Manhattan, NY",Data Scientist/Engineer in NYC,False
694,"Yapta's mission is simple: To give our customers confidence in travel. To that end, we are one of the world’s leading companies for fare transparency and cost savings. We analyze billions of rates every month, and turn all that data into meaningful notifications and reports. We provide automated services for corporate travelers to save money, by tracking prices on airline tickets and hotels, and sending alerts when prices drop.
We were recently named to Deloitte’s Fast 500, are highly profitable, and have grown 400% in the past 4 years. Our team is fast paced and focused, but we maintain a healthy work/life balance and have fun. We value integrity, flexibility, accountability, drive, and collaboration.
Are you excited about the convergence of technology and travel? Do you want to be a part of a cohesive agile team? Do you fall asleep thinking about indexes, transforms, and solutions for streaming data ingest? Come help us build our cutting-edge data platform, powering insights used by some of the biggest brands in the world.
How you’ll succeed on the team:
You're passionate about getting at data and creating high quality, easy to consume views into that data
You continually improve by learning from others, and you jump in when a teammate could use your help
You care about your customers, and understand how your data contributes to the goals of the business
You have an agile mindset, and are comfortable refining vague requirements
You can sense miscommunications among team members, and do your part to improve understanding
You thrive in and contribute to a positive work environment, where everyone shares constructive thoughts and suggestions
Requirements: (Must have experience to be eligible for consideration.)
Minimum of 3 years of professional software development experience in a hands-on data-centric role in data engineering, architecture, streaming or warehousing is REQUIRED.
Comfortable working with a mix of structured & unstructured data from a variety of sources is REQUIRED.
Experience with at least one modern server-side language (such as Python, Java, or similar) is REQUIRED.
Experience with at least one relational database technology (MySQL, Postgres, MS SQL, etc.) is REQUIRED.
Proficient with SQL and schema design is REQUIRED.
Preferred Experience: (Very helpful skills as they will be important in this role.)
Familiarity with cloud services and infrastructure, preferably AWS
Familiarity with columnar store databases (Vertica, Redshift, Snowflake, etc.)
Experience building out data ingest pipelines
Experience building out or working within Extract, Load, Transform / Data Lake architectures
Experience with Spark, DataFrames, pandas a big plus
Experience transforming partially or fully unstructured data into more easily queryable formats
Experience tuning databases for performance
Experience working in an agile environment such as Scrum or Kanban
Knowledge of machine learning tools & concepts a plus
What we offer:
Fun, collaborative environment
Optional work-from-home Wednesdays
Competitive compensation and benefits package, including medical, dental, and vision insurance
5 weeks of PTO and 10 paid holidays (total 7 weeks)
401k
Stock options
Commuter benefits
Stocked kitchens, with coffee, soda, and snacks
Regular team activities, including Mariners games, ping pong tournaments, movies, etc.
This position is based in Pioneer Square in Downtown Seattle. Candidates must be eligible to work in the US.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","Seattle, WA 98104 (First Hill area)",Data Engineer,False
695,"Overview
Imagine a career where your creative inspiration can fuel BIG innovation. Immerse yourself in our award winning culture while creating breakthrough Retail solutions that simplify the lives of customers worldwide.

7-Eleven is expanding its social, mobile, and digital footprint with a full suite of products and services that are revolutionizing the industry. At the newly formed Digital Center of Excellence team, we are looking to foster innovation by focusing on designing experiences for customer delight and Test & Learn methodologies. The breadth and depth of these customer-driven innovations mean limitless opportunities for you to turn your ingenious ideas into reality at 7-Eleven.

Discover what it’s like to be part of a team that rewards taking risks and trying new things. It’s time to love what you do!
Responsibilities
Looking for individuals with an unmistakable passion for building elegant and intuitive data integration pipelines. Need to be opinioned and passionate about what you do both at a tech level and at a business level. Not afraid to challenge the norm.
Qualifications
3-5 years of experience in architecting and building enterprise scale systems is required.
Expertise with HDFS-based computing frameworks like Spark, Kafka, Storm OR SIMILAR are required.
Experience with R, Python, SH/Bash and JVM-based languages including Scala and Java; Hadoop family languages including Pig, Hive; high performance data libraries including Spark, NumPy, TensorFlow or similar with the ability to pick up new languages and technologies quickly.
Ability to manually and programmatically interact with data stored in traditional (relational) databases as well as No-SQL databases like Cassandra and MogoDB.
Experience in HDInsights, Azure Data Lake, Azure Data Lake Analytics and Azure Data Factory is required.
Experience in cloud and distributed systems principles, including load balancing, networks, scaling, in-memory vs. disk, etc.is required.
Marketing Technology/CRM functional expertise highly desired but not required.
Experience dealing with data in a Retail/marketing organization preferred but not required.
Internal Posting Dates: 08/28 - 9/11","Irving, TX",Big Data Engineer,False
696,"Are you passionate about delivering building robust and scalable solutions using Big Data technologies? Do you have a curious nature, always interested in how to innovate? We’re looking for someone like that to help us:
– build a Data Hub for applications supporting WMA business
– analyze data on mainframe and Big Data platform
– build relationship and work closely with developers, data modelers, data stewards and data governance teams
You'll be working in the WMA IT team in Weehawken, NJ. We are a team of open minded, highly motivated and forward thinking professionals who care about strategy and quality. You will be cooperating with multiple teams located in US and in India working on a strategic data architecture platform. We are the central data hub for core WMA data, the single version of the truth, providing data for downstream processing, reporting and analytics. All teams are highly skilled and capable in Big Data and ETL technologies. You will collaborate closely with the Business, the office of the CDO, CTO, Quality Assurance and IT professionals. This initiative has a very high profile and is recognized to be a core component of our business strategy over the next three years. You will be based in the UBS office in Weehawken, NJ. If you love data, new technologies and innovation, then this position is for you.

You have:

at least 10 years of IT experience , of which 3-5 years of experience in a developer role in Big Data environment with Hadoop distributions such as Cloudera or Hortonworks , who will play a major role in implementation of Big Data Refinery for UBS Wealth Management organization
strong programming skills using Informatica BDM, Scala/Spark, Python, SQL
experience with Hortonworks or Cloudera (HDFS, Hive, HBase, Impala, Spark, Kafka, Flume, Pig, etc.)
experience of developing streaming applications using Scala/Spark, Cassandra
hands on UNIX shell scripting experience.
experience preferably with Autosys JIL

You are:
– proactive, keeping stakeholders well informed of development effort status
– comfortable working in dynamic environments with fast-paced deliveries and changing requirements
– expected to work interdependently, collaborate, negotiate
– skilled at recognizing system deficiencies and implementing effective solutions

#DiceTech
Expert advice. Wealth management. Investment banking. Asset management. Retail banking in Switzerland. And all the support functions. That's what we do. And we do it for private and institutional clients as well as corporations around the world.

We are about 60,000 employees in all major financial centers, in more than 50 countries. Do you want to be one of us?
Together. That’s how we do things. We offer people around the world a supportive, challenging and diverse working environment. We value your passion and commitment, and reward your performance.

Keen to achieve the work-life agility that you desire? We're open to discussing how this could work for you (and us).

Why UBS? Video
Are you truly collaborative? Succeeding at UBS means respecting, understanding and trusting colleagues and clients. Challenging others and being challenged in return. Being passionate about what you do. Driving yourself forward, always wanting to do things the right way. Does that sound like you? Then you have the right stuff to join us. Apply now.
UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.","Weehawken, NJ",IT Big Data Engineer,False
697,"Role Summary:
The Data Engineering team helps solve our customers' toughest challenges; making flights safer, power cheaper, and oil & gas production safer for people and the environment by leveraging data and analytics. The Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies.

Essential Responsibilities:
As a Data Engineer, you will be part of a data engineering or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in GE business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization.

In this role you will:

Performs a variety of data loads & data transformations.
Working knowledge of methods for parsing, formatting, & transforming data into units consistent with analytical needs.
Demonstrates proficiency in implementation of logical/physical data models that support MDM best practices.
Performs integration of multiple data source-formats into master data load.
Proficient in the use of at least one ETL tool.
Must have good communication skills - both oral and written

Qualifications/Requirements:
Basic Qualifications:

Bachelor's Degree in business, computer science or in ""STEM"" Majors (Science, Technology, Engineering and Math)
Fluent in relational database platforms and programming languages: Oracle, SQL Server, Open Source databases with minimum 5 years of experience as data
engineer.
Experience and understanding of Microsoft Azure DBMS related services and Virtual Machines platforms
A minimum of 3 years of technical experience along with established credentials across disciplines and functions within a product

Eligibility Requirements:

Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job
Must be willing to work out of an office located in Providence, RI.

Desired Characteristics:
Technical Expertise:

Working knowledge of DB technologies, distributed systems Automation, Oracle Exadata, SQL Server, Open Source db's
Deployment & Automation and Configuration Management tools: Docker,Chef/Puppet, Refactory and Jenkins
Understands the technology landscape, up to date on current technology trends and new technology, brings new ideas to the team and ability to analyze impact of technology choices
Skilled in breaking down problems, documenting problem statements and estimating efforts

Leadership:

Demonstrates clarity of thinking to work through limited information and vague problem definitions
Influences through others; builds direct and ""behind the scenes"" support for ideas
Proactively identifies and removes project obstacles or barriers on behalf of the team
Shares knowledge, power, and credit, establishing trust, credibility, and goodwill

Personal Attributes:

Self-motivated, able to work independently
Excellent communication skills and the ability to interface with leadership with confidence and clarity
Ability to work well with global teams, including time-zone flexibility, manage multiple priorities across the technology stack
Ability to identify, quantify and resolve technical problems along with deep passion for learning

#DTR

About Us:
At GE Digital, we are creating technology and solutions to enable social, mobile, analytical and cloud capabilities for the Industrial Internet. The Industrial Internet is an open, global network that connects people, data and machines. It’s about making infrastructure more intelligent and advancing the industries critical to the world we live in. At GE, we believe it’s about the future of industry—energy, healthcare, transportation, manufacturing. It’s about making the world work better. GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

Locations: United States; Rhode Island; Providence

GE will only employ those who are legally authorized to work in the United States for this opening.","Providence, RI 02902 (Downtown area)",Data Engineer,False
698,"Data Engineer

At GumGum our ad server produces over 50 TB of new raw data every day. It amounts to ~100 billion events per day that needs to be processed. Dealing with data at this scale is challenging in a number of ways. We deal with number off-the-shelf frameworks including Spark, Kafka, Cassandra, DynamoDB, RedShift, but often push them past their limits. This team is responsible for providing critical ad reporting data for GumGum’s internal and external customers.

As a Data Engineer, you will be building and maintaining exciting systems, services and data tools. You’ll bring your experience with complex distributed systems, passion for performance and optimization, and ability to write highly scalable and fault tolerant code.

Responsibilities: Refining our data infrastructure technologies such as Kafka, Spark, Druid, Fluentd to support real time analysis of data
 Own the core data pipelines and scale our data processing flow. Build scalable systems with various AWS & Big Data technologies, lead technical discussions, participate in code reviews, guide the team in engineering best practices. Must be able to write quality code and build secure, highly available systems.
 Work on GumGum’s proprietary Reporting Server Work on various reports using Groovy, SQL and Java Works on GumGum’s proprietary forecasting system
Requirements: At least a bachelor's degree in computer science or equivalent 3+ years of software engineering experience (Java/Scala/Python) Experience with large scale distributed real-time systems with tools such as AWS, Spark, Kafka, Hadoop Familiar with various AWS services, Serverless architecture and containers Experience with high volume, high availability production systems. Strong problem solving skills, strong verbal and written communications skills

Perks Stock options 401k, great medical/dental plans Unlimited PTO Great co-workers, monthly happy-hours Located in hotbed of tech startups, just blocks from the beach!","Santa Monica, CA",Data Engineer,False
699,"Do you want to have an impact? It starts with the Data. We absorb billions of data points a day and transform into a Full Resolution Identity graph. As part of an early stage team, you will make an impact in creating a robust and scalable data platform to fuel Drawbridge’s data needs. You will architect and build powerful data pipelines that will enable business insights and efficient. If you are a motivated individual that wants to work on bleeding-edge technology, making vast sums of data accessible to others, we've saved you a seat on our team!

About Drawbridge
Drawbridge is the leading digital identity company, building patented cross-device technology that fundamentally changes the way brands connect with people. We use large-scale AI and machine learning technologies to build democratized solutions for consumer-friendly identity. Our technology is driving the intersect between mar-tech and other categories with applications including advertising, personalization, content management, product recommendations, authentication, and risk detection. We’re headquartered in Silicon Valley, backed by Sequoia Capital, Kleiner Perkins Caufield Byers, and Northgate Capital, and have been listed on CNBC’s annual Disruptor 50 list of groundbreaking companies twice, named one of Fortune’s 50 Companies Leading the AI Revolution, and has been included on the Inc. 500 list of fastest growing companies in America for two consecutive years. For more information visit www.drawbridge.com.
What You'll Do
Work with Analytics and Product Management to ensure optimal data design and efficiency.
Identify and drive scalable solutions for building and automating reports, pipelines and dashboards.
Partner with Engineering teams to ensure data quality.
Build scalable data pipelines and create data models to support analysts and other stakeholders.
What You Bring To The Table
Knowledge of Hadoop Ecosystem and willingness to learn more (Spark, Hive, Presto, Pig… ect).
Experience with relational databases (Mysql,Vertica, Redshift… ect).
Experience with ETL design, implementation and maintenance.
Experience with BI Reporting & Visualization tools. Knowledge of Tableau is preferred.
Development experience with Python.
You are a pro-active self starter with a willingness to learn and adjust in a fast-paced working environment.
What's In It For You
Contribute to a fast-paced, high-growth company that will challenge you, and which was recently featured in Forbes, Fortune, TechCrunch, and AdWeek.
Have a huge impact in the hottest growth area in digital advertising.
Get rewarded with competitive pay, benefits, and pre-IPO stock options.
Work with fun, intelligent people in a great work environment.
Enjoy gourmet coffee, free lunches, sweet & salty snacks, and keg.
Medical (PPO and HMO options), dental, and vision coverage.
Flexible Spending Account (FSA) benefit.
Life and disability insurance.
401K savings plan.
Attractive vacation, sick, gym/health, and holiday benefits.
An excellent work culture of fun, respectful, open minds, diversity, and teamwork.
The Drawbridge “X” Factor
Sure we’ve got the usual requisite stuff like happy hours, brown-bag talks, free lunches, snacks, and more, but what sets Drawbridge apart is the authentic sense of community. We actually like hanging out together, whether we’re working to move the company forward, or getting to know each other a little better – in and out of the office. Our culture makes us excited to come to work, and we know that this strength helps us attract talented individuals who go above and beyond. If you’re different, you’ll fit.

As part of our dedication to the diversity of our workforce, Drawbridge is committed to Equal Employment Opportunity without regard to race, color, national origin, ethnicity, gender, protected veteran status, age, disability, sexual orientation, gender identity, or religion, or any other unlawful factor. Drawbridge complies with all applicable laws, including those regarding consideration of qualified applicants with criminal histories (such as the San Francisco Fair Chance Ordinance). We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our Drawbridge participates in E-Verify.

Note to Recruiters and Placement Agencies: Drawbridge does not accept unsolicited agency resumes. Please do not forward unsolicited agency resumes to our website or to any Drawbridge employee. Drawbridge will not pay fees to any third party agency or firm and will not be responsible for any agency fees associated with unsolicited resumes. Unsolicited resumes received will be considered the property of Drawbridge.","San Mateo, CA 94403 (Sugerloaf area)",Data Engineer,False
700,"Job Description
Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access?

Amazon's Finance Technology team is seeking an outstanding Data Engineer to join the team that is shaping the future of the finance data platform. The team is committed to building the next generation big data platform that will be one of the world's largest finance data warehouse to support Amazon's rapidly growing and dynamic businesses, and use it to deliver the BI applications which will have an immediate influence on day-to-day decision making. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable. Our platform serves Amazon's finance, tax and accounting functions across the globe.

As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint.

Responsibilities:
Design, implement, and support a platform providing secured access to large datasets.
Interface with tax, finance and accounting customers, gathering requirements and delivering complete BI solutions.
Model data and metadata to support ad-hoc and pre-built reporting.
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Tune application and query performance using profiling tools and SQL.
Analyze and solve problems at their root, stepping back to understand the broader context.
Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.
Basic Qualifications
Bachelor’s degree in CS or related technical field
6+ years experience in dimensional data modeling, ETL development, and Data Warehousing
Experience with Redshift and/or other distributed computing systems.
Excellent knowledge of SQL and Linux OS
SQL performance tuning
Server management and administration including basic scripting
Basic DBA tasks
Solid experience in at least one business intelligence reporting tool
Preferred Qualifications
Master’s degree in Information Systems or a related field.
Knowledge of Big Data Solutions. Experience with Hadoop, Hive or Pig.
Experience with Redshift and other AWS services.
Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.
Knowledge of a programming or scripting language (R, Python, Ruby, or JavaScript).
Experience with Java and Map Reduce frameworks such as Hive/Hadoop.
Strong organizational and multitasking skills with ability to balance competing priorities.
An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm.","Seattle, WA",Data Enginner,False
702,"Job Description
Amazon seeks an experienced Business Intelligence Engineer (BIE) to join a newly created Enterprise Risk Management and Compliance (ERMC) team. Amazon has a diverse set of global businesses, and each business has some level of compliance requirements across multiple areas, including Payments, Trade, Human Resources, Tax, Social Responsibility, and others. While the responsibility for maintaining compliance is with the individual business areas, this team ensures each business has an effective ERMC program with processes, controls and self-testing in place to address their regulatory responsibilities. This team also assists in risk ranking and mitigation of gaps identified throughout the organization.

In this role, you will own the root cause analysis for the transactions reported to the regulatory bodies for the Denied Party Screening program by Enterprise Risk Management and Compliance (ERMC). The root cause analysis will drive long-term strategic decisions for ERMC on where to invest preventative technological resources. You will need to collaborate effectively with internal stakeholders and cross-functional teams to solve problems, create operational efficiencies, and deliver successfully against high organizational standards. You should be able to apply a variety of tools, data sources, and analytical techniques to answer a wide range of high-impact business questions and present insights in a concise and effective manner. This is a high impact role with goals that directly influence the bottom line of the business.

Responsibilities include, but not limited to:
Apply analytics and data mining techniques to solve complex problems and drive business decisions
Employ appropriate tools, methodologies to discover patterns of risks, gaps, and help reduce reportable transactions to the regulatory bodies.
Solve analytical problems and effectively communicate methodologies and results
Design and develop automated dashboards to monitor and highlight key trends, drivers, and anomalies
Partner with Operations, Tech , and Compliance, to quantify risks and opportunities for the improved screening
Basic Qualifications
BA/BS in Computer Science, Engineering, Statistics, Mathematics, Finance, or related field3+ years’ experience as a BIE, data analyst, data scientist, data engineer, or similar job function in a technology companyDemonstrated strength in SQL, data modeling, ETL development, and data warehousingAdvanced skills in Excel as well as any data visualization tools like Quicksight, Tableau or similar BI tools.Experienced working in very large data warehouse environmentsAdvanced knowledge of Microsoft Office(predominantly MS Excel and MS SharePoint)Knowledge of AWS solutions such as Redshift, S3Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management
Preferred Qualifications
MBA or Master in Computer Science, Engineering, Statistics, Mathematics, Finance, or related fieldExperienced supporting projects involving complex data sets and high variabilityExperienced conducting complex data analysis (what-if scenarios, A/B testing, etc.) and using ML modelsExcellent communication (verbal and written), interpersonal skills, and ability to effectively communicate with both business and technical teamsExperienced handling confidential and sensitive dataDesigned and developed data infrastructure to support business growth

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation","Seattle, WA",Business Intel Engineer I,False
703,"The WebbMason Analytics Data Engineer helps our clients turn data into knowledge so they can make better decisions faster. The data engineer will work with clients and other team members to analyze and help define requirements, mine and analyze data, integrate data from a variety of sources, and participate in the design and implementation of reports, algorithms, and other data processing and analysis techniques. The most fundamental role of the data engineer to is deliver high-quality data pipelines for producing analytics-ready datasets.

Data Engineer Responsibilities:

Deliver end-to-end analytics projects, including data ingest, data transformation, data science, and data visualization
Design and deploy databases and data pipelines to support analytics projects
Clearly document datasets, solutions, findings and recommendations to be shared internally & externally
Learn and apply tools and technologies proficiently, including:
Languages: SQL (standard and DB-specific), Python, R, Spark/Scala, Bash
Frameworks: Hadoop, Spark, AWS
Tools/Products: Data Science Studio, Alteryx, Jupyter, RStudio, Tableau, PowerBI
Build compelling visualizations and dashboards that address the analytic needs of the end-user/customer
Performance optimization for queries and dashboards
Develop and deliver clear, compelling briefings to internal and external stakeholders on findings, recommendations, and solutions
Analyze client data & systems to determine whether requirements can be met
Test and validate data pipelines, transformations, datasets, reports, and dashboards built by team
Develop and communicate solutions architectures and present solutions to both business and technical stakeholders
Provide end user support to other data engineers and analysts

Requirements:

Expertise in SQL and Python. Other programming languages (R, Scala/Spark, SAS, Java, etc.) are a plus
Experience with data and analytics technologies, including RDBMS, ETL, and BI
Experience with Hadoop or other big data technologies
Experience with AWS or other cloud technologies
Experience with agile delivery methodologies and/or JIRA
Experience working on Linux command-line
BS or higher in related field
Master’s degree in related field

","Sparks Glencoe, MD 21152",Data Engineer,False
704,"QuantumBlack helps companies use data to drive decisions. We combine business experience, expertise in large-scale data analysis and visualization, and advanced software engineering know-how to deliver results. From aerospace to finance to Formula One, we help companies prototype, develop, and deploy bespoke data science and data visualisation solutions to make better decisions.
Who You'll Work With
Our Consultant Data Engineers work closely with our clients and our Data Scientists in order to curate, transform and construct features which feed directly into our modelling approach.
This would be a hybrid client-facing/technical role using cutting edge technologies, whilst also being able to communicate complex intractable ideas to non-technical audiences. Gathering clear requirements is a key part of this role and will define the technical strategy the team employs on the study.
Our projects cover a wide range of industries and may expose you to problem areas such as: Disease epidemiology, athlete injury prediction or salesforce effectiveness optimisation and many more. In order to gain insight from previously ignored and unconnected data you will need to extract information from vast array of different data sources such as: Data Warehouses, SQL databases, legacy applications, unstructured data, documents, emails, APIs, Kafka endpoints and graph databases.
What You'll Do
Work with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Understanding, assessing and mapping the data landscape.
Maintaining our Information Security standards on the engagement.
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Defining the technology stack to be provisioned by our infrastructure team.
Building modular pipeline to construct features and modelling tables.
Use new and innovative techniques to deliver impact for our clients as well as internal R&D projects
Mentoring and developing junior Data Engineers on engagements
Requirements
Strong experience with at least two of the following technologies: Python, Scala, SQL, Java
Commercial client-facing project experience is beneficial, including working in close-knit teams
The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Good experience in multiple database technologies such as:
Distributed Processing (Spark, Hadoop, EMR)
Traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL)
MPP (AWS Redshift, Teradata)
NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan)
A proven ability in clearly communicating complex solutions
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
Experience and interest in Cloud platforms such as: AWS, Azure, Google Platform or Databricks
Strong experience in traditional data warehousing / ETL tools (Informatica, Talend, Pentaho, DataStage)
Exceptional attention to detail","Boston, MA",Data Engineer (Boston),False
705,"Apple is seeking a highly skilled data engineer to join the Data Engineering team within Apple Media Products. AMP (home to Apple Music, App Store, iTunes and more) has some of the most compelling data in the world. We are looking for a talented engineer who is motivated by challenging problems and well versed with big data technologies. This is a unique opportunity to join a focused team and work collaboratively with other groups to make a significant impact.

Key Qualifications
Experience in high level programming languages such as Java, Scala, or Python.
Proficiency with databases and SQL is required.
Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience with large scale data warehousing, mining or analytic systems.
Ability to work with analysts to gather requirements and translate them into data engineering tasks
Aptitude to independently learn new technologies.
Description
As a member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction. This role is inherently cross functional and the ideal candidate will work across disciplines. We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipeline. This position involves working on a small team to develop large scale data pipelines and analytical solutions using Big Data technologies. Successful candidates will have strong engineering skills and communication, as well as a belief that data driven processes lead to great products. You will need to have a passion for quality and an ability to understand complex systems.

EducationBachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems.","Santa Clara Valley, CA","Senior Data Engineer, Apple Media Products",False
706,"Title: Data Engineer with AWSLocation: Sanjose, CAPosition: FulltimeNote: AWS Expert Responsibilities: Work closely with the business teams and translate business problems into analytical requirementswork with a team of people to design various components of the solution, such as data movement, storage, computation / processing, and insight deliveryLead the deployment and maintenance of this platform and contribute to continuous improvementWork collaboratively using iterative methods, design and code reviews, and contributing ideas for business growth RequirementsBachelors/ Masters/ PhD degree in Math, Computer Science, Information Systems, Machine Learning, Statistics, Econometrics, Applied Mathematics, Operations Research or related technical degreeA minimum of 5 years’ experience in a related position, as a Technology Architect providing technical leadership in handling for various types of business problems4+ years of building data platforms for analytics, advanced analytics in AWS or AzureExposure to Traditional ETLs and in depth exposure to orchestrating pipelines in AWS or AzureData Modelling, Query optimization in MPP systemsAble to design the technical architecture, platforms selection, develop & test solutions to address the client problems & requirementExperience in designing & developing data processing solutions & custom ETL pipeline for varied data formatsKnowledge on scalable processing frameworks, RDBMS & NoSQL platformsStrong fundamentals in Hadoop, SparkStrong individual planning and project management skills, able to juggle multiple tasks and prioritiesTrack record of delivering strong business resultsAbility to communicate technical architecture to business audienceThanks & RegardsAkthar Pasha Sr. Technical RecruiterCell : +1 (732)723-5773Job Type: Full-timeExperience:AWS: 3 years (Required)Azure: 2 years (Required)building data platforms for analytics, advanced analytics: 4 years (Required)Traditional ETLs and in depth exposure to orchestrating pipe: 3 years (Required)Strong fundamentals in Hadoop, Spark: 2 years (Required)Education:Bachelor's (Required)","San Jose, CA",Data Engineer with AWS,False
707,"As a member of the Pfizer Analytics Lab team, a component of Pfizer’s Business Technology organization, the Data Engineer will join a team of highly collaborative data scientists & engineers dedicated to leveraging data and advanced analytics to create a healthier world. This team member will contribute their dynamic perspective and knowledge to data engineering and advanced analytics, inspire colleagues and peers to develop and implement critical data driven solutions within Pfizer’s drug discovery efforts.

Specifically, this group is focused on developing a set of capabilities designed to enable highly efficient exploration, experimentation, and rapid hypothesis generation based on internal, public, and commercially available datasets to continue supporting Pfizer’s data driven, forward thinking approach to data science

Day-to-day, the Data Engineer will
Build services and tooling around “scraping” databases, loading logs, fetching data from external stores or APIs
Automate data consumption from other source systems, files etc.
Collaborate with other engineering, cloud infrastructure , security and product management teams to understand requirements and develop highly scalable system designs and architecture
Integrate new data management technologies and software engineering tools into existing structures
Create custom software components and analytics applications
Employ a variety of languages and tools to marry systems together
Participate in the assessment of new technologies as well as identifying next-generation solution architectures.
Develop efficient analytic pipelines that include components related to data acquisition, exploratory analysis, feature engineering, modeling, and interactive storytelling.
Shared-ownership of advancing team's data engineering capabilities through the ability to implement and execute on state-of-the-art approaches
Co-develop re-usable components that will serve as the foundation for a scalable approach for Pfizer’s analytic maturation
Partner with other Business Technology teams to define and execute technology POCs using innovative technologies to advance Pfizer’s analytic capabilities
Directly engage with key business stake-holders (Director level)
Informal leadership of project teams comprised of Associate/Sr. Associate level colleagues

QUALIFICATIONS

Bachelor’s Degree in Computer Science, Operations Research, physics, applied mathematics, statistics required
Advanced Degree in Computer Science, Operations Research, physics, applied mathematics, statistics or related field strongly preferred
5 years’ experience working as a Data/ML Engineer
3 years working with semi-structured and unstructured data
2 years working in a cloud based ecosystem, preferably Amazon Web Services.
Ability to thrive in a fast-paced multi-disciplinary environment; with the ability to effectively communicate with a diverse audience
Ability to create technical examples, prototypes, and demonstrations based on rapidly changing data sets
Excellent written and verbal communication skills

Technical Qualification
Proven experience in at least two of the three following categories:

Data Science / Machine Learning
Expertise with general-purpose statistics/machine learning algorithms and at least one of the following sub-disciplines: Natural Language Processing, Deep Learning, Network Analysis.
Expertise with the implementation of algorithms within Python, R or Scala
Expertise with model tuning, validation and evaluation

Data Engineering
Expertise with SQL development, database administration and performance tuning
Expertise with data manipulation and extraction using modern programming languages (Java, C++, C#, Python, Scala, Spark, etc.)
Experience with Unix/Linux development – package management, knowledge of filesystems, performance monitoring/troubleshooting
Experience with sourcing data from APIs; experience building APIs is a plus
Experience with a variety of data stores for unstructured and columnar data as well as traditional database systems, for example, ElasticSearch, MongoDB, Cassandra, HBase, MySQL, Postgres and Vertica

Machine Learning Engineering
Experience building production implementations of data science and engineering pipelines
Building and running high throughput real-time and batch data processing pipelines using Spark, Flink, Storm, Kafka or equivalent technologies

EEO & Employment Eligibility

Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.

Sunshine Act
 
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.

Other Job Details:
Eligible for Employee Referral Bonus
This position can sit in La Jolla, CA, New York, NY or Collegeville, PA


Pfizer is an equal opportunity employer and complies with all applicable equal employment opportunity legislation in each jurisdiction in which it operates.","Los Angeles, CA",Big Data and AI Engineer,False
708,"New York Life Insurance Company (“New York Life” or “the company”) is the largest mutual life insurance company in the United States*. Founded in 1845, New York Life is headquartered in New York City, maintains offices in all fifty states, and owns Seguros Monterrey New York Life in Mexico.

New York Life is one of the most financially strong and highly capitalized insurers in the business. The company reported 2016 operating earnings of $1.954 billion. Total assets under management at year end 2016, with affiliates, totaled $538 billion. As of year-end 2016, New York Life’s surplus was $23.336 billion**. New York Life holds the highest possible financial strength ratings currently awarded to any life insurer from all four of the major ratings agencies: A.M. Best, A++; Fitch AAA; Moody’s Aaa; Standard & Poor’s AA+. (Source: Individual Third Party Ratings Report as of 8/17/16).

Financial strength, integrity and humanity—the values upon which New York Life was founded—have guided the company’s decisions and actions for over 170 years.

New York Life, the largest writer of retail life insurance in the U.S. and a top player in annuities, long-term care and mutual funds, is seeking a Data Engineer in its Center for Data Science and Analytics.

The Center for Data Science and Analytics is the innovative corporate Analytics group within New York Life. We are a rapidly growing entrepreneurial department, which aims to design, create and offer innovative data-driven solutions for many parts of the enterprise. We are aided by New York Life’s existing business with a large market share in individual life insurance. We have the freedom to explore external data sources and new statistical techniques, and are excited about delivering a whole new generation of Analytical solutions.

In fact, we are designing and will build one of the first multivariate model-based continuous risk differentiations in the industry. This model will incorporate current underwriting best practices (including medical rules) as features and add other data sources, patterns/ideas and variables to essentially create a rating plan to support the next generation underwriting process at New York Life. This is just one of several projects with large business value. Geographic analytics on agents and customers, application fraud detection, agent success prediction and client prospecting analytics (off-line and on-line) are other exciting examples of enormous incremental value from analytics. Our products will be implemented into real-time core business processes and decisions that drive the company (e.g. underwriting, pricing, agent recruiting, prospecting, new product development).
We work with data ranging from demographics, credit and geo data to detailed medical data (medical test results, diagnosis, prescriptions) and social media information. We have a modern computing environment with a solid suite of data science/modeling tools and packages, and a large (but manageable) group of well-trained professionals at various levels to support you. Life insurance is on the verge of huge change. This is a chance to be part of, actually to drive, the transformation of an industry.
You will be part of Data & Platform sub-function team under Center for Data Science and Analytics. The Data & Platform team services internally to Data Scientists who focus on Statistical analysis.
You will be part of a fast paced, high-impact team who will work with an entrepreneurial mindset using some of the best of breed tools as part of our Enterprise Data Hub (Hadoop) using R, Spark and Python.
You will apply your data engineering skills to build pipelines, workflows to gather, cleanse, test and curate datasets from Oracle, MSSQL Server, 3rd party data and create datasets in Enterprise Data Lake (Hadoop), which will be used by several teams of predictive modelers.
You will perform Proof of Concepts and test out new software tools under the umbrella of Data Science but geared more towards data engineering.
Responsibilities
Ingests, merges, prepares, tests, documents curated datasets from various novel external and internal datasets for a variety of advanced analytics involving multi-variate models
Utilizes data wrangling/data matching/ETL techniques while to explore a variety of data sources, gain data expertise, perform summary analyses and curate datasets
Functions as data expert, contributes to analytics/solutions design and productizing decisions
Collaborate with Business leaders to understand business challenges and devise solutions by using business acumen and mining vast amounts of data to draw insights
Can work independently with some supervision and be part of a collaborative team
Work with Project Managers and Scrum Masters to provide milestones and stories
Proactively and effectively communicates in various verbal and written formats with senior level member of the team and partner
Actively participates in proof of concept tests of new data, software and technologies. Shares knowledge within the team
Follows industry trends and related data/analytics processes and businesses. Attends conferences, events, and vendor meetings as needed

Required qualifications
Graduate-level degree in computer science, engineering, or relevant experience in the field of Business Intelligence, Data Mining, Database Engineering, Programming
3-5 years of overall experience working in the field of data wrangling and programming with a minimum of 1 year experience with ingesting, cleaning, merging and applying necessary data wrangling logic in Hadoop
1+ years in writing complex SQL queries in any of the following and/or similar databases - Oracle, SQL Server, DB2, MySQL
Proficiency using Python for all data related work such as Numpy, Pandas, PySpark
Experience working with Linux Operating System
Experience working with data visualization tools or packages
Experience building Exploratory Data Analysis reports such as Histograms, Box plots, Pareto, Scatter Plot using R, Python or a Data Visualization tool such as Tableau and Spotfire

Preferred:
Understanding of statistical modeling concepts, designs and analytics-based products
Any experience in using ETL tools such as Ab Initio, Talend, Informatica, Pentaho
Any experience working with Data Warehouses and/or Data Marts
Any experience in Life Insurance business

Other Notes:
Our technology stack is RStudio Pro, SAS, Enterprise Data Hub (using Hortonworks Hadoop Data Platform), Waterline, Trifacta, R, Python, Spark, PySpark, SparkR, Linux

EOE M/F/D/V

SF: LI-TK1
EF: EF-TK1
EOE M/F/D/V

If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811.
Based on revenue as reported by “Fortune 500, ranked within Industries, Insurance: Life, Health (Mutual),” Fortune Magazine, June 17, 2016. See http://fortune.com/fortune500/ for methodology.
**Total surplus, which includes the Asset Valuation Reserve, is one of the key indicators of the company’s long-term financial strength and stability and is presented on a consolidated basis of the company.

1. Operating earnings is the key measure use by management to track Company’s profitability from ongoing operations and underlying profitability of the business. This indicator is based on generally accepted accounting principles in the US (GAAP), with certain adjustments Company believes to be appropriate as a measurement approach (non GAAP), primarily the removal of gains or losses on investments and related adjustments.

2. Assets under management represent Consolidated Domestic and International insurance Company Statutory assets (cash and invested assets and separate account assets) and third party assets principally managed by New York Life Investment management Holdings LLC, a wholly owned subsidiary of New York Life Insurance Company.","New York, NY",Data Engineer - Center of Data Science Team,False
709,"System1 is looking for engineers with production data experience to join Data Engineer team. This team is the horizontal layer that supports business intelligence, optimization, machine learning, and external & internal reporting. We process and report on hundreds of millions of events and user attributes per day, gathered from an extremely heterogeneous set of data streams.
Our bread and butter is Python and PostgreSQL, but we also utilize a range of technologies including AWS Lambda, SNS, SQS, Redis, and Dynamodb for caching and mapping, as well as Redshift, Kinesis, and Spark for large dataset munging and ad hoc analysis.
The Role You Will Have
Create, build, and maintain a coherent and performant data architecture
Prototype, develop, deploy, and debug data ingestions and data management services
Participate in peer code reviews and produce high quality documentation
Construct queries and reports to guide architectural design, business decisions, and optimization algorithms
Take projects through the full engineering lifecycle: designing, ticketing, building, testing, deploying, and debugging tools and products
Help grow a team and work with a tight knit group of engineers and data stakeholders
What You Will Bring
Bachelor’s in Computer Science or equivalent
3+ years of experience with Python development
3+ years of experience working with large SQL datastores (PostgreSQL, Redshift)
Understanding of NoSQL datastores like DynamoDB and Redis
Experience with Linux and the AWS ecosystem
What We Have To Offer
Free Uber/Lyft to and from work every day!
Collegial and collaborative team with highly intelligent and motivated coworkers
Cross-team lunches and demos to foster learning
Unlimited Paid Sick Time, Competitive PTO and Benefits package
Daily catered meals and fully stocked kitchen
Bi-weekly happy hour at various bars, restaurants, and venues across Los Angeles
Bi-weekly onsite happy hour
Catered dinner on Tuesdays and Thursdays
Weekly fitness class with private trainer: high intensity training, yoga, beach volleyball, beach soccer, ultimate frisbee
Company parties and outings: Skyzone indoor skydiving, Medieval Times, Karaoke, etc.","Venice, CA",Data Engineer,False
710,"Interested in an opportunity to work in a robust, challenging, data-focused environment with an outstanding team of technical and research professionals? At TGS, financial data is at the heart of our business. The Financial Data Engineering role is mission critical - requiring uncommon reliability, a passion for writing great software and tools, experience working with a wide variety of data (preferably financial data, but not necessarily), and the ability to support large-scale production systems. It’s an exciting, challenging role that provides clear opportunities to contribute to the success of an exceptional organization.
In addition to writing programs and working with data, the successful candidate will work closely with our research team and is likely to interact with a variety of external resources such as data providers, brokers, dealers, and software vendors.
To be considered for this role, you will need to demonstrate expertise in a number of the following general areas:
Programming: experience developing data management tools and/or applications using languages such as Java, Perl, Python, or C/C++
System tools: experience with scripting languages (Perl, Python, shell scripts, etc.), and Unix-based operating systems (especially Linux)
Production support - willingness and ability to support large, complex production systems in an on-call or rotational capacity
Analysis of large data sets: experience developing programs to parse, process, analyze, and comprehend large data sets
Applications: experience designing, developing, and maintaining software applications, tools, and systems
Vendor interaction: working with external resources to solve problems, acquire data, and improve relationships
Financial data: familiarity with financial terms and experience working with data from a variety of vendors and sources

About Us

For over two decades, the TGS team has built quantitative trading systems that have produced exceptional results across a range of financial markets. We use scientific methods and engineering discipline to solve challenging problems and develop technology solutions. Our Irvine office is as unique as our Southern California location, combining elements of high tech, finance, and applied research in a collegial atmosphere and beautiful workspace.

As an employer, we are small, discreet, and highly selective. We look for talented people with proven track records of performance and achievement, and are far more interested in aptitude and potential than expertise in any particular technology, tool set, or professional domain. If you're inspired by the idea of working on interesting problems with talented colleagues, we invite you to share your resume and explore the possibility of joining our team.","Irvine, CA",Financial Data Engineer,False
711,"JOB DESCRIPTION
DATA ENGINEER

WHO IS SPR?

SPR is a digital technology consultancy that develops elegant solutions to transform the way people do business. We’re 300+ strategists, developers, designers, architects, consultants, thinkers, and doers in Chicago and Milwaukee. We work with 160 clients in 10 unique industries – everything from corporate finance and global logistics to local breweries and startups.

We think about the end users and rigorously apply the latest technologies and frameworks to address our clients’ needs. We enable companies to do more with data, engage with other people, build disruptive solutions, and operate productively. To do this, we hire smart technologists and sharp business leaders who are excellent communicators and have an interest in working on multiple projects across industries.

SPR offers a great environment for employees to learn, to build systems that make an impact, and to tackle exciting challenges. We operate in a fun, casual work environment and have great benefits including: competitive salary, bonuses, generous vacation time, big fitness incentives, and medical/dental/vision insurance.

By joining the SPR team, you’ll be using your brain, working hard and making an impact through your projects – and you’ll be rewarded for it.

WHAT IS THE POSITION?

As a Data Engineer at SPR, you must have experience building and operating data pipelines (both streaming and batch, utilizing both ETL and ELT architectures). You will be building data pipeline solutions by designing, adopting and applying big data strategies and architectures. You must be experienced in large-scale system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration best practices, and expertise in data integration, data transformation, data modeling and data cleansing. The Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for our clients.

PROFESSIONAL QUALIFICATIONS

Experience in designing and implementing innovative data integration solutions, utilizing Python with Spark clusters.
Familiarity with architectural patterns for data-intensive solutions
Expertise in real-time streaming and migrating batch-style data processing to streaming and micro-batch solutions
Knowledge of the RDBMS core principles; set up, tune, design, as well as newer unstructured data tools
Familiarity with consulting and traditional application design
Excellent written and verbal communication skills
Display solid problem-solving abilities in the face of ambiguity
Must be a hands-on individual who is comfortable leading by example
Experience with Agile Methodology
Possess excellent interpersonal and organizational skills
Able to manage your own time and work well both independently and as part of a team

TECHNOLOGIES WE USE

Cloud (Azure, AWS, Cloud Foundry, Heroku, Mesos, DC/OS) / / RDBMS (SQL Server, PostgreSQL, Oracle, DB2) /NoSQL (Mongo, Raven, DocumentDB, Cassandra, Maria, Riak) / Python (including Databricks) / / Big Data (Cloudera & Hortonworks Hadoop distributions, including Hive, Pig, Sqoop, Spark) / Integration Tools (Apache Nifi, Cloudera Streamsets, Azure Data Factory, AWS Glue, Talend) / ELK (ElasticSearch, Logstash, Kibana) / Machine Learning (Azure ML tooling, TensorFlow, AWS Sagemaker, scikit-learn) / Data Visualization (Grafana, Kibana) / Microsoft PowerShell / AWS SDK / Fast Data (Apache Ignite / Gridgain, Apache Geode/Pivotal Gemfire)

EDUCATION & EXPERIENCE

3-5 years of professional experience
BA or BS, preferably in Computer Science, Engineering or Science/Technology-based discipline

If this sounds like the kind of challenge you would be up for every day, we would love to hear from you.","Chicago, IL",Data Engineer,False
712,"$60,000 - $100,000 a yearStriiv is building the industry's leading Wearable OEM platform for the Healthcare and Group market. We provide an end-to-end solution - from the data/services, mobile applications, to a family of wrist-worn devices.As a Backend/Data Engineer at Striiv, you will design and build data processing systems and backend services on the Google Cloud Platform that impact people's lives worldwide helping them stay healthy. You will working closely with other smart, dedicated Engineers and Product Managers, shape our platform scaling and data processing and cloud service strategy, write maintainable code, and advocate for best practices and exceptional quality.An ideal candidate has:- Bachelor's Degree in Computer Science or Engineering (or relevant experience)- Experience working with backend technologies- Knowledge developing APIs using Python or Java- Experience with a Python web framework (Django, Flask, Webapp2)- Experience with database technologies and database design (e.g. SQL or noSQL)- Experience with Google Cloud Platform such as App Engine, Datastore, BigQuery, Pub/Sub, Dataflow or equivalent technologies on Amazon Web Service- Experience with OAuth 2.0- Experience with version control using GitBenefits & Perks:- Competitive salary and equity options- Working with latest and greatest technology- Weekly free lunches- Unlimited snacks- Close to Downtown Redwood City and Caltrain- Casual dress code- Wear the latest wearable gearStriiv is an industry leader of OEM fitness tracking and smartwatch devices. Powering technology branded by Walgreens, Acer, and more; Striiv specializes in creating custom solutions for businesses interested in offering wearable technology. Striiv is located in Redwood City, CA and is an equal opportunity employer.Job Type: Full-timeSalary: $60,000.00 to $100,000.00 /yearWork authorization:United States (Required)","Redwood City, CA",Backend / Data Engineer,False
713,"Major Duties and Responsibilities:
","Oak Ridge, TN",Senior Data Engineer,False
714,"Seeking a Data Developer for a contract role at a major investment bank in Jersey City, NJ! Below is the job description:Responsible for the design, development, testing, integration, operation and support of infrastructure services that meet stated business requirements and adhere to coding best practices and architecture standardsAdheres to architectural design standards, risk management and security policies, data management policies, leading presentations in architecture review, strategic technology directions, best practice development (e.g., estimating models), mentoring less experienced team members, code reviewsResponsible for all elements of Software Development LifecycleDevelops integration elements, data models, Application Programming Interfaces (APIs)Assists in the building of open 3rd-party Software Development Kits (SDKs)All interested and available Data Developers, please apply NOW!Job Type: Full-timeExperience:APIs: 1 year (Preferred)investment banking: 1 year (Preferred)Data Development: 1 year (Preferred)Kafka: 1 year (Preferred)NoSQL data stores: 1 year (Preferred)Location:Jersey City, NJ (Preferred)Work authorization:United States (Required)","Jersey City, NJ",Senior Data Engineer,False
715,"Austin, TX
Novus is an innovative company that is changing the way the world invests. We are a high growth, disruptive technology firm bringing big data analytics to the alternative investment industry. Our platform enables investors to consistently maximize their performance potential through discovery of true investment acumen and risk, proprietary industry insights and expertise, and effortless data management and enlightenment.

We are looking for a Data Engineer to join the team and play a critical role in the design and implementing of sophisticated financial data feeds that serve as the foundation of our platform.

Background

We operate in an industry in which over $120 Trillion is invested annually on behalf pension / endowment / sovereign funds, and private investors / family offices. And yet, the lack of data intelligence and insight results in much of that money being invested based purely on historic track record, with little transparency into performance acumen and / or risk management. This is our challenge and our purpose: to help investors make better investments. And in doing so, we aim to provide the single network on which trusted professionals will engage, promote, and manage their business. In short, we want to help the world invest better. And we need you.

Role

You will report directly to our Vice President of Engineering and be based in Austin.

The Engineering team at Novus is a fast-paced group of individuals who are passionate about improving the way the world invests. The Data Engineer will work with vast amount of securities data from many sources, and portfolio data from prominent hedge funds, fund of hedge funds and some of the most sophisticated institutional investors.

Responsibilities around client data integration include:

Help lead a green-field redesign and implementation of industry-leading ETL
Lead and coordinate development efforts with our international team
Applying industry best practices for database development
Maintaining and performance tuning database code
Culture

Our culture is critical to us. It’s how we push ourselves every day to do better than the day before. We expect our team members to deliver on their responsibilities, understand how each and every component of our company works to generate success, and hold themselves and their colleagues accountable to the highest standards. As a result, we will enjoy talking to you if:

You are excited to solve problems you haven’t seen before – “turning every page” to understand the domain
You can work independently – removing roadblocks and answering your own questions
You want to solve problems quickly – to satisfy your users, to get feedback & iterate, and to move onto the next problem
You want to make an impact
Qualifications, Traits & Experience
B.S. in Computer Science or related discipline from top university or college. Double major or minor in Economics or Finance a plus.
Experience with Python is a huge plus
Working experience with Microsoft SQL Server version 2008 R2 or higher
Working experience in building ad-hoc T-SQL queries, stored procedures, views, and functions
Working experience with ETL.
Knowledge on definitions and parameters of financial instruments, e.g. equities, fixed incomes, options, futures, swaps, forwards, etc. is a plus
Interest in investments and/or the fund of funds and hedge fund industry","Austin, TX",Data Engineer,False
716,"Responsibilities
Manage data warehouse plans for a product or a group of products.
Interface with developers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Integrate external data sources into data pipeline.
Build API’s for internal and external consumption.
Define and manage SLA for all data sets in allocated areas of ownership.
Qualifications
BS/BA in Technical Field, Computer Science or Mathematics.
Experience in the data warehouse space.
Experience in custom ETL design, implementation and maintenance.
Experience with a data integration tools (e.g. SSIS, Pentaho)
Experience with object-oriented programming languages (e.g. Python, Java, C#)
Experience with schema design and data modeling.
Experience in writing SQL statements.
Experience in analyzation of data to help identify deliverables, gaps and inconsistencies
Experience with a visualization tools (e.g. Power BI, Tableau, Qlik)
Preferred Qualifications
Knowledge in Python
Knowledge in SSIS
Knowledge in MSSQL Server (2014-2017)
Knowledge in Power BI or SQL Server Reporting Services
Experience in payment processing or clinical trials","King of Prussia, PA",Data Engineer,False
717,"$120,000 - $160,000 a yearContractOver 9+ Years of experience in development, design, testing, and implementation with major focus on Data Warehousing, Database Applications and Business intelligence solutions using ETL tools like Ab Initio Informatica etc.Requirement Gathering and Data Analysis of all supporting systems. Designed Informatica mappings and data flows.Full Software Development Life Cycle (SDLC) experience including Analysis, Design and Review of Business and Software Requirement Specifications; Development and Testing as per the SDLC Agile/Scrum methodology.Designed, Developed, and tested ETL processes in AWS environment.Worked on data ingestion, encryption etc.Optimized and refactored existing ETL processes from SQL Server environment to AWS environment.Worked on AWS Redshift database, Distribution key, Sort key, Compression analysis in Redshift.Worked on the unloading into S3 buckets.Worked on external hive tables.Worked in Sprit based releases and participated in scrum meeting and daily standup meetings.Worked with BSA to understand requirements and prepared LLD and HLD.Prepared Estimates for small modules.Worked on Servigistics tool.Created ETL mappings using Informatica Power Center to move information from multiple sources into a common target area such as Data Marts and Data Warehouse.Extensively worked in developing ETL for supporting Data Extraction, transformations and loading using Informatica Power Center.InformaticaMetadata Manger(MM), Designing and developing Metadata Manager resources and loading into MM warehouse. Performed Developer testing, Functional testing, Unit testing for the Informatica Metadata ManagerExcellent experience in implementing push down optimization in Informatica.Developed Complex mappings in Informatica from various transformations like Router, Filter, Expression, Aggregator, Joiner, Update Strategy using the Informatica Power Center and IDQ.Worked on complex transformations like Java and Normalizer.Integrated Sales force sources into Informatica power center.Worked as developer for Informatica migration from 9.6 to 10.1Worked on SAP ETL tool BODS and prepared Reverse Engineering sheets.Developed and maintained ETL (Extract, Transformation and Loading) mappings to extract the data from multiple source systems like Oracle, SQL server and Flat files and loaded into Teradata.Knowledge in Business Intelligence areas such as Data Integration, Data Masking, Physical Expertise knowledge improving Performance Worked on various scheduling tools like Control-M, Autosys, Tivoli TWS and ESPKnowledge in Business Intelligence areas such as Data Integration, Data Masking, Physical data modeling in Teradata.Extensively worked on Teradata indexes, Join strategies and Performance tuning of long running Teradata sql queries.Experience using NoPI tables and multi value compression and Temporal tables.Job Types: Full-time, ContractSalary: $120,000.00 to $160,000.00 /year","Nashville, TN",Data Engineer,False
718,"Requirements:

4+ years of experience.
Spoken English.
DWH structures creation.
Develop and monitor SSIS Packages for ETL Processes.
SSAS Cubes development (both MDX and Tabular)
Writing Stored Procedures.
Developing SSRS and PowerBI reports.
Performing ETL functions to migrate the legacy data to SQL databases.
MasterData management
Mandatory Skills:

SQL Server 2012,2014,2016
SSIS
SSRS
SSAS
SQL Server Agent
Nice to have:

math background for ML tasks
other relational and\or no-sql databases
different reporting tools like Tableau, QlikView,
Big Data experience
C#, Python","McKinney, TX",Microsoft SQL Server\BI Data Engineer,False
719,"$75 - $85 a dayContractAs Big Data Engineer, you will work on building the next generation a top security analytics platform. You will play a crucial role in building a platform to collect and ingest several billion (and growing) log events from the globally distributed security infrastructure and provide actionable insights to customers and security researchers.


Required Skills & Experience
4+ years of experience in Java development
Excellent interpersonal, technical and communication skills
Ability to learn, evaluate and adopt new technologies
Bachelor's Degree in computer science or equivalent experience
Candidates must be local
Desired Skills & Experience
Familiarity with Hadoop & Hive and other data processing frameworks such as Spark, Kafka, Storm and Elastic search
Experience working with data processing infrastructure
Experience with data serialization techniques and data stores for persisting events
What you will be doing
You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data
Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
The Offer
Competitive Pay: Up to $90/hour, DOE
Contract Duration: 6 – 12 Months
You will receive the following benefits:
Medical Insurance & Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit


Applicants must be currently authorized to work in United States on a full-time basis now and in the future.
Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) across 11 major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.","Mountain View, CA",Data Engineer,False
720,"Reimagining healthcare requires a sensitivity towards understanding complex data problems and drawing substantiated insights that help build novel products that truly improve people's lives. Our role in administering health plans puts us a position to dig into the most granular level of America's healthcare payment infrastructure. This gives us control over our data quality, which is key for getting specific and actionable insights.

We are looking for a creative Software/Data engineer with a data science mindset. This position requires strong foundation in building data centric algorithms at scale. Ideal candidate would have built/shipped production code using open source Big Data Technologies (Hadoop, Spark, Airflow wtc). This candidate should also be willing go beyond what is available in current Open Source frameworks and customize the these stacks if needed to make them better fit for business needs, and contribute back to open source community. Besides engineering foundational infrastructure components, you will also be building scalable Data-Mining and Machine Learning pipelines. You will be working very closely with rest of the Data Science team and collaborate with wider Engineering teams at company level.

We Expect to See:
-----------------


Strong Data Structures and Algorithms background.
Developed scalable data pipelines.
Shipped at least one product/service.
Development experience in Linux environment.
Distributed data computing experience: you know how to parallelize computing across cluster of machines using modern stacks.
Written production code in any modern Languages like C++, Java, Python etc.
Comfortable with basic Statistics.
Superb communication skills in both Technical and Non-technical setting.

Nice to Have:
-------------


Experience with Big Data Technologies like Hadoop, Spark, Airflow.
Relational database modeling and SQL queries.
Familiar with AWS services like EC2, EMR etc or similar cloud computing environment.
Python development experience.
Experience with productionization of models developed in ML libraries like Scikit-learn, MLLIB, Tensorflow, Keras, (Py)Torch etc.
Experience with management of BI frameworks like Tableau, Looker etc.
Interest in Healthcare.

Collective Health is a technology company working to create the healthcare experience we all deserve. Founded in 2013, our team of engineers, designers, product managers, and actuaries are redefining the $1 trillion market of employer-sponsored health benefits with data-driven and people-focused products. Our complete health benefits solution helps great companies like Activision Blizzard, Palantir, Restoration Hardware, and Pinterest take care of their people by harnessing the power of design and technology. Based in San Francisco, CA, we're backed by some of the best investors in Silicon Valley including Google Ventures, Founders Fund, NEA, and Redpoint Ventures. For more information, visit us at https://www.collectivehealth.com ( https://www.collectivehealth.com/ ).
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","San Francisco, CA",Data Science Engineer,False
721,"Tabula Rasa Health Care– TRHC is a family of companies that leverages technology to improve healthcare. TRHC works with providers and insurers to identify multi-drug interactions and to reduce risk of medication-related problems. We have developed the first multi-drug interaction tool that identifies risk across a variety of safety factors and presents meaningful opportunities to mitigate that risk. This technology can be used to assess safety at the individual patient-level or to stratify medication risk at the population-level, and can be embedded within any EMR or other healthcare IT system.
Tabula Rasa Health Care is looking for a Data Engineer to work with IT, business analysts and application developers to define and program ETL solutions. The Data Engineer will be responsible for creating, developing, investigating, mapping and testing the various data needs. This position requires that the Developer be an expert on data availability and quality, have a strong understanding of the business requirements, and the creativity to develop workable solutions for a wide range of customers.
RESPONSIBILITIES:
Build and maintain SQL Server Integration Services (SSIS) jobs
Migrate SSIS jobs over to new Informatica environment
Document ETL processes, programs and solutions
Develop and test new ETL processes
Design and build data structures and processes to populate data warehouse tables
Manage post-production data quality monitoring
Quickly identify and resolve data- and migration-related production issues
Acquire data from primary or secondary data sources and maintain data systems and integrity
Develop procedures to enhance reporting and query capabilities to improve efficiency and accuracy
Develop innovative approaches to manipulate data sets in a way that enables clients to see patterns and trends
Work with key stakeholders to ensure integrity and proper integration for all sources of enterprise data
Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, user access, and security.
Stay current on big data trends and research various technologies as they become relevant.
Provide software coding leadership, guidance, and quality adherence in languages such as Python, Java, JavaScript, React, Node.js, SQL, NOSQL, etc.
QUALIFICATIONS:
Education, Experience & Training:
A BS degree in computer science, computer engineering, other technical discipline, or equivalent work experience
Exceptional skills in relational database systems (MS SQL Server, MySQL, Oracle)
Strong ability with dimensional data modeling and OLAP principles
3+ years’ experience working with ETL Tools (MS SSIS Packages, SSMS, Informatica)
Experience with NoSQL (Mongo, Couch) and columnar databases a plus
Knowledge of Python will be a plus
Hands-on expertise with application design, software development, and automated testing
Prior experience with compliant (e.g. HIPAA, 21 CRF Part 11) systems, processes and operations development is preferred
Ability to identify and diagnose ETL and database related issues, perform root cause analysis, and recommend corrective actions
Specific Skills:
Solid understanding of Data Warehouse methodologies
High proficiency with SQL (MySQL, SQL Server, Oracle, DB2)
Excellent communication skills
Working knowledge in Software Development Life Cycle
3 years of experience with Big Data Components/Frameworks (Hadoop, HBase, MapReduce, HDFS, Pig, Hive, Sqoop, Flume, Ozie, etc.)
Competencies:
Overall competency in business and technology
Excited & motivated to learn new technologies
Sense of urgency to attend to IT leadership demands
Responsive and prompt communication
Customer-service oriented
Excellent time management
Excellent verbal and written communication skills
Self-motivated, team player, highly organized
Attention to detail and accuracy of facts and documentation
Ability to manage multiple priorities and work independently
Ability to problem-solve and ability to follow through on tasks","Moorestown, NJ 08057",Data Engineer,False
722,"Lirio is looking for a skilled Data Engineer to build a data pipeline and warehouse to support its Data Science team. Lirio is a communication and technology company that applies behavioral science and persona segmentation to optimize individualized communication at scale. In other words, we deliver the right message to the right person at the right time to drive the customer of one to take action. This is an opportunity to join a hardworking team with the mission of truly impacting lives for the better.

The Data Engineer will be responsible for building a data architecture from the ground up that will ingest internal and external data, transform it when necessary, and store it in an analytics-friendly warehouse. The Data Engineer will work closely with Data Scientists, Machine Learning Engineers, and Developers to understand the provenance of data, information needed from the data, and potential ETL techniques. An ideal candidate is comfortable with building systems from the ground up.

Core Responsibilities:
Design, build, and maintain a scalable cloud-based data warehouse that provides standardized access to internal and external datasets.
Evaluate new data sources to understand requirements for acquisition, parsing, and transformation of the data via API or flat-files.
Build automated data ingestion pipelines to continuously ETL external data into a data warehouse.
Work with Developers to capture valuable internal events and ingest into the data warehouse.
Work with Data Scientists to harden and automate feature extraction and feature engineering scripts.
Ensure that production data products have consistent access to input data.
Additional Opportunities:
Feature engineering from unstructured or semi-structured data
Data acquisition
Education & Experience:
An ideal candidate has:

A bachelor’s degree or equivalent and proven work experience
Demonstrated capability in data engineering:
3+ years of directly relatable experience
5+ years of software engineering experience
Expertise in:
ETL
Data Flow Diagrams
Workflow management platforms (Airflow, Luigi, or similar)
Database Schemas & Normalization
Matching storage technology to data size, structure, and use
Python or Java • SQL DB and NoSQL DB technology (particularly key-value)
Ability to work with multiple teams and internal customers
Experience with cloud platforms, preferably AWS
The Benefits:
Lirio is based in Knoxville, Tennessee, with support for remote employees for the right fit. We offer competitive benefits, including ample paid time off and health insurance, among others. We have a great culture comprised of people who are committed to delivering results. We are growing and want great people to join our team.

To apply, please send resume and LinkedIn profile to talent@lirio.co and specify the position for which you are applying in the subject line.","Knoxville, TN 37923",Data Engineer,False
723,"Do you have strong product instinct and an appreciation for data-centric analytical products? Are you passionate about data and how data can be used in investing? We are looking for people like you who can help us:

Gather requirements and scope out project details with relevant stakeholders
Create and maintain optimal data pipeline architecture across different domains
Coordinate with data quality team on business rule implementations and day-to-day operations
Fix defects and implement enhancements in existing pipelines
Support data analysts in building dashboards and visualization models
Identify inefficiencies in queries and ETLs, and investigate solutions for performance tuning
Work with business teams to assist with data-related technical issues and support their data needs

Evidence Lab is one of the most innovative and highly regarded teams in UBS Investment Bank. We thrive on innovation and work as a startup within a well-established and well-funded investment bank. We specialize in various analytical techniques including web harvesting, geospatial, social media, market research, and data science. You will be joining a team of Product Managers and Business Analysts that sit within Evidence Lab that are helping to scale and grow the business. You can learn more about Evidence Lab here.

You have:

Bachelor’s degree in Computer Science, CIS / MIS, or related engineering or technical fields
Outstanding SQL, python and PL/SQL skills (R and Java knowledge is a plus)
Experience in enterprise data warehousing concepts (data modeling, architecture, etc.)
Working experience with cloud-based data solutions (Redshift, Azure Data Warehouse/Lake, HDInsight, etc.)
Working knowledge of message queuing, stream processing and scalable ‘big data’ stores
Proficiency in ETL tools such as Dataiku, Cask, and/or Pentaho

You are:

Ready to join a fast growing team in a dynamic and challenging environment
Relentless in pursuing new ideas and self-improvement
Problem solver with strong (data) analytical skills
A team player willing to learn / share solutions and best practices from your colleagues
Able to manage multiple assignments simultaneously and follow up on unfinished business

#DiceTech #DicePref
Expert advice. Wealth management. Investment banking. Asset management. Retail banking in Switzerland. And all the support functions. That's what we do. And we do it for private and institutional clients as well as corporations around the world.

We are about 60,000 employees in all major financial centers, in more than 50 countries. Do you want to be one of us?
Together. That’s how we do things. We offer people around the world a supportive, challenging and diverse working environment. We value your passion and commitment, and reward your performance.

Keen to achieve the work-life agility that you desire? We're open to discussing how this could work for you (and us).

Why UBS? Video
Are you truly collaborative? Succeeding at UBS means respecting, understanding and trusting colleagues and clients. Challenging others and being challenged in return. Being passionate about what you do. Driving yourself forward, always wanting to do things the right way. Does that sound like you? Then you have the right stuff to join us. Apply now.
UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.","New York, NY","Data Engineer – Data Warehouse, Analytics & BI",False
724,"Samba TV, recognized by Inc. Magazine as one of the fast-growing companies in the US and one of the ""most interesting ad-tech upstarts of the year"" by Business Insider, is seeking to hire a software engineer to join our Data Engineering department.

Samba TV is uniquely positioned at the forefront of the TV revolution. The way people discover, watch, and engage with television has fundamentally changed, and we're connecting the dots to help better understand audience trends and viewership habits for marketers.

The Core Data and Analytics team is one of several in the Data Engineering department. Our team is responsible for the data processing pipelines that produce key datasets consumed by our data scientists, research analysts, and external customers, as well as power our analytics platform. This team deals with data at scale — on a continuous basis, we ingest, process, and ultimately make sense of incoming viewing data from millions of televisions.

As a member of this team, you will help architect, build, operate, and maintain our data pipelines responsible for aggregating television viewing data and deriving metrics and insights that power a variety of our data products and offerings. You will not only work on our pipeline jobs, but you will also have the opportunity to help build out and further evolve our internal frameworks upon which we process and deliver data at scale.

Responsibilities


Analyze and improve the efficiency, scalability, and stability of data collection, storage, and retrieval processes for our core systems.
Create and manage platform-specific APIs.
Create new data processing systems as necessary to support our Data Scientists and Research Analysts.
Ultimately, build robust, high-volume production software.

Requirements


Strong command of a programming language or two – while we code primarily in Python, we acknowledge that engineers with sound fundamentals can pick up new languages relatively quickly.
Excellent problem solving skills. Ability to interpret and analyze data is a must. Consequently, mathematical inclination is a major plus.
2+ years of professional development experience building high-performance, large-scale applications/pipelines.
Solid foundation in computer science, with strong competencies in data structures, algorithms and software design.
Experience with Hadoop, Spark, or similar technologies is desirable.
Experience with running production systems on AWS is also a plus.

","San Francisco, CA",Data Engineer - Core Data and Analytics,False
725,"$60 - $63 an hourContractJob SummaryJob Functions / ResponsibilitiesProvide data engineering on modern, cloud-based and legacy data processing technology stacks.Build data pipelines, data validation frameworks, job schedules with emphasis on automation and scale.Contribute to overall architecture, framework, and design patterns to store and process high data volumes.Ensure product and technical features are delivered to spec and on-time.Design and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology.Proactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt.Qualifications and Skills5+ years of experience in large-scale software development with emphasis on data analytics and high-volume data processing.3+ years of experience in data engineering development.2+ years of experience implementing scalable data architectures.2+ years of experience with AWS and related services (e.g., EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake).Experience in data-centric programming languages (e.g., Python, GO, Ruby, Javascript, Scala).Proficiency with ETL tools and techniques.Knowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, IMS, MySQL, Postgres, SAP HANA, and Teradata.Experience with participating in projects in a highly collaborative, multi-discipline team environment.Work settings:Requires frequent sitting and walking.Availability to work “on-call” 24 hours/day for emergencies.Position could be required to minimal traveling up to 20%.EducationMasters' or Bachelors' degree in Computer Science or a related field.Job Types: Full-time, ContractSalary: $60.00 to $63.00 /hourExperience:AWS: 1 year (Required)","Renton, WA",Data Engineer with AWS,False
726,"AppZen has developed the world’s first artificial intelligence (AI) solution for business process automation. AppZen’s AI for business solutions uses patent-pending natural language processing, computer vision and machine learning algorithms to analyze data and automate functions. Our technology is used to automate business processes for both small and large enterprises, including Fortune 50 organizations. The machine learning based technology automatically detects accidental and intentional fraud, providing real-time compliance to IRS rules, FCPA regulations, and general company policies.

We are looking for a Data Engineer to help expand and build out our data infrastructure and pipelines. Data Engineers at AppZen work on architecting data pipelines to analyze and process streams of data throughout our system, both in offline and online contexts. You will also work with data scientists to productionize machine learning models and the pipelines that build them.
Requirements:
3+ Years experience in engineering or data science
Experience writing production quality Python code
Knowledge in databases (schema design, querying, optimization) SQL, Postgres
Knowledge in offline batch processing and/or online real-time stream processing and queueing systems
Interested in working across our entire data science stack including model building, data pipelining, and performance/scale analysis
Nice- to-haves:
Experience using systems like Apache Spark, Airflow, or other data processing tools
Knowledge in AWS tooling such as RDS, SQS, Kinesis, and S3","San Jose, CA",Data Engineer,False
727,"Elasticiti is an analytics solutions company dedicated to the media industry. Our clients are top media brands who need to innovate on multiple large datasets to inform their strategy and tactics.
The ideal candidate has a strong AWS skillset and is well rounded enough to interface effectively with business users.
Job Functions
Build an understanding of data sources and downstream systems
Liaise with key stakeholders to understand requirements, business definitions and the potential value of different data Sets.
Support, design, implement and document solutions for loading, piping and exposing data from multiple sources.
Support and build well-engineered data systems to support analytical needs using AWS .
Assure accuracy of data processing and outputs through consistently high software development skills, adherence to best practice, thorough testing and peer reviews.
Habitually approach problem solving with creativity and resourcefulness; carefully evaluate risks and determine correct courses of action when completing tasks.
Skills/Abilities:
Demonstrable professional experience designing, building, and maintaining data systems and processes using AWS Big Data Platform
Demonstrable, hands-on professional Data Analytics skills using Python, SQL. Java is a plus
Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions
Demonstrated willingness and ability to effectively work with various team members when gathering requirements, delivering solutions, and eliciting suggestions and feedback
Extremely quick learner both in terms of new technical skills and acquiring domain knowledge
Experience:
MUST HAVE - 4+ years hands on Big Data (Spark/Hive) experience
Experience 4+ years working in Python development environments for data wrangling and analytics.
Expertise using cloud-based systems and services to acquire and deliver data via APIs and flat files
Extensive hands-on experience working with data using SQL.
Extensive hands-on experience working with AWS S3, EMR, Redshift, Glue
Education:
Bachelor's Degree in Computer Science or closely related discipline","New York, NY",Senior Data Engineer,False
728,"Data Engineer
Requisition ID: 1810310

Description

Responsibilities include, but are not limited to the following: Implement best practices for data engineering Develop processes and documentation Extract and prepare data for analysis Develop and revise model code Understand data path from source to destination Troubleshoot connectivity and data integrity issues Prepare and manage infrastructure (servers, databases etc.) for data storage Coordinate with NOV software engineers, product engineers, data scientists and other data users to define and configure DataVault tag lists Prepare reliability-centered equipment maintenance templates, including tag lists and analytical models, for deployment to rigs Coordinate efforts with field engineers and NOV cross-functional teams to ensure that rigs have the proper hardware, software, and network to collect data Develop code for automating data processing pipelines Perform other work-related tasks as required


Qualifications

A minimum education of Bachelor's in Computer Science, Engineering, or related field is required.
A minimum of 2 years of experience specifically in Data Engineering is required.
Experience with programming language is required (ex: Python, R, Batch Scripting, Java, STEP7)
Experience with data processing is required (ex: Hadoop, Cassandra, Spark)
Excellent written and verbal communication skills
Experience in the following areas preferred:
o Reading control systems documentation
o Data acquisition
o Data integrity
o Data management: OSIsoft PI System (strongly preferred), Microsoft SQL, MongoDB
o Revision management: Git, Stash
o Data science platform: Domino
o Data visualization: Grafana, TIBCO Spotfire, Microsoft Power BI
o Server network installation, configuration, and troubleshooting
Drilling data analysis experience preferred
Candidates must be highly self-motivated and work well as part of a fast-paced, collaborative team

Job: Science
Schedule: Full-time
Shift: Day Job
Job Posting:","Houston, TX",Data Engineer,False
729,"ContractData Engineer Job DescriptionLocation: Based in Austin, TXTravel:  0-20%About Us: We are the premier Data Science and Machine Learning firm. We are scientists, innovators and strategists. We bring tangible value to our clients through mathematics, applied science and machine learning. We transform information into intelligence and intelligence into initiative. We formed to address the need across industries to solve complex problems with a specialized approach. Valkyrie has honed that approach for many clients with impactful, innovative outcomes. Simply put, we help our clients make the right decisions, craft the right initiatives, and answer the right questions through applied science.Data Engineer Position: The Data Engineer is the right fit for a motivated engineer that is excited to be part of a growing business, interested in Artificial Intelligence, and is passionate about data infrastructure. As an Engineer you will be assisting in developing data set processes for data modeling, mining, and production. The right candidate will have strong experience architecting systems for collecting, storing, processing, and analyzing data and has demonstrated data-wrangling excellence. In collaboration with Data Scientists, you will be a part of efforts to improve data reliability, efficiency, and quality, as well as researching opportunities for data acquisition and new uses for existing data. The right candidate for this role will leverage knowledge of large-scale processing systems to enable innovation in Data Science and Machine Learning.Data Engineer Qualifications: Bachelor’s or master’s degree in STEM (Science, Technology, Engineering, Math) or related field4+ years of professional database and distributed-computing experience3+ years’ experience with AWS (Athena, Lambda, Airflow, Kinesis, and/or Firehose preferred)5+ years of strong experience in designing, constructing, installing, testing, and maintaining highly scalable data management systemsExtensive knowledge on databases and engineering practicesExperience with various databases: SQL, PostgreSQL, MongoDB, and/or Neo4JStrong Python programming skills (including NumPy, SciPy, Pandas, and scikit-learn)ETL development and Spark experience is a PLUSStrong skills and experience in effective collaboration and technical problem solving within multidisciplinary teamsExperience in a client facing role is a PLUSPerks: Competitive CompensationTremendous growth potentialHealthcare, dental, HSAFlexible hours, work from home as neededOnsite gym & poolParkingOpen PTO & Sick timeStocked kitchen with healthy snacks, drinks, coffee, etcLots of team events including happy hours, catered lunches, and other fun outingsDog friendly officeJob Types: Full-time, ContractExperience:AWS: 2 years (Required)Location:Austin, TX (Preferred)Work authorization:United States (Required)","Austin, TX",Data Engineer,False
730,"Global video game publisher/developer headquartered in Rockville, MD, seeks a Big Data Engineer. This position works within the Enterprise BI Team and is responsible for the development of the Big Data Platform for Enterprise-wide reporting. The Big Data Engineer will be supporting a broad range of data pipelining needs from all facets of the business, including e-commerce, financial, and game event data.

The incumbent will have at least 2+ years of previous experience partnering with both technical and business teams to facilitate implementation across the enterprise. The Big Data Engineer will facilitate the creation of data pipeline processes to move data from enterprise data sources such as relational databases and log files.

Responsibilities:
Work within the Enterprise BI team, supporting the creation of data pipeline processes for ingesting data at large scales
Work directly with Data Modelers, Enterprise Architect, and Analysts, ensuring that business requirements are being met
Directly work with the Data Engineers and Data Modelers to understand the source and target structures
Coordinate with the analysts and report developers to ensure data can be easily digested by Business Intelligence tools
Be able to straddle differing subject areas such as in game vs business data sources

Requirements:
2+ Years of Experience with a major programming language (C, Java, Scala, Python, etc)
Comfortable working with structured, semi-structured, and unstructured source data
Understanding of Amazon Web Services, especially data related components
A strong communicator and is comfortable interfacing directly with differing customers across the organization in addition to the BI team
Working experience with the SCRUM development framework

Preferred Skills:
Apache Spark experience a major plus (Spark RDDs, Spark DataFrames, Spark SQL)
SQL skills – able to query data sources and generate results from complex structures
A clear understanding of both row and columnar storage databases
Experience working within the VideoGame/MMO industry is desired, though candidates from outside of the industry are also welcomed
Understanding of the Free to Play/Microtransaction Business Model is a plus
A personal interest in video gaming is a plus","Rockville, MD",Big Data Engineer,False
731,"ContractDescription: The Data Center Network Engineer assists in the Installation team in maintaining our key role of being a valued and trusted technology employee through effective installation and support of data (routing/switching), wireless, and network security products in a large Application Centric Infrastructure framework.Responsibilities: Data Center Engineer Position in large ACI environmentDesign, implementation, and support of network architectures, primarily CiscoInstall and configure network solutions using routers, switches, and security devicesInstall and configure software systems that support the network infrastructure such as network monitoring systems, log monitors, DNS servers, and firewalls.Troubleshoot network errors and performance problems.Documenting procedures and changes related to design, installation, and support.Experience in hardware provisioning, installation, configuration, maintenance, and troubleshooting.Ability to handle complex problems effectively. Identify key issues and generates multiple solutions.Understand client needs, identify root causes of problems, and develop and implement creative and pragmatic solutions.Create as-built documentation and provide knowledge transfer to customers upon completion of the installation and configuration.Requirements: Desired Skills and ExperienceACI experience a mustCisco Certifications: CCNP Route Switch, CCNA Data Center or CCNP Data CenterNexus R/S (9k/7k/5k/2k)Catalyst Switching Experience (9k,6k/4k/3k/2k)Programmability experience preferred e.g. Python, Ansible and JSON scriptingJob Type: ContractExperience:Nexus R/S (9k/7k/5k/2k): 5 years (Required)ACI: 5 years (Required)Python, Ansible and JSON scripting: 4 years (Required)License:CCNP Route Switch (Required)CCNA Data Center (Required)CNP Data Center (Required)","Franklin, TN",Data Engineer,False
732,"$70,000 - $76,000 a yearHydromax USA is looking to add a Data Engineer to the team. The ideal candidate will show enthusiasm towards building and maintaining distributed data systems. Initially this role will be focused on the set up of a new Big Data analytics production environment which will eventually merge with existing infrastructure. The ideal candidate possesses advanced understanding of data storage and usage at large scale. After the initial deployment of a production environment, this role will translate data science algorithms into production ready services and help support production data systems. Our team includes some of the brightest minds in the industry and we hope to add you to our roster.
Duties:
Develop, deploy, and manage distributed systems in a production environment
Turn data science algorithms into production pipelines
Design scripts for data management
Support analysts and developers in query optimization
Work with developers to establish new data services
Tune analysts’ queries for optimized performance
Containerize developer created applications for deployment
Build and maintain external data ingress and egress protocols
Requirements:
Bachelor degree in a technical field or equivalent experience in a related position
3+ years of experience with data systems
Advanced understanding of SQL
1+ years of experience building data pipelines
Fluency with one or more coding languages (Java, Scala, and Python preferable)
Experience with Apache Spark or similar Big Data platforms
Familiarity with Rest Endpoints
Preferred Qualifications:
Master’s Degree in a technical field
Experience working with spatial data
Formal education in Mathematics
Experience building containerized applications (Kubernetes or Docker)
Experience deploying and using distributed systems (NoSQL, Apache Kafka, Apache Airflow, Presto…etc)
Familiarity with the Linux terminal
Benefits: Health, Dental and Vision Insurance; 401(k) company matching, up to 5% after year 1; Paid Personal Time Off (PTO), and Paid Holidays; Profit Sharing.

EOE: We are an equal opportunity employer and do not discriminate against otherwise qualified applicants on the basis of race, color, creed, ancestry, religion, orientation, age, sex, marital status, national origin, disability, genetic information, handicap or veteran status.
Qualifications","Louisville, KY 40299",Data Engineer,False
733,"Our customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets. We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in design and implementation of a top-notch Big Data solution to be deployed at massive scale.

Responsibilities:
Participate in design and development of Big Data analytical applications
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Implement large-scale near real-time streaming data processing pipelines
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale
Requirements:
Strong knowledge of Scala
In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)
Understanding of the best practices in data quality and quality engineering
Experience with version control systems, Git in particular
Desire and ability for quick learning of new tools and technologies
What will be a plus:
Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)
Experience with Github-based development processes
Experience with JVM build systems (SBT, Maven, Gradle)

What we offer:
Work in the Bay Area with terrific customers on large, innovative projects.
High-energy atmosphere of exponentially & successfully growing company.
A very attractive compensation package with generous benefits (medical, dental, vision and life), 401K and Section 125 pre-tax offerings (POP and FSA plans).

About Us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, omnichannel services, DevOps, and cloud enablement","Cupertino, CA",Big Data Engineer,False
734,"Movable Ink powers meaningful experiences in email and on the web for the biggest brands in the world. Data is at the heart of these experiences - we are collecting many terabytes of data each quarter, and all of it must be partitioned and aggregated for many different use cases.

The Principal Data Engineer will be responsible for all data access patterns across the business. Data Scientists will want access to the billions of events tracked across our customers' web sites each day. Data Analysts will want connect that usage back to configuration data in our relational database. The product itself will need to aggregate this constant flood of data in real time.

Fast-forward one year. Here's what you will have accomplished:


Supported data initiatives in three different products using a combination of stream processing, messaging queues, and batch ETL
Become an expert in our existing storage technologies and our use cases to suggest and implement enhancements
Enabled the Data Science team by providing them with the tools and the dataset they need to be effective
Connected product data to business data for ad-hoc analysis with BI tools
Performed a cost analysis for moving from a unified data storage approach to regional isolation
Partnered with Information Security to define and implement recommend procedures for data storage and access

Experience:

You've done a lot of work with Big Data tools, such as Spark, Storm, Hive, Hadoop, etc
You've implemented storage mechanisms for high-throughput workloads
You're comfortable in AWS and have run production systems there

","New York, NY",Principal Data Engineer,False
735,"Description

Job Purpose
The Data Engineer I is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.

Key Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.

Qualifications
Knowledge, Experience & Qualifications:
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience with relational databases and knowledge of query tools and statistical software is required including but not limited to:- SQL Management Studio - SQL Integration Services- SQL Database Services- SQL Job Agent- SQL Server Data Tools- SQL Reporting Services- SQL Analysis Services Tabular- Power Query/M
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, and data storage techniques
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)


About Cox Automotive
Cox Automotive Inc. makes buying, selling and owning cars easier for everyone, while also enabling mobility services. The global company’s 34,000-plus team members and family of brands, including Autotrader®, Clutch Technologies, Dealer.com®, Dealertrack®, Kelley Blue Book®, Manheim®, NextGear Capital®, VinSolutions®, vAuto® and Xtime®, are passionate about helping millions of car shoppers, tens of thousands of auto dealer clients across five continents and many others throughout the automotive industry thrive for generations to come. Cox Automotive is a subsidiary of Cox Enterprises Inc., a privately-owned, Atlanta-based company with revenues exceeding $20 billion. www.coxautoinc.com
Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual’s age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.

Organization: Cox Automotive

Primary Location: US-IN-Carmel-11799 N College Ave

Employee Status: Regular

Job Level: Individual Contributor

Shift: Day Job

Travel: No

Schedule: Full-time

Unposting Date: Ongoing","Carmel, IN",Data Engineer I,False
736,"Requisition ID: 25606

There are currently more than 20,000 objects in Earth's orbit. Some of these objects are operational satellites performing critical civil, scientific, and military missions. Tracking, monitoring, controlling and defending these satellites is of key national interest. The Information Integration and Decision Support Group develops the mission critical decision support tools needed to perform key functions such as rapid event detection and dynamic scheduling of assets in order to develop courses of action for the Space Operator. The decision support tools are driven by integrating information from various sensors, sources, and systems.

The focal point of the group's activities resides in its Lexington BMC3 Testbed (LC3T) facility .It provides a real-time framework environment to ensure interoperability of several service-oriented-architecture (SOA) systems, to evaluate and operationalize the contributions of new sensors and sources, and to develop and assess the performance of new decision support algorithms using live data.

We are seeking a data engineer to help us build the next generation automated decision support tools in the LC3T facility. These tools will help the space operator understand the behavior of objects in space and the intent of this behavior. This engineer will be working with a team to create the end to end design, as well as development and deployment of new capability. Tools developed in the LC3T are commonly used in space operations today.

Qualifications: BS in Electrical Engineering, Physics, Applied Mathematics or similar technical field with 0 - 3 years is required. Experience with Java, SQL and NoSQL databases, data analytics (such as pattern recognition & change detection) is highly desired.
Position will require candidate to acquire and maintain a Secret clearance.





MIT Lincoln Laboratory is an Equal Employment Opportunity (EEO) employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, veteran status, disability status, or genetic information; U.S. citizenship is required.",Massachusetts,Data Engineer,False
737,"Description:
The data science and engineering team at Target is a hyper-growing, dynamic and collaborative team. Data engineers work closely with data scientists to create valuable insights using voluminous data collected from internal and external systems on a large-scale. Business operations are empowered with these insights to achieve Target’s strategic initiatives while providing world-class shopping experiences.

Use your skills, experience and talents to be a part of groundbreaking thinking and visionary goals with Target’s Data Science and Engineering team in Sunnyvale. As a Sr. Data Engineer, you will take the lead as you work on:

Designing and Implementation

Ownership of all technical aspects of software development for assigned application
Utilize the data structure, and algorithms to make the system more efficient
Support ongoing refactoring of code, utilize visualization and other techniques to fast-track concepts and deliver continuous improvement
Analyze, monitor and optimize for performance
Write code and unit tests, work on API’s, automation and testing
Produce and maintain high-quality technical documentation
Scope of Work

Will gain new knowledge from upcoming research and applying it to design and development
Develop deep understanding of tie-ins with other systems/platforms within the supported domains
Share the gained knowledge with other team members through interactive sessions
Identify opportunities to adopt innovative technologies
Works with Product Owners and Principal Data Engineers to prioritize features for ongoing sprints and managing the list of technical requirements based on new known defects and issues
Provides continuous support for design, development and application availability
Requirements:

2+ years of experience in developing software applications including: analysis, design, coding, testing, deploying and supporting of applications
BS degree in Computer Science, Applied Mathematics, Statistics or area of study related to data sciences and data mining
Understand application/software development and design in an Agile environment
Collaborative personality, able to engage in interactive discussions with the rest of the team
Inquisitive on Big Data technology; current on new ideas and tools
Good understanding of Data structures and Designing of Algorithms
Good understanding of basic mathematical concepts and ability to grasp complex mathematics
Should know at least one programming language i.e. Java, Python, C/C++
Preferred Qualifications:

Experience with Hadoop stack (Map Reduce, Kafka, Pig, Hive, HBase, Cassandra, YARN) and/or BDAS (Scala/Spark)
Extensive experience designing and developing API’s in large scale environments
Experience working on projects facilitating true CI/CD (Jenkins, Drone, etc.)
Exposure to DevOps tools and culture (i.e. Kubernetes, Docker, Spinnaker, Jenkins, Git, etc.)
Awareness of new and emerging Big Data technologies
Qualifications:","Sunnyvale, CA",Sr Data Engineer - APIs for Test/Measurement Platform,False
738,"$150,000 - $200,000 a year*Tech Team Extension is looking for a Director of Data Science/Engineering for one of our high growth clients! AdTech experience is a huge plus!*The Data Engineer / Science Director will lead a team responsible for designing and improving our big data infrastructure. The team is also responsible for providing various advanced mathematical solutions for large-scale problems in other areas of our system. We process a few billion events daily and the amount of data we process is growing rapidly. We believe in being smart, innovative, pragmatic, polished, and efficient and are looking for people that fit those values. This role reports to our CTO.What you’ll do: Develop various predictive models to help solve various big data problemsAdapt & improve various parts of the Our big data infrastructure/stack to handle the above modelsWork with various team members to help improve areas which deal with high volume low latency problems, using various statistical/mathematical modelsWhat you’ll need: Good background and experience with statistical and other advanced math problem solvingProven experience writing advanced algorithms on top of Big Data infrastructureKnowledge in a variety of non-SQL big data open source products such as Spark, Hadoopetc.Programming experience, ideally in Python, Scala, Java or R, but we are open to other experience if you’re willing to learn the languages we useKnowledge of machine learning libraries and such as Spark MLlib, Mahout,scikit-learn,theano,*vowpalwabbit, H2O, or*xgboostand ability to apply machine learning at scale.Knowledge of data mining and natural language processing is a plusAbility to operate in an agile, entrepreneurial start-up environmentExcellent teamwork skills with an ability to reach out and use team strengths to get the work done with minimal supervisionGood communication skillsYou must be motivated, driven, and passionate about programming and technologyJob Type: Full-timeSalary: $150,000.00 to $200,000.00 /yearExperience:Data Mining: 4 years (Preferred)Python: 3 years (Preferred)Natural Language Processing: 1 year (Preferred)Big Data: 3 years (Preferred)Java: 3 years (Preferred)","El Segundo, CA","Director, Data Science/Engineering",False
739,"The Data Analytics and Visualization Group (http://dav.lbl.gov/) has an immediate opening for a Scientific Data Engineer.

The Data Engineer will work on development and applied research in the area of data modeling, management, and analysis in the area of neuroinformatics applications. The position is in support of a research project, funded by the National Institutes of Health (NIH) that aims to develop methods for standardization, sharing, and analysis of neurophysiology data. The overall objectives for this position are to develop new methods and software tools that enable scientific knowledge discovery using high performance computational platforms and advance the state-of-the-art in data-intensive analysis. This positions will be part of an experienced team conducting R&D in the areas of data-intensive, high-performance analysis, visualization, and data management. The candidate will be working as part of a multi-disciplinary team composed of computer scientists, mathematicians, and experimental/computational neuroscientists.

The Scientific Data Engineer will:

Design, implement and maintain HPC ready software tools for querying and visualizing complex multidimensional neurophysiology data, and design, implement and maintain software tools for creating and running parallel data intensive analysis workflows.
Develop methods for tracking provenance of analyses of experimental data, and develop capabilities for integrating standardized data with web-based data archives.
Ensure and exercise software development best practices around version control and continuous integration and deployment
Work closely with the community of developers of the Neurodata Without Borders (https://www.nwb.org/) open source data ecosystem, and work closely with neuroscientists from institutions such as UCSF and the Allen Institute for Brain Science to develop new capabilities of the Neurodata Without Borders ecosystem

Minimum Qualifications:

Bachelor’s degree in a field with an emphasis on mathematics and/or computer science (or equivalent experience) and at least 2 years of related experience designing and developing software for data modeling or analysis.
Demonstrated capability with programming languages, such as C/C++, Python, Java
Demonstrated capability with version control systems, such as Git, Mercurial or SVN
Experience developing complex software solutions
Experience testing large code bases
Experience contributing to community-driven open source software
Demonstrable experience in one or more of the following areas: data modeling, data management, data query, machine learning, visualization

Additional Desired Qualifications:

Master’s or PhD in Computer Science or related field, with 5 or more years of professional experience designing and developing scientific data modeling or analysis software.
Demonstrated capability with parallel libraries and environments, such as, MPI, OpenMP, CUDA, pthreads, OpenCL, OpenACC
Demonstrated capability with CI/CD systems, such as TravisCI, CirlceCI, or AppVeyor
Experience working in shared compute resources (e.g., clusters)
Experience working with relational and/or non-relational database systems, e.g., MongoDB, PostgreSQL, MySQL, Redis
Experience working with HDF5
Experience developing REST APIs
Experience with the Neurodata Without Borders data standard and PyNWB

Notes:

This is a full time, 2-year, term appointment with the possibility of extension or conversion to Career appointment based upon satisfactory job performance, continuing availability of funds and ongoing operational needs.
The level of the position will depend upon the applicant's demonstrated skills, knowledge, and abilities.
Full-time, M-F, exempt (monthly paid) from overtime pay.
Salary is commensurate with experience.
This position may be subject to a background check. Any convictions will be evaluated to determine if they directly relate to the responsibilities and requirements of the position. Having a conviction history will not automatically disqualify an applicant from being considered for employment.
Work will be primarily performed at: Lawrence Berkeley National Lab, 1 Cyclotron Road, Berkeley, CA.

Berkeley Lab (LBNL) addresses the world’s most urgent scientific challenges by advancing sustainable energy, protecting human health, creating new materials, and revealing the origin and fate of the universe. Founded in 1931, Berkeley Lab’s scientific expertise has been recognized with 13 Nobel prizes. The University of California manages Berkeley Lab for the U.S. Department of Energy’s Office of Science.

Equal Employment Opportunity: Berkeley Lab is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or protected veteran status. Berkeley Lab is in compliance with the Pay Transparency Nondiscrimination Provision under 41 CFR 60-1.4. Click here to view the poster and supplement: ""Equal Employment Opportunity is the Law.""","San Francisco Bay Area, CA",Scientific Data Engineer,False
740,"In addition to the responsibilities listed below, this position is responsible for planning, designing, and building database systems that are stable, recoverable, and integrated with other available technology stacks for the purpose of efficient, secure data utilization. This includes understanding the interoperability between databases and other dependent technology stacks (performance scalability, stability, capacity planning, etc.), possessing in-depth knowledge in one dependent technology stack (e.g. windows admin, networking, storage, etc.), negotiating with other engineering platform partners, and consulting as 2nd level technical expert on specific platforms. This role also requires familiarity with at least one other DBMS platform in addition to specialty and administrative skills in at least one other platform.Some of the unique challenges this position will face include database considerations for a large, corporate enterprise and a high degree of complexity and non-uniformity.


Essential Responsibilities:

Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.

Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.

As part of the IT Engineering job family, this position is responsible for leveraging DEVOPS, and both Waterfall and Agile practices, to design, develop, and deliver resilient, secure, multi-channel, high-volume, high-transaction, on/off-premise, cloud-based solutions.

Provides consultation and expert technical advice on IT infrastructure planning, engineering, and architecture for assigned systems by assessing the implications of IT strategies on infrastructure capabilities.

Provides some recommendations and input on options, risks, costs, and benefits for systems designs.

Leverages partnerships with IT teams and key business partners to troubleshoot complex systems.

Serves as a functional expert and collaborates with architects and software engineers to ensure functional specifications are converted into flexible, scalable, and maintainable system designs.

Translates business requirements, and functional and non-functional requirements, into technical specifications that support integrated and sustainable designs for complex or high impact infrastructure systems by partnering with Business Analysts to understand business needs and functional specifications.

Ensures system designs adhere to company architecture standards.

Builds partnerships with counterparts in various IT Teams (e.g., database, operations, technical support) throughout system development and implementation.

Serves as a technical expert for project teams throughout the implementation and maintenance of assigned enterprise infrastructure systems by defining and overseeing the documentation of detailed standards (e.g., guidelines, processes, procedures) for the introduction and maintenance of services.

Mentors other technical resources throughout infrastructure systems development.

Reviews and validates technical specifications and documentation for complex or multi-dimensional solutions.

Leads the development and modification of solutions by identifying technical solutions to business problems.

Collaborates with business leaders, Solutions, and lead enterprise architects to review business drivers, and establish a foundation for enterprise systems planning.

Reviews benchmarking results, and provides information to support current and future infrastructure needs and projects to IT leadership. Provides preliminary conclusions.

Benchmarks and evaluates IT trends and technologies to identify opportunities and considerations that impact ROI.

Makes recommendations on resources required to maintain service levels and meet new demands.

Guides and drives physical architecture design for new initiatives.","Pleasanton, CA",Principal Big Data Engineer,False
741,"Job Description:
Mid-level position participating in data conversion, data cleansing, data integrity solutions, as well as generating user documentation for our application. There is opportunity to attain high visibility to our clients and become entitled to bonus participation based on performance and contribution. Great opportunity to acquire oil and gas experience, as well as client service skills.
Assists in the compilation, analysis and manipulation of massive sets of data in order to convert raw data into meaningful information in client systems. This responsibility requires exemplary skills in SQL. MS Excel and MS Access skills are also valued.
Defines processes and procedures associated with unique client requirements for data management and reporting.
Researches and recommends innovative approaches for project execution. Recommends areas for improvement in internal processes along with possible solutions and provides status reports to stakeholders and addresses issues as appropriate.
Participates with team in problem resolution regarding systems and procedures. Assists team, or supervisor with special consulting projects to identify client data anomalies and provide business solutions.
Documents procedures using text and workflow diagrams and screen shots from application. Develops and maintains installation and configuration procedures. Provides documentation for institutional workflow processes associated with application.
Responsible for the provisioning of application access accounts: create, modify, and remove user accounts or permissions as required.
Ensures systems documentation follows best practices, is up-to-date, accurate, and tested. Maintains inventory of equipment, software subscriptions, troubleshoot user computer and application issues, coordinate with client IT personnel for implementation and trouble-shooting. Recommends policies and procedures which govern the applications.
Performs other duties as assigned and works under minimal supervision with some latitude for independent judgment.
Qualifications/Knowledge/Experience Required
Requires bachelor's degree in Computer Science, Information Systems, Finance, Accounting, or related curriculum. Must possess strong customer service and interpersonal skills. Additional skills, talents and languages:

Data warehousing solutions
In-Depth knowledge of database architecture
Systems development
ETL, spreadsheet and BI tools
Data modeling
Communication & visualization
Data APIs
Database systems (SQL, etc.)
Math, Stats, Machine Learning
SQL, XML, C++, R, Python","Oklahoma City, OK 73102",Data Architect / Data Engineer / Data Analyst / Business Ana...,False
742,"Data Engineer

 At MediSpend, we are on a mission to transform and simplify how the life science industry complies with global healthcare industry regulations. MediSpend Global Compliance Solutions are recognized market leaders fully integrated within a first-mover, born in the cloud technology platform. We’re gearing up for the next phase of growth, looking to onboard hungry, humble, and smart people who thrive on solving business problems with innovative technology. Read on to see what’s different about this opportunity and begin to visualize how you will advance your career by joining our team.

Check this out:

Get to know the regulatory compliance and healthcare entity data domain
Work with “big data” architects to create analytic data structures
Use modern data wrangling tools and techniques to inspect and transform data
Participate in the renaissance of the functional programming paradigm within the industry’s hottest data transformation/analytics framework
Learn and practice the tricks to working with data at scale
 A day in the life:

You are a Data Engineer at MediSpend. You practice ETL and ELT. You create data transformations to standardize three aggregate spend transaction files into a standard format. You build innovative algorithms to detect duplicate healthcare providers. You figure out that a particular file won’t load properly because there are missing delimiters. You build custom crosswalks that transform client specific data to standardized formats. You build a set of routines to create a set of denormalized data structures that will enhance analytic execution speeds. You help build operational metrics to report on data quality and volumes. You aid the product team to mine for client data inconsistencies.

What you bring to the table:
You’re a professional with a great blend of practical experience, education, and achievement. You’re efficient at getting your points across in written and verbal mediums. You are passionate about working with data. Exposure to the healthcare data domain is a plus, but not required.

Significant experience with ETL/ELT tools and platforms is expected. We’re a Java shop, so CloverETL, Pentaho, Talend experience helps. If you’ve used modern data wrangling tools like Paxata, Tamr, or Trifacta, even better. Spark experience is a plus. You should be efficient with at least one programming language such as Java, Scala, or Python. SQL skills are a given. Knowledge of data storage systems which include traditional RDBMS (Oracle, PostgreSQL, MariaDB), analytic data stores (Vertica, Greenplum, Redshift, Presto), and object stores (AWS S3, OpenStack Swift) helps also.
You’ve been in the trenches delivering commercial applications requiring data accuracy. You have contributed to the development of repeatable, modern data processing pipelines.

Your move:

Growth story, startup feel, life science domain, passionate technologists, and born in the cloud

Looking for full-time employees only","Portsmouth, NH 03801",Data Engineer,False
743,"Data Engineer - Downtown Boston - $65-90K

A stable healthcare company reinventing the primary care system
in downtown Boston is looking for a Data Engineer to join their growing team! This
is a mid-size company but the data team is small, so they are looking for someone
who thrives in a team environment. Some additional details on this position:

Requirements:
Contribute to the full life cycle of ETL
processes

1-2 years’ experience with SQL, relational
databases, Python

Strong communication skills are a must

Past experience working in the healthcare
industry

Pluses:
Experience with Looker, Tableau, SSIS,
Matillion, Talend

Benefits:
Health Dental Insurance

20 days PTO

Tuition Reimbursement

Equity",United States,Data Engineer,False
744,"Description:
Lead Data Engineer – High Performance Computing
Target EDABI (Enterprise Data Analytics Business Intelligence) is revolutionizing the way how Target retail uses data. Located in Sunnyvale, CA, it’s just across the street from the local train station in the heart of Silicon Valley. Originally opened in 2014, the Sunnyvale office is now home to more than 100 team members who work to make Target a more modern data-driven retail company. To learn more, view this article: https://corporate.target.com/article/2016/11/meet-target-sunnyvale
Team Introduction
The High-Performance Computing Group at Target EDABI not only aims to enable teams at Target to stream, filter, transform, and analyze high-bandwidth data in real-time, but also provide tools for data analysts and other team members to analyze and take action on their data streams.
What will you be doing:
The High-Performance Computing Engineer will design and develop parallel computing algorithms for solving very large machine-learning problems on heterogenous platforms including vector engines (AVX512), FPGAs, and other types of super-computing hardware platforms.
Principal Duties/Responsibilities
Design and develop parallel computing algorithms
Manage life-cycle of HPC products from inception to deployment
Requirements
MS or PhD degree in Computer Science, Electrical Engineering or Computer Engineering
Ability to analyze systems at all levels
Ability to make decisions based on strict scientific analysis
Outstanding communication skills
Ability to create, innovate, think out-of-the box
Understand application/software development and design.
Collaborative personality, able to engage in interactive discussions with the rest of the team
In-depth understanding of computer, processor architecture
Ability to create new solutions beyond available open source code
Ability to explain strengths and weaknesses of generally available open source, such as Hadoop, Spark, Storm, Flink. (We do not use these Open Source solutions, but it is important to know, understand how they fail in achieving super-computing performance levels).
In-depth understanding of shared-memory multiprocessor, multi-core programming
In-depth understanding of parallelization paradigms
Experience in designing and developing large-scale real-time systems
Deep understanding of Operating system kernels
Deep understanding of machine-level architecture and programming
Deep understanding of computer performance, benchmarking, latency, throughput
High-performance linear algebra libraries
Expert-level knowledge of C (or C++).
Some familiarity with Tensorflow or similar
Conceptual understanding of Opencl, Verilog, FPGA development, CUDA
Qualifications:","Sunnyvale, CA",Lead Data Engineer - High Performance Computing,False
745,"THE CHALLENGE
The data engineering team is building the next generation of data infrastructure. We work with cutting-edge big data technologies to power the most efficient, secure, and performant data ecosystem to power live experiences. Operating at the base of Eventbrite's infrastructure, we feed data to analysts, customers, engineers, and the platform alike. Our core foundation is modular and strong. We hope you can help us extend its use across the company as we explore and implement new technologies.

THE TEAM
The Data Engineering team is charged with building and maintaining all streaming and batch data pipelines across the company. We also support and empower analysts, engineers and data scientists with the tools they need to create a data-driven product and power data-driven decision making. We are pragmatic and meritocratic. We work hard. We play hard. We are strongly connected to each other. Eventbrite engineers perform frequent demos of the code we ship. We also hone our skills through code reviews and tell the world about it on Eventbrite’s Engineering Blog. We value community, we believe in the power of live experiences, and we regularly host free events with top technical speakers. Learn more about the team from some of our engineers here.
THE ROLE
We are hiring someone to help us build a scalable, reliable, secure, and highly performant data platform. You'll help reinforce and extend the infrastructure that powers the use of data at Eventbrite. From infrastructure development to data analysis to ETL jobs, you will need a broad range of big-data engineering skills. The team has strong and versatile engineers. You will grow. We hope to grow with you.

TECH STACK
Our primary stack includes usual suspects in big data engineering: Spark, Hadoop, Presto, Spark SQL, Hive, Spark Streaming, Kafka, MySQL, Redis, AWS, Yarn, Ansible, Terraform, Python, Git, and more.
THE SKILL SET
1-3 years of experience building high quality software in Python, Java, or Scala.
1+ years of working experience in rapid product development building data infrastructure, ETL, or MapReduce jobs
Working knowledge of SQL and relational database design and modeling, approaches and techniques for extracting, transforming, loading and integrating data
Understanding of Data Engineering, Data Science, Machine Learning, Business Analytics, and the relevant technologies that support them
Familiarity with a server-side framework, such as Django, Express, Rails, or .Net
Excellent customer service skills
Outstanding verbal, written, presentation, and facilitation skills. In particular, a demonstrated ability to effectively communicate technical and business issues and solutions to multiple organizational levels
Ability to teach and mentor engineers with a variety of skill levels and backgrounds.
Strong analytical and problem solving skills and attention to detail
BONUS POINTS
Bachelor’s degree or higher in a technical field (CS/Math/Stats/Engineering)
Passionate about live entertainment, and eager to help build Eventbrite into the world’s leading event technology platform
ABOUT EVENTBRITE
Eventbrite is the world’s largest ticketing and event technology platform, powering millions of live experiences around the globe. We build technology that allows anyone to create, share, find and attend events of all kinds. Music festivals, marathons, conferences, hackathons, political rallies, fundraisers, gaming competitions— you name it, we power it. Meet some of the team.

IS THIS ROLE NOT AN EXACT FIT?
Sign up to keep in touch and we’ll let you know when we have new positions on our team.

Eventbrite is a proud equal opportunity/affirmative action employer supporting workforce diversity. We do not discriminate based upon race, ethnicity, ancestry, citizenship status, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), marital status, registered domestic partner status, caregiver status, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, genetic information, military or veteran status, mental or physical disability, political affiliation, status as a victim of domestic violence, assault or stalking, or other applicable legally protected characteristics.

FLSA Status: Exempt

Please read our Applicant Privacy Notice to understand how we process your personal information when you apply for a job with us.","Nashville, TN",Data Engineer,False
746,"Job Description

This role works as a member within Decision Sciences of Consumer Banking Group at First National Bank of Omaha. The principle purpose of this position is to conduct assignments to support initiatives related but not limited to the following:

Perform data import, wrangling and visualization on very large datasets. Data validation and Integration from multiple sources, including SQL and NoSQL databases.

End to end programming support of machine learning initiatives, including coding, testing, and code validation.

Assist automating production code, optimizing model efficiency, improving quality assurance, and deploying model into production on big data platform.

Visualizing data in a way that tells a story.

Work independently and collaboratively within the team to help to achieve the desired outcomes of an analytical project.

Job Qualifications:
Basic Qualifications

Minimum of 3 years of working experience in data science.

Must have a Master’s degree in Computer Science, Management Information System, Statistics or other related STEM fields.

Strong working knowledge in computer programming languages, including R, SQL, and Python.

Experience with data visualization tools (e.g. R Shiny, Markdown, Microsoft Visual Studio, PowerBI)

Depth of knowledge in leveraging latest big data tools/platforms (i.e. machine learning techniques, deep learning), highly-preferred experience in big-data computation, such as Hadoop, Spark, and MapReduce.

Demonstrate excellent oral and written communication skills.

Demonstrate the ability to handle stress, meet deadlines and work with minimal supervision.

Demonstrated experience working in multi-disciplinary teams.","Omaha, NE",Data Engineer,False
747,"Digital Pharmacist is a rapidly growing digital health company that powers technological communication and adherence solutions for 7,500 pharmacies, national pharmacy wholesalers, hospital systems and pharmaceutical brands. Millions of patients use the company's products every month. The company is headquartered in Austin, Texas, with offices in Newark, New Jersey. Digital Pharmacist is the official digital partner of the National Community Pharmacy Association and a winner of the 2017 Austin A-List Awards.
The Role
Own all of the data infrastructure, the data lake/warehouse, and ETL jobs
Architect data patterns to support our organization’s needs
Maintain security of HIPAA data while using it to power business decisions and analytics, as well as other customer facing systems
Build a robust, comprehensive data architecture to support the goal of building tools to utilize large sets of data to help improve medication adherence and promote patient health
Qualifications
B.S. in Computer Science or Engineering, or equivalent experience
2+ years of ETL/data engineering work
3+ years of professional software engineering
Experience working with Relational and noSQL databases
Experience working with engineers and business analysts to solve data needs
Experience working with a data pipeline and ETL jobs
Preferred
Experience with big data frameworks (Spark, EMR, etc)
Experience with Airflow, Luigi, Pinball or equivalent batch job managers
Experience working with cloud infrastructure
Experience with BigQuery, Redshift, Vertica, Cassandra or others
Experience consuming and building RESTful APIs
Why work with us?
Stock options
401(k) plan
Dog-friendly workplace
Three weeks of PTO plus holidays
Paid medical, dental and vision plan
Breakfast taco Mondays and a fully stocked kitchen for snack breaks daily
Free Orange Theory and yoga classes","Austin, TX 78751 (Hyde Park area)",Data Engineer,False
748,"ObjectRocket is a young company with big goals. We want to build the next generation of Database as a Service, and we need your help. We need folks who want to build something that hasn't been done yet, is hard, yet fantastically rewarding.

We are located in the heart of beautiful downtown Austin, TX. Austin is highly regarded as a wonderful place to live, work, and play. It's the Live Music Capital of the World and has a serious night life.

ObjectRocket has a fast paced, and exciting culture. We are a small team, and move quick. We are building something quite amazing and look to be leaders in our field and community. We are growing like crazy, we need more help!

We own the entire stack (hardware and software) so that means you’ll need to find and resolve issues at all levels. To do this successfully you’ll be switching hats often, from Systems Administrator resolving a configuration problem on a machine to DBA digging into production datastores to identify issues, to Software Developer troubleshooting customer code. You’ll have the opportunity to work with a broad range of technologies, in particular Hadoop, MongoDB, ElasticSearch, and Redis. Additionally, you’ll be responsible for owning the full customer experience; this includes communication with customers via our ticketing system and phones, working complex problems and owning issues through to resolution.

You’re a seasoned Systems Administrator and technical jack of all trades with top-notch people skills looking to join a fast-paced team building huge things. You prefer an agile work environment and possess the self-motivation to thrive in it. Your written communication skills are the stuff of legends.

You have an uncanny ability to solve issues with technologies you’ve never seen before after a few minutes of research. You are, above all, a problem solver.

Qualifications:
Tenured Linux knowledge with CentOS, RHEL and/or Ubuntu
Functional virtualization knowledge with KVM, Xen and/or OpenVZ
Prior training or experience installing, configuring and optimizing Hadoop, MongoDB and/or Redis
In-depth knowledge of bash/shell scripting and working knowledge in at least one other language.
Python or Java preferred.
Excellent written communication skills
Ability to learn new technologies quickly
Passion for collaboration

Nice to haves

Working knowledge of storage tech such as SSDs, RAID and lvm2
Nagios/Icinga and New Relic experience.
Experience administering web and application servers running Apache, Nginx and Gunicorn
Active in the technical and open source community (e.g. participating in blogging, tweeting and social coding are all very desirable)
Database administration experience with other NoSQL solutions (e.g. HBase and Elasticsearch).
Knowledge of PostgreSQL, Cassandra and/or other RDBMS technologies.
Experience with the Hortonworks sandbox environment","Austin, TX",Customer Data Engineer I - ObjectRocket,False
749,"What You'll Do
We need a self-motivated and highly curious individual that is excited to work with ever-evolving groundbreaking technology in the big data field. You will apply a variety of big data technologies to the security space to ingest, transform, index, aggregate, correlate, provide API's, visualize and enable a spectrum of organizations across Cisco.
Who You'll Work With
As a member of the Threat Intelligence Platform team, you'll be part of a highly empowered collaborative group who's passionate about using data to help secure Cisco, enable security products, and cultivate security research.
Who You Are
You are a self-starter who loves to dig into data and who your teammates trust to throw challenging problems in your direction. As a result, you help make data much easier to understand and consume for others and have managed to pivot to new technologies rapidly.
At minimum, we expect you to have:
BS in Computer Science or related technical degree
Active proficiency with Python, Git and test automation
Minimum 3 years of experience on big data platform (ex. Hadoop, Spark, Kafka, Clickhouse, NiFi)
Experience with crafting and managing data pipelines
DevOps mindset with experience on agile team
Strong technical writing and communication skills
Excellent problem solving and decision-making skills
The following would be beneficial, but not required:
Experience with containerization (specifically Docker & Kubernetes)
Experience with SQL and Linux
Security experience
Splunk dashboards, reports, and alerting
Why Cisco
At Cisco, each person brings their unique talents to work as a team and make a difference. Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.
We connect everything securely– people, process, data and things – and we use those connections to change our world for the better.
We innovate everywhere- from launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.
We benefit everyone- We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us! #WeAreCisco
Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.LI-KM1","Research Triangle Park, NC",Big Data Engineer,False
750,"Job Description
Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to be part of team building a new machine learning platform that processes billions of records a day in a scalable fashion using AWS technologies?

Amazon's Finance Technology team is seeking an outstanding Big Data Engineer to join the team that is shaping the future of the finance machine learning platform. The team is committed to building a large-scale machine learning platform solving complex and ambiguous problems in the finance space to service customers such as payments, treasury, and finance operations. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and machine learning.
Basic Qualifications
Bachelor's degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
Demonstrated ability in data modeling, ETL development, and data warehousing.
Coding proficiency in at least one modern programming language (e.g. Python, Java, Scala)
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Query performance tuning skills using Unix profiling tools and SQL
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Linux/UNIX including to process large data sets.
Experience with AWS services
Some experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements
Strong ability to interact, communicate, present and influence within multiple levels of the organization
Master's degree
Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions
Previous experience with distributed or load balanced system design and development.
Strong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams.

Amazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.","Seattle, WA",Big Data Engineer: ML Platform,False
751,"
Design, access and implementation of extensive databases and analytical tools
Recognize trends and put puzzles together that are relevant to understanding value creation in the intersection of data sets
Formulate and apply mathematical modeling and other optimizing methods to develop and interpret information that assists management with decision making and policy formulation
Collect and analyze data and develop decision support software, services and products
Develop and supply optimal time, cost or logistics networks for program evaluation, review and implementation
Gather and process raw data at scale including writing scripts, web scraping and calling APIs
Perform validation and testing of models to ensure adequacy and reformulate models as necessary
Collaborate with management to identify and solve operational problems to clarify management objectives

– Present the results of mathematical modeling and data analysis to management and other end-users


Scrub noisy datasets using programming language (SQL, Python, R, MATLAB, C++) as well as statistical software
Use understanding of how structured and unstructured data sets intersect to provide creative solutions
Develop code to predict correlation between number of operating rigs in US and oil prices
Use T-SQL to analyze data an build correlation models
Develop automated well production forecast model using T-SQL to load data into Python, implementing pandas, numpy and scipy modules to analyze data and predict well production for every well in US
Perform database management by automating data update processes and building stored procedures and table-functions for various users
Write code for automated data scraping and data download assignments using Python and selenium module

Requirements:
This position requires a Bachelor’s degree in Mathematics or Computer Science followed by six months of experience as a Software Developer using Python.

Contact:
Applicants interested in this position should mail their resume to:

Justin Carlson – VP & Managing Director, Research

East Daley Capital

5161 E. Araphaoe Road

Ste. 411

Centennial, CO 80122","Centennial, CO",Data Engineer,False
752,"The job

Akili Interactive is looking for a Data Engineer to join our development teams in Boston, MA or Larkspur, CA (just north of San Francisco). You will help develop the data infrastructure that supports our Data Science, Business Intelligence, and Data Integration initiatives, and you will be an integral part of the broader effort to launch of our digital therapeutic products that combine rigorous scientific validation with engaging video game mechanics.

You will join during the early stages of building new mobile applications, web applications, event streaming pipelines and supporting API services. Our stack is Scala, Play, lots of Docker, Postgres, ElasticSearch, several AWS services and JavaScript. Ideally we'd like someone who's comfortable with these technologies, but we're always excited to meet great engineers who can quickly learn new technologies.

Here's what it entails


Participate in technical discussions with other engineers, architects, partners and colleagues to discuss application and platform solutions
Serve as a hands on software engineer using the language and tools best suited to solve the problem
Establish expertise in our existing data infrastructure
Develop data storage and pipeline infrastructure to support Data Science, Business Intelligence, and Data Integrations
Be hands-on with cloud deployments, load balancing, CI/CD, provisioning and security of the production and staging environments
Ship clean well crafted code

Here's what you need


Bachelor's Degree or Master's Degree in Computer Science
4+ years experience as a Data Engineer
Hands on experience using Amazon Web Services (AWS) SDKs - such as SQS, S3, SNS, RDS, Lambda
Strong understanding of Scala, or expertise in Java with a desire to learn Scala
Strong understanding of both relational and NoSQL databases
Experience working in a containerized infrastructure deployed in the cloud
Avid user of open source projects and enjoy constant learning
Enjoy both analytical and hands-on work to solve problems
Able to focus on a problem to completion with a strong attention to detail

It's great if you also have


Experience as a backend software engineer developing services and APIs
HIPAA experience
Continuous Integration and Development
Agile Development
Written technical documentation samples, or links to open source projects (github projects or documentation you have written), that you are able to share

Akili - The company

Akili Interactive (www.akiliinteractive.com ( http://www.akiliinteractive.com/ )) is pioneering the field of digital therapeutics, combining scientific and clinical rigor with the ingenuity of the tech industry to develop novel digital medicines designed to treat cognitive dysfunction and brain-related conditions.

Akili's flagship product, AKL-T01 in pediatric ADHD, is now under review by FDA for clearance following Akili's recent filing. Akili previously announced positive top-line results of a multi-center, randomized, double-blind, controlled pivotal study evaluating the safety and efficacy of AKL-T01. If cleared by FDA, AKL-T01 would be the first prescription video game to treat a medical condition and the first prescription digital medicine for children with ADHD. The product is based on technology developed at UCSF that was featured as the cover article ( http://www.nature.com/news/gaming-improves-multitasking-skills-1.13674 ) in Nature under the headline ""Game Changer.""

Akili was founded by leading neuroscientists and top-tier game design veterans to create gaming experiences that are designed to directly treat cognitive deficiency and improve symptoms associated with medical conditions across neurology and psychiatry. Akili has partnerships with top pharmaceutical companies, and has received recent recognition in major media outlets including the Wall Street Journal, CNBC, Fast Company, and The Verge.","San Francisco, CA",Data Engineer,False
753,"As a Data Engineer at Amne, you’ll join a young, growing team as we build the world’s fastest and simplest home transaction experience.
Responsibilities will include:
Building data infrastructure and back-end services to support user-facing and internal-use applications
Identifying, researching, and analyzing new data sources
Developing and maintaining REST APIs for Amne's back-end services
Managing and developing ETL pipelines to ensure and improve the accuracy, quality and usability of data across Amne's infrastructure.
Improving and maintaining industry-leading home valuation models and methodologies using millions of real-estate transactions and complementary data sets.
Collaborating with Software Engineers, Product Managers, and Product Designers
Maintaining development best practices, including test coverage, continuous integration, A/B testing, and documentation
Becoming a domain expert in real-estate
Required Skills and Abilities
Strong Python programming skills, including experience with NumPy, SciPy, Pandas, and scikit-learn
Familiarity with common Python web frameworks (Flask, Django, etc.)
Experience with the AWS ecosystem (S3, Elastic Beanstalk, EC2, etc.) and application deployment
Ability to write clean, re-usable, and production-ready code.
Ability to translate complex data-oriented challenges into solutions for business objectives, and vice versa.
Well-developed SQL skills, preferably including experience with PostgreSQL and PostGIS.
Desirable Skills and Experience
Experience with Apache Airflow
Previous experience with continuous integration tools (e.g., CircleCI, TravisCI, etc.)
Previous GIS experience
Programming experience in Node.js is a plus
We’re looking for individuals who bring:
Experience with databases, statistics, web services, algorithms, and Python
Strong communication skills
Experience building modular, scalable, cloud-based systems
An interest in learning new technologies and taking the lead on the integration of new technologies into Amne's stack.
A willingness to learn new technologies and a whatever-it-takes attitude towards building the best possible home buying and selling experiences for our users
Amne is currently hiring for our Austin office. Amne team members are thoughtful listeners, fast learners, results driven, self-motivated, self-aware, and self-disciplined.","Austin, TX",Data Engineer,False
754,"Data Engineer
Duration : 6+ Months (C2H)
Only W-2 consultants
Phone + In-Person
Owings Mill,MD

Job Description:

The Data Engineer is an accomplished technical leader, proactive customer-focused advocate, a team player with substantial software engineering experience, preferably with some experience within the healthcare industry. The Data Engineer must have hands-on experience with data security and knowledge of HIPAA expert determination. The ideal candidate will have an advanced understanding of data discovery/analysis/classification, data masking, data transformation, and SQL/data modelling. The candidate must demonstrate the ability to evaluate cutting edge technologies and overcome technical challenges in a fast paced environment. The Data Engineer will play a key role of migrating three enterprise applications into a consolidated application which leverages DevOps, cloud computing, and data lake / big data technologies.

Job Duties:
Design, develop, implement, and maintain code, information architecture, and conceptual models to support data de-identification effortDevelop data & metadata policies and proceduresReview and evaluate database performance, risk and financial analysis feasibility studiesInvestigate and repair application defects regardless of component including platform, business logic, data process logic, or database (SQL and data modeling).All other duties as assigned or directed
Qualifications:
Bachelors of Science (or higher) in computer science or related field8+ years of systems/application analysis & design experience5+ years of combined experience data masking tools (e.g. Dataguise, Privacy Analytics) or ETL (Talend or Informatica)
 Preferred experience with PHI data de-identification/ anonymization / masking

Technical Skills
Excellent knowledge of relational databases (PostgreSQL, Oracle) including SQL & stored proceduresExcellent knowledge of rules engines particularly used for complex data (masking) operationsBasic knowledge of Unix/Linux commands especially in processing data filesPreferred experience with ATTD and associated technologies (Fitnesse, DBSlim, Junit)Preferred experience with delivering code using Continuous Integration and Continuous Delivery (CI/CD) best practices and DevOps (Jenkins Pipelines, Docker, Groovy, Ansible)Preferred experience with AWS or other cloud platforms.
Other
U.S. citizen or legal right to work in the United States without sponsorship

Thanks and Regards,

SINDHURI
Direct - 201-815-4942
sindhuri.gajula@siarptech.com","Owings Mills, MD",Data Engineer-Data Analyst,False
755,"$120,000 - $177,000 a year (Indeed Est.) S&P Global Ratings is looking for an experienced Data Science Engineer to join Data Engineering team within Chief Data Office, a team of data and technology professionals who define and execute the strategic data roadmap for S&P Global Ratings. The successful candidate will participate in the design and build of S&P Ratings cloud based analytics platform to help develop and deploy advanced analytics/machine learning solutions.

The Team
You will be an expert contributor and part of the Rating Organization’s Data Services Team. This team, who has a broad and expert knowledge on Ratings organization’s critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.

Our Hiring Manager Says
If you are an individual that brings demonstrated experience of delivering big data projects as a data science engineer,, this is an excellent opportunity. I am looking for someone with sound technical knowledge, can be hands-on, worked on transformational initiatives, and can drive results.

Responsibilities:
Design and develop efficient and scalable data pipelines between enterprise systems and analytics platformWork closely with Data Science team and participate in development and deployment of machine learning models and feature engineering pipelinesProvide technical expertise in the areas of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and SparkBuild and maintain a data environment for speed, accuracy, consistency and ‘up’ timeSupport analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organizationWork with the machine learning engineering team to build a data eco system that supports AI products at scaleEnsure data governance principles adopted, data quality checks and data lineage implemented in each hop of the dataPartner with the chief data office, enterprise architecture organization to ensure best use of standards for the key data domains and use casesBe in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologiesEnsure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards
Experience & Qualifications:
BS or MS degree in Computer Science or Information Technology8+ years of experience as data engineer at an innovative organization4+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and SparkExpert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environmentExperience With Machine Learning Frameworks, such as TensorFlow , PyTorch, H2O, scikit-learn, Theano, Caffe or Spark MLib is an added advantageExposure to R, SparkR, SparklyR or Other R packages is a plusExperience in constructing fast data staging layers to feed machine learning algorithmsExperience in building data APIs to consume analytic model outputFamiliarity with machine learning model training and deployment process is a plusExperience with development frameworks as well as data and integration technologies such as Python, Scala or InformaticaExpert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.Monitors industry trends and directions; develops and presents substantive technical recommendations to senior managementExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partnersAbility to prioritize and manage work to critical project timelines in a fast-paced environmentFinancial services industry experience","New York, NY",Data Science Engineer,False
756,"The Role

We're looking for a Business Intelligence Engineer to help design and build the data environment that supports the Iora Health vision. As a key member of our Business and Clinical Intelligence (BCI) team, you will be responsible for designing, implementing, and maintaining operational data stores and data warehouses which form the foundation for innovative applications that directly impact the quality of care delivered to our patients. You will be passionate about quality data, clean architecture, effective collaboration, and continuous learning. You will be happy to get your hands dirty while building towards a bigger vision. You will serve as the resident data architecture and informatics expert and you will be instrumental in building the systems and culture of an innovative company that is working to transform health care, positively and directly impacting the lives of our patients every day.

Responsibilities


Be a key contributor to the design, implementation, automation, and documentation for Iora ETL processes
As a member of an agile team, design, implement, and document data infrastructure for Iora Health's data warehouse
Develop key reports and dashboards using Looker to provide reporting analysis and analytics insights

Required Skills


1-2 years experience with relational databases and SQL
1-2 years experience building reports and dashboards using BI tools such as Tableau or Looker
1-2 years experience building data pipelines and ETL processes using tools such as SSIS or Talend
Excellent verbal and written communication skills
Working knowledge and curiosity required to debug and analyze complicated system

Preferred Skills


Experience with Redshift
Experience with Matillion / SSIS / Talend
Experience with Looker / Tableau

We believe in building a diverse team, and we strive to make our office a welcoming space for everyone. We encourage talented people from all backgrounds to join us. Help us restore humanity to healthcare!

About Iora

Iora Health is transforming health care, starting with primary care. We created a high-impact relationship based care model, that particularly benefits adults on Medicare and those who might need more attention. Our care model changes everything - the team, outcome-focused payment, customer service, and the technology that supports our care.

We know that when you invest in relationships with people, you can help them live happier and healthier. Our patients get a team that respects and listens to them. We get paid to keep our patients healthier, and it works - we are successfully improving the lives of our patients while lowering costs.","Boston, MA",Healthcare BI / Data Engineer,False
757,"ContractAbout Us:
We are a growing company with a solid customer base, excellent compensation and benefits, and a collaborative yet flexible work environment. If you enjoy challenging projects, using new technologies to deliver innovative business solutions, and you're interested in working for an entrepreneurial company, we may have a home for you.

BI Data Engineer


PTP is seeking an experienced BI Data Engineer to join our team of qualified, diverse consultants. This position is located in Sacramento, California.

The BI Data Engineer position requires strong business and technical skills. Responsibilities include data modeling, batch processing, data matching, data search, and duplicate record checking. It also includes integrating data quality tools in a Microsoft environment. Excellent communication skills and working as part of an integrated team are critical.

Tasks and Responsibilities:

Assist with the design, development, implementation, and support of the customer’s Business Intelligence and Business Analytic (BIBA) platform’s data architecture, including assessment of the client's current model, and offer suggestions to improve performance, data handling, and development process efficiencies.
Lead developer to integrate new data quality components within the client's BIBA Platform
Develop and support of detailed dimensional data models.
Develop detailed multi-dimensional (i.e. OLAP) databases in conjunction with team members.
Provide ETL design, development, implementation and support of the customer’s Business Intelligence & Business Analytic data for the warehouse.
Assist and guide customer technical staff in extracting data from the source system into the data warehouse staging area, ensuring data validation, data accuracy, data type conversion, and business rule application.
Prepare documentation, as part of the knowledge transfer process, which will enable the customer’s technical team to continue with the ongoing maintenance and operations of the BIBA system.
Design, develop, refactor, and support web service calls to stored procedures to consume data.
Mandatory Qualifications:
A BS degree in information systems, computer science, or software engineering related field.
Experience with Microsoft SQL Server 08/12/14, SSIS, SSAS, SSRS, PPS; including experience with customer systems that have over 15 million record updates daily.
Experience developing and calling web service APIs
Experience with Microsoft Visual Studio 2010 and Team Foundation Server 2010.
Experience with data requirements gathering and analysis.
Experience with integrating data quality tools from companies such as Experian, Trillium, Informatica, etc.
Experience with data requirements gathering and analysis
Familiarity with multi-dimensional (OLAP) databases.
Experience translating dimensional data models into physical data structures.
Familiarity with data management and database administration concepts, principles, and processes.
Experience working as a BI Data Engineer on at least one project involving the design, development, implementation, and deployment of a BI software solution using the Microsoft BI Toolset. For this requirement, the amount of experience gained on each project must have been at least six full-time equivalent months.
Experience working as a BI ETL Developer on at least one IT project involving the design, development, implementation, and deployment of a BI software solution relatively large in size and complexity (over 15 million records).
Desired Qualifications:
Experience with Government technology projects.
Excellent verbal and written communication skills and the ability to interact professionally with a diverse group including executives, managers, and subject matter experts (technical and non-technical).

Salary is DOE and is extremely competitive. We are able to offer relocation assistance to the right candidate.","Gold River, CA 95670",BI Data Engineer,False
758,"Data Engineer Consultant
As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure and Google Cloud using core data warehousing tools, Hadoop, Spark, Event Stream platforms, and other big data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.
Who are you?
You’re a smart, collaborative person who is passionate about technology and driven to get things done.
You’re not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
What technologies will you be using?
Everything. It’s about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board.
Why do we work here?
Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in San Francisco leveraging the coolest technologies. At Slalom, we found our people.
What does our recruitment process look like?
Our process is highly personalized. Some candidates complete their process in one week, others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless how long or short the process may be for you, the most important thing is that you find your dream job.
Qualifications:
Bachelor’s degree in Computer Engineering, Computer Science or related discipline
5-7+ years relevant experience
Understand different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
4+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
2+ years of experience working on AWS, GCP or Azure
Experience working with relational databases
Strong analytical problem-solving ability
Great presentation skills
Great written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and cloud specific data pipeline tools such as Airflow, Glue and Dataflow","San Francisco, CA 94105 (Financial District area)",Data & Analytics- Data Engineer,False
759,"Data Engineer

Lake Trust’s Business Intelligence team is a cornerstone to achieve future business objectives. This team will be primarily responsible for supporting the Business Intelligence & Analytics objectives of Lake Trust Credit Union. We are seeking a Data Engineer to perform a key role on this team solving problems through the use of extracting information from data.

What You’ll Do

The data engineer will work on implementing complex data projects with a focus on collecting, parsing, managing, analyzing large sets of data to turn information into insights using multiple platforms. The Data Engineer will make the appropriate data accessible and available for use by Lake Trust. He or she should be able to decide on the needed hardware and software design needs and act according to the decisions. The data engineer should be able to develop prototypes and proof of concepts for the selected solutions. Some of the tools used will be Microsoft SQL Server, Alteryx Designer and Server, R, Microsoft PowerBI and Azure. The Data Engineer role requires creativity with a high attention to detail and drive for data accuracy. If you enjoy working within an agile team this may be a great role for you.

ESSENTIAL FUNCTIONS
Ability to solve problems with data
Discovery of data across many different systems, data sources and data types
Have strong SQL querying skills
Ability to profile data to measure quality, integrity, accuracy, and completeness
Poses data preparation and blending for reporting and visualization purposes
Have experience with the cataloging of data sources
Hands-on experience on all aspects of data warehousing and schema
To be proficient in designing efficient and robust ETL workflows
Helping to streamline a better data supply chain for analytics that goes from experimentation into production
Working with cloud computing environments
Tune solutions to improve performance and end-user experience
Document requirements and resolve conflicts and clear ambiguities
Work in teams and collaborate with others
Contribute to group knowledge sharing platforms and best practice
Perform other duties and responsibilities as required or assigned

KNOWLEDGE, SKILLS AND ABILITIES
High attention to data accuracy
Ability to work in an agile team
Critical thinking to ask questions, determine best course and offer solutions
Complete work independently
Act as a change agent
Continuous improvement mindset
Ability to understand the big picture
Effective analytical and decision-making skills
Strong interpersonal skills to build relationships and communicate effectively with managers, staff, and vendors
Strong written and verbal communication skills
Teamwork skills within the department and on project teams
Demonstrated ability to work effectively in a fast-paced, complex, and dynamic business environment
Effectively provide open and honest feedback via performance reviews and 1-1 conversation.
Enjoy being challenged and to solve complex problems on a daily basis
Proven ability to support a strong member/customer service culture
Demonstrated and dynamic analytical/problem solving skills
Proven effective communication and collaboration

What You’ll Bring

BA/BS data analytics/computer science/information or similar degree is required
Four years job related experience is preferred
Data warehouse architecture experience is required
Hands-on experience writing SQL code is required
Building reporting semantic layers and BI dashboards is preferred
Hands-on ETL experience is required
SQL Server Integration Services is preferred
Microsoft SQL and/or Oracle querying is required
Experience with Microsoft Power BI, Alteryx Designer and Microsoft Azure preferred

What You’ll Get

We know that pay and benefits are important. And, we’ve really got that covered. But, we also know that those are not the only things that you need to decide if this is the place for you. Join our team of Lake Trusters and you’ll enjoy:
Working with an energetic team focused on making our members wildly successful
An opportunity to work with others that have your back every step of the way
Opportunities to make a difference both inside and outside of our walls
Being treated like you are more than the work you do","Brighton, MI",Data Engineer,False
760,"Data Engineer III

In the hospitality industry, people matter.

That's why here at Choice we're always looking for exceptional people - people who will challenge us, make our team stronger, smarter and more complete. People who know how to roll up their sleeves and tackle the job at hand; who go the extra mile to get the job done - and done well.

At Choice we are looking for employees to connect the world through the power of hospitality - and we offer support, training and a collaborative workplace atmosphere that makes us a great place to bring together people, brands, and technology that enable success.

Who are we looking for? Maybe it's you.

The role…
The Data Engineer III is a technical leader in the department, responsible for development of solutions using all commonly used tools by the team and leading others to complete projects through technical guidance. This person will play a critical role to support the analytic data development efforts of the ongoing DAP (Data Analytics Platform) project.

This role requires expert skills in building datasets in big-data environments such as Cloudera. This role requires strong skills in ETL, programming and application development using Java, Scala, Python and other data science languages and programs such as R. Experience using ETL tools such as Syncsort DMX-h is a plus.

The Data Engineer III also drives and leads the team in the following areas:
-Design Reviews
-Production Release Reviews
-Code Repository Management
-Documentation

In this individual contributor role, the Data Engineer III handles the most complex projects/assignments and helps drive the strategy for business intelligence, data development and data visualization under limited direction from management and in alignment with Choice business strategy, and the department’s objectives.

This role ensures all yearly Key Program initiatives are supported.

What you will do…
Completes DAP Analytics Data development & ETL projects (BIG Data)
Completes application development projects
Researches and completes development of leading-edge/future technology solutions
Manages and supports the departments infrastructure (apps, tools, code bases, performance)
Attends, leads, and ensures the occurrence of design reviews, production release reviews and code reviews
Maintains knowledge of future technologies and industry practices related to existing technologies used in the department
Drives risk mitigation activities, including creation of back-up strategy/process, disaster recovery process, code repositories, and documentation on Wiki sites

Skills you have…

Education
Bachelor’s Degree in Computer Science, Information Technology, or related field, from a four-year college or university, or one to two years related experience and/or training; or equivalent combination of education and experience. Relevant industry/technology/application certification is preferred

Experience
5+ years relevant experience in business intelligence, data development and data visualizations, or equivalent technical environment

Knowledge
Exceptional knowledge of business intelligence, data development and data visualization tools and solutions
Exceptional knowledge of database management system technologies and tools
Strong knowledge of application development techniques using Java

Skills:
Expert proficiency in:
Data Visualization tools, such as Tableau Desktop/Tableau Server
ETL and Data Development tools, such as Syncsort DMExpress
Advanced proficiency in:
Application development using Java
Programming languages, such as SQL or 4GL
Expert proficiency with reporting tools, analytic tools, query tools and interfaces
Advanced proficiency in data warehousing concepts and database technologies
Exceptional analytical skills
Exceptional verbal, written and listening communication skills
Data Integration experience, including ability to design, document, develop and test data integration processes from data analyst specifications
Prior experience on analysis and resolution of data quality and integration issues
Demonstrable experience implementing Big Data solutions using current technologies
Experience in data formatting, cleaning up the data, understanding of schemas and metadata
Practical skills with efficient file movement inside of big data platform
Experience with large scale ( >1TB raw) data processing and ETL
Ability to design, architect and code
Demonstrable experience on Hadoop ecosystem (including HDFS, Spark, Sqoop, Flume, Hive, Impala, Map Reduce, Sentry, Navigator); Hadoop data ingestion using ETL tools (e.g. Nifi, Kafka Talend, Pentaho, Informatica, DMX-h) and Hadoop transformation (including MapReduce, Scala)
Demonstrated experience and knowledge of relational SQL databases such as Vertica, Cloudera/Impala, Informix, SQL Server, Oracle and ability to write SQL commands with expert level SQL skills for data manipulation (DML) and validation (Informix, Vertica, SQL Server, Oracle)
Working knowledge or experience with Tableau to create Reports and Dashboards is desired
Working knowledge or experience with Business Objects XI, Business Objects Web Intelligence is a plus
Ability to code in either Python or R for data analytics is a plus
Exceptional knowledge sharing skills, such as creating user-friendly documentation and instructions, and professionally responding to requests in audits
Exceptional interpersonal skills and demeanor
Excellent leadership skills, such as planning and prioritizing the use of limited resources, promoting a sense of team, developing good relationships with peers and customers, and dealing appropriately with ambiguity
Proficient in the use of MS Office applications, such as Outlook, Word, PowerPoint and Excel

Abilities
Ability to lead and motivate others and to drive results
Ability to provide after hours support, as needed
Ability to work effectively in a team-oriented environment, both independently and collaboratively
Ability to uphold Choice’s Values & Performance Principles of collaboration, performance excellence, sense of urgency, openness to new ideas, inclusion & diversity, integrity, customer focus, and respect.




Must be able to uphold Choice's Values & Performance Principles of accountability, collaboration, performance excellence, sense of urgency, innovation, inclusion & diversity, integrity & trust, customer focus, and respect.","Phoenix, AZ 85032 (Paradise Valley area)",Data Engineer 3,False
761,"At SpringML, we are all about empowering the ‘doers’ in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to today’s most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.
Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.


Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers","Pleasanton, CA",Big Data Engineer,False
762,"$85,000 - $125,000 a yearAre you a Data Engineer that loves working with very large data sets? Are you skilled in using Hadoop, Pig, Hive, Java or Python to integrate large data sets into meaningful assets that can be used by the business for analytics? If you answered yes to any of the question above, please read on!The Business Intelligence team needs Data Engineers to help us architect and build robust data solutions that can be used by the Data Science & Applied Analytics teams, as well as, the analysts throughout the business. The role requires you to collaborate with both technical and non-technical folks so, unfortunately you won’t be able to speak techie all the time. However, you will be involved in a variety of projects allowing you to grow your knowledge and skills beyond what you thought was possible.We spend a lot of time and effort architecting, building, and automating our solutions so, hopefully it’s no surprise that we take data quality very seriously! We’ll ask you to use your Jedi engineering skills on data quality efforts from time to time. It’s fun and a great way to learn our data!WHAT YOU WILL BE DOING:Transforming large, complex data into business assets that serve both the Enterprise Business Intelligence team and analysts throughout the organizationProviding appropriate data for a given analysis. This would require you to work with data modelers/analysts to understand the business problems they are trying to solve and create or augment data assets to feed their analysis.Explore and recommend innovative solutions to complex problems. How cool is that?Ensure our data assets meet our data quality standards. It’s important!Have fun in a fast paced energetic environmentWHAT YOU NEED:5+ years of relevant employment experienceTeradata experience. We’re looking for power users not administratorsStrong SQL, we mean REALLY strong. We want you to be excited about SQL scripts that are hundreds of linesExperience transforming large datasets into consumable assets for self-service analytics and reportingExperience designing, implementing and supporting Data MartsMust be familiar with Linux systems, including basic shell scriptingDesign, develop, and maintain data aggregation, summarization jobs (i.e. automation)You need to be flexible to changing priorities and comfortable in a fast passed dynamic environmentKnowledge of Amdocs and/or CSG billing systems a plusGood generalist experience is a plus, ideally working with all layers in the technology stack. If you’re “good” in various technologies, we should talk.Job Type: Full-timeSalary: $85,000.00 to $125,000.00 /year","Philadelphia, PA 19103 (Belmont area)","ETL Developer (SQL, Big Data)",False
763,"About Us

Our ambitious goal of helping innovators build a better world with data started in 2009. Today, we are one of the fastest growing enterprise companies in history, exceeding $1B in revenue at the end of 2017.

The world is experiencing a technological revolution driven by AI, machine learning, virtual reality, quantum computing and self-driving cars -- all of which require large amounts of data and put Pure's technology literally in the driver's seat. Our solutions deliver real-time, secure data to power critical production, DevOps, and modern analytics in multi-cloud environments. With a Satmetrix NPS score in the top 1% of B2B companies worldwide, our customer-first culture and commitment to innovation build a fast-growing company that employees, customers, partners, and investors love. For more information on our business, check out the corporate fact sheet ( https://www.purestorage.com/content/dam/purestorage/pdf/PureStorage_FactSheet.pdf. ).

Our Team
Puritans come from various backgrounds and we thrive off of challenging the norm. This cross pollination of backgrounds led to numerous ground breaking ideas and has helped us build one of the most reliable and easy to use storage systems in the industry. ​We strive to hire the best and brightest people, who excel in a cutting edge, fast paced, collaborative and transparent environment. We are seeking enthusiastic individuals to solve real world problems while having fun along the way.

Summary:
Davenport & Patil have written in the Harvard Business Review article entitled ""Data Scientist: The Sexiest Job of the 21st Century"" that the role requires ""a high-ranking professional with the training and curiosity to make discoveries in the world of big data.""
Pure Storage is looking for an experienced data engineer with a strong programming background coupled with excellent analytical and communication skills to join us in building up a data science practice. Our charter is to partner with IT to create a marketing data lake on Pure Storage technology that enables the ingestion and blending of many disparate sources (both structured and unstructured) for predictive analytics and operational reporting. This will be the platform for next-gen business decision support focusing on: the use of statistical techniques for sales and pipeline forecasting, new customer acquisition, propensity-to-buy prediction for program targeting, rules for cross/up-sell, segmentation/classification, real-time analytics and media mix optimization.

What You Will Be Doing
The Data Engineer will join a growing data science community in Pure Storage and report directly to the Senior Director, Data Science as part of the marketing group. The data scientist will blend both structured and unstructured data and team with statisticians/mathematicians to use the latest datamining and data visualization techniques to craft actionable models and insight in order to provide a sustainable competitive advantage for Pure Storage.
The candidate must have a formal education in an area related to computer science, and be exceptionally interested in lifelong learning to keep pace with the latest techniques and technology related to big data analytics, while having the skill to manipulate large, disparate datasets from a wide range of sources.
The ability to identify, synthesize and communicate technical and analytical findings and business recommendations to a non-technical audience is critical.

What You Bring To The Team


Graduate degree in computer science or a related technical field required.
2-5 years experience data experience.
Excellent knowledge of database systems and architectures, big data programming languages and machine learning. Experience in working in sales and marketing analytics a plus. Graduate degree in computer science or a related technical field required.
Excellent knowledge of database systems and architectures, big data programming languages and machine learning. Experience in working in sales and marketing analytics a plus.
Programming experience in SQL and/or Python for data manipulation necessary.
Familiarity with data visualization programming languages such as JavaScript, in addition to software such as Tableau, R Shiny, Chartio or Spotfire beneficial.
Familiarity with big data technology such as Spark, Kafka, Hadoop and Hive.
Ability to program in R, base SAS and/or Python to implement machine-learning techniques a plus.
Excellent interpersonal skills: collaboration, creativity and communication all key to success.
If a career in data science with a company that is at the center of the big data renaissance sounds like a good fit for you, please reach out to us to get in on the cutting edge of big data analytics!

Pure creates opportunities for your development and career growth.

At Pure, we believe that each Puritan is a leader contributing to the success of our business, regardless of role. We offer an assortment of learning options available to all Puritans, including workshops on leadership, management, career development and more! We're here to change the world and we hope you join us!

Popular Perks

Pure offers an unlimited vacation policy, free lunches, meditation rooms, free yoga classes and employee resource groups to inspire all of our employees to maintain mind and body wellness. Through our Pure Good Foundation, we also offer numerous volunteer opportunities for employees to give back not only to the Bay Area, but across the globe.

Pure is Committed to Equality

Pure is proud to be an equal opportunity and affirmative action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or any other characteristic legally protected by the laws of the jurisdiction in which you are being considered for hire. If you need assistance or an accommodation due to a disability, you may contact us at TA-Ops@purestorage.com ( TA-Ops@purestorage.com ).","Mountain View, CA 94041",Data Engineer (Data Science Group),False
764,"S&P Global Ratings is looking for an experienced Big Data Engineer to join Data Engineering team within Chief Data Office, a team of data and technology professionals who define and execute the strategic data roadmap for S&P Global Ratings. The successful candidate will participate in the design and build of S&P Ratings cloud based analytics platform to help develop and deploy advanced analytics/machine learning solutions.

The Team
You will be an expert contributor and part of the Rating Organization’s Data Services Team. This team, who has a broad and expert knowledge on Ratings organization’s critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.

Our Hiring Manager Says

If you are an individual that brings demonstrated experience of delivering big data projects as a data engineer, this is an excellent opportunity. I am looking for someone with sound technical knowledge, can be hands-on, worked on transformational initiatives, and can drive results.

Responsibilities:
Design and develop efficient and scalable data pipelines between enterprise systems and analytics platform
Work closely with Data Science team and participate in development of feature engineering pipelines
Provide technical expertise in the areas of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Build and maintain a data environment for speed, accuracy, consistency and ‘up’ time
Support analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organization
Work with the machine learning engineering team to build a data eco system that supports AI products at scale
Ensure data governance principles adopted, data quality checks and data lineage implemented in each hop of the data
Partner with the chief data office, enterprise architecture organization to ensure best use of standards for the key data domains and use cases
Be in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologies
Ensure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards
Experience & Qualifications:
BS or MS degree in Computer Science or Information Technology
5+ years of experience as data engineer at an innovative organization
3+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Expert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environment
Experience With Machine Learning Libraries and Frameworks (TensorFlow , MLlib) is an added advantage
Exposure to R , SparklyR , and Other R packages is a Plus
Experience with development frameworks as well as data and integration technologies such as Informatica, Python, Scala
Expert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.
Monitors industry trends and directions; develops and presents substantive technical recommendations to senior management
Excellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners
Ability to prioritize and manage work to critical project timelines in a fast-paced environment
Financial services industry experience","New York, NY",Big Data Engineer (S&P Global Ratings),False
765,"The individual will be principally responsible for overseeing a team of Dispute Resolution Investigators and Administrative support staff to ensure their work is completed with high quality and within the time lines established in our policies and procedures.

This team is responsible for handling the review of an Insured/Servicer appeal to a prior decision to rescind coverage. The team is accountable for an independent investigation of each appeal based upon the loan underwriting and policy requirements, and providing a timely and reasoned response to the Insured/Servicer, including any new documentation provided by the Insured/Servicer.

In conjunction with the AVP-DRU, the individual provides training and acts as a mentor to newly hired employees and is available to assist in guiding all of the employees in the direction of handling the customer appeals/rebuttals. Among their existing responsibilities will include; counseling existing staff on performance and quality standards; organizing and leading escalation calls with our key customers; lead special projects with our Client Advocate Group at Radian on key accounts specific to a customers’ rescissions of claims. This individual will also partner with IT Department and Risk Analytics to identify enhancements to DRU functionality in Investigations DB, and to develop accurate reporting to support DRU’s surveillance management ability.


The individual will be principally responsible for overseeing a team of Dispute Resolution Investigators and Administrative support staff to ensure their work is completed with high quality and within the time lines established in our policies and procedures.

This team is responsible for handling the review of an Insured/Servicer appeal to a prior decision to rescind coverage. The team is accountable for an independent investigation of each appeal based upon the loan underwriting and policy requirements, and providing a timely and reasoned response to the Insured/Servicer, including any new documentation provided by the Insured/Servicer.

In conjunction with the AVP-DRU, the individual provides training and acts as a mentor to newly hired employees and is available to assist in guiding all of the employees in the direction of handling the customer appeals/rebuttals. Among their existing responsibilities will include; counseling existing staff on performance and quality standards; organizing and leading escalation calls with our key customers; lead special projects with our Client Advocate Group at Radian on key accounts specific to a customers’ rescissions of claims. This individual will also partner with IT Department and Risk Analytics to identify enhancements to DRU functionality in Investigations DB, and to develop accurate reporting to support DRU’s surveillance management ability.


EEO Statement
Radian complies with all applicable federal, state, and local laws prohibiting discrimination in employment. All qualified applicants will receive consideration for employment without regard to gender, age, race, color, religious creed, marital status, sexual orientation, national origin, ethnicity, ancestry, citizenship, genetic information, disability, protected veteran status or any other characteristic protected by applicable federal, state, or local law.

If you are a person with a disability and need assistance in the application process please send an e-mail message to recruitment@radian.biz.","Philadelphia, PA",Sr. Data Engineer,False
766,"Knowledge is our product, and data is our platform. We need engineers who look at a data set and want to unlock the answers it holds inside. Engineers who look at a data set and think about how to make sure it is correct. Engineers who look at a data set and want to make infrastructure to help build it better, faster, and stronger.

As a Data Engineer, you will work closely with oncologists and statisticians to build software that will help our customers discover novel insights into their data. You will design our data infrastructure, and use it to develop extensible, robust data and analytics pipelines, tools, visualizations, and services for accessible and flexible data analysis. You will learn more than you ever thought possible about how cancer is treated in the real world, and your work will directly support oncology research and publications.

About you:

You hold a BS, MS, or Ph.D. in computer science or related field
You have 2+ years work experience
You have experience with languages like Python, C++, Java, or C#
You are passionate about performance, reliability, and scalability of systems
You are inspired by our mission to improve cancer research through technology
You seek simple approaches to complex problems
You like science and/ or medicine just because it's cool

Bonus points if you have any of the following:

Have a good understanding of relational databases like PostgreSQL, MySQL or MSSQL
Have real passion for data and a strong understanding of statistics
Have developed distributed data processing systems against large, heterogeneous data sets
Have taken a leading role in delivering complex software systems all the way to production
You almost decided to go to med school

","New York, NY",Senior Data Engineer,False
767,"Overview
TISTA Science and Technology Corporation, a CMMI Maturity Level 3 company, focuses on delivering information technology and professional services to Federal and State agencies. TISTA is an Inc. 500 company, a recipient of the 2010 Top 100 Service-Disabled Veteran-Owned Businesses from Diversity Business, recognized in Washington Technology's FAST 50 list of the fastest growing small businesses in government contracting in 2012 & 2013, recognized as the Top 25 Fastest Growing Small Technology companies by the Washington Business Journal in 2014 & 2015, and selected as the Veteran Owned Company of the Year in 2014 by the Montgomery County MD Dept. of Economic Development.

TISTA Science and Technology Corporation is seeking Data Engineers to join our growing team using groundbreaking cloud and big data technologies. These Data Engineers will collaborate with data scientists from prominent ivy league institutions to identify user needs, find solutions, and take our products from concept to launch. We have Mid, Senior and Lead roles available.
Responsibilities
Build data pipelines using Apache Spark, Scala, Python, Apache Airflow etc.
Collaborate with User Experience and Engineering teams in the planning of new products
Write unit tests and get close to 100% code coverage
Work on AWS – S3 for storage, EC2 and EMR for processing/analysis
Follow Agile methodology for the software development
Identify problems and propose resolutions
Qualifications
 6+ year of engineering experience; focus on back-end development and/or data engineering
 In-depth programming knowledge with Java, Python, and Scala
 Experience with Spark, Hadoop, or HIVE
 Strong experience with AWS; including EC2, EBS, RedShift, EMR, ELB, SNS, RDS, CloudFormation, and more
 Experience with tools like Maven, Jenkins, Git
 Able to perform as an effective member of a geographically dispersed team across multiple time-zones

Education:
Bachelor’s degree in computer science or closely related field
Clearance:
Must be eligible to obtain a Public Trust
Location:
Rockville, MD

Here at TISTA Science and Technology we value our Veterans and encourage all to apply!

TISTA is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.","Rockville, MD",Data Engineer,False
768,"By clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that I agree with Takeda’s Privacy Notice, Privacy Policy and Terms of Use.

Job Description
Are you looking for a patient-focused company that will inspire you and support your career? If so, be empowered to take charge of your future at Takeda. Join us a Data Engineer, Visualization in our Cambridge, MA office.
Here, everyone matter and you will be a vital contributor to our inspiring, bold mission. As a Data Engineer, Visualization working in the Data Engineering and Emerging Technologies team, a typical day will include:

OBJECTIVES:
 As a Data Engineer, you will be tasked with creating an ecosystem to have the right data, to ask the right question, at the right time.
 Apply advanced techniques to complex problems in R&D and other organizations.
 For this specific role, there will be a focus on visualization of data.
 Work directly with the Data Science in R&D at Takeda along with other advanced analytics organizations across the company.
 Apply advance techniques in structured, partly structured and unstructured data across different partner organizations.
 Implement solutions for both big data and difficult to structure data sets.
 Maintain up-to-data knowledge on modern data technologies, explores new platforms and beta tooling.
 Independently use own judgement to identify data requirements and influences the design.
 Influence new computer science platforms to design, analyze and implement complex and new data driven solutions that impact the company.
 Provide leadership to complex data analysis, uses and explores data, languages, tools and software to best construct data for predictive modeling, tests the model, trains data to deploy the modeling within a complex R&D, Medical, Mathematical environment and a large complexity of IT systems and data.
COMPETENCES, EDUCATIONAL AND SKILLS
Required:
Bachelor’s Degree in Computer Science or Data Science
2+ years’ experience or relevant project / coursework
Up-to-date specialized knowledge of data wrangling, manipulation and management of technologies.
Ability to manipulate voluminous data with different degree of structuring across disparate sources to build and communicate actionable insights for internal or external parties.
Possesses strong communication skills to portray information.
Experience with Rapid UI Prototyping.
Experience with Tableau or Qlik
Experience with R or Python UI tools (R Shiny, ggplot, Seaborn, Matplotlib …)
Ability to work in an agile environment with high quality deliverables.
Experience with two of the following languages: Java, Scala, R or Python
Experience with UI frameworks like R. Understanding of Web Services as well as JSON formats
Working knowledge of SQL and Relational Databases
Experience with one of the following NoSQL datastores (Cassandra, MongoDB, Neo4J, …)
Experience with concepts of Hadoop and Spark
Preferred:
Experience with Tableau, R Shiny, Pandas and ggplot
Additional Languages: Chef, R, JavaScript
Experience with Multiple NoSQL datastores (Cassandra, MongoDB, Neo4J, …)
Experience with data formats including Parquet, ORC or AVRO
Understanding of AWS (S3, EC2, Redshift, EMR, Athena)
Experience with a Rapid UI tools: EX: Tableau
#LI-JV1","Boston, MA","Data Engineer, Visualization",False
769,"Strength Through DiversityGround breaking science. Advancing medicine. Healing made personal.Role and Responsibilities: The Data Engineer II will focus on data collection, movement, storage, transformation processing, and storage of Big Data. The incumbent will work with both current ETL/Data Warehousing and future Big Data/Streaming/Pipeline architectures. The focus will be on choosing optimal solutions to use for these purposes, then implementing, maintaining, and monitoring them. Always keeping in mind, the overarching goal of accelerating translational research and improving clinical care.Facilitate data collection from a variety of various sources, getting it in the right formats, assuring that it adheres to data quality standards, and assuring that downstream users can get that data quickly and with a common standard interfaceEnsure that data streams/pipelines are scalable, repeatable, and secure, and can serve multiple users within the InstituteDevelop as a core member of an Agile team, using Agile tools and methodology. Work closely with other team members including Application Developers, Database Developers, and Data ScientistsResponsible for creating the infrastructure that provides insight from raw data and handles diverse sources of data seamlesslyEnable big data and batch/real-time analytical solutions that leverage emerging technologiesAdditional responsibilities include developing prototypes and proof of concepts for the selected solutions and implementing complex big data projects with a focus on collecting, parsing, and managing large sets of data using multiple platforms to allow for Research and Data Science initiativesTranslate business requirements into modern data pipeline solutions. Create centralized documents and diagrams of all solutionsCreates a data catalog store of all metadataDesigns and implements monitoring, backup, and disaster recovery of data systemsApproaches all relationships with a world-class customer service approach. Maintains a customer-focused approach with users to provide solutions that are science/research-drivenResponsible for the integrity and security of data in all forms of storage throughout the Data ArchitectureWork with other IT professionals through Mount Sinai effectively. Comply with the Institutional Review Board and HIPAA to follow all applicable policies and proceduresAssists in the development of standards and procedures affecting data management, design and maintenance. Documents all standards and proceduresProvides presentations and training to other team members in the aboveExtremely flexible attitude. Willing to work with multiple types of technologies and languages with an open mind and without technology bias. Continuous interest in updating skill sets and knowledge of trends in the Big Data Technology spaceRequirementsBachelor’s degree in Computer Science or a related discipline; Advanced degree preferred with the following experience4+ years relevant professional development experience, preferably in a Linux environmentProficiency with Python development. Flexible to learn another language as needed - Scala, Java, and/or C# knowledge is a strong plusExperience with SQL and NoSQL databases such as Oracle, MS SQL Server, PostgreSQL/MYSQL, and Mongo DB (or similar such as CosmosDB or DynamoDB)Experience in RESTful service development (preferably with Node JS, Django and PHP)Familiarity with the big data technology space and the ability to leverage a wide variety of open source technologies and tools. Knowledge of Hadoop, Spark, Kafka and other big data technology stacks and streaming tools or related Cloud Service technologies on Azure or AWSExperience with Configuration Management software – Ansible (preferred), Puppet, or Chef or an equivalent AWS/Azure Infrastructure as Code experience. Also, experience with version control (Git)Experience with installation and configuration of big data software and technology or equivalent Cloud Service technologies on Azure or AWSExperience working in an Agile methodologyExperience as a plus: Working knowledge of cloud architecture and implementation on Azure or AWS is a big plus.Experience with Serverless computing (e.g. AWS Lambda or Azure Functions), creating VMs, cloud security, and other cloud services is also a big plus.Experience with micro-services and SOA is a plus.Knowledge of healthcare data, HL7, and Mirth are also a big plus.Strong skills in data structures, data/file formats, algorithms and object-oriented design.Experience working with JIRA is a plus.Strength Through DiversityThe Mount Sinai Health System believes that diversity is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize medicine together.We work hard to acquire and retain the best people, and to create a welcoming, nurturing work environment where you can develop professionally. We share the belief that all employees, regardless of job title or expertise, can make an impact on quality patient care.Explore more about this opportunity and how you can help us write a new chapter in our story!Who We Are: Over 35,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve.Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospital campuses, including Mount Sinai Beth Israel, Mount Sinai Beth Israel Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai Roosevelt, Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai.The Mount Sinai Health System is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) savings plan, and much more.The Mount Sinai Health System is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.EOE Minorities/Women/Disabled/VeteransJob Type: Full-timeExperience:Linux: 4 years (Required)Python: 4 years (Required)SQL: 4 years (Required)Education:Master's (Required)Location:New York, NY (Required)Work authorization:United States (Required)","New York, NY","Data Engineer II, Scientific Computing",False
770,"Bluestem Brands, Inc. has a full-time opportunity for a Lead Data Engineer in Eden Prairie, Minnesota.

The Lead Data Engineer uses smart technology to provide timely, accurate, and actionable information to empower stakeholders enterprise-wide to make the best decisions.

This position requires a Bachelor's degree or equivalent in Information Technology, Computer Science, Computer Information Systems, Electronic Engineering, or a related field and 7 years related (progressive, post-baccalaureate) experience.

Must also have each of the following:
1) 3 years of demonstrated experience leading a team of technical resources on complex projects;

2) 5 years of demonstrated experience in working with Data Warehousing which includes designing and building enterprise data warehouse solutions for different business subject areas; and

3) 6 years of demonstrated experience with technical data analysis, data quality, responsibility for complex analysis deliveries, and SQL experience with the ability to write complex SQL statements.

Employer will accept experience gained concurrently.

Please apply online at www.bluestem.com.

EOE.

Required Skills

Required Experience","Eden Prairie, MN 55344",Lead Data Engineer,False
771,"Under minimal supervision, the Data Engineer Senior accepts and validates all student level data for assigned projects in order to ensure data integrity. Combines several sources of data to be used for reporting test results. The Data Engineer Senior also provides input and support into internal products and systems that affect data in the company. Follows established protocols and standards when performing tasks. This senior position also uses prior experience and expert judgment to make decisions on how contracts should be executed. Participates in and recommends process improvement projects. May mentor junior staff members on the execution of projects. Works in consultation with other team members to find solutions to non-standard complex situations. Additional responsibilities include the following:
Validate and accept student level data including student demographic and test data from various sources and provided in various formats.
Perform cleanup activities to data per specifications created.
Perform documentation of data processing specifications to ensure common understanding of processes and procedures used on a project.
Participate in project meetings in order to ensure data integrity issues are discussed and resolved.
Develop code and process to support validation, cleanup, transformation and delivery of student data
Provide data support to internal products and systems.
Provide input into the development of internal products and systems.
Develop appropriate quality assurance steps to be implemented on projects to ensure data accuracy.
Provide support to team members on projects as needed.
Mentor junior team members.
Attend cross functional team meetings to contribute to solution definition for new projects or other complex situations.


Qualifications
Bachelor’s degree and five (5) to eight (8) years related experience in SQL development or an equivalent combination of education and experience.
Comprehensive knowledge of Microsoft SQL Server or an equivalent database software
SQL programming of complex views, stored procedures, functions, and scripts
Strong understanding of the fundamentals of developing user-facing reports and forms
Experience with Visual Studio and C# preferred
SAS programming, or experience in similar software such as R or SPSS is a plus
Experience with ETL tools is a plus
Working knowledge of Microsoft Office required","Dover, NH 03820",Data Engineer Senior,False
772,"SummaryAs a member of our Data team, your role is critical in powering the data-driven culture at Gaia. You will be working in a collaborative team environment in order to understand the needs of all stakeholders in the business and build corresponding data products to meet them. You will be working within an AWS, Python, Spark, Hive and Airflow stack in order to transform our data warehouse into a scalable data lake. You are self-motivated, ambitious and come to work with a great attitude every day. You live to make information accessible and beautiful for those who need it!ResponsibilitiesCollaborate with data engineers, data scientists, software engineers, data analysts, and business stakeholders in order to power all of the data needs across the businessConsume, ingest, and transform raw large-scale event streams of dataBuild, maintain, monitor and optimize ETL data pipelinesHelp design, implement, and enforce production requirements and data governance protocolsContribute to the design and construction of business critical KPIs, dashboards and reportsEvangelize the proper use of data and data-driven decisions across the companyQualificationsBachelor’s degree in computer science, math, a related discipline, or equivalent work experience2+ years of software development experience, ideally with Python or Scala1+ years of experience working with distributed big data tools like Spark and HiveAn extremely strong command of SQL and comfort with database design and architecture principlesComfort working with data in both structured and unstructured formsExcellent analytical, problem-solving, and communication skillsThe ability to gather stakeholder requirements and translate them into tangible next stepsStrong familiarity with an affinity for our content is a prerequisite to applyingMembership in Gaia is strongly preferredMust be able to work in our office in Louisville, ColoradoUS citizenship or an existing work visa is requiredNice to have: Experience with Airflow and/or other data pipeline toolsExperience with database administration, preferably MySQL and/or PostgreSQLFamiliarity with event-driven architectureComfort within the AWS data lake ecosystemExperience with BI tools like Domo, Looker or TableauMore about GaiaAt Gaia, we believe when enough of us wake up, everyone wakes up. Gaia (NASDAQ: GAIA), headquartered in Louisville, Colorado, is a global digital video streaming service and online community that provides curated conscious media content to its subscribers in over 120 countries. As the world’s largest subscription video on-demand (SVOD) provider of transformational media, dedicated to empowering you in body, mind and spirit, we have a unique opportunity to drive meaningful change in our world through streaming content that awakens and transforms. With over 7,000 titles of exclusive video content, we serve 100’s of thousands of subscribers every day. Over 90% of our videos are available for streaming exclusively on Gaia through most devices connected to the Internet and 80% of the views are generated by content produced or owned by Gaia. Our applications include gaia.com, iOS, tvOS, Android, and Roku as well as select content on Comcast, Verizon, and Amazon. We are a young, rapidly growing public company that offers a fast-paced, entrepreneurial working environment with plenty of opportunities to take risks and achieve outcomes previously thought to be unobtainable. Our opportunity is to become the undisputed global leader in the delivery of conscious media.This opportunity expands our community of creative, open-minded, conscious living enthusiasts that make up our employee base - a strong, established team of professionals. Some of the perks of working collaboratively with a team dedicated to sharing this mission, aside from sharing the values of growth, personal responsibility, creativity and innovation; comes with access to the on-site Olympic quality gym and complimentary yoga and fitness classes; a beautiful solar-powered orchard campus, complete with hiking and running trails and labyrinth; and an on-site café which serves breakfast and lunch daily including a full service espresso bar featuring locally roasted coffee.Full-time employees are offered alternative and traditional medical benefits including preventative coverage; as well as dental, vision, 401K, life insurance and more.We are having an impact – want to join us?Job Type: Full-timeExperience:software development: 2 years (Preferred)distribute big data tools like Spark and Hive: 1 year (Preferred)Education:Bachelor's (Preferred)Work authorization:United States (Preferred)","Louisville, CO",Data Engineer,False
773,"Summary of Major Responsibilities
The Data Engineer II will provide technical expertise, perform data analysis, and be an expert of the data generated by business systems and used to drive decisions. They will build meaningful, effective, and performant data visualizations and reports, serve as a point of contact for production support and issue resolution, and analyze, troubleshoot and tune performance.
Essential Duties and Responsibilities
Provide expertise in data analysis, management, and visualizations
Expand and improve existing Tableau dashboards and reports; create new ones
Debug data issues in dashboards, reports, data warehouses, operational data stores, and other disparate data sources
Share knowledge of advanced Tableau features like data blending, parameters, and calculated fields
Improve monitoring, automated testing, and deployments
Be a resource for providing insight, recommendations, and assistance with technical and non-technical questions
Monitor and recommend enhancements to the technical architecture to keep pace with changing business demands and scale
Stay informed of technology industry trends and solutions with ability to analyze for possible application in our environment
Promote the role and capabilities of IT to enhance the professional development of all staff through example, recommendation, and accommodation
Qualifications
Minimum Requirements
Bachelor’s degree in Computer Science, Information Systems or related field or equivalent professional experience
5+ years of experience designing and developing reports and dashboards using a reporting tool, preferably Tableau
5+ years of experience developing BI reports preferably in a data warehouse environment
Experience working with SQL for data exploration, debugging, and performant data access
Ability to query disparate data sources, including relational structures, OLTP systems, and dimensional data models
Experience creating data visualizations, designing and developing dashboards
Experience developing BI reports, preferably in a data warehouse environment
Ability to translate business requirements to technical specifications
Proven ability to work in a fast-paced, cross-functional team environment
Adaptable, open to change and able to work in ambiguous situations and respond to new information or unexpected circumstances
Excellent interpersonal and communication skills, ability to operate in a cross cultural and complex matrix environment, and ability to build consensus across functions
Desired Experience
Developing dashboards and reports on data from sales, call center, and custom software applications
Tableau, ideally including performance optimization
Microsoft EDW/BI/OLAP technologies: SQL Server, SSRS, SSIS
ETL
Physical Requirements
Ability to work in an office setting, operate telephony devices, and a computer
#LI-JK1

Exact Sciences is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to age, color, creed, disability, gender identity, national origin, protected veteran status, race, religion, sex, sexual orientation, and any other status protected by applicable local, state or federal law. Applicable portions of the Company’s affirmative action program is available to any applicant or employee for inspection upon request.","Madison, WI",Data Engineer II,False
774,"CSpring is seeking a Data Engineer to join our team in Indianapolis, IN. At CSpring, you truly do make a difference. Be heard, work on challenging assignments, improve your community, and join a growing team that will both challenge you and value your contributions. When you join CSpring, you become part of our Ohana – or extended family and our most valuable asset. You are NOT just another “resource” as is the case with many other consulting or staffing firms.
If you possess the qualifications listed below, please apply for this position and a member of our recruiting team will contact you directly. Your information will remain confidential and you will never be submitted for a position without your approval.
You will be a member of the Data team responsible for delivering high-value BI solutions to support strategic business needs. This person will be working on new development creating a new Data & Insight Platform for our client to enable insight based decisions. This is a highly innovative and dynamic environment.
Responsibilities:
Implement a high-performance, next generation data platform.
Work with a team to identify, design and build appropriate dataset and linkages for complex data.
Refactor legacy data platforms to integrate with the next generation data platform.
Implement data quality infrastructure and processes.
Implement data infrastructure for emerging data classes.
Contribute to the team’s growing set of development platforms, tools, processes and promote industry standard best practices.
Identify analytics tool guidelines and standards for performing required data preparation, analysis, visualization, and reporting.
Provide business and architectural context to show how the analytics tools fit within the overall infrastructure and high-performance data architecture.
Organize, deliver, and ensure data integration support of a corporate computing capability.
Qualifications:
3-5 years of experience with large scale data technologies such as Hadoop, Spark, machine learning, etc.
Knowledge of informatics, analytics, computational science, service management, delivery.
Knowledge of toolsets and capabilities utilized by expert data scientists and modelers.
Java, C++
Knowledge of Perl/Python and Unix scripting.
NoSQL and RDBMS databases
Hadoop-based tools (Hive, Hbase, MapReduce, MongoDB, Cassandra)
Data modeling and ELT / ETL
Data analytics and visualization
Solid communication skills and team player


CSpring offers a comprehensive compensation, training, and benefits package; including Paid Time Off, PPO and HSA Medical Insurance Options, Dental, Vision, STD, LTD, Life Insurance, and IRA Match. For more information about CSpring, please visit our website at http://www.cspring.com.
Unsolicited resumes from third party recruiting firms will not be accepted or considered for this position.","Indianapolis, IN 46204",Data Engineer,False
775,"Who We Are
We’re America’s largest mortgage lender, closing loans in all 50 states. J.D Power ranked Quicken Loans “Highest in Customer Satisfaction in Primary Mortgage Origination” for the past eight consecutive years, 2010 – 2017. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for four consecutive years, 2014 through 2017, each year the company was eligible. There’s a simple reason we’ve been so successful: We care about the people we work with.

If you’re tired of stuffy, bureaucratic workplaces, then you’ll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply won’t find anywhere else. Quicken Loans was named #1 in ESSENCE Magazine’s first ever list of “Best Places to Work for African Americans” in 2015. We've been on Computerworld's ""Best Places to Work in IT"" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazine’s list of ""100 Best Companies to Work For"" in 2018, remaining in the top-30 for the past 15 years.
What You'll Do/Need
The Big Data Engineer develops the ETL processes, designs structured data models from unstructured data sets and integrates data in Hadoop with a SQL server data warehousing environment.

Responsibilities

Collaborate with other Engineers, Architects, Quality Assurance and Product Owners on solving new and existing technical issues
Work with or without complete business requirements or specifications
Work directly with technology partners
Assist with delivery estimates
Maintain ownership of assigned projects
Requirements

Bachelor’s degree in computer science or equivalent experience
5 years of development experience in Java, Python or C#
Experience with RESTful API design and implementation
Experience in Workflow and BPMN development using Activiti, Camunda BPM, etc.
Experience with data migration, transformation and scripting
Proficient understanding of code versioning tools
Passionate about engineering quality, testing, automation and documentation of code and systems to ensure easy maintenance over a long period
Experience working on high-volume server software
Strong verbal and written communication skills and a strong attention to detail
What’ll Make You Special

Master’s degree
Experience in building high-performance, scalable backend services in the cloud
Experience with NoSQL databases such as Amazon DynamoDB
Experience with data frameworks, including Hadoop, Hive, Pig and Spark
Exposure to test-driven development, behavior-driven development frameworks and libraries
Experience with Collibra, administration, management and working with APIs
Experience with Anypoint APIs
Experience with MuleSoft application development
What You'll Get
Excellent benefits package that includes a 401(k) match, medical/dental/vision insurance and much more
Opportunities to participate in professional and personal development programs, including personal empowerment coaching, leadership training and ongoing personal growth training
Other incentives, contests and rewards, including trips, event tickets, cash prizes and more
Why We're Different

Corporate politics not your strong suit? The anti-corporate culture of Quicken Loans gives our team members the initiative to build solutions together and grow both personally and professionally. At Quicken Loans, we’re in the business of putting roofs over our clients’ heads, but we certainly aren’t putting ceilings on our team members’ careers. If you’re interested in working in a place with a philosophy that’s truly different, apply today.

Quicken Loans is an equal opportunity employer.

Disclaimer: Quicken Loans received the highest numerical score in the proprietary J.D. Power 2010 – 2016 Primary Mortgage Origination studies and the 2014 – 2017 Primary Mortgage Servicer studies. 2016 Origination (or Sales) based on 5,182 total responses and measures the opinions of customers who originated a new mortgage or refinanced within the past 12 months, surveyed in July – August 2016. 2017 Servicing based on 7,374 total responses and measures the opinions of homeowners on their mortgage servicing company, surveyed in March – April 2017. Your experiences may vary. Visit JDPower.com.

Quicken Loans is the #1 online lender based on the ranking of Quicken Loans in comparison to online residential mortgage lenders included in the Inside Mortgage Finance ""Top 50 Mortgage Lenders"" report from Q2 2017.","Detroit, MI",Big Data Engineer,False
776,"Part-timeWhat is the opportunity?
This role is within a technology team focused almost exclusively on the needs of Municipal Sales and Trading desk. The role will be focused on development and delivery of Predictive Analytics Platforms to transform processes and optimize technologies in aiding decisions.
What will you do?
Employ the existing (and develop new) Machine Learning algorithms that can find (predictive) patterns in large multi-modal data.
Provide innovative solutions for business problems (e.g., by translating complex commercial problems to Machine Learning problems).
Be an active member of teams that provide the business with AI-first apps, and data-driven insights and strategies.
Participate in, lead, and create cross-functional projects and training. Performs qualitative and quantitative assessments of all aspects of models including theoretical aspects, model design and implementation as well as data quality and integrity.
Analyzes complex data and associated quantitative analysis.
Uses quantitative tools and techniques to measure and analyze model risks and reaches conclusions on strengths and limitations of the model.
Prepares and analyzes detailed documents for validation and regulatory compliance, using applicable templates.

What do you need to succeed?
Must-have
Master's degree in Mathematics, Statistics, Economics, Computer Science, Operational Research, Physics, and other related quantitative fields
Scientific expertise and applied experience in Machine Learning (ideally, a combination of excellent academic research and high-impact commercial projects).
In depth understanding of common Machine Learning algorithms (e.g., for classification, regression and clustering).
In depth knowledge of advanced statistical theories, methodologies, and inference tools (e.g., hypothesis testing, (generalized) linear models, additive models, mixture models, non-parametric models).
Proven track record in some of the advanced topics such as Bayesian inference, hierarchical models, deep learning, Gaussian processes, and causal inference.
Advanced programming skills in Python. (and their related data processing, Machine Learning, and visualization libraries).
Practical experience in preparing data for Machine Learning (e.g., using SQL and/or NoSQL technologies).
Integration of Machine Learning algorithms with big-data platforms (e.g., Spark) and high-performance computing ecosystems.
Excellent (written and oral) communication skills.

Nice-to-have
Programming in C++ and/or Java.
Deployment of algorithms as real time / highly available services.
Integration with front-end systems (e.g., HTML5/ native mobile apps).
Employing Machine Learning in collaborative commercial settings (e.g., using DevOps methodologies and tools such as GitHub), ideally, in collaboration with product development teams.
Experience of working with engineering and design / product teams.

What’s in it for you?
We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team

About RBC
Royal Bank of Canada is Canada’s largest bank, and one of the largest banks in the world, based on market capitalization. We are one of North America’s leading diversified financial services companies, and provide personal and commercial banking, wealth management, insurance, investor services and capital markets products and services on a global basis. We have over 80,000 full- and part-time employees who serve more than 16 million personal, business, public sector and institutional clients through offices in Canada, the U.S. and 37 other countries. For more information, please visit rbc.com.

Inclusion and Equal Opportunity Employment
RBC is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veterans status, Aboriginal/Native American status or any other legally-protected factors. Disability-related accommodations during the application process are available upon request.

JOB SUMMARY
City: Jersey City
Address: 30 Hudson Street
Work Hours/Week: 40
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salaried
Position Level: PL07
Required Travel(%): 0-25
Exempt/Non-Exempt: Exempt
People Manager: No
Application Deadline: 11/24/2018
Req ID: 173449","Jersey City, NJ",Predictive Analytics Data Engineer,False
777,"Fractal Analytics is a strategic partner to Fortune 500 companies and helps them to leverage Big Data, analytics and technology to drive smarter, faster and more accurate decisions in every aspect of their business.Our Big Data capability team is hiring Data Engineers who can produce efficient & functional codes to solve complex analytics' problems. If you are an exceptional developer and who loves to push the boundaries to solve complex business problems using innovative solutions, then we would like to talk to you.Requirements: Overall 3-6 years of work-experience.Ability to perform ETL operations on both On-Prem and Cloud based technologies.Deep familiarity with Hadoop, Hive, Spark and programming in Java/Scala/Pyspark.Integration and testing of data of models in development and production environments.Medium proficiency and experience in implementing AIML codes in distributed node environment such as NLP.Job Type: Full-timeExperience:Hive: 2 years (Required)AWS: 1 year (Required)Natural Language Processing: 1 year (Required)Hadoop: 2 years (Required)Java: 2 years (Required)Spark: 2 years (Required)Scala: 2 years (Required)","San Francisco, CA",Sr. Data Engineer,False
778,"null
Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.
Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.
https://www.ten-x.com/

The Role:
Data and our ability to leverage it is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data science team on building out proprietary tools and models around our customer and asset data (both internal and external sets). You will be working on key projects that have board level visibility.
Responsibilities
Play a leading role in designing, developing and implementing Big Data databases (Hadoop, Graph, MySQL, NoSQL, MongoDB) that contains multiple data sets from both internal and external sources
Lead the setup of data pipelines of new internal and external data sets into the database
Work with Data Scientists to help dedupe and fuzzy match data
Work with software engineers on developing APIs
Experience
Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
5+ Years’ Experience in data engineering, including: 2+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data science requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analysesLI-ES1","San Mateo, CA",Data Engineer,False
779,"At Hasbro, we embrace the unique skills, experiences, talents and perspectives of our global workforce which, combined with our culture of curiosity and innovation, generates the best ideas. We live our values of Community, Passion, Integrity and Creativity, and we’re committed to giving our 5,000+ employees opportunities to build their individual capabilities, balance work and home, deliver excellence through team work and thrive personally. This enables us to deliver results in all aspects of our business.

Position Summary:
As a Data Engineer you will work with partners in IT and stakeholders across the organization to enable data-driven decisions. Your primary responsibility will be to expand the usable pool of data available for conducting analysis. You will also help execute the roadmap for Analytics within Hasbro and scope, plan, and execute strategic Analytics projects to help Hasbro’s brands achieve profitable growth.

A day in the life as a Data Engineer:

Write routines and build data pipelines to ingest, clean, and prepare data for analysis. Develop views and aggregated datasets that can be easily loaded into analytical tools.
Support analytics projects and conduct analysis. Leverage descriptive and exploratory techniques, text analytics, and statistical methods to help answer business questions. Prepare results and communicate findings to stakeholders. Projects may be small, one-time requests or larger, ongoing programs.
Expand Hasbro’s data model by incorporating Hasbro internal and external third-party data. Identify data sources and catalog metadata. Understand relationships between datasets. Maintain and update data model as new data sources are identified.
Advise on appropriate infrastructure and tools for data storage, ingestion, preparation, and maintenance. Understand how business requirements map onto infrastructure needs and toolset.
Support architecture plan to accommodate existing and future datasets. Create business processes for on-boarding new data sources.

What you'll bring:

2-4 years of experience in Data Engineering, Analytics, consulting, or a related data/quantitative field.
B.S. in a quantitative field (e.g., comp sci, engineering, economics), advanced degree/MBA a plus.
Intermediate experience with a scripting language like Python (preferred), R, Scala, C#, UNIX shell, Javascript.
Intermediate to advanced SQL skills, must be able to design and optimize queries and create data structures.
Experience developing in AWS, Google cloud, or Azure (Azure preferred).
Exposure to data preparation tools (e.g., Alteryx, Informatica, Teradata, Tamr).
Hands-on experience with next generation data storage tools and methods (e.g., NoSQL, Hadoop, Spark) as well as associated data modeling tools.
Knowledge of statistical package like R, SAS, SPSS, Matlab, Knime, Rapidminer a plus.
Excellent written and verbal communication skills – experience working with business stakeholders and senior leaders.
Strong project management capabilities, able to coordinate multiple projects and prioritize a variety of incoming requests.","East Providence, RI",Data Engineer,False
780,"*******************
Why do we want you?

*******************

You are an ambitious, driven developer looking to play a central role in the design, development and implementation of cutting edge, highly-scalable applications based on large amounts of unstructured/structured data. You're passionate about open source and Big Data technologies. You find yourself thinking about complex, cutting-edge systems in the shower. You want the opportunity to collect, parse, manage, analyze, and visualize large data sets to extract meaningful knowledge. And you dream of working with NoSQL database technologies and fun, emerging data science techniques. As part of a small team of experts, you have a strong desire to contribute to important endeavors to address critical needs of Fortune 500 companies.

****************
Responsibilities
****************


Work with an amazing team to design and develop modern distributed Big Data processing and analysis systems
Utilize primarily open source development tools and code frameworks (e.g. Git, Storm)
Define and manage development tasks within an agile team
Utilize unit tests and deploy code to live systems
Work closely with customers and incorporate feedback into development activities
Gracefully accept criticism when you make a less-than-stellar lunch recommendation

***************************
Skills & Qualifications
***************************

Must have:

5+ year software development in a professional setting
Strong functional language experience (Clojure or Scala preferred)
Programming with scripting languages (e.g. Python, Perl, Bash, etc)
Proficient in UNIX/Linux environments (we work on Macs and deploy to Ubuntu on AWS)
Strong knowledge of programming structures and algorithms
Excellent oral and written communications skills

Nice to have:

Distributed, fault-tolerant architectures (Storm experience preferred, Spark or Hadoop ok)
Working with NoSQL databases and JSON data (ElasticSearch preferred)
Working with messaging/queueing systems (RabbitMQ preferred)
Working with SQL databases and large-scale data integration (MySQL preferred)

***************
About Signafire
***************

Founded in 2013, Signafire is best known for its industry-leading fusion and content analysis technology, which enables companies to fuse, access, and analyze data at an unprecedented speed and scale. Initially developed for Intelligence Agencies and Special Operations Forces, Signafire's technology has evolved to help companies in any industry find, identify, and accurately turn data into actionable insights, across nearly any public or private databases.

Signafire is currently in a period of high growth, and is expanding our technology teams to create and develop new products and enhanced features. We are looking for ambitious, intelligent, and innovative candidates to help us continue to grow.","New York, NY",Senior Data Engineer,False
781,"You will love Chesterfield and we'd love to have you!
Civic pride and making a difference are just two reasons to join the Chesterfield County team. Want to help Chesterfield achieve the vision of ""being an extraordinary and innovative community in which to live, learn, work, and play""?

Chesterfield County is the fourth largest local jurisdiction in the state of Virginia and is a recognized leader across the country for being innovative, delivering high-quality results, and embracing new technology. The Center for Digital Government has recognized Chesterfield as one of the top five counties of our size over the past several years. Awards and recognitions for innovative business solutions and openness and transparency of information are received annually. Chesterfield IT teams have performed well at the Governor of Virginia's Data-thon challenge the past three years, earning the coveted Governor's Cup last year.

Our Data Goals
Evangelize the concept of ""data as an asset""
Formulate insights that foster innovation
Enable effective decision-making through shared county data
Unleash the capability of data science to empower business users
 Data Engineer Role
Advanced SQL Queries
Metadata Management
Complex Data Modeling
Cloud Data Architecture
Efficient Data Integrations
Innovative Data Analytics
Data Mining Techniques
Data Governance
Qualifications & Experience
Bachelor's degree in Information Systems, Computer Science, Mathematics, Statistics or related field from an accredited university or college. Advanced degree preferred.
At least six years progressive work experience in data focused roles (ten years preferred)
Experience with Microsoft technologies, including SQL Server, SSIS, SSAS, SSRS, Power BI, and Azure cloud services, preferred
Proficient with advanced statistical techniques and concepts such as regression and distributions.
Building skill in variety of machine learning techniques such as clustering and decision tree learning.
Advanced experience in developing custom data models, algorithms, data integration, data cataloging, and metadata management.
 Skills & Knowledge
Advanced analytical and problem-solving skills
Expert skills in database query language and semantic modeling
Expert skills in data analytics, reports, visualizations, and dashboards
Drive to learn and master new data technologies and techniques.
Highly skilled on processes related to project life cycles, software development life cycles, requirements gathering, testing methodologies, and source control
Be dependable and have a good respect for diversity
Excellent communication, team building, and interpersonal skills with a customer-focused approach
Embrace values of building trust, staying agile, and focusing on innovation
Pre-employment drug testing and FBI criminal background check required. This position is subject to working in high security areas governed by the US Department of Justice's ""Criminal Justice Information Services (CJIS) Security Policy"" and therefore requires successfully passing a more stringent criminal background check. Must be a US citizen or have been a lawful resident of the US for the past ten years. Must maintain personal mobile technology as a condition of employment.
 
Shift:

Monday - Friday; 8:30a.m. - 5:00p.m.
 
Work Location:

Information Systems Technology","Chesterfield, VA",Data Engineer,False
782,"Who are we?
Pixalate helps Digital Advertising ecosystem become a safer and more trustworthy place to transact in, by providing intelligence on “bad actors” using our world class data. Our products provide benchmarks, analytics, research and threat intelligence solutions to the global media industry. We make this happen by processing terabytes of data and trillions of data points a day across desktop, mobile, tablets, connected-tv that are generated using Machine Learning and Artificial Intelligence based models.
We are the World’s #1 decision making platform for Digital Advertising. And don’t just take our word for it - Forrester Research consistently depends on our monthly indexes to make industry predictions.


What does the media have to say about us?
Harvard Business Review
Buzz Feed
Forbes
NBC News
CNBC
Business Insider
AdAge
AdAge
CSO Online
Mediapost
Mediapost
The Drum
Mediapost
Mediapost


How is it working at Pixalate?
We believe in Small teams that produce high output
Slack is a way of life, short emails are encouraged
Fearless attitude holds high esteem
Bold ideas are worshipped
Chess players do really well
Titles don’t mean much, you attain respect by producing results
Everyone’s a data addict and an analytical thinker (you won’t survive if you run away from details)
Collaboration, collaboration, collaboration
What will you do?
Support existing processes running in production
Design, develop, and support of various big data solutions at scale (hundreds of Billions of transactions a day)
Find smart, fault tolerant, self-healing, cost efficient solutions to extremely hard data problems
Take ownership of the various big data solutions, troubleshoot issues, and provide production support
Conduct research on new technologies that can improve current processes
Contribute to publications of case studies and white papers delivering cutting edge research in the ad fraud, security and measurement space
What are the minimum requirements for this role?
Bachelors, Masters or Phd in Computer Science, Computer Engineering, Software Engineering, or other related technical field.
A minimum of 3 years of experience in a software or data engineering role
Excellent teamwork and communication skills
Extremely analytical, critical thinking, and problem solving abilities
Proficiency in Java
Very strong knowledge of SQL and ability to implement advanced queries to extract information from very large datasets
Experience in working with very large datasets using big data technologies such as Spark, BigQuery, Hive, Hadoop, Redshift, etc
Ability to design, develop and deploy end-to-end data pipelines that meet business requirements.
Strong experience in AWS and Google Cloud platforms is a big plus
Deep understanding of computer science concepts such as data structures, algorithms, and algorithmic complexity
Deep understanding of statistics and machine learning algorithms foundations is a huge plus
Experience with Machine Learning big data technologies such as R, Spark ML, H2O, Mahout etc is a plus
What do we have to offer?
Located in sunny Palo Alto and Playa Vista, CA the core of Pixalate’s DNA lies in innovation. We focus on doing things differently and we challenge each other to be the best we can be. We offer:
Experienced leadership and founding team
Casual environment (as long as you wear clothes, we’re good!)
Flexible hours (yes, we mean it - you will never have to sit in traffic anymore!)
FREE Lunches! (You name it, we’ve got it)
Fun team events
High performing team who wants to win and have fun doing it
Extremely Competitive Compensation
OPPORTUNITY (Pixalate will be what you make it)","Palo Alto, CA 94306 (Barron Park area)",Big Data Engineer,False
783,"Job Title: Data Engineer
Reports to: Head of Upstream Data – Americas

Location: Houston, TX USA

Company
For the past 40 years, Wood Mackenzie has established its reputation as a trusted source of intelligence, enriching lives by empowering clients with unique insight on the world’s natural resources.

Now, as part of the Verisk Analytics family, that legacy is even stronger. Aligning with the world’s leading data analytics company extends our ability to help clients overcome the toughest challenges with our unique analysis and advice.

We will continue to build on the power of our existing approach to assess and value individual assets and companies, allowing our clients to pursue the most promising opportunities.

Together, we inspire and innovate the markets we serve – providing invaluable intelligence that informs the strategic decisions that will ultimately shape the future direction of our global natural resources.

Role Purpose
Wood Mackenzie seeks an experienced professional to play a key role in improving and growing our US upstream supply chain research coverage. Your team will source oilfield service (OFS) supply and demand data and craft proprietary models to derive detailed cost estimates and forecasts to complement our core upstream research offerings. Working as part of the Wood Mackenzie L48 Upstream Research team, you will be at the forefront of our upstream analysis and engage with clients at a senior level. You will be expected to contribute and make an impact from day one.

Team Profile
The Americas Upstream Data Team leads the ingestion, processing, curating and analysis of data that underpins our products and supports our extraordinary research. We are a global leader in commercial analysis within the energy sector, and the completeness, integrity and quality of our datasets are critical to maintain this position. This is an exciting opportunity to join a team comprised of data analysts, engineers and scientists, working together to demonstrate greater value from our data.

Main Responsibilities
Maintain data and systems used to deliver content to clients and/or internal partners
Execute the operation of business processes including data gathering, collation and formatting
Understand the implications of system changes, tools and processes that are used in the team, and the impact of making changes for internal partners and clients
Create and maintain the documentation of data processes, databases, data tools and models, according to policies, procedure and standard methodologies.
Follow set procedures to complete regular data processes and analysis, ensuring completeness, accuracy and integrity of data
Collect and compile industry data on a regular basis from various sources to maintain and supplement our data platform
Create new ways to visualize and analyze our data for both internal tools and commercial products
Form a good understanding of data domain, business activities and client requirements from the organization's products and services
Ensure integrity of our own sourced data
Develop new processes to improve the transformation and curation of our data
Develop good working relationships with research and data colleagues to provide data support or process changes
Knowledge & Experience Required
Minimum of bachelor's degree required in related field of study
Experience in working with data and demonstrate an affinity with numbers
Understanding of how web scraping methods can be implemented
Additional skills desired but not required include: Python, R, VBA, Kapow, XML, HTML, JSON, C#, ArcGIS, SQL, AWS, Kibana, Elasticsearch, Kafka
A knowledge of the upstream industry would be advantageous
Experience of using MS Office suite including Excel
Understanding of Data visualization tools (such as Spotfire, PowerBI, Tableau, Quicksight)
Accuracy and attention to detail
Process driven
Information gathering
Team working and communication
Time management and organizational skills
Efficiency focused
Core Competencies
Accuracy and attention to detail
Process driven
Information gathering
Team working and communication
Time management and organizational skills
Efficiency focused
Wood Mackenzie Core Values
Wood Mackenzie is a place where we are committed to supporting our people to grow and thrive. We value different perspectives and aspire to create an inclusive environment which encourages diversity and fosters a sense of belonging.

Wood Mackenzie values each individual's contribution and helps them reach their full potential while sustaining an organisational culture of health and well-being.

Our core values are:
Respect for the Individual
Integrity
Passion
Persistence
Confidence with humility
Excellence
Teamwork
We understand the importance of bringing your whole self to work and to achieving balance between work, family and other life commitments. We are open to considering flexible working arrangements to enable the greatest spectrum of talent to contribute to Wood Mackenzie's success.

EEO statement
Unsolicited resumes submitted to Wood Mackenzie by any external recruitment agency via Internet, e-mail, fax, or U.S. mail become the property of Wood Mackenzie and we are not responsible for any fees associated with those resumes.

In compliance with the Civil Rights Act of 1964 and 1991, the Age Discrimination in Employment Act of 1967, Section 504 of the Rehabilitation Act of 1973, the Americans with Disabilities act of 1990 and all other relevant federal and state laws, the policy of this company prohibits discrimination in employment because of race, color, religion, national origin, sex, gender identity and/or expression, age, veteran’s status, disability, genetic information or any other group protected by law. Applicants are considered for all positions without regard to race, color, religion, national origin, sex, gender identity and/or expression, age, veteran’s status, disability, genetic information or any other group protected by law.

If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access WoodMac.com/careers on-line as a result of your disability. You can request reasonable accommodations sending an email to hrenquiries@woodmac.com.

Wood Mackenzie is an Equal Opportunity Employer M/F/V/D, and a member of E-Verify.

http://www.eeoc.gov/","Houston, TX 77057 (Galleria-Uptown area)",Data Engineer,False
784,"(NYSE: WRK) partners with our customers to provide differentiated paper and packaging solutions that help them win in the marketplace. WestRock’s 45,000 team members support customers around the world from more than 300 operating and business locations spanning North America, South America, Europe, Asia and Australia. Learn more at www.westrock.com.
Sr. Data Engineer - Norcross, GA
3169 Holcomb Bridge Rd, Jefferson Plaza
Norcross, Georgia, 30071
United States


The opportunity:
This role will have an emphasis on Data Engineering practices and will be part of a center of excellence team leading the evaluation and implementation data engineering technology, tools, and frameworks leveraging the Integration platform and cloud services supporting our Integrations investments. The efforts will help drive the strategic vision for EAI and B2B integrations within WestRock.

How you will impact WestRock:

Establish best practices in Integration technologies and developing self-servicing tools
Lead a technical team of data engineers delivering a wide array of EAI and B2B solutions that use cutting edge technologies
Guide the organization in efficient data and resource management best practices with cloud based integration services


What you need to succeed:

At least 6 years of general software and data engineering experience
At least 4 years of experience building and maintaining Integration platforms and services
Hands - on experience in the following domains

Enterprise Architecture Frameworks
Processes, methodologies, standards, products and frameworks applicable to support enterprise wide Integrations.
Business, systems and applications analysis and defining requirements for applications supported by WEBMETHODS (Services/BPMS/CAF/Terracotta in Memory Caching).
Security concepts such as OAuth, SSO, and Attribute Based Access Control and Master Data Management concepts in supply chain management.
User interface and user experience technologies including mobile-first architectures.
Developing and supporting complex interfaces between internal business systems using various adapters, file based mechanisms, batch and real-time processing.

Developing and supporting complex interfaces between internal business systems and external partners.
Technologies and ecosystem supporting B2B integrations, with good understanding of tools, applications and frameworks in a typical supply chain management system that includes document exchange, interacting with third party service providers, customer related web interfaces and self-service tools.
Analysis and Reporting
Experience providing solutions that provide insights into the transaction processing through analysis and reporting with hands on experience with Trading Networks
good understanding of reporting requirements for various business needs and costs allocation.

Operational experience with RDBMS (SQL Server, Oracle, MySQL, etc.) data stores
Experience in Java programming language
Strong software engineering skills including full agile software development lifecycle including design, implementation, unit testing techniques and able to participate in peer review
Ability to identify problem root cause quickly and provide solutions with pros and cons
Communicate effectively with product owner, project manager, and team members to ensure commitments, dates and expectations are delivered
Must be a self-starter with good interpersonal skills and the ability to manage multiple tasks, meet deadlines, and adapt to changing project requirements
Education: Bachelor's Degree in Computer Science, or similar

What we offer:
Corporate culture based on integrity, respect, accountability and excellence
Comprehensive training with numerous learning and development opportunities
An attractive salary reflecting skills, competencies and potential
A career with a global packaging company where Sustainability, Safety and Inclusion are
business drivers and foundational elements of the daily work.

WestRock Company is an Equal Opportunity Employer committed to creating and maintaining a diverse workforce: Minorities/Females/ Disabled/Veterans.","Norcross, GA",Sr. Data Engineer,False
785,"We are looking for an experienced Data Engineer to join our growing data warehouse team. You will be responsible for profiling, analyzing, processing and loading the vast amounts of payment data we have. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys the challenge of precision-based quality and high-throughput performance engineering. You will be part of a small team that works on data initiatives to deliver information products to our Business Intelligence community. You will be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing our existing architecture and re-shaping it to leverage the next-generation, cloud-native tools.

Essential Duties and Responsibilities:

 Collaborate on design and maintain optimal data pipeline architecture.
Analyze and process large, complex data sets to meet functional and non-functional business requirements.
Assist team to build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and related ‘big data’ technologies.
Deliver quality of work via unit testing.
 Ensure performance of code meets non-functional requirements.
Work with stakeholders including the Management, Product, Data and Design teams to assist with data-related technical issues.
Responsible for engaging with production support staff and remediating chronic and/or critical production support issues.
Responsible for monitoring more junior developer and contracted staff.
Qualifications

 Minimum Required Qualifications for Consideration:
Bachelor’s degree in Computer Science or Information Systems.
 Minimum 4 years+ experience designing and coding data pipeline programs, particularly in a data warehousing or analytics environment.
 Experience with Informatica, Talend, or an equivalent ETL tool.
 Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
 Strong analytic skills related to working with structured and unstructured datasets.
 Build processes supporting data transformation, data structures, metadata, dependency and workload management.
 A successful history of manipulating, processing and extracting value from large disconnected datasets.
Excellent verbal and written communication skills.
Preferred Qualifications:
 
Experience in a high-volume, high-throughput data processing environment.
 Experience with AWS cloud services: EC2, RDS, S3, Redshift, Glue, Athena.
 Experience with Python is a plus
 Knowledge of financial and/or payment systems.
 Experience with source code version control.

At WEX, we reward innovation, hard work and excellence.

Benefits include:
401(k) Plan
Adoption Assistance
Bonus Plan
Dental Insurance
Dependent Life Insurance
Employee Assistance Program
Employee Referral Award Program
Expedition - WEX’s Sabbatical Program
Extended Parental Leave
Flexible Spending Accounts (Medical and Dependent)
Health Insurance
Life Insurance/AD&D
On Site Fitness Facility in South Portland location
Pet Insurance
Paid Time Off(PTO)
Short- and Long-Term Disability Programs
Tuition Reimbursement
Vision

Equal Opportunity Employer/Vets/Disability
Primary Location: U.S.-ME-South Portland
Schedule: Full-time
Job: Information Technology - Corp","South Portland, ME",Data Engineer,False
787,"As a Data Engineer, you will use your technical capabilities to enable data driven decisions to inform financial and operational insights across the Fossil Group. You will deliver insights that will inform and drive key business decisions, and partner with business teams to structure problems, extract and analyze data and present findings and recommendations to our leadership teams.

Specific responsibilities include:
Collaborating with internal business clients to identify and develop enterprise scale analytic algorithms on large data sets
Acquiring, curating, and cleaning data from a wide variety sources, both public and private
Developing and maintaining ETL pipelines and workflows
Selecting and integrating big data tools and frameworks to support the business’ analytics needs
Provisioning and maintaining cloud based databases and cloud computing environments
Ability to draw conclusions from data and recommend actions
Continuing to grow knowledge of data engineering tools and processes
Your Skills
Aligning with Fossil’s core values, we are looking for someone who:
Develops strong partnerships across our organization
Brings a positive, service-oriented approach to work
Is driven by creating unique and efficient solutions for business challenges
Can articulate thoughts clearly as well as listen to and considers others’ ideas
Is passionate about their contribution to Fossil
Has a great time at work and encourages others to do the same
Required Skills and Experience:
BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience. A Masters degree in Computer Science, Mathematics or related technical field is a plus.
Experience architecting and developing end-to-end enterprise scale Big Data analytical solutions in serverless environments such as Google Cloud Platform
Experience with Cloud Dataflow, BigQuery, Hadoop/Spark, and Tableau
Experience implementing ETL processes in Big Data analytical solutions using a variety of sources (Text, databases, JSON, XML, etc..)
2-3 years of experience in statistical and database languages (e.g., Python, R, advanced SQL)
2-3 years of experience working with Big Data, data mining or machine learning, data visualization to draw actionable insights
3+ years of experience programming in Java and Unix shell scripts
2-3 years of experience with Agile and Scrum development
Familiar with Apache BEAM SDK
Working knowledge of basic financial concepts: P&L, margins, pricing, etc.
Prior experience in financial modeling is a plus.
Excellent oral and written communication skills, including the ability to communicate complex findings in a structured and clear manner to a non-technical audience","Richardson, TX",Data Engineer,False
788,"Company Overview
At Proofpoint, we have a passion for protecting people, data, and brands from today’s advanced threats and compliance risks. We hire the best people in the business to:
Build and enhance our proven security platform
Blend innovation and speed in a constantly evolving cloud architecture
Analyze new threats and offer deep insight through data-driven intel
Collaborate with customers to help solve their toughest security challenges
We are singularly devoted to helping our customers protect what matters most. That’s why we’re a leader in next- generation cybersecurity—and why more than half of the Fortune 100 trust us as a security partner.

The Role
Proofpoint is seeking a Senior Data Engineer to work on the Cloudmark Security Platform for Mobile. We are on a mission to delight our customers and partners by helping them achieve their business goals.
We are a creative and data-driven team, focused on continuous learning about our customers’ needs and behaviors. We care deeply about customer experience, and cherish the insights we uncover through experimentation, analysis and prototyping.
Your day-to-day
As a Senior Data Engineer, you will help shape the continued development of the Cloudmark Security Platform for Mobile Solution. You will split your time between working with the team in our San Francisco office, and travelling to work onsite with our mobile carrier customers. Onsite you will be deploying and integrating the solution, and doing data analysis in Splunk and ELK to train the platform for optimal performance in the customer’s messaging environment. You will bring valuable customer insights and real-world experiences back to Product Management and the rest of the development team to help evolve the solution and keep it the best in the industry.
You will work alongside the mobile solution product manager and other development team members to design and build solutions in a fast, collaborative process. We will iterate quickly, operate on evidence and evolve as we learn. You are biased towards action, pragmatism, and results.
Deploy and integrate the Cloudmark Security Platform for Mobile Solution into mobile operator environments
Perform data analysis using operational intelligence tools and scripting tools including Splunk, ELK, Python, Perl and GoLang, to optimize and train the mobile solution for specific environments
Iterate on product ideas and work with Product Management to help shape the direction of the solution
Build rapport, trust and credibility with customers, delivering presentations on the product results
Produce customer facing documentation
Assist with building test harnesses and test plans
Produce customer facing reports using Splunk and ELK
 What you bring to the team
BS Degree in CS, data science, statistics or related fields (or equivalent level of industry experience).
Minimum 7-10 years of experience in a software related field (development, testing, data analysis, solutions architect, etc)
Experience in data analysis using Splunk, ELK, Loggly, or similar analytical tools
Experience with Linux system and application administration
Ability to write scripts needs to perform integration tasks
Pre-Sales, Solutions Architect or consulting experience is useful – particularly implementation experience in carriers or large enterprises
Experience in a technical role in the anti-spam, cyber-security and/or messaging space is a plus
Knowledge of networking topology, TCP/IP protocol, network configuration and components (firewalls, routers, proxies, etc.), and mobile protocols such SMPP or messaging protocols such as SMTP, SMS, MMS or RCS is very useful
Strong technical presentation and communication skills, both verbal and written is essential
Ability to work both independently and in a team environment
Ability to gather requirements, and interact effectively with both customers and team members
Ability to travel up to 50%
Why Proofpoint
As a customer focused and driven-to-win organization with leading edge products, there are many exciting reasons to join the Proofpoint team. We believe in hiring the best the brightest and cultivating a culture of collaboration and appreciation. As we continue to grow and expand globally, we understand that hiring the right people and treating them well is key to our success! We are a multi-national company with locations in 10 countries, with each location contributing to Proofpoint’s amazing culture!
#LI-VW1","San Francisco, CA",Senior Data Engineer,False
789,"Data Engineer
Navigator works with clients across the country serviced by offices in Columbus (Headquarters), Atlanta, Baltimore, Boston, Cleveland, and Phoenix
Submit your resume knowing that our breadth of services require new recruits all the time!

We are looking for an experienced Data Engineer to join our growing team of analytics experts. This resource will join our consulting organization with a focus on optimizing data and data pipeline architectures, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our analytics project teams on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities for Data Engineer

Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Hadoop ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights
Work with executive, functional, and technical stakeholders regarding data-related technical design, development, and architecture initiatives
Create data tools for analytics and data scientist team members
Qualifications for Data Engineer

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Willingness to travel.


We are looking for a candidate with 2+ years of experience in a Data Engineer role, who has experience using the following software/tools:

Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases
Experience with data pipeline and workflow management tools
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.


Education:

Bachelor’s degree.
Navigator Management Partners, LLC is an equal opportunity employer. It is the policy of Navigator Management Partners to provide equal opportunity to all employees and applicants; to provide equal opportunity for advancement of employees; and to administer its business in a manner that does not discriminate against any person because of age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.","Cleveland, OH",Data Engineer,False
790,"The Data Engineer will ensure the smooth functioning of processes to ingest third-party data into internal data systems, as well as manage delivery of data to API endpoints or data repositories. The engineer will have responsibility for both data ingest and delivery. The engineer typically will be responsible for data transformation scripts and oversight of one or more data QA pipelines, per client requirements. The data engineer may work with client or provider systems to write specifications detailing the type and expected quality of data deliveries. The data engineer will also with work with systems engineering, product development, solutions engineering and data entry teams as needed to assess and meet needs.
Highly knowledgeable about industry data management strategies and practices, such as ACID compliance, data backup/restore, and data encryption/access. Ability to write data transformation scripts in at least one common scripting language, such as Groovy, Python or Javascript. Functional competence in Java. Familiarity with W3C standards for linked data and related technologies such as triple stores, RDF, SPARQL and SHACL is strongly preferred. Bachelors degree in Computer Science or Information Systems or 5+ years’ experience with corporate data management systems in high-compliance contexts.
The Data Engineer for the data technology team manages and operates systems to acquire and deliver content data to client systems within Disney. The data engineer’s primary responsibility is to maintain the systems that ensure data meets internal and client specifications and is delivered on-time. The data engineer will work as part of a data technology team with the senior staff data engineer and the data quality engineer. 588472","Burbank, CA",Data Engineer- Project Hire,False
791,"About the role:
Audley Travel is now seeking a Data Engineer to play a pivotal role in our Digital Analytics team. In this role you will be responsible for the creation and management of Client & Marketing data sets across the Audley journey, further enabling advanced data science and analytics. You will collaborate closely with our Digital Marketing Analysts, Data Scientist and Marketing channel owners to scope, build and visualize extensible data sets to better our client communication strategies.
Basic Qualifications:
Exceptional problem solving skills
Strong attention to detail and managing deadlines
Bachelor’s degree in computer science or related field
Strong understanding of relational database systems (both row & column oriented)
Experience with several AWS features
Preferred Qualifications:
Experience creating data models and data city planning
Experience migrating data infrastructure and workflows into the cloud
Solid knowledge of software development methodologies (such as Agile)
What’s in it for you:
As a data engineer you will be a crucial piece of a brand new client acquisition strategy. You will have the ability to work with key stakeholders and the freedom to think independently and be creative in your efforts. In addition, we offer a comprehensive benefits package where we cover 100% of the cost of Medical, dental and vision and match your 401k contribution! We find though that the real benefit to a career at Audley turns out to be the people.
About Audley Travel US Inc.:
Audley connects discerning travelers to a more rewarding travel experience in nearly 80 destinations around the world. Each of our Country Specialists has first-hand knowledge of the best guides, food, lodging, and local secrets in a specific region of the world, having lived or traveled there extensively. This allows them to tailor each journey to a client’s individual travel style and interests. The local knowledge and personalized service creates a streamlined and stress free planning experience.
Our carefully curated experiences, candid advice and level of service including personalized travel packs go above and beyond client expectations
Audley Travel Group is an Equal Employment Opportunity/Affirmative Action employer. We are committed to the policy of providing equal employment opportunities without regard to race, color, religion, sex, sexual orientation, gender identity, veteran status, disability or national origin.
This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.","Boston, MA 02114 (Back Bay-Beacon Hill area)",Data Engineer,False
792,"We are seeking an experienced Data Engineering Consultant with a proven track record for high quality and impactful delivery.Responsibilities: Advise, enable and assist software data architects, data engineers, product and business owners to evolve fast data architectures and design technical solutions.Enable our clients to successfully design and implement software solutions mainly through pairing, mentoring, code and architecture reviews, etc.Assist in creating and maintaining tools and templates for professional services engagements such as methodologies, perspectives, training and other enablement materials.Ability to travel (25% of the time).This is much more than a coding position - we are looking for a true consultant and technology leader with an advanced technical background.Qualifications: ...enjoy being a fast learner and being a part of a fast-moving and evolving technical environment....are an advocate and evangelist of stack and related technologies; Scala or Java, Spark, Kafka, HDFS, Mesosphere DC/OS or Kubernetes, and optionally Platforms for microservices, including Akka, Lagom, and Play....have significant experience in big data batch and streaming architectures and concepts, including deployment, monitoring and operations....have experience in distributed architectures and functional programming concepts....have a consulting background and experience working directly with clients....have excellent written and verbal communication skills in English....are skillful at interacting and working with people in a leadership role; working with a self-organized lean and agile team to mitigate key project technical risks, managing effort and ensuring quality....are dedicated to helping clients produce high quality solutions and dedicated to best practices such as automated testing, performance testing, code reviews, continuous integration, and continuous deployment....hold at least a Bachelor's degree or equivalent experience....have specific experience one or more of the following data engineering and architecture areas:IoT solutionsSpark, Hadoop, Big Data/Fast Data systems.Mentoring and leading software teams.Job Type: Full-timeExperience:Hadoop, Big Data/Fast Data systems: 2 years (Preferred)Scala or Java, Spark, Kafka, HDFS, Mesosphere DC/OS: 4 years (Required)big data batch and streaming architectures and concepts: 3 years (Required)Education:Bachelor's (Required)Language:English (Preferred)Work authorization:United States (Required)","San Francisco, CA",Data Engineer,False
793,"R183245 Data Engineer
Job Description

Your next adventure at VMware is only a click away!

At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights.
If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community.
VMware, the global leader in virtualization and cloud infrastructure, delivers customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery.

Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform? Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?

VMWare Data Team is looking for a Data Engineer to help build on Next generation Near Realtime BI Platform based on SAP HANA and Hadoop. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.

Responsibilities:Understand the business capability/requirements and transform them into robust design solutionsPerform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as neededPerform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLTProtect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.Help data consumers to correctly understand and use the data.Building reports based on the business need.

Qualifications:5+ years of experience in as a BI/Data Engineer handling large volumes of data.Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.Expertise in writing advanced SQL queries.Experience working with Informatica, SAP SDI/SLTExpertise in SAP HANA, Hive/Hadoop/HawqWorking knowledge of BI Reporting tools like BOBJ and Tableau is a plus.Experience in Python ScriptingFamiliarity with Amazon Web Services (AWS), Redshift is a plusStrong analytical and troubleshooting skillsExcellent verbal and written communication skillsBachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.

EEO Statement:
VMware is an equal opportunity employer committed to the principles of equal employment opportunity and affirmative action for all applicants and employees. Equal opportunity and consideration are afforded to all qualified applicants and employees in personnel actions, which include: recruiting and hiring, selection for training, promotion, rates of pay or other compensation, transfer, discipline, demotion, layoff or termination. VMware does not unlawfully discriminate on the basis of race, color, religion, sexual orientation, marital status, pregnancy, gender identity, gender expression, family medical history or genetic information, citizenship, national origin or ancestry, sex, age, physical or mental disability, medical condition, veteran status, military status, or any other basis protected by federal, state or local law, ordinance or regulation. VMware also makes reasonable accommodations for disabled employees consistent with applicable law. Further, it is the policy of VMware to maintain a working environment free of all forms of harassment","Palo Alto, CA",Data Engineer,False
795,"$90,000 - $110,000 a yearAre you a Data Engineer who wants to utilize your talents to help foster the leaders of tomorrow? Do you want your work for a non-profit to help society? If yes, read on

Whats The job?

As a Data Engineer on our team, you will be tasked to take the lead on building out data pipelines from the ground up using Python.

The data youll be shaping will help improve and track academic performance across our array of award-winning educational facilities with the goal of creating the infrastructure to double our capacity. You will be processing the data with Spark / AWS.

This is a position where youll have a lot of insight into how we move forward as a team. We are very feedback-driven and value professional growth. If you feel it can be done better, youll be able to take ownership on steering us in that direction.

Who Are We?

We are an award-winning educational system founded in NYC, changing the game on how education can develop our leaders of tomorrow.

We have grown rapidly since our inception and are looking to double to outreach soon.

What Are We Looking For?


Data Pipeline background using Python
Experience with Spark AWS
Someone who wants to take point on projects

Compensation


$80-100K
Robust Full Benefits Package (Medical, Dental, Vision)

Whats In It For You?

If helping develop the leaders of tomorrow isn't enough, we also offer a robust benefits package (Medical, Dental, Vision, 20 Days PTO etc.).

You will be in an environment that allows for ownership on how we are going to move forward, you will be a key figure in shaping our direction.","New York, NY 10005 (Financial District area)",Data Engineer (Education),False
796,"ThirdLove is looking for a Data Engineer to provide technical leadership and hands-on development of our data driven projects. This role will lead the charge for selecting and implementing the right technologies and infrastructure for data warehousing, analytic reporting, recommendation engines and analytics engines based on business requirements and best practices.

THE JOB


Identify database requirements by interviewing stakeholders, evaluating existing systems, and analyzing department applications and operations
Recommend, document, and implement database systems, data architecture, schema design, security, backup, and disaster recovery
Master of ETL/ELT
Relational Data modeling including de-normalized dimensional modeling (Star and Snowflake schema design)
Work closely with engineers to design and maintain scalable data models
Develop, document and maintain enterprise ETL processes
Be the expert on end-to-end data flow for the enterprise
Develop an enterprise reporting and data warehouse solution to track business metrics
Implement systems for tracking data quality and consistency
Work with the tech team to establish data standards, ensure standard adherence and maintain data quality
Collaborate with business and product stakeholders to develop clear business objectives, KPIs and measurements
Programming/Scripting of tools for task automation

THE QUALIFICATIONS


3+ years experience in data warehousing and business intelligence
Expertise in ETL (eg, SSIS, Ansible, Informatica, SAP, OWB or Scripts)
Prior experience working with data processing platform
Proven ability to work with varied forms of data infrastructure, including relational databases
Experience with configuring and deploying databases on AWS
Experience with creating frameworks to extract data via APIs
Expertise with various database platforms (SQL, NoSql, On-Prem, Cloud)
Scripting and programming in one or more of the following: PHP, .NET, Python, R., Javascript
Ability to write, analyze, and debug SQL queries
Experience with non-structured data (eg, free form, json, xml, images, audio, video)
Experience with data visualization, data mining, and preferably statistical tools
Good understanding of the Big Data technology trends
A master of all trades mentality and an ability to embrace new challenges regularly
Able to take individual ownership of a project from start to finish
Excellent critical thinking, problem solving, and analytical skills
Excellent communication skills, and the ability to work effectively with others
BS or MS degree in Computer Science or a related technical field

THE PERKS


Comprehensive health benefits
401k plan
Equity
Subsidized lunches
Quarterly product allotment
Wellness benefits including in-office massages visits

ThirdLove® is empowering women to feel comfortable and confident in their everyday lives. We make bras that fit perfectly, feel incredible, and look stellar.

We're a rapidly growing team (over 250 today and doubling in size this year) based in San Francisco with offices in Chico, CA and Argentina. We're funded by Tier 1 investors and are disrupting a $100 billion global market. We're one of the fastest growing consumer brands in the country, and known as 'the brand to catch Victoria's Secret'.

Our culture is collaborative, fast-paced, and data-driven with a strong focus on designing beautiful products and creating a seamless user experience. Instead of using standard industry molds, we developed proprietary half-cup sizes based on real women's measurements and created a Fit Finder® Quiz that removes the hassle in finding the best size. To date, over 9 million women have taken our easy-to-use Fit Finder® Quiz.

Five core values drive all of the work we do:

Every Day is a New Day. Learn from the past but keep moving forward. Stay positive and optimistic.
Make it Happen. Be proactive. Be thoughtful about the how.
Defy Conventions. Question the status quo. Ask why. Be nimble and embrace change.
We're Stronger Together. Give your full attention, share information, and help others learn and develop.
Put Customers First. Make every interaction count. Listen to, respect, and delight customers.

If you want to impact millions of people each and every day, and you share these values, we'd love to connect.

ThirdLove is an equal opportunity employer and values diversity at the company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity or expression, sexual orientation, age, genetics, marital status, veteran status, or disability status.","San Francisco, CA",Data Engineer,False
797,"An Engineer is part of a key team of Nordstrom Technology professionals that applies scientific, mathematical and social principles to design, build, and maintain technology products, devices, systems and solutions. These technology products and solutions provide amazing customer experiences while meeting the needs of the business. The Data Engineer will focus on leading the design and development of business intelligence, ETL and database solutions. The ideal candidate is creative, customer-driven and has a passion for supporting e-commerce BI products.

A day in the life...
Possesses deep proficiency of engineering best practice(s)
Support the development and evolution of our data warehouse and BI platform.
Partner with the BI Manager, Data and BI Engineers, Program Managers and Analysts on building a best-in-class suite of tools and reporting mechanisms to bring the most salient, insightful data more directly into key business functions.
Coding proficiency in at least one modern programming language (e.g. Python, Java, Scala)
Experience with Big Data Technologies (Hadoop, Hive, Presto, Pig, Spark, etc.).
Design and implement modernized ETL and data processing solutions through modernized cloud based solutions (S3, Redshift, etc) and deprecate legacy on-premise solutions (Oracle, etc)
Develop data integration solutions leveraging multiple disparate sources.
Continual performance tuning and capacity planning for future growth potential.
Provide a constant flow of new and innovative ideas into the BI roadmap.
Integrates broad working knowledge in related disciplines to create integrated technical innovations/ solutions for complex business situations
Serves as lead resource for dealing with challenging technical issues
Makes decisions which influence and impact the success of cross-team initiatives
Works with larger team to drive to resolution on complex engineering problems
Accountable for resolving specific issues within a particular area, application, technology or system
Some exposure to working cross discipline and driving solutions for moderately complex business situations
Leads end-to-end engineering support for projects and problems of complex scope and impact within practice
Viewed as trusted engineering resource
Makes decisions which require understanding of both internal and external impacts to team/project

You own this if you…
4+ years professional experience
2+ years of experience writing advanced SQL, data modeling, building ETL solutions, performance tuning of BI queries, and data mining from multiple sources.
2+ years of experience in Analytics, OLAP tools, and engineering automated solutions.
2+ years of experience in BI reporting technologies such as MicroStrategy or Tableau.
2+ years of experience Coding proficiency in at least one modern programming language (e.g. Python, Java, Scala)
2+ years of Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).
Cloud Computing Experience (e.g AWS, Azure) is a major plus
Bachelor’s or Master’s in Computer Science, Engineering, or equivalent














We’ve got you covered…

Our employees are our most important asset and that’s reflected in our benefits. We listen to what’s most important and continue to evolve our offering to support both our employees and their families.

Beyond strong health, retirement and time off benefits, Nordstrom is proud to offer:
Commuter Benefits
100% Paid Parental Leave
Charitable Giving and Volunteer Match
Merchandise Discount
Nordstrom Stock Purchase Plan

A few more important points...

The job posting highlights the most critical responsibilities and requirements of the job. It’s not all-inclusive. There may be additional duties, responsibilities and qualifications for this job.

Nordstrom will consider qualified applicants with criminal histories in a manner consistent with all legal requirements.

Applicants with disabilities who require assistance or accommodation should contact the nearest Nordstrom location, which can be identified at www.nordstrom.com .
© 2018 Nordstrom, Inc. | Nordstrom Careers Privacy Policy

Current Nordstrom employees: To apply, log into Workday, click the Careers button and then click Find Jobs.","Seattle, WA",Engineer - Big Data,False
798,"LendKey is solving a complex challenge – to improve lives with lending made simple – by helping financial institutions compete in the digital age and provide a delightful customer experience, while providing borrowers with the simple, transparent, digital borrowing experience they have come to expect and desire. LendKey works with hundreds of credit unions and banks to conduct their education finance and home improvement loan programs.
We are looking for an experienced Engineer and Data Architect to help us build out our data warehouse and technical data structure across the firm. Our data capabilities and culture are still in early stages, so this is an opportunity to build a data platform from the ground up.
What you’ll do:
Partner with the product and engineering teams to develop scalable, extensible systems
Be the driving force behind the roadmap to normalize all of our transactional data, disparate systems, and transfer of data in way that creates a flexible and scalable data solution
Develop data governance policies & appropriate structures to ensure adherence to those policies
Ensure that solutions work well within our current code environment, that technical data initiatives are aligned effectively with development staff, and work to understand how other tiers in the technology stack influence data quality including APIs, ORMs (Object Relational Mapping) and User Interface
Responsible for managing the full life-cycle of the data warehouse solution; including the architecture, design, development, implementation, and support of the data warehouse
Participate in creating the data design of all transactional data stores across business units and technology stacks
Work with end users to translate business questions and requirements into applications that employ the appropriate reporting tools
Assist in monitoring and troubleshooting system performance, reliability, availability, and recoverability of all data stores
Requirements
What we’re looking for:
Culture Fit:
Strong desire to work for a mission-based organization that emphasizes the importance of providing exceptional customer service and adherence to our core values: Truthful at all times; Helpful to teammates, clients, and customers; Present, committed & engaged to their teams and work; Driven to be courageous to make an impact; and Diligent & conscientious in executing every element of work.
Technical/Business Experience
Bachelor’s degree in Computer Science or related field
10+ years overall experience working in development and enterprise data architecture
Experience and background in building data platforms for financial services
Minimum 5+ years of experience with enterprise data architecture/design
Minimum 5+ years of experience in software development with deep experience in Object-Oriented, Functional, Object-Functional Language and one of the major SQL relational datastores (we are a SQL Server and MySQL shop)
Deep expertise in relational and dimensional data modeling and database design skills.
Experience in leadership roles a plus
Hands-on experience with data management and movement platforms like Hadoop, Kafka and Airflow
Ability to investigate complex business problems, develop effective recommendations, negotiate and/or present solutions, and resolve problems in a highly professional and tactful manner.","New York, NY",Principal Data Engineer,False
799,"ContractOMNIGON is looking for a Data Engineer to build, maintain, monitor, and improve a real time scalable, fault tolerant, data processing pipeline.

The Data Engineer will support the building of a Redshift-based data mart, implementing ETL processes and integrating with various marketing platforms (SAS, Salesforce) and other systems.

Our preference is that this person is onsite in Los Angeles; however, we will accept candidates who can frequently travel onsite. This is a contract position.

Responsibilities


Implementing ETL processes
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies
Collaborate with team members to help shape requirements

Requirements


4+ years of data engineering or related experience
Strong Java and/or Scala experience
Experience with AWS services including S3, Redshift, EMR, Lambda, and RDS
Experience with stream processing using Spark Streaming/Storm/Beam/Flink
Experience with messaging systems, such as Kafka or Kinesis
BS in Computer Science, or a related field
Excellent communication skills

Preferred


Experience with NoSQL databases such as MongoDB, Cassandra, or DynamoDB
Experience with Elasticsearch
Experience with Machine Learning using Mahout/Deeplearning4j/Spark ML

About OMNIGON

OMNIGON is a team of digital strategists, artists and technologists working exclusively in the areas of consumer loyalty, audience growth and digital content delivery. Since its founding in 2008, OMNIGON has established itself as a market leader, focused on helping clients achieve returns on the strategic, creative and technical investments they've made. OMNIGON, headquartered in New York and with teams in Los Angeles, London, Toronto, Kiev and St. Petersburg, works with celebrated, global brands including Fox Broadcasting, Verizon, the PGA TOUR, FC Bayern Munich, AS Roma, the German Football Association (DFB), IRONMAN, NASCAR, World Rugby, the United States Golf Association and countless others.","Los Angeles, CA",Data Engineer,False
800,"Are you passionate about solving challenging problems?
Do you thrive being a critical part of an elite team of like-minded people?
How would you like for your next career move to take you to the next level?
If any of this sounds appealing, look no further.
Job Description:
Design and build robust and scalable solutions for managing structured and unstructured data using traditional databases (PostgreSQL, SQL Server, etc.), Massively Parallel Processing (MPP) databases and NoSQL (Hadoop, Spark, etc.) tools.
Work with business users and the data operations group to develop automated ETL routines to ingest disparate sources of data into SQL databases.
Orchestrate server environments on the data platform with tools such as Puppet and Ansible
Develop tools to support a team of Data Architects, Data Analysts and Data Scientists
Basic Qualifications:
5+ years software development experience with languages such as Python, Java, Rust, and Scala
2+ years Python experience
3+ years Experience with SQL Database design and querying
3+ years Experience with using Linux systems from the command line
Desired Skills:
Ansible
Hadoop Ecosystem
Apache
Open Source Development via GitHub


So what does Novetta do?
We focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.
Our culture is shaped by a commitment to our Core Values:
Integrity: We hold ourselves accountable to the highest standards of integrity and ethics.
Customer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.
Employee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.
Innovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.
Get a referral bonus for the great people you know! With our amazing referral program, you could be eligible to win outstanding rewards for referring qualified new hires to Novetta.

Novetta is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.","Washington, DC",Data Engineer,False
801,"OVERVIEW
Sentry Insurance is seeking a Data Engineer at our Stevens Point, Wisconsin headquarters. The individual filling this position is responsible for, under broad supervision, gathering and assessing business information needs and preparing system requirements. The person will perform analyses, development and evaluation of data requirements in a data management environment which includes data design, database architecture, metadata, repository creation and data consumption methods. The individual will use data mining and data analysis tools and will review and validate data loaded into the data repositories for accuracy. The person will participate in the detailed identification and development of program and data repository specifications and the design, development and implementation of new and existing programs and systems.
WHAT IT TAKES
Bachelor’s degree in Computer Science, Information Systems or a related field and five years of progressive data engineering experience. Sentry will also accept individuals with a master’s degree in Computer Science, Information Systems or a related field and three years of progressive data engineering experience.
As part of the 5 years (bachelor’s degree) or 3 years (master’s degree) of data engineering experience required, the individual must have:
2 years’ experience with business intelligence tools and systems, specifically Business Objects, Analysis Services and Tableau;
2 years’ experience with scheduling and workload automation tools, including at least one year with Zena;
2 years’ experience with SQL and PowerShell Scripting;
2 years’ experience using Java and Apache Kafka to create pipeline data messaging solutions;
2 years’ experience with Informatica, Power Center and Power Exchange; and
2 years’ experience with Agile methodology and JIRA.

WHAT YOU'LL RECEIVE
There is a reason why Sentry made Forbes’ list of America’s Best Midsize Employers in 2017 and 2018. At Sentry, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. So, we provide a perfect mixture of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that you’ll receive.
Competitive compensation to reward you for your hard work every day.
Generous Paid-Time Off Plan for you to enjoy time out of the office.
401(K) plan with a dollar for dollar match on your first eight percent, plus immediate vesting to help fund your future.
Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
Pretax Dependent Care and Health Expense Reimbursement Accounts to ease taxes on health spending.
Extensive Work-Life Resources to lend a helping hand.
Volunteer Time Off so you can dedicate time to the community.
Sentry Foundation gift matching program to encourage charitable giving.
HOW YOU’LL APPLY
If you are interested in joining Sentry’s team, select the one position that you are most interested in being considered for and complete your online application details. If you have applied with us before, you will only need to provide your email address and password. If this is your first time applying, you’ll need to create an account. Please upload your resume directly, in addition to completing your online application details.
WHO YOU’LL WANT TO CONTACT
Laura Kaczmarski at 715/346-6373
LAURA.KACZMARSKI@SENTRY.COM
ABOUT SENTRY
All of us at Sentry—more than 4,000 associates—have various talents, skills, and backgrounds. We work together to deliver on our promises to our policyholders every day. We’re proud to offer a full line of property, casualty, and life insurance products to help protect businesses, cars, homes, lives, and retirement income.

Our headquarters is in Stevens Point, Wisconsin, with claims and service offices located throughout the United States. From sales to claims, and information technology to marketing, we enjoy a rewarding and challenging work environment with opportunities for ongoing professional development and growth.

Our bright future is built on a long track record of success. We got our start in 1904 and have been helping businesses succeed and protect their futures ever since. Because of the trust placed in us, we’re one of the largest and financially strongest mutual insurance companies in the United States. We’re rated A+ by A.M. Best, the industry's leading rating authority.

Get ready to own your future at Sentry. Opportunities await.
EQUAL EMPLOYMENT OPPORTUNITY
It is our policy that there be no discrimination in employment based on race, color, national origin, religion, sex, disability, age, marital status, or sexual orientation.","Stevens Point, WI 54482",Data Engineer,False
802,"Location
New York, New York
Shift:
Day (United States of America)
Description:
Join the Information Technology team at New York’s #1-ranked hospital.

This is your opportunity to provide world class technology solutions that will directly impact the quality of a patient’s life. At New York Presbyterian Hospital Information Technology is at the forefront of our patient experience. Joining our team will give you the opportunity to develop your career while creating solutions and services that will improve the welfare of others. If growing your career in technology while creating solutions that improve the lives of others inspires you, then a career at New York Presbyterian Hospital awaits you.

The Senior Big Data Engineer will be assisting in the configuration and implementation of batch and streaming data processes related to ingesting data, cleaning and transforming it on a Big Data platform. You will be responsible for documenting work and management of cluster/services. The Senior Big Data Engineer will assist in the monitoring and maintenance of the data processes deployed in production.

Required Criteria
Bachelor’s Degree in Computer Science
Minimum 3 years of work related experience
Minimum 3 years of experience in software development, preferably with Java, excellent understanding of Object Orientated concepts
Minimum 2 years’ experience with at least one scripting language: Python, Scala, Linux shell preferred
Minimum 2 years of experience with a Hadoop/Map Reduce preferably Hortonworks or Cloudera, worked with Hive and Kafka
Minimum 1 year experience with noSQL database such as HBase (preferred, Cassandra or MongoDB)
Preferred Criteria
Master’s Degree
Minimum 1 year of experience in tuning Hadoop cluster to improve performance and end-user
Minimum 1 years of experience with workflow tool and data processing platform: Oozie and Impala
Strong SQL skills preferred
Familiarity with Graph Databases a plus
Familiarity with search indexes such as Lucene/SOLR a plus
Familiarity with statistic tools such as R, SAS, etc. a plus
Familiarity with Spark a plus
Enjoy being challenged and solving complex problems
Able to work in a team and collaborate with other teams to define requirements and remove ambiguities
Join a hospital where employee engagement is at an all-time high. Enjoy competitive compensation along with benefits such as tuition reimbursement, hospital retirement contributions, and financial planning assistance. Start your life-changing journey today.
__________________
#1 in New York, ""America's Best Hospitals 2017-2018."" - U.S.News & World Report
2018 ""Best Places to Work: Employee's Choice."" - Glassdoor
2018 ""Employees' Choice: Top CEOs"" - Glassdoor
2018 “America’s Best Employers.” - Forbes
2018 ""150 Best Places to Work in Healthcare."" - Becker's Healthcare
2018 ""Top-Rated Work Places: Best Hospitals"" - Indeed
Discover why we're #1 in New York and a best employer at: nyp.org/careers

NewYork-Presbyterian Hospital is an equal opportunity employer.","Manhattan, NY",Senior Big Data Engineer,False
803,"Data Engineer - Analytics - 18123100

Hill-Rom is a $2.7B leading worldwide manufacturer and provider of medical technologies and related services for the health care industry, including patient support systems, safe mobility and handling solutions, non-invasive therapeutic products for a variety of acute and chronic medical conditions, medical equipment rentals, surgical products and information technology solutions. Hill-Rom's comprehensive product and service offerings are used by health care providers across the health care continuum and around the world in hospitals, extended care facilities and home care settings to enhance the safety and quality of patient care.

Description

The QA/RA Engineer - Metrics will manage key corporate metrics to drive business decisions and implement sound data-driven business solutions with an emphasis on quality system compliance. The primary function is to lead Hill-Rom’s global efforts to collect, analyze, and track the resolution of key data related to entity quality and performance indicators by interfacing with key business partners across the corporation. The incumbent will manage a dashboard of Key Performance Indicators for Executive Management that provides consistent timely analysis and publication of quality and key performance indicator metrics. Trend analysis will be performed to identify and mitigate critical quality issues along with the corresponding level of risk. The incumbent will promote a culture that Quality Matters to everyone in the corporation.

ESSENTIAL DUTIES AND RESPONSIBILITIES – Other duties may be assigned:
Manage the evaluation of quality systems to determine how metrics and measures can be used to drive business decisions.
Provide leadership to monitoring and analyzing key elements of quality performance to identify product and process quality trends, quality system integrity and compliance with internal, as well as external standards and guidelines. Intervene when necessary to implement solutions and drive continuous improvement.
Lead the Quality Improvement Program for Hill-Rom including identifying state-of-the-art quality standards and practices; quality system planning and implementation; and influencing strategic planning.
Manage defined KPI’s/other metrics to identify process efficiency and improvement items.
Coordinate Executive Management Review and quality scorecards.
Recommend goals and objectives for quality performance as part of annual and long range business plans.
Manage a set of best practices on what Hill-Rom should consider when defining quality system outcomes/best practices based on proven successes within the medical device industry.
Lead the review of quality process issues and determine the best solution approach using a combination of people training, transformation, process updates, service improvements or technology changes.
Provide oversight to the development of queries and reports from various data sources and data warehouse activities.
Ensuring standardization, harmonization and reuse of information across the global corporation.
Support the escalation process when compliance issues cannot be resolved at the local level.
Provide training and support to department staff and business users on the use of metrics.
#INDHR

Qualifications

Must possess sound knowledge of analytical data interpretation and trending tools.
Must be articulate in both verbal and written communication skills, including strong questioning and listening skills and ability to look beyond obvious answers and understand the impact on other areas.
Ability to understand and apply mathematical concepts especially as they relate to statistics and trending.
Ability to define problems, collect data, establish facts, and draw valid conclusions.
Proven presentation skills are required.
Must be adept at independent decision-making.
Strong leadership skills, strong interpersonal skills, and the ability to deal effectively with system users, is required.
Strong data analysis skills and creativity in identifying new opportunities and evaluating alternatives is required.
The proven ability to prioritize and manage multiple projects and meet deadlines is required.
Must have the capability of developing effective working relationships with staff at all levels in the organization.
Willing to travel (5%-10%) as business responsibilities require.

EDUCATION AND/OR EXPERIENCE:
Bachelors degree in a business or technical discipline or relevant experience
Knowledge of Quality Systems, Business Process Management and Process Improvement is preferred.
Experience in statistical analysis is required (the ability to understand and apply data to the business is extremely important).
Experience with the creation and ability to influence business processes using KPIs and metrics is required.
Must be capable of working on several projects concurrently under tight deadlines and be able to prioritize to meet organizational goals, with attention to regulatory requirements.
Demonstrated proficiency with Microsoft systems (Excel, PowerPoint, Word, Access, Project, SharePoint); Cognos BI; Minitab; SAP and JD Edwards is preferred

Hill-Rom is an equal employment employer F/M/Disability/Vet/Sexual Orientation/Gender Identity

Job: Quality

Primary Location: United States-Indiana-Batesville

Other Locations: United States-Illinois-Chicago, IL

Schedule: Full-time

Travel: Yes, 10 % of the Time

Posting Entity: Hill-Rom","Batesville, IN 47006",Data Engineer - Analytics,False
804,"At Bossa Nova we create service robots for the global retail industry. Our robots’ mission is to make large scale stores run efficiently by automating the collection and analysis of on-shelf inventory data. We drive autonomously through aisles, navigating safely among customers and store associates. If we were a self- driving car we’d be operating at level 5 autonomy.
Oh, we should add, it’s real, happening today, you can meet our robots in some of the world’s biggest retailers.
Position: Python Robot Data Engineer
Location: Pittsburgh
You’ll be joining our data engineering team as a data engineer. You’ll be responsible for the movement of various forms of data off our robots to the cloud. You'll write Python code that is deployed on field robots and used for: logging, monitoring and remote troubleshooting. You will work daily with Roboticist's
Required:
2+ Years professional experience with Python
Python data structures and best practices
Strong Linux skills (Ubuntu)
Writes organized code with appropriate exception handling and logging
Designs code to handle degraded or constrained network conditions.
Understanding of HTTP network requests and responses
Understanding of TCP, with ability to troubleshoot network issues
Ability to write technical documentation and comment code
Ability to write test suites
Desire to collaborate with domain experts.
Nice to Have:
Experience with Containers
Production experience working with Cloud
Configuration management of some form (Ansible, Chef etc)
Test Automation","Pittsburgh, PA 15222 (Strip District area)",Python Robot Data Engineer,False
805,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Facebook is seeking an experienced Software Engineer to join the Warehouse Product Infrastructure team. The Warehouse Product Infrastructure team builds large scale logging, data processing and analytics at Facebook. Our stack serves all Facebook products to monitor and make critical product decisions. We handle everything from Facebook scale logging across client and server, to metrics computation, to unified pipeline management across streaming, batch and machine learning workloads. We are looking for candidates who share a passion for tackling complexity and building platforms that can scale through multiple orders of magnitude and for enable Facebook analytics to be fast and high quality. This position is full-time and is based in our office in Seattle, WA.
RESPONSIBILITIES

Design core, backend or frontend software components

Code using primarily PHP, Hack, Python, Scala or Java

Interface with other teams to collaborate in transforming the landscape

Conduct design and code reviews

Analyze and improve efficiency, scalability, and stability of various system resources

Design and implement a workflow language for efficient data processing and machine learning
MINIMUM QUALIFICATIONS

4+ years software engineering experience or coding experience in C, C++, Java

2+ years coding experience in PHP, Hack, Python, Java or C++

2+ years experience building logging infrastructure, metrics infrastructure or pipeline management
PREFERRED QUALIFICATIONS

Experience working directly with data engineer or data scientist teams

Experience working with product team in logging instrumentation on both client and server side

Experience working on data modeling from logging to analytics

Experience building metrics computation or consumption framework

Experience extracting and implementing analytics patterns","Seattle, WA",Software Engineer - Data Flow,False
806,"$112,000 - $142,000 a year (Indeed Est.) We’re on a mission to understand and structure the world’s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world’s largest health systems.
We’re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.
As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning

We’re looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You’ve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.

Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python’s NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive.","San Francisco, CA","Software Engineer, Data",False
807,"Position Summary

We are currently seeking a Data Engineer who will assist in the design and implementation of a hospital-based clinical data-warehouse system. Your role will be to support existing software and integrate new data sources with a focus on efficiency and availability. Etiometry has the world’s largest collection of pediatric intensive care unit monitoring data and the volume and data types are continually increasing. The data sets provide numerous storage and normalization challenges but can offer important insights into the efficacy of existing treatments and reveal new and innovative patient management strategies.

Responsibilities

Interface with software stakeholders to understand infrastructure and user requirements.
Develop and support Etiometry’s data-warehouse solution both for clinical personnel and internal research staff.
Build, test and deploy software and database upgrades into a production environment.
Utilize and improve Etiometry’s clinical data cleaning tools and techniques, which include data extraction, de-identification, and clinical measure normalization & cleaning.
Design and implement requirements for company research.
Basic Qualifications

BS in computer science, systems engineering, or a similar technical field with relevant work experience.
An understanding of data model design, database schemas, and optimizing database applications.
A breadth of understanding of database technologies including both relational and non-relational solutions.
Experience manipulating large data sets of time-series and intermittent data.
Experience using version control software.
Desired Qualifications

Experience designing and developing database access layers for schemas that contain multiple databases containing unique data types and access requirements.
Experience working with Python as a primary development language with an emphasis on data management and processing.
Experience with MySQL and MongoDB
Experience producing software for a clinical setting that utilizes clinical patient data, e.g., labs, physiologic signals, and administrative data.
An understanding of clinical (or any data-driven) research from a data aggregation and methodologies standpoint (study design, subject protections, and statistical analysis).
Experience with Agile software development methodologies, and continuous integration and delivery.
Contact Us

If you are interested in this opportunity, please email careers@etiometry.com with your resume.","Boston, MA 02134 (Allston-Brighton area)",Data Engineer,False
808,"Mission Link Data Engineer-268726
Description
We are pioneers. We were the first to break the sound barrier and design the first functional jetpack. We were aboard NASA’s first lunar mission and brought advanced tiltrotor systems to market. Today, we are defining the future of on-demand mobility. At Bell, we are proud to be an iconic company with superb talent, rapidly creating novel and coveted vertical lift experiences.


The Data Engineer is a member of Bell’s team of analytics experts. This position is responsible for cleaning and analyzing aircraft sensor data from many types of aircraft. The ideal candidate is an enthusiastic, clean coder who enjoys designing and programming systems to analyze, store, and display data. The Data Engineer works on a small team of developers and uses aircraft data to provide value to internal and external customers. This position is based at Bell’s headquarters facility in Ft. Worth, TX.
Position Responsibilities:
Find, explore, and implement value from aircraft dataLink the data analysis to customer use-cases and valueHelp design and optimize data presentationExtract, transform, and load data into relational database systemsDesign and maintain database schema and stored proceduresContribute to a clean and maintainable codebaseReview data and code with peers

Qualifications
Education:

Bachelors degree in Engineering or Computer Science

Experience:

At least 5 years of experience in data analysis is required.Experience with object-oriented programming in Python, Java, C++, Scala, etc. Experience in coding with Python 3.X is preferred.Experience with MS SQL Server – administration, design, programmingKnowledge of aircraft systems and aircraft maintenance procedures is preferred
Experience in Linux (Ubuntu, SUSE or others) – command line, shell scriptingExperience supporting and working with cross-functional teams in a dynamic environment.Experience in application of best practices in software development including source control, object-oriented development, and clean programming principles.

Don’t miss the chance to join a diverse, inclusive environment where you feel a sense of belonging. As a member of our global workforce, you will collaborate with dedicated, enthusiastic teams where unique experiences, backgrounds and ideas combined with a strong passion for our products take us above and beyond flight.


EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

This position requires use of information which is subject to the International Traffic in Arms Regulations (ITAR) and/or the Export Administration Regulations (EAR)., Non-U.S. persons selected must meet eligibility requirements for access to export-restricted information. , The ITAR/EAR defines a U.S. person as a U.S. Citizen, U.S. Permanent Resident (i.e. 'Green Card Holder'), Political Asylee, or Refugee.

Pay Transparency Policy Statement
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise, have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.
Job Field: Information Technology
Primary Location: US-Texas-Fort Worth
Recruiting Company: Bell Flight
Schedule: Full-time
Job Level: Individual Contributor
Shift: First Shift
Job Posting: 09/19/2018, 10:10:52 AM
Textron (and its subsidiaries) participates in E-Verify. We will provide the U.S. Social Security Administration (SSA) and","Fort Worth, TX",Mission Link Data Engineer,False
809,"InternshipData Engineer Grad Intern - (00049974)
Description

Do you want to help analyze data and do the analysis needed to contribute to solving our nation’s most critical problems? Do you want to be mentored by engineers and scientists that are experts in their fields? Do you want to join over 300 other interns for a summer full of learning, networking and fun?

MITRE’s people are committed to tackling our nation's toughest challenges. We are different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&D centers we operate for the government create lasting impact in fields as diverse as cybersecurity, healthcare, aviation, defense, and enterprise transformation. We're making a difference every day—working for a safer, healthier, and more secure nation and world.

Our workplace reflects our values. We want you to come as an intern and then join us upon completing your degree so that you can experience the gratifying work, our competitive benefits, exceptional professional development opportunities, and a culture of innovation that embraces diversity, inclusion, flexibility, collaboration, and career growth.
Key Function: What do MITRE Data Analytics / Operations Research / Statistics / Math Interns do?
MITRE’s Data Analytics /Operations Research / Statistics / Math Interns help strengthen our sponsor’s effectiveness by using data analytics, statistical analysis, and modeling and simulations methods to drive analytically-defensible decisions. They apply advanced analytical and mathematical techniques to solve complex decision problems—those with multiple alternatives that require quantitative analysis to confidently select the best one. Analysts use high performance computing, cloud Computing, big data analytics, and data visualization tools and techniques to improve system designs, to assist in making difficult policy and acquisition decisions, to maximize operational effectiveness and efficiency, and to achieve the highest quality engineering solutions.

Qualifications

Required Qualifications:
Completed BS undergraduate degree in computer science or equivalent technical degree and currently pursuing graduate degree in computer science or equivalent technical degree.

Candidate will have also have strong capabilities in the following areas:

Software design, development (Java, C++) and scripting (Perl, Python)Data engineering (managing massive data volume and velocity, information cleansing & refinement, information models & architectures)Deep working knowledge of specific tools and technologies (e.g., DBMS, ETL)Design techniques and tradeoff analysis for scalability, availability, extensibility, redundancy, load balancingCloud Computing and virtualizationService Oriented ArchitectureWeb ServicesInformation AssuranceAttribute Based Access ControlInformation ManagementInformation Exchange/interface specificationAgile Development and Agile Acquisition
Must have good analytical, written, presentation, and interpersonal communication skills and leadership experience. Must possess a strong aptitude to work in a research-focused program.


Primary Location: United States-Virginia-McLean
Work Locations: Washington 22102
Job: SW Eng, Comp Sci & Mathematics
This requisition requires a clearance of: None
Travel: No
Job Posting: Jul 3, 2018, 10:23:04 AM","McLean, VA",Data Engineer Grad Intern,False
810,"ContractHi,One of our esteemed client is open for a job position of Data Engineer (Tableau Admin) a 6+ months contract position in Sunnyvale, CABelow are the details of requirement, if interested please share your resumeThe requirement is for a classic Data Engineer, with primary skills on the infrastructure (Tableau Server Admin, Cloud deployments and Data Visualization. The project management skill is secondary and preferable to have; as the candidate needs to self-manage the whole data life cycle. The resource is not required to create ETL jobs but enables other team members to do it in a self-driven mode as part of self-service BI platform team. So, strong knowledge on Full Data Life Cycle is requiredSenior Data Engineer (Tableau Server Admin)Location: Sunnyvale, CADuration: 6-12 months Tableau Server Administration, Cloud deployments and Data VisualizationETL experience (Informatica, Hadoop/Spark),Scripting/development experience (Python, SQL)Data engineering, User training/consulting, Software/Product architectureVerbal & Written communication, customer focus, teamwork/collaboration, eye for automationThanks and Regards,Vishal SharmaVings Technologies""Making Technology Cheaper and More Efficient""Job Type: Contract","Sunnyvale, CA",Data Engineer (Tableau Admin),False
811,"ContractW2 only. 12mos+. AA/EOE. This is not an entry level position. 2-3 yrs. professional exp needed.Responsibilities:-Our team enables seamless execution so they can make radically better things to bring the best of the client to everyone.-We organize information and data through the management and creation of centralized tools, processes, programs and analysis for the senior leadership team, Business units, and functions, to land excellent products.-In this role, you will enable data driven decision making within the central Chief of Staff team.*Our team handles multiple core processes for the business: Product Development Process and Schedule; Headcount planning and the annual planning process; OKRs -Staples (key metrics for senior leadership team) Within this role, you will have a chance to impact each of these areas by leveraging your analytical skills, business judgement, and excellent communication.-Assist on the creation of the spreadsheet-based headcount planning tool that will.-Unify the approach our teams use for headcount planning-Significantly reduce planning burden on teams-Dramatically improve ability to visualize org needs-Automatically calculate gaps in the org-Specify which products and functions drive those gaps-Enable scenario exploration to understand how roadmap or timing changes impact resource needs-Create dashboards with headcount analysis, that includes but is not limited to: Front-end development to create a web-based central tool for the headcount planning trix above + Peeps entry for allocation-Creating visualizations for headcount plan targets vs. actuals, scenario planning, trends/projections.-Automating headcount allocation data visualization (go/hwpa-peeps)-Implementation of headcount modeling for new products based on complexity.-Lead ad hoc quantitative analysis and research deep dives.Qualifications: -2+ years of working experience in an analytical role, with proficiency in SQL, appscript/javascript experience and Spreadsheet modeling. -Experience with visualization tools (e.g. Tableau). -Effective problem-solving, statistical and business judgment skills, with the ability to translate quantitative analysis into a business recommendations. -Excellent project management, presentation and communication skills.Job Type: ContractExperience:Data Engineering/Analytics: 3 years (Preferred)","Mountain View, CA",Data Engineer III,False
812,"ContractSkill Set: HDFS, Spark, Hive, MapReduce, Scala, JAVA, Linux, Spring, SVN/GIT, HbaseRules based engine LUCY hands on knowledge is a plus.American express experience is a plus.Job Type: Contract","Phoenix, AZ",Big Data Engineer,False
813,"Coursera is scaling a global platform to provide universal access to the world’s best education, and we’re motivated by the passion and mission to transform lives through learning. Our platform has reached over 35 million learners worldwide and we have partnered with 170+ elite universities & industry partners around the globe with over 2,900 courses in our catalog. We offer Courses, Specializations, Certificates, and Degrees to meet the needs and goals of the diverse learners who come to Coursera.

Several years ago, we began hosting accredited online Masters degrees provided by our university partners, which provide a more convenient, lower-cost, “stackable” means of earning credentials comparable to their traditional on-campus counterparts. We also launched Coursera for Business, partnering with over 1,000 companies around the world to provide access to curated skill development for their employees.

At Coursera, our data engineering team is unique, with the goal to democratize data and empower our internal and external users with data to build and enhance next generation of learning experience. We are responsible to model and build our foundational core data lake that feeds our key data solutions. We build analytical products to serve our partners and customers with key insights. We believe the next generation of teaching and learning is personalized, accessible, and efficient - reaching a world of learners who need it -, and that with our scale, data, technology, and talent we are best positioned to make that vision a reality.

We’re looking for a talented and driven senior data engineer with a keen eye for data. Our ideal candidate is an independent, analytically-minded individual with strong data modeling and software engineering skills, who shares our passion for education. In this role, you’ll directly work with cross-functional teams to design, develop, and deploy data solutions.
You personally exhibit a conviction that the world needs Coursera to be wildly successful and alignment to our core values:
Betterment: a tireless pursuit to drive results
Boldness: take risks and act decisively
Deep Honesty: invite and offer candid feedback in order to learn, change, and grow
Solidarity: recognize that we are part of something bigger than ourselves and are committed to our mission
Your responsibilities:
Architect scalable data models and build efficient and reliable ETL pipelines to bring the data into our core data lake
Design, build, and launch visualization and self-serve analytics products that empower our internal and external customers with flexible insights
Be a technical leader for the team; guide technical and architectural designs for the major team initiatives; mentor junior members of the team
Build data expertise, and partner with data scientists and product engineers to define and standardize business rules and maintain high-fidelity data
Define and partner with other engineers in the development of new tools to enable our customers to understand and access data more efficiently
Work cross-functionally (eg: product managers, engineers, business teams) to support new product and feature launches
Your skills:
5+ years experience in a data-related field, including data engineering, data warehousing, business intelligence, data visualization, and/or data science
Strong software engineering skills and at least one scripting language (e.g., Python)
Proficient with relational databases and SQL
Familiarity and experience with big data technologies (eg: Hive, Spark, Presto) preferred
Ability to communicate technical concepts clearly and concisely
Independence and passion for innovation and learning new technologies
If this opportunity interest you, you might like these courses on Coursera -
Data Warehousing for Business Intelligence Specialization","Mountain View, CA",Senior Data Engineer,False
814,"As the Senior Data Engineer, you'll be joining a team of passionate engineers, designers and product managers. Together, we answer business questions through data.

Blue Bottle Coffee is growing, presenting us with exciting opportunities to solve interesting challenges and help create solutions to serve our production teams, our retail teams and HQ teams. You'll create impact that you can see and taste, working on all parts of the data stack.
You will:
Build, maintain, and troubleshoot ETL pipelines for several data sources
Work with business partners to build data specs
Establish best practices around ETL and other data services
Build micro-services to augment or supplant more involved ETLs
Build automated testing, performance evaluations, monitoring tools and dashboards
Design experiments, analyze data, visualize results, and present findings
Work with and build APIs from/for other microservices and outside services
Evaluate, and when necessary, rebuild existing ETLs
You are:
Deeply collaborative; comfortable jumping in and working closely with a group of different stakeholders, excited to share knowledge, and welcome support.
Constantly learning and eager to iterate; hungry to build better software and are constantly finding better ways.
Able to take over someone else’s code and know when refactoring is needed
Able to break-down complex problems into solvable pieces of work
Vigilant about test coverage and code quality
Curious, adaptable and versatile; able to create compromises in situations where there is not one right answer
Not shy about making estimates, and are always trying to get better at it.
A leader, not just Senior in title
You have:
3+ years of production experience with data engineering
3+ years of experience with cloud infrastructure
Ability to model data and write SQL
Familiarity with testing a data application, integration testing, and processes that rely on CI/CD
Familiarity with machine learning
A few benefits we offer:
Medical, dental, and vision coverage for all full-time employees and their dependents starting on their first day of work
401(k) plan
Paid time off and parental leave
Annual conference budget
Free drinks at any of our cafes and a complimentary bag of beans to take home each week
Discounts on any Blue Bottle food items and merchandise
Blue Bottle is an Equal Opportunity Employer. We value an open mind, dedication to work, and a collaborative spirit. We hire based on these qualities, a job’s requirements, our business’s needs, and an applicant’s qualifications. We do not tolerate discrimination or harassment of any kind—in the hiring process or in the workplace.

We comply with the ADA and consider reasonable accommodation measures that may be necessary for eligible applicants/employees to perform essential functions. If you have a disability or special need that requires accommodation, please contact us at careers@bluebottlecoffee.com.

We may refuse to hire relatives of present employees if doing so could result in actual or potential problems in supervision, security, safety, or morale, or if doing so could create conflicts of interest.

We will consider for employment qualified applicants with arrest and conviction records.

We participate in E-Verify. We will provide the federal government with employees’ Form I-9 information to confirm authorization to work in the U.S. We will only use E-Verify once an employee has accepted a job offer and completed the Form I-9.","Oakland, CA 94607 (Acorn-Acorn Industrial area)",Senior Data Engineer,False
815,"Responsibilities:
Work within the structure of an agile / scrum development team
Experience at working within all levels of the software development lifecycle from requirements through development and post production support.
Able to work within a fast pace release cycle using automated (DevOps / CI) technologies
Expected to produce quality code in a timely fashion with high emphasis on testing using BDD and other automated unit testing practices.
Adhere to continuous practices to improve and meet code quality standards through code reviews and compliance with patterns, practices and standards set forth on various projects
Work well with other developers, analysts and managers to understand business and technical requirements in order to develop friendly intuitive solutions that are easy to use while making practical sense of complex data.
Enthusiastic about learning and adapting to new technologies quickly
Required Skills:
Experience in full stack server side web development (SQL, DAL, Middle tier, Routes, Controllers, APIs)
Experience with the Laravel PHP framework
Experience in full stack client side web development
Various JS frameworks, Angular, jQuery, etc,
HTML5, CSS3 and preprocessors, including responsive concepts such as Bootstrap / Flexbox etc..
Exposure in developing mobile based application (as either native, or in responsive web technologies or PWA’s)
Strong experience in OOP in many languages both server side, desktop and browser client is a necessity. (PHP, Python, JavaScript)
Experience with RESTful API’s (Ajax / JSON / CORS)
Experience with using GIT as a code repository / version control system.
Strong core understanding and experience in multi-tiered development patterns such as (MVC, MVVM etc.)
Strong experience in multiple database concepts (OLAP / OLTP, Relational Data, and Data Warehouse concepts). Oracle PL/SQL, and SQL Server T-SQL
Experience in other data persistence solutions (No-SQL Document DB concepts).
Firm understanding of presenting complex data in a visually simplified reporting manner
Experience in working in POSIX based environments (Oracle Linux / RHEL specifically)
Working with Linux shell scripts is a plus
An understanding of web based networking concepts, specifically an advanced understanding of HTTP.

Qualifications

Minimum Requirements:

College degree in a Technical or a related field and 2-4 years professional level experience; or 6+ years professional level related Technical experience; or an equivalent combination of education and professional level related Technical experience required.","Little Rock, AR",Data Engineer II,False
816,"ContractAkraya is looking for a Software Data Engineer for one of our client in Oakland, CA If the job description below is a fit, please apply directly or call Sushil at 408-816-2465. If this position is not quite what you’ re looking for, visit akraya.Com and submit a copy of your resume. Our team will get to work finding you a job that is a better match at one of our many clients.

Primary Skills: Python/Java/Scala, Spark, AWS RedShift, SQL, Airflow
Duration: CTH (or direct FTE)
Contract Type: W2 or C2C

Responsibilities:
Design and build robust data pipelines using scripting in SPARK, Airflow, Python and SQL.
Design data warehouse/data marts in AWS Redshift and other databases as appropriate.
Use Optimization techniques in data load and query processing
Validate and build audit, balance, and control of mission-critical data pipelines
Develop cool viz using Tableau and other open source Viz tools as needed
Identify best data sources among multiple sources to use for data pipelines to improve trust in data
Fix bugs, work collaboratively with team members
What you bring to the team:
Masters or equivalent in CS/Engineering or another comparable discipline
You have at least 6 years of technical experience and strong data warehouse & data modeling skills
Need a person who can write high quality Python code OR Java OR Scala along with Spark
Very strong skills in Python, SQL, SPARK, Redshift, Airflow, AWS
Familiarity with Agile methods (we use agile tools)
Experience with reporting tools like Tableau is a plus.
Team player, agile, highly accountable, curious, willing to learn, implement and teach
Ability to juggle multiple responsibilities and deliver to timelines
Experience in the consumer lending industry required
Bonus Points
Experience with open source tools such as Kafka is a plus
Experience in any JVM based language


Please apply directly with your updated resume or call Sushil at: 408-816-2465

About Akraya
Akraya, Inc. Is an award-winning staffing firm that works with many of the leading, technology-based companies around the world. We have been ranked as one of the “ Best Staffing Firms to Temp for” by Staffing Industry Analysts on multiple occasions and are a preferred staffing vendor within numerous staffing programs. Please visit akraya.Com to search through all.","Oakland, CA",SOFTWARE DATA ENGINEER : 18-03932,False
817,"Job Title:

Data Engineer


Reports to:

Chief Technology Officer


Location:

Jersey City



Company

Verisk Maplecroft’s data-driven solutions enable multinational companies to identify and manage the key political, human rights, economic and environmental risks impacting their operations, supply chains and investments.

As part of the Verisk Analytics family, we are aligned with the world’s leading data analytics organisation. Together, we aim give our clients the insight they need to make better, data-driven decisions through unrivalled analysis and advice.

Team Profile

The Technology team at Maplecroft is responsible for designing, developing, and running the infrastructure and products that drive the company. Maplecroft is a data-driven company, and the Technology team sits at the heart of what we do. We're a small but capable team that values independent thought, critical thinking, and pragmatism.

Role Purpose

This will be the first hire in a new Data Science team reporting into the CTO. We need someone to work with people (and data sources) across the business to consolidate, aggregate, and make sense of various types of data, ranging from website analytics to structured API feeds from vendors to web crawls. A few example projects that the successful candidate would work on are:

1. Customer engagement reports & dashboards to support the Sales & Marketing team, pulling data from Google Analytics and our CMS & entitlement databases to show who is (or isn’t!) accessing what content

2. Usage metrics for the Research teams to better understand how customers are consuming their content (using similar data to 1.)

3. Using web scraping tools to assist Research analysts with their work by aggregating and annotating a variety of news sources

4. Working with Data Scientists in our Analytics team to help them gather and process structured and unstructured data sets in support of their work

5. Working with the Software Engineering and Infrastructure teams to turn pilot projects from around the business into production applications

Knowledge & Experience

Key skills are Python programming and a working knowledge of range of databases. We aren’t looking for a web developer, though someone with that background would be well suited to the role. Specific technologies you will be expected to work with include:
Python
MySQL
PostgreSQL
ElasticSearch
Apache Airflow
Various HTTP APIs

 Minimum 3 years professional experience

Key Competencies

Issue identification, problem solving & analysis
Communication
Planning, implementation and control
Building and maintaining relationships
Collaboration
Continuous improvement


Maplecroft Core Values

Maplecroft is a place where we are committed to supporting our people to grow and thrive. We value different perspectives and aspire to create an inclusive environment which encourages diversity and fosters a sense of belonging.

Maplecroft values the contributions of each individual and helps them reach their full potential while sustaining an organisational culture of health and wellbeing.

Our core values are:
Respect for the Individual
Integrity
Passion
Persistence
Confidence with humility
Excellence
Teamwork

We understand the importance of achieving balance between work, family and other life commitments. We are open to considering flexible working arrangements to enable the greatest spectrum of talent to contribute to our success.

EEO Statement

Unsolicited resumes submitted to Wood Mackenzie by any external recruitment agency via Internet, e-mail, fax, or U.S. mail become the property of Wood Mackenzie and we are not responsible for any fees associated with those resumes.

In compliance with the Civil Rights Act of 1964 and 1991, the Age Discrimination in Employment Act of 1967, Section 504 of the Rehabilitation Act of 1973, the Americans with Disabilities act of 1990 and all other relevant federal and state laws, the policy of this company prohibits discrimination in employment because of race, color, religion, national origin, sex, gender identity and/or expression, age, veteran’s status, disability, genetic information or any other group protected by law. Applicants are considered for all positions without regard to race, color, religion, national origin, sex, gender identity and/or expression, age, veteran’s status, disability, genetic information or any other group protected by law.

If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access WoodMac.com/careers on-line as a result of your disability. You can request reasonable accommodations sending an email to hrenquiries@woodmac.com.

Wood Mackenzie is an Equal Opportunity Employer M/F/V/D, and a member of E-Verify.

http://www.eeoc.gov/","Jersey City, NJ",Python Developer/Data Engineer,False
818,"$130,000 - $170,000 a yearOur client is looking for a savvy Data Engineer well versed in moving many TB of data per hour. You will be working to design and create low latency data processing pipelines.

Required
Scala, Spark, Hadoop
Kafka, Redis, NoSQL, Akka
Java or Python
ETL pipeline development
High throughput data volume
Why work here?
Generous salary and bonus package
Unlimited PTO
401K
Free snacks, breakfasts, coffee
Medical dental vision benefits","New York, NY",Data Engineer (Spark),False
819,"Oath Ad Platforms is our unified ad tech solution for both advertisers and publishers. Our innovative ad tech gives one stop access to Oath's trusted data, high quality inventory and demand, creative ad experiences and industry-leading machine learning, at global scale.


The Yahoo Gemini team is developing next-generation technologies to enrich advertiser and user experience with ever growing and interesting data challenges. We work on all Yahoo user data - building the data pipelines and statistical data models to process billions of events, and making machine learning work.

We are looking for world-class, fun-loving Big Data engineers to join our team where you will have the opportunity to help develop low latency and large scale data systems. You will analyze requirements; investigate optimal software solutions; architect, design, implement and test those solutions.

Minimum Job Qualifications:

Hands-on experience in developing scalable data solutions
Proficiency in Hadoop, Spark, Hive, HBase, and Oozie technologies
Hands-on experience in real-time query engines (Druid, Presto) preferred
Solid understanding of data structures and algorithms
Strong in Java and PIG
Good skills and experience in Linux, XML, JSON, REST
Experience with fault-tolerant system design and high-performance engineering
Experience with machine learning algorithms and/or statistical methods is preferred
Able focus and deliver results in a fast paced and entrepreneurial environment
Strong analytical and problem solving skills
BS/MS degree in Computer Science or industry relevant field



Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","Sunnyvale, CA",Big Data Engineer,False
820,"Contribute to the definition and refinement of processes and procedures for the data engineering practice. Work closely with data scientists, data architects, and other IT and business counterparts to identify, capture, collect, and format data from the external sources, internal systems, and the data warehouse to extract and profile features of interest. Contribute to the evaluation, research, experimentation efforts with batch and streaming data engineering technologies in the context of data projects assigned. Work with data engineering related groups to inform and showcase capabilities new techniques within data engineering. Assist the data architects in implementing the data architecture roadmap and related data projects.
Basic Qualifications:
Problem Solving Aptitude
Data Mining and Profiling proficiency in various techniques
SQL knowledge
Basic knowledge of data storage technologies (relational, Big Data, NoSQL)
Familiarity with ETL and Data Ingestion techniques
Tableau or similar Reporting/Analytics/Dashboarding UI tool experience
Desired Skills:
Strong Communication and Facilitation Skills
Strong Interpersonal Skills
Presentation Skills
Learning Agility in picking up business processes specific to Aerospace and Defense
Knowledge of data modeling
Knowledge of DevOps and Agile Methodologies
Ability to work as part of a team to solve business process and data problems
BASIC QUALIFICATIONS:
Problem Solving Aptitude
Data Mining and Profiling proficiency in various techniques
SQL knowledge
Basic knowledge of data storage technologies (relational, Big Data, NoSQL)
Familiarity with ETL and Data Ingestion techniques
Tableau or similar Reporting/Analytics/Dashboarding UI tool experience
Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.

Join us at Lockheed Martin, where we’re engineering a better tomorrow.","Fort Worth, TX 76108",Data Engineer,False
821,"ContractData Engineer – Long term contract or contract to hireWe need a Data Engineer to help our client on a contract, and possibly contract to hire role. In this role, you will be working with a small team of data engineers to ultimately impact how our client makes crucial decisions. This opportunity will suit candidates who enjoy working in a small team and enjoy seeing the impact of their work.This is a contract role and can either be a long-term contract position or a contract to hire role.Key Technical Requirements for the Data EngineerAt least 5 years of experience with Netezza or similar MPP tool.Some Data Modeling experienceStrong SQL Skills ie writing queries, joins etcPreferred Technical Skills for the Data EngineerExperience building custom ETL’sAny experience with new cloud/big data technologies like Hadoop, EMR etc.If you're interested in this role, please email Dave Wilson at david @ primeteampartners.comPrime Team Partners is an equal opportunity employer. Prime Team Partners does not discriminate on the basis of race, color, religion, national origin, pregnancy status, gender, age, marital status, disability, medical condition, sexual orientation, or any other characteristics protected by applicable state or federal civil rights laws.Job Type: ContractExperience:ETL: 2 years (Required)Education:Bachelor's (Required)Work authorization:United States (Required)","Seattle, WA 98119 (Queen Anne area)",Data Engineer - ETL/Data Modeling,False
822,"ContractTitle: Data EngineerLocation: San Jose, CA- Locals only- in person interview must, no exceptionsRequired Experience/Skills:- 4-6 years of hands on experience)- Integration Engineer- Kafka, Storm, Java/python,- DB Utilties,Scripting ,Batch/Streaming- ETL experiences- Direct experience in building high volume data pipeline- Working with highly available systems- Hands-on & Agile- infrastructure, h/w fundamentals for installations/resolving OS/product conflicts)Job Type: ContractLocation:San Jose, CA (Required)Work authorization:United States (Required)","San Jose, CA",Data Engineer,False
823,"Job Summary*Title: Big Data Engineer**Location: Plano, TX - 75024W2 only*Responsibilities and Duties8 + years of professional experience3+ years of experience with Big data technology and analytics3+ years of experience in ETL and ELT data modelingExperience working with traditional warehouse and correlation into hive warehouse on bigdata technologiesExperience setting data modeling standards in HiveProficiency in using query languages such as SQL, HiveExperience with streaming stacks like Nifi, Spark and PySparkExperience working with ELK stack or Solar indexingUnderstanding of data governance, bigdata security like kerberos, rangerKnowledge of setting standards around data dictionary and tagging data assets within datalake for business consumption.Understanding of Big Data tools (e.g., NoSQL DB, Hadoop, Hbase) and API development consumption.Understanding of data preparation and manipulation using Datameer toolKnowledge of SOA, IaaS, and Cloud Computing technologies, particularly in the AWS environmentExperience with Hadoop/Hive, Spark and Scoop highly desirableExperience in one or more languages (e.g., Python or Java, Groovy)Experience with data visualization tools like Tableau, Power BIJob Type: Full-timeExperience:data modeling: 3 years (Required)Big data technology: 3 years (Required)","Plano, TX",Big Data Engineer,False
824,"---------------

About the Role:
---------------

If you're passionate about building large scale data processing systems, and you are motivated to make an impact in creating a robust and scalable data platform used by every team, come join us. You will jump into an early stage team that builds the data transport, collection and orchestration layers. You will help shape the vision and architecture of WeWork's next generation data infrastructure, making it easy for developers to build data-driven products and features. You are responsible for developing a reliable infrastructure that scales with the company's incredible growth. Your efforts will allow accessibility to business and user behavior insights, using huge amounts of WeWork data to fuel several teams such as Analytics, Data Science, Sales, Revenue, Product, Growth and many others as well as empowering them to depend on each other reliably. You will be a part of an experienced engineering team and work with passionate leaders on challenging distributed systems problems.

---------------

About the Team:
---------------

Data is at the core of our business, providing insights into the effectiveness of our products and enabling the technology that powers them. We build and operate the platform used by the rest of the company for streaming and batch computation and to train ML models. We're building an ecosystem where consumers and producers of data can depend on each other safely. We thrive to build high quality systems we can be proud to open source and an amazing experience for our users and ourselves. We regard culture and trust highly and are looking forward to welcoming your contribution to the team.

----------------
Responsibilities
----------------


To architect and Design large scale data infrastructure in production (performance, reliability, monitoring)
You'll design, implement and debug distributed systems
Thinking through long-term impacts of key design decisions and handling failure scenarios
Building self-service platforms to power WeWork's Technology
Focused on team over individual achievements.
Building software incrementally and make consistent progress.
You love to learn. mentor and teach others.
You're empathetic, you build long-lasting relationship characteristic of highly efficient teams.
You keep up-to-date with the latest developments in the field.

------------
Requirements
------------


5+ years programming experience with: Java, Scala, Haskell, JavaScript
2+ years experience in stream processing with Flink, Spark, Storm, or Beam
Experience with one or more of the following technologies:
Distributed logging systems Kafka, Pulsar, Kinesis, etc
Batch processing: Spark, Hadoop, …
IDL: Avro, Protobuf or Thrift
MPP databases Redshift, Vertica, …
Query execution Columnar storage, push downs: Hive, Presto, Parquet, ...
Workflow management Airflow, Oozie, Azkaban, ...
Cloud storage: S3, GCS, ...
Understanding of distributed systems concepts and principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)
Eager to learn new things and passionate about technology
Experience with contributing to open source software a plus
Experience with the following Cassandra, DynamoDB, RocksDB/LevelDB, Graphite, StatsD, CollectD a plus

------------
About WeWork
------------

WeWork Technology is bridging the gap between physical and digital platforms, providing a delightful, flawless & powerful experience for members and employees. We build software and hardware that enables our members to connect with each other and the space around them like never before.

We augment our community and culture teams through the tools we build. We believe there's a macro shift toward a new way of working—one focused on a movement towards meaning and purpose. WeWork Technology is proud to be shaping this movement.

We are a team of passionate, fearless and collaborative problem-solvers distributed globally with one goal in mind - to humanize technology across the world.

We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","San Francisco, CA",Staff Data Engineer - Streaming / Metadata,False
825,"Trinisys is looking for a Healthcare Data Engineer to join our fast-growing team. This position will extract, transform, and load healthcare data into Trinisys ClearView® and EPIC, Cerner, Athena, Allscripts and other EHR / EMR systems. The successful candidate will possess solid SQL skills in a variety of dialects (MS, MySQL, AWS Aurora, and Oracle). Experience working with healthcare data is required, and in-depth knowledge of at least one EMR systems will be a factor in candidate selection.An Inc. 500 award winning company, Trinisys revolutionizes the way businesses collect and process information. Clients have drastically reduced the time and cost involved in acquiring data from paper and the web with Trinisys’ innovative data capture software. The Trinisys Integration Engine enables companies to automate complex business processes and get data to the systems they use every day.Here’s your opportunity to be a part of a growing company that values individuals and their contributions. The Trinisys team is a tightly‐knit, collaborative group of creative, enthusiastic and thoughtful people. Trinisys offers an impressive benefits package, great flexibility and an amazing working environment.What you’ll be doing:Extracting, transforming, and loading healthcare data to support data archiving and migrations between EHR/EMR SystemsAnalyzing client requirements, developing extraction and transformation plans, and implementing those plansUsing your judgment to recommend alternatives when appropriateWorking as part of an agile team, and contributing to the team on a proactive basisWhat’s in it for you:Being part of an expanding company recognized nationally for its growth by Inc. 500 and Best Place to Work by the Nashville Business Journal. Sharpening your skills as a member of one of the most talented organizations in Nashville. Working in a laid-back, collaborative and fun environment. Unmatched benefits including health/dental/vision, disability and life insurance plus a 401k with a company contribution and much more!What you need to qualify:Bachelor’s in Computer Science, Computer Engineering, Information Systems or related fieldMinimum of three (3) years of programming and/or systems analysis experience are preferredBackground in technologies such as client/server, relational database management systems, object-oriented and distributed object development a plusStrong technical skills around data access and transformation. Must have the ability to write complex SQL queries firsthandMinimum two years programming experience in JSP, JavaScript and SQLExperience with Hibernate and Spring or similar frameworksExperience reverse engineering systemsSuperior ability to analyze and manipulate clinical dataAbility to write technical specs and create diagrams such as data flow diagramsExperience with Financial/Revenue Cycle data is a plusSoft Skills:Must have above good communication skillsMust feel comfortable communicating to groups and defending technical decisionsTenacious problem solver with a consuming curiosity, and facility with intellectual abstractionsPhysical DemandsThe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.The employee is frequently required to reach with hands and arms. The employee is occasionally required to stand; walk and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision and ability to adjust focus.Work EnvironmentThe work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The noise level in the work environment is a quiet, professional office.Trinisys is an Equal Opportunity EmployerJob Type: Full-time","Brentwood, TN 37027",Healthcare Data Engineer,False
826,"Company Name: Kroger General Office
Position Type: Employee
FLSA Status: Exempt
Line of Business: Research & Development
See what life is like at Kroger Technology
at https://www.kroger.com/livekt

Additional Technology Information:
The Operations Research team and Data Science team is a world-class data science team. This team is dedicated to solving Kroger's biggest problems. The team combines data science and data engineering skills to provide ""full-stack"" data science support to multiple, high-profile initiatives. We collaborate with multiple business partners including R&D, Operations, Merchandising, Finance, and HR to deliver state-of-the-art data science solutions. The team is looking for Data Engineers that are focused on data-related tasks such as sourcing data, ensuring data is clean and high quality, administering ETL workflows, managing databases, system administration and providing operational support to the team. If interested, the candidate could also have the opportunity to learn data science skills such as modeling, simulation, forecasting, and machine learning and may transition into more of a data science role over time.

Position Summary:
The Operations ReSearch Analyst 1 will crEate detailed analysis, operational recommendations and decision Support Tools through intense mathematical analysis, collaboration with Subject Matter Experts, model design and Development in order to maximize the benefit to The Kroger Co. This position will provide assistance to Operation ReSearch Analysts and Senior Analysts on large Projects. Collaborate with cross-Functional teams to deliver sustainable, continuous Improvement.
Essential Job Functions:
Assist in the initial definition of the problem statement and Data requirements.
Understand and translate operational complexities.
Design and Develop models to visualize the problem.
Assist in the crEation of measurement Systems to track Project success and goal attainment.
Perform extensive analysis of the problem and outline possible Solutions.
Test possible Solutions utilizing Operations ReSearch Tools and techniques. These techniques and Tools include, but not limited to, mathematical Optimization, quantitative decision Support, simulation, linear Programming, dynamic Programming, regression analysis, cluster analysis, queuing theory, statistics, forecasting, Data mining, and Project management.
Draw conclusions from previous tests in order to crEate New hypotheses.
Work independently and Accountable for multiple concurrent Projects while delivering accurate and appropriate findings.
Publish findings and recommendations to Project team, management and Operations ReSearch peers.
Collaboratively Develop Plans to implement suggested Improvements and confirm the Plans are realized.
CrEate sustainable control Plans for Improvement efforts.
Participate in the approval of Project closures and formal transition to the Process owners.
Complete other assigned work as necessary.
Key contributor in the Process Improvement Engineering activities within a cross-Functional design team. Collaborate in Developing sustainable Processes and Solutions that drive manufacturing, supply chain and in-Store efficiencies, eliminate non-value-added activities and enhance our Internal and External Customer experience.
Consult with Subject-Matter Experts to Develop potential Business and Technology Solutions; advise on impact to Business Processes.
Establish and Maintain Relationships throughout the Enterprise with Division Associates, manufacturing, vendors, logistics, and Store-level personnel to understand, document and define Business strategies and Processes (current and future state).
Must be able to perform the essential functions of this position with or without reasonable accommodation.
Minimum Position Qualifications:
Bachelor's Degree, BA/BS degree in Engineering/Sciences/Quantitative discipline or equivalent.
Minimum of 2 years experience with Data collection and analysis.
Strong analytical and problem solving skills, balanced by the ability to apply common sense.
Strong mathematics and statistical background.
Effective organizational skills and solid team background Maintained through effective communication skills.
The ability to travel independently as required.
Desired Previous Job Experience:
Experience analyzing complex sets of Data and Developing recommendations and/or publishing findings.
Experience in computer Programming, Data visualization, and/or mathematical analysis.
Experience in Retail, supply chain, and/or manufacturing a plus.
Education Level: None
Required Certifications/Licenses: None
Position Type: Full-Time
Shift(s): [[mfield4]]
States: Alabama; Alaska; American Samoa; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; District of Columbia; Federated States of Micronesia; Florida; Georgia; Guam; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Marshall Islands; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Northern Mariana Islands; Ohio; Oklahoma; Oregon; Palau; Pennsylvania; Puerto Rico; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virgin Islands; Virginia; Washington; West Virginia; Wisconsin; Wyoming
Keywords:

Jobs at Kroger: At Kroger, we hire people who have a passion for helping others and who want to build a relationship with our Customers. No matter what stage of your career, you can build your future at Kroger. We look for people who want more, aspire to be more and work hard to achieve their goals. Our focus on keeping the Customer first is what makes us successful. As the largest traditional grocery chain in the U.S. and one of the world's largest retailers, we employee nearly half a million Associates across 35 states. We offer many opportunities not only in our stores, but in Manufacturing, Logistics, Marketing, Finance, Human Resources, and many other fields.

Company Overview
Kroger Family of Companies employs nearly half a million associates who serve customers in 2,782 retail food stores under a variety of local banner names in 35 states. Our Family of Companies also operates 2,268 pharmacies, 274 fine jewelry stores, 1,489 supermarket fuel centers and 38 food production plants in the United States. Kroger is dedicated to our Purpose: to Feed the Human Spirit™ by serving America through food inspiration and uplift and creating #ZeroHungerZeroWaste communities by 2025. Careers with The Kroger Co. and our family of companies offer competitive wages, flexible schedules, benefits and room for advancement.","Blue Ash, OH 45242",Data Engineer/Operations Research Analyst,False
827,"Job Description
The IBM’s Watson Health business unit is now looking for talented individuals ready to usher in the next era of healthcare. We live in a moment of remarkable change and opportunity. The convergence of data and technology is transforming healthcare and life sciences organizations in every way.


Position: Data Engineer
Location: Cambridge, MA (Boston) or Raleigh, NC

Job Description:
Are you passionate about DATA and want to work with Data Scientist to solve real-world problems? The AI Data Curation team, develops and operates a data platform to ensure the utmost quality of our datasets that are used for training the next generation of AI solutions to enhance clinical decision making. We’re looking for a senior data engineer to help design, develop, and manage elastic and highly-available data platform for the large volume of medical datasets.

Essential Responsibilities:
Design and develop scalable data infrastructure for a large volume of medical data, structured and unstructured from various sources in batch mode and near real-time.
Develop data ingestion, integration, transformation pipelines and manage data warehouses.
Work with Data Scientist to develop tools/automation and build analytics capabilities to expedite the use of data for AI training purposes.
Develop tools to analyze large data sets and perform data verification, data integrity and quality check.
Responsibilities will vary from developing and maintaining new data sets, data warehouse development and generating reports on data usage, access controls and performing data integrity checks.
All team members are expected to contribute broadly, with an agile and growth mindset.

Required Professional and Technical Expertise:
Undergraduate degree in Computer Science or related field of study for software development.
8+ years of experience in building data platform, data engineering, or software engineering.
Software engineer skills in one more language (Java, Python), data manipulation (SQL).
Experience in designing and building efficient and scalable solutions for big data.
3+ experience working with agile methodologies and cloud technologies.
Experience working with medical data and knowledge of any one or more of the following: HL7, FHIR, DICOM, PACs, VNA, EMR, Epic, Cerner, data anonymization
Strong communication, negotiation and consensus building skills when dealing with stakeholders and team members.

Preferred Professional and Technical Expertise:
Have developed statistical models, machine learning algorithms

Required Technical and Professional Expertise

Undergraduate degree in Computer Science or related field of study for software development.
8+ years of experience in building data platform, data engineering, or software engineering.
Software engineer skills in one more language (Java, Python), data manipulation (SQL).
Experience in designing and building efficient and scalable solutions for big data.
3+ experience working with agile methodologies and cloud technologies.
Experience working with medical data and knowledge of any one or more of the following: HL7, FHIR, DICOM, PACs, VNA, EMR, Epic, Cerner, data anonymization
Strong communication, negotiation and consensus building skills when dealing with stakeholders and team members.


Preferred Tech and Prof Experience

Have developed statistical models, machine learning algorithms

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.","Cambridge, MA 02139 (Area IV area)",Watson Health - Data Engineer,False
828,"$80,000 a yearSales Engineer / Customer Success Engineer / Data Engineer
Leading edge data analytics SaaS provider is hiring for a Sales Engineer / Data Engineer to work in their Customer Success department. If you enjoy working in a close-knit collaborative environment, where everyone cares about each other and in turn, the success of the company - then apply now!

Skills:

Strong customer focus, with a solution-oriented attitude and mindset
Excellent SQL skills (able to write SQL code)
Background working with complex data sets, and familiar with various reporting tools and products
Familiar with various public cloud systems (Amazon AWS, Microsoft Azure, Google Cloud)
Prior experience working for a startup-like environment

Compensation / Perks:

Salary ranging up to $80K, depending on experience
Medical / Dental / Vision
Snacks, unlimited coffee, tea, and other beverages
Catered breakfast and happy hour team outings
Casual and collaborative work environment

","Cambridge, MA",Customer Success/Data Engineer,False
829,"About MapR Technologies
MapR Technologies, a provider of the industry’s leading data platform for AI and Analytics, enables enterprises to inject analytics into their business processes to increase revenue, reduce costs, and mitigate risks. MapR addresses the data complexities of high-scale and mission-critical distributed processing from the cloud to the edge, IoT analytics, and container persistence. Global 2000 enterprises trust the MapR Data Platform to help them solve their most complex AI and analytics challenges. Amazon, Cisco, Google, Microsoft, SAP and other leading businesses are all part of the MapR ecosystem. For more information, visit www.mapr.com.

Data Engineer- Operations
Data Engineers will report into MapR’s Professional Services Organization. MapR Data Engineers are responsible for delivering a variety of engineering services to MapR customers throughout North America. Assignments will vary based on the candidate’s skills and experience. Typical assignments may involve cluster sizing, installation, configuration, health checks, performance tuning, data migration, security, and automation. MapR’s growing customer base includes most of the Fortune 50 companies which makes the work assignments technically challenging yet rewarding. This role provides a significant opportunity to learn and apply big data technologies and solve related complex problems. MapR Data Engineers report to the Director of Data Engineering located at company headquarters in San Jose, CA.
Responsibilities:
Master the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.
Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security, and automation.
Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.
Achieve proficiency with MapR Streams sizing, performance tuning and multi-master replication.
Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR Service products and Service offerings.
Work closely with MapR sales in scoping and estimating customer professional service projects.
Write, deliver and present formal SOW’s (Statement of Work).
Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.
Be a technical voice to MapR’s customers and technical community via blogs, Hadoop User Groups (HUG’s) and through participation at leading industry conferences.
Stay current in best practices, tools, and applications used in the delivery of professional service engagements.
Requirements:
5+ years experience administering any flavor of Linux
Strong scripting skills (Bash or Python preferred)
Familiarity with commercial IT infrastructures including storage, networking, security, virtualization and systems management
DevOps background and familiarity with either Ansible, Puppet or Chef
Proficiency in basic Java or Scala programming preferred but not required
2+ years in a customer facing, professional services software delivery role
Bachelor's degree in CS or equivalent experience
Familiarity with Hadoop and the Hadoop Eco-System framework a significant advantage (MapR is willing to train otherwise promising candidates)
Strong verbal and written communication skills are required
Ability to professionally manage multiple priorities with minimal supervision and deliver on schedule
Willingness to travel about 70%
The ideal candidate will have any/all of the following:
RHCE certification, Bash or Python scripting, Automation using Ansible, Puppet or Chef, basic knowledge of Hadoop, Hive, or Spark","Santa Clara, CA",Data Engineer - Operations,False
830,"SharpSpring is seeking a talented Data Engineer to join our engineering team in Gainesville, FL. Our team is a group of dedicated individuals working to provide the best service possible to our customers using the most innovative solutions. SharpSpring provides excellent benefits and an engaging workplace with talented, friendly coworkers.
The Data Engineer will be responsible for the code and processes required to extract, transform, and load data into a data warehouse or data store and should possess knowledge of schema design, concurrency, API design, MapReduce, and aggregation. This role represents an opportunity to directly shape and impact a newly created team within our business and bring fresh ideas to the table regarding our long-term data strategy.
As a key member of our data team, you’ll work across departments to assist with the provisioning, analysis, and interpretation of business intelligence data as it relates to the adoption of our flagship SaaS platform while also working alongside our development team to provide centralized access to data we use for real-time reporting inside of our application.
You’ll also be responsible for evaluating the available ecosystem of Big Data tools and will advise our senior technical staff members regarding what tools best fit the needs of our organization. Upon completing our initial assessments of these tools, you’ll assist with the implementation and deployment of the solutions we collectively decide upon.

Responsibilities
Extract data from multiple data sources, such as SQL, MongoDB, Google Analytics, and other platform APIs, and load them into a centralized data warehouse to facilitate unified reporting.
Assist with the creation of dataflow pipelines–or a comparable technology–to regularly aggregate and summarize data sets for consumption by our application and other business intelligence tools.
Use scientific methods to work alongside other departments such as finance, customer success, and marketing to understand trends and key performance indicators affecting the health of our business.
Assist our product and development teams with data needs as they relate to our product development.
Create dashboards inside of Sisense and disseminate reports across the organization.
Maintain data feeds for dynamic spreadsheets and other ad hoc reporting tools.
Provide consultation regarding big data toolsets and storage solutions.
Administer and maintain our data infrastructure.
The Person
Degree in Computer Sciences, Mathematics, Statistics, or a similar discipline.
5+ years of professional, industry experience as a software engineer.
Experience with the design and operation of large distributed systems.
Expert in a programming language such as Python or Golang, and their respective standard data processing libraries.
Strong working knowledge of relational databases and SQL.
Extensive experience with at least one queueing system, such as ActiveMQ, SQS, etc.
Rigor in high code quality, automated testing, and other engineering best practices.
High level of comfortability with command line tools and data pipeline processing from a terminal.
Knowledge of build systems and version control systems such as Git.
Strong background in statistical modeling methods.
Basic business acumen, customer empathy, and a team player attitude.
Excellent spoken and written communication skills.
Enjoys a fast-paced work environment and the challenges it brings.
Self-starter with the ability to work independently, take initiative, and learn new skills.","Gainesville, FL",Data Engineer,False
831,"InsideSales.com offers the sales industry’s first comprehensive sales acceleration platform that creates high performance sales teams with breakthrough technology, and increases the revenue of their customers’ world-wide by applying our machine learning and innovative technology. Our product is pure technology, supported by a predictive machine learning engine.
InsideSales.com is growing incredibly fast and is funded by some of the top venture capital firms, as well as by Salesforce and Microsoft. We have high expectations and have attracted great leaders who have enjoyed a similar ride elsewhere, and are excited to be here.
Joining a fast growing company is a great way to accelerate your career. We’re transforming how SALES is done, applying technology to revolutionize and modernize. Want to bet on your career and learn a lot? Come join us.
Engineering for scale:
InsideSales.com builds for the cloud, deploying as a software as a service (SAAS) platform. We have a portfolio of seven products and a large amount of data helping to provide automation, insights and intelligence to sales organizations around the world. You’ll learn about building a large scale platform of distributed micro-services, using cutting edge technologies. You will learn how to build and scale data pipelines and data structures required for InsideSales.com to continue to innovate and grow.
We need you if you:
 Love a challenge – the best technology problems are those tackling really hard business and scaling problems
 Delight in building scalable, fast, robust and elegant software in modular components
 Are proud in producing high quality solutions and iterate until they are truly awesome
 Learn constantly about new technology, challenges and lessons from your mistakes
 Cooperate in a team environment where you can learn from your peers and they can learn from you
Many job descriptions read like a shopping list for the impossible. At the core, we’re looking for great software engineers. Technologies change, but great minds transition. That said, the following set of qualifications are relevant and useful to the problems we’re solving:
Position Responsibilities:
 Work closely with product owners, senior engineers, designers, programmers, and QA to deliver industry-leading solutions
 Responsible for constructing solutions based on customer requirements
 Contribute to the implementation of major features and components from requirements and designs
 Contribute to the creation of functional and technical specifications
 Responsible for contributions in technical design, task estimation, implementation, automated testing, debugging, and deployment
 Participate as a productive member of an agile team of engineers
 Produce high quality, test-driven software
 Assist in planning, costing, designing and testing
 Communicate well with product managers, customer support, and other team members
 Promote established standards, processes, procedures, and tools throughout the software development life cycle
 Work with large sets of data, building systems to facilitate flow, management, and consumption of the data.
 Requirements:
 Bachelor’s or Master’s Degree in Computer Science or Engineering or equivalent and/or the ability to show us that you are a great engineer
 1+ year of software development, ideally with experience with data structures and pipelines
 A demonstrable track record of building great software
 OOP, web, and services experience. All the skills to build great distributed and complex systems
 Database experience – both SQL and NoSQL (MongoDB, Cassandra, etc)
 Experience with queuing technologies including RabbitMQ or Kafka
 Core knowledge of cloud services, like AWS or Azure.
 Experience in building services and applications across some combination of Node.js, Go, Scala, Python, C#, Java.
Nice to haves:
 History of working in small Agile teams, iterating to build great solutions
 Ability to work in a Linux environment
 Web development skills in Javascript, preferably with React or AngularJS.
InsideSales.com offers competitive compensation, generous benefits including a matching 401(k) program, healthcare insurance and reimbursement accounts, as well as a gym membership and ongoing training & education programs.","Provo, UT 84606",Data Engineer,False
832,"Senior Data Engineer

WW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encompass healthy eating, physical activity and positive mindset. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all. To learn more about WW and jobs with a purpose, visit http://www.ww.com/us/corporate-careers

Collecting data from many unique sources we are strongly positioned to derive deep knowledge on our member’s behaviors. Using this data we can provide improved & personalized experiences for users as well as providing insights to the executive team that will drive the evolution of our company.

To help us achieve our goals, we are seeking Data Engineers to join our team.
Description
Uniquely positioned to lead the way in the exciting HealthTech industry, WW is rebuilding most of our core experiences and embracing modern engineering practices and techniques.
Collecting data from many unique sources we are strongly positioned to derive deep knowledge on our member’s behaviors. Using this data we can provide improved & personalized experiences for users as well as providing insights to the executive team that will drive the evolution of our company.
As we rebuild many of our core experiences, we are seeking talented people who are excited to join our team. This is a rare opportunity to join a company embracing a modern Technology culture, where you will have the ability to improve people’s lives in a very meaningful way and have a major impact on WW offerings to our members. As reliance on health and wellness awareness increases, you can be part of the team that is leading the way.

To help us achieve our goals, we are seeking Data Engineers to join our team.

Some of the opportunities this role has to offer include:
Work closely with our data scientists to help build complex algorithms that provide unique insights into our data.
Build data pipelines that clean, transform and aggregate data from many different sources.
Develop models that can be used to make predictions.
Build complex functions that answer questions for the business.
Model data at rest and enable powerful data analysis.
Enable machine learning, natural language processing and other data science methods within WW.
Work with engineers across the organization to identify data quality issues in source systems and help keep the data clean.
Provide solutions that help share data with the enterprise.
Be an advocate for best practices and continued learning.
You should have:
Experience with a Cloud Data Platform such as GCP or AWS
If GCP: Big Query, Cloud Dataflow, Cloud Dataproc, Pub/Sub
If AWS: Redshift, EMR, Datapipeline, RDS, Lambda ,and Kinesis.
Experience programming in Scala, Python, or Java.
Experience building data pipelines using Apache Beam or Apache Spark.
Experience with some of the following data stores: Big Query, Redshift, Postgres, Cassandra, or Mongo.
It would be great if you also have:
Experience with real time streaming solutions such as Kinesis, Pub/Sub, or Kafka.
Large scale Data Modeling from a Modern Big Data perspective.
Experience producing and consuming event driven data.
Experience working on a data team building functions, models and complex algorithms.
We hire only the best people. Here are the benefits to being top-notch:
The opportunity to work with some of the best innovators in the industry
Generous healthcare coverage
401(K) with company match
Paid Time Off
Paid parental leave
Tuition reimbursement
Wellness allowance
Profit Sharing
WW is an equal opportunity employer. WW does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.

Any offer of employment is contingent upon the satisfactory results of reference and background checks.","New York, NY",Senior Data Engineer,False
833,"In order to apply for a position at Lumeris, you must create an account using your email address and a password of your choosing. This account will allow you to receive notifications each step of the way through the job application process. With these updates, you’ll never have to wonder where you are in the process. Additionally, we can easily send pertinent documents to you for your review. Once you create the account, you may apply to any position you feel is a good fit without having to re-enter information. Thank you for your interest in Lumeris.


Position:
Data Engineer


Position Summary:
This is an exciting opportunity to join a first-class technology team who is developing accountable delivery solutions for the healthcare industry. The Datab Engineer will be responsible for developing analytical data solutions focused on healthcare cost and quality analytics. The candidate will participate in all phases of the development lifecycle from initial requirements gathering and design through to coding and testing of our data solutions.


Job Description:
Work directly with data scientists, clinicians and other stakeholders to understand requirements and incorporate feedback through collaborative iterations of analysis, development, and testing
Develop database solutions for all phases of data transformation, including, but not limited to, data ingestion, data cleansing, data validation, CRUD operations, data enrichments, ETL, etc. on the Microsoft SQL Server stack
Translate business requirements into functional and technical specifications, develop, implement and test appropriate solutions
Adhere to best practices and standards
Provide documentation where needed
Actively participate in agile sprint ceremonies (standup, retrospectives, sprint planning, etc.)


Experience, Qualifications and Education:

Strong analytical skills
3-5 years of working experience coding SQL
Motivated learner willing to learn new technologies, including open source and cloud
Self-starter, ability to identify actionable steps towards completing objectives
Solid communication and interpersonal skills
Bachelor's Degree in Computer Science or equivalent experience
Process oriented, ownership for projects and deadlines


Experience in following areas is a strong plus:
Experience working with healthcare administrative (claims) or EMR/EHR data
Experience with the Microsoft SQL stack, such as SQL Server and SSIS
Experience with source control tools and branching/merging strategies
Experience with automated code deployment tools (Jenkins, Bamboo)
Experience working with 1TB plus datasets

Lumeris is an EEO/AA employer M/F/V/D.


Location:
St. Louis, MO


Time Type:
Full time


Status:
2 - CO","St. Louis, MO",Data Engineer,False
834,"Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.

Roles & Responsibilities:
The Data Engineer II will focus on data collection, movement, storage, transformation processing, and storage of Big Data. The incumbent will work with both current ETL/Data Warehousing and future Big Data/Streaming/Pipeline architectures. The focus will be on choosing optimal solutions to use for these purposes, then implementing, maintaining, and monitoring them. Always keeping in mind the overarching goal of accelerating translational research and improving clinical care.

Duties and Responsibilities:
Facilitate data collection from a variety of different sources, getting it in the right formats, assuring that it adheres to data quality standards, and assuring that downstream users can get that data quickly and with a common standard interface
Ensure that data streams/pipelines are scalable, repeatable, and secure, and can serve multiple users within the Institute
Develop as a core member of an Agile team, using Agile tools and methodology. Work closely with other team members including Application Developers, Database Developers, and Data Scientists
Responsible for creating the infrastructure that provides insight from raw data and handles diverse sources of data seamlessly
Enable big data and batch/real-time analytical solutions that leverage emerging technologies
Additional responsibilities include developing prototypes and proof of concepts for the selected solutions, and implementing complex big data projects with a focus on collecting, parsing, and managing large sets of data using multiple platforms to allow for Research and Data Science initiatives
Translate business requirements into modern data pipeline solutions. Create centralized documents and diagrams of all solutions
Creates a data catalog store of all metadata
Designs and implements monitoring, backup, and disaster recovery of data systems
Approaches all relationships with a world-class customer service approach. Maintains a customer-focused approach with users to provide solutions that are science/research-driven
Responsible for the integrity and security of data in all forms of storage throughout the Data Architecture
Work with other IT professionals through Mount Sinai effectively. Comply with the Institutional Review Board and HIPAA to follow all applicable policies and procedures
Assists in the development of standards and procedures affecting data management, design and maintenance. Documents all standards and procedures
Provides presentations and training to other team members in the above
Extremely flexible attitude. Willing to work with multiple types of technologies and languages with an open mind and without technology bias. Continuous interest in updating skill sets and knowledge of trends in the Big Data Technology space


Requirements:
Bachelor degree in Computer Science or a related discipline; Advanced degree preferred
4+ years relevant professional development experience, preferably in a Linux environment
Proficiency with Python development. Flexible to learn another language as needed - Scala, Java, and/or C# knowledge is a strong plus
Experience with SQL and NoSQL databases such as Oracle, MS SQL Server, PostgreSQL/MYSQL, and Mongo DB (or similar such as CosmosDB or DynamoDB)
Experience in RESTful service development (preferably with Node JS, Django and PHP)
Familiarity with the big data technology space and the ability to leverage a wide variety of open source technologies and tools. Knowledge of Hadoop, Spark, Kafka and other big data technology stacks and streaming tools or related Cloud Service technologies on Azure or AWS
Experience with Configuration Management software – Ansible (preferred), Puppet, or Chef or an equivalent AWS/Azure Infrastructure as Code experience. Also, experience with version control (Git)
Experience with installation and configuration of big data software and technology or equivalent Cloud Service technologies on Azure or AWS
Experience working in an Agile methodology

Experience as a plus:
Working knowledge of cloud architecture and implementation on Azure or AWS is a big plus
Experience with Serverless computing (e.g. AWS Lambda or Azure Functions), creating VMs, cloud security, and other cloud services is also a big plus
Experience with micro-services and SOA is a plus
Knowledge of healthcare data, HL7, and Mirth are also a big plus
Strong skills in data structures, data/file formats, algorithms and object oriented design
Experience working with JIRA is a plus


Strength Through Diversity

The Mount Sinai Health System believes that diversity is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize medicine together.

We work hard to acquire and retain the best people, and to create a welcoming, nurturing work environment where you can develop professionally. We share the belief that all employees, regardless of job title or expertise, can make an impact on quality patient care.

Explore more about this opportunity and how you can help us write a new chapter in our story!

Who We Are

Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve.

Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospital campuses, including Mount Sinai Beth Israel, Mount Sinai Beth Israel Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai.

The Mount Sinai Health System is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.

EOE Minorities/Women/Disabled/Veterans","New York, NY","Data Engineer II, Scientific Computing",False
835,"ContractNew York NY

Full Time

Mandatory Technical Skills:

Directing a team of engineers to design, develop, and implement data pipelines for business operational support, analytical workflows and machine intelligence opportunities.
Identifying the right data sources, establishing data modeling best practices, and installing/administering or designing a data modeling tools.
Analyzing existing datasets creating requirements for incorporation into data flow pipelines as well logical and physical data models.
Establishing a continuous process for optimization by developing and enforcing data modeling best practices.
Facilitating the communication of the architecture, design and project status to project stakeholders.
Providing business insight through integration of data with available machine learning and business intelligence tools.
Optimizing workflows so that they might be migrated into (and out of) public cloud infrastructure where sensible.

Desirable Technical Skills:

Need to bring in Thought Leadership and be Innovative.
Need to Keep Pace with latest trends and technologies in Analytics and Visualization area.

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","New York, NY",Data engineer,False
836,"InternshipReq. ID: 123929
Micron Technology’s vision is to transform how the world uses information to enrich life and our commitment to people, innovation, tenacity, collaboration, and customer focus allows us to fulfill our mission to be a global leader in memory and storage solutions. This means conducting business with integrity, accountability, and professionalism while supporting our global community.

As an IoT /Data Engineer Intern within the Data Science department at Micron Technology, Inc. in Manassas, VA, you will be part of a team responsible for expanding the Facilities “Internet of Things”. The primary role will be the setting up and analyzing new sensors, and sensor data streams. The Data Science Team is responsible for transforming and analyzing enormous amounts of data generated by Fab Facilities and Fab Processing Equipment.

You will apply a mix of Computer Science/ Mechatronics concepts to:
Device Hardware and Software Setup
Device Hardware and Software Monitoring
Documenting issues for tracking and improvements
Conducting and managing small scale experiments with new sensors.

Qualified candidates will have:
Strong understanding of IoT Edge Devices, including Sensors, sensor modules, and related boards (Raspberry Pi, Odroids, Arduinos, etc)
An understanding of Mechatronics basics, with a good awareness of; electrical engineering, mechanical engineering, and sensor-data-collection.
Ability to prepare and format large sets of data
Ability to visualize large sets of unstructured data
Ability to troubleshoot Edge-node devices, at a basic level.
Good analytic and problem solving skills
Excellent multi-tasking skills
Strong written and verbal communications.
Previous experience in a semiconductor environment a plus.
Able to work inside a clean room when needed

Education:
In one of the following disciplines: Engineering, Computer Science, Data Science, Mechatronics.

About Us
As the leader in innovative memory solutions, Micron is helping the world make sense of data by delivering technology that is transforming how the world uses information. Through our global brands - Micron, Crucial and Ballistix - we offer the industry's broadest portfolio. We are the only company manufacturing today's major memory and storage technologies: DRAM, NAND, NOR and 3D XPoint™ memory. Our solutions are purpose built to leverage the value of data to unlock financial insights, accelerate scientific break throughs and enhance communication around the world.
We recruit, hire, train, promote, discipline and provide other conditions of employment without regard to a person's race, color, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, pregnancy, veteran’s status, or other classifications protected under law. This includes providing reasonable accommodation for team members' disabilities or religious beliefs and practices.
Each manager, supervisor and team member is responsible for carrying out this policy. The EEO Administrator in Human Resources is responsible for administration of this policy. The administrator will monitor compliance and is available to answer any questions on EEO matters.
To request assistance with the application process, please contact Micron’s Human Resources Department at 1-800-336-8918 (or 208-368-4748).
Keywords: Manassas || Virginia (US-VA) || United States (US) || Frontend Manufacturing || Entry || Internship || Engineering || Not Applicable ||","Manassas, VA",Intern- Internet of Things Data Engineer,False
837,"TPC Energy Fund is a proprietary energy trading firm located in Washington, DC. Our business is primarily focused on the wholesale power markets. We combine quantitative analysis, fundamental knowledge of the grid, cutting edge technology and regulatory know-how to shape our success. Our team is young, dynamic, collaborative and results oriented. We are looking for a candidate who can rapidly build and evolve our analysis and trading platform and who possesses the following skills:

Relevant research experience, such as a PhD, Masters, or Architect Role in Math, Computer Science, Physics, Machine Learning or a relevant quantitative field

3-5 years managing and optimizing large sets of data

Ability to work with trading team and make creative suggestions for evaluating the data

Knowledge of time series data classification and anomaly detection

Excellent programming skills (proficient in Python, GO or Java)

AWS experience and/or experience architecting cloud based system

Eager to conquer large sets of data and improve both our capabilities and speed of analysis

Driven by solving complex problems

Strong communication skills and a self-starter

Ability to learn from mistakes and make improvements

Experience in the energy markets is preferred but not a must

Apply Now","Washington, DC",Streaming Data Engineer,False
838,"A snapshot of what you would do:

The Data Engineer possesses the knowledge necessary to collect, store, process and analyze huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. Data Engineer will work closely with data scientists and is mainly in charge of architecting the solutions for data science and business intelligence projects.

Build data pipelines to pull together information from different source systems
Integrate, consolidate and cleanse data
Structure the data for use in individual analytics applications
Work with operations to understand all different pieces of information and learn how data is being captured in the business process
Provide data in ready to use form for data scientists who are looking to run queries and algorithms against the information for predictive analytics, machine learning and data mining purposes
Work with business units and department to deliver data aggregations to executives and other end users for more basic types of analysis to aid in ongoing operations
Generate meaningful data visualizations
Research opportunities for data acquisition and new uses for existing data
Recommend ways to improve data reliability, efficiency and quality
Troubleshoot data issues within analytics databases and presents solutions
 Required Skills:

Comprehensive knowledge of the following areas:
SQL programming in MS SQL Server (or similar SQL based database server) to do data modelling, data validation, table building and data joins, efficient indexing, complex query building, query optimization and analysis
SSIS (SQL Server Integration Services) or other ETL/data integration tool
Database Management Systems
Programming in any of Python, Java, C#, C++ etc.
Fundamental knowledge of the following areas:
BI tool such as PowerBI, Tableau etc.
Knowledge data security practices
Knowledge in Microsoft Excel
Knowledge in data science tool is a plus (R, Python, Stata etc.)
Knowledge in Natural Language Processing and text analysis is a plus
Knowledge in Rest APIs is a plus
Required Experience:

Bachelor's degree (Master’s degree is preferred) in Computer Science, Software/Computer Engineering, Applied Mathematics, Physics, Statistics or related technology or quantitative field
Two years SQL script writing
One year in ETL
One year programming
About Us:

APEX Analytix is a cloud based software and services company that works with over 200 of the Global Fortune 1000 to recover lost profits, protect against procurement fraud, and manage the process of supplier onboarding, compliance, and payment profit optimization. We receive from our clients annually roughly 9 million Supplier records and $4.6 trillion in invoices and payments. We work with clients like the world’s largest retailer, electronics company, entertainment company, financial institutions, pharmaceutical manufacturers, automotive manufacturers, services companies, and many others, with shared services locations in the US, UK, and Hong Kong.

Delivering our solutions to the world’s largest companies has enabled us to develop what the world’s largest and most respected IT industry analyst describes as possibly the ONLY trusted source for accurate and high value supplier information. We are making this APEX data available to the world (solution software developers, enterprises, and other entities) through the launch of an on-demand API business model. Initially targeting Global Fortune 1000 companies, our API service to this validated and enhanced “SmartVM” data source is supplemented with over 400 real-time data sources through direct integration, subscription and robotics. In turn - our clients will have the trusted data they need to ensure compliance, manage risk, improve B2B transaction processing, enable strategic initiatives, predict outcomes and prescribe action. If you want to be a key contributor, creating and building the world’s only trusted API based Supplier Data Brokerage solution ….never seen before in the industry…. this opportunity is for you!

APEX Culture:

Our culture drives everything! We live our culture of performance, respect, candor and fun—in that order. Performance is measured by delivering value to our clients, generating goodwill, trust and partnership. Sincere respect for each other, our diverse backgrounds and our well-being are our cornerstones. We value open and honest relationships with each other, our clients and our communities. And, life is too short to not have fun! We look for team players who possess the qualities of being humble, hungry, and smart. We recruit candidates who will strive for accountability in performance and career growth, serving fortune 1000 clients in long-term respectful relationships, and working in a diverse and caring environment.

Perks:

We owe our growing success to our team of bright, passionate, and innovative individuals. We truly value our associates and strive to provide the highest quality benefits program offering competitive compensation packages with tailored bonus plans and generous benefits. Our benefits include: health plans (medical, dental, optical, life insurance, disability insurance, FSA, HSA employee assistance program, supplemental insurance options and pre-paid legal), generous paid time off plan, 401k plan, flexible work schedules, wellness programs (company fitness and weight loss challenges, financial wellness software tool, and gym membership stipend), associate and customer referral bonus programs, and paid community hours. And here at APEX, you won’t get lost in the shuffle. Our focus on internal training, growth and development, results in over a quarter of our open positions, filled with internal promotions annually! With resources such as a strong Mentor Program, Internal Training Portal, Education, Tuition, and Certification Assistance, we provide the tools for our associates to grow and develop. If you are looking for a place to shine, come join the team!","Greensboro, NC 27410",Data Engineer,False
839,"Jetblack is the first portfolio company within Store Nº8 ( https://www.storeno8.com/ ), the incubation arm of Walmart, which is focused on transforming the future of retail. Jetblack is a members-only personal shopping and concierge service that combines the convenience of e-commerce with the customized attention of a personal assistant. Our ultimate goal is to create a new standard of consumer shopping with the fastest most delightful end-to-end consumer experience.

Our unique positioning as the first stand-alone company incubated by Walmart gives us access to resources needed to have impact at massive scale and reinvent consumer shopping behaviors. Our journey has just begun and we are building a hard-working passionate founding team to join us changing the retail landscape and driving technical innovation.

Jenny Fleiss, the CEO and co-founder of Jetblack, previously co-founded Rent the Runway, a business that has transformed the retail industry by making designer clothing rentals a convenient and accessible luxury experience for millions of women.

About the job:
Data is core ingredient to our success in conversational commerce; the data lake developed by the data engineering team will materialize our customer interactions in a flexible and powerful way that drives our experimental learnings, product insights, and personalization research. Our ultimate goal is to create a new standard of consumer shopping with the fastest most delightful end-to-end consumer experience. Our unique positioning as the first stand-alone company incubated by Walmart gives us access to resources needed to support data-driven learnings and the ability to have impact at massive scale and reinvent consumer shopping behaviors. Our journey has just begun and we are building a hard-working passionate founding team to join us changing the retail landscape and driving technical innovation.

We're blushing. Some of our favorite press links:
https://www.entrepreneur.com/article/314307 ( https://www.entrepreneur.com/article/314307 )

https://www.inc.com/zoe-henry/rent-the-runway-co-founder-launches-jetblack-takes-on-amazon.html ( https://www.inc.com/zoe-henry/rent-the-runway-co-founder-launches-jetblack-takes-on-amazon.html )

You will love this job if you:

Are passionate about technology and motivated by building cool things that bring tangible value
Thrive in loosely structured environments and help create increased structure as we mature
Consistently deliver and have an appreciation for practices that keep you from slowing down
Play well with others and want to work with and inspire other great engineers

What you'll do:

Own the design and implementation of Jetblack's production data lake, as both a store of our schematized event logs and a central source of truth for business users and analysis
Optimize dataflows to power our machine learning practice for research and live systems
Own documentation and best practices around data materialization and usage
Implement audit and anomaly detection logic to guarantee the reliability of data views
Thoughtfully partner with Product, Business Intelligence and Back-end Engineering to materialize new data products, serve as an expert on data schemas and uses to the rest of the business

What you should have:

4-6 years of industry experience
BS / MS in Computer Science or related field
Experience with Spark / Flink / Hive and/or Hadoop is a big plus
Familiarity with a JVM-based language (Java / Scala / Clojure)
Prior work with textual data, document/semantic indexing and natural language analysis
Experience with SQL query optimization and RDMS data modeling; familiarity with NoSQL solutions a plus
Experience with workflow schedulers (Airflow / Oozie)
Previous work with microservices and containerization (Docker / Kubernetes) a plus
Effective communication, interpersonal and teamwork skills

Pay, perks & such

At Jetblack, we function like a startup and have the benefits of the largest retailer in the world, Walmart. Our benefit package includes a monthly gym stipend, a generous parental leave program, leading healthcare options, 401k matching, and more! We offer unlimited vacation and a great culture filled with quarterly outings, happy hours, work out classes, celebrations, and clubs. We host bi-weekly lunch 'n learns and weekly all-hands meetings. Most importantly, we empower employees to dig in and focus on solving big consumer problems. Join us!","New York, NY",Senior Data Engineer,False
840,"Job Description

The Spine Global Product Engineering team seeks an accomplished Data Engineer and aspiring Data Scientist for an exciting opportunity on the Data team. You will be involved with designing a new workflow for data science and analytics that are a big part of the roadmap for 2018 and implement workflow processes for data science projects that scale for data in the billions. You will design large distributed technical solutions, manage research projects resulting in real world project and data pipelines that support data products. To start you will assist with, but eventually lead data science POC projects.
Your day to day will include:
The design and management of data pipe-lines using Luigi or other ETL system
Uncanny ability to look at code, identify bottlenecks and adjust through code refactor and/or tuning Spark parameters
Writing Java and Scala applications for data processing and engineering
Implementing standard and custom machine learning techniques across big data
Facing challenging and complex business problems daily
The investigation, procurement and ramp up to new technologies
Collaboration and team work, you enjoy fostering relationships and partnering with others daily and will work closely with the Product Management team

Qualifications

We are looking for a talented team member who is also a delight to work with, which usually includes:
Bachelor's degree in Mathematics, Computer Science, Engineering, Statistics
Data engineering experience in Java, Python and/or Scala
Accomplished in the design of efficient and robust ETL workflows
A background in software engineering
Clear understanding of Web Services
Knowledge of Big Data Architectures (Hive/Hadoop, Redis and/or Dynamo DB)
Experience with SQL and/or NoSQL
Excellent oral and written communication skills
A collaborative spirit and a drive to solve problems creatively
Extra credit for implementing Machine Learning on parallelize infrastructure
Got the goods? We would love to hear from you
Additional Information

All your information will be kept confidential according to EEO guidelines.","New York, NY",Data Scientist Spine (N902),False
841,"Reed Exhibitions (RX) is one of the largest trade show organizers in the world and is one of the four group companies within RELX (formerly Reed Elsevier), which is a FTSE 100 company. As part of a set of C-suite supported strategic programs that aim to use data to deliver the best of face-to-face and digital services around trade shows, a number of projects have been identified under the ‘Data & Analytics’ program. The first of these projects is to globally rollout a single lead/badge scanning capability across 100s of RX 500+ shows over the next two years.
The opportunity here is to provide a consistent data platform for this data, so exhibitors and visitors can easily access who they have met, but additionally underlying business networks can be identified and understood to improve business outcomes for RX’s 7 million annual customers through the development of world-class products and services in the B2B and B2C trade show space.
The role
RX is putting together a project team for this and subsequent projects under the Data & Analytics program. An important member of the team will be a Data Scientist who with a Data Engineer and Product Manager will make up the central team who will work with local show teams and business units around the world. Additional project resources will come from Central IT and 3 rd party agencies as required.
The ideal candidate will be a ‘true’ multi-skilled data scientist with a business leaning: a genuine hybrid of computing, statistical, communication skills and business acumen. Combined with the imagination to see what and how data is currently or can be collected and then developed into new customer valued services.
Due to the semi-decentralized nature of RX the following list of skills and personal attributes would be prized. If these skills can be evidenced by past projects that would be ideal.
Statistical modelling, e.g., general linear model, survival models (Cox regression)
Hypothesis testing, ideally for product development A/B testing, e.g., ANOVA, t-tests, Chi-Squared, McNemar, non-parametric methods
R or Python script writing
Understands data pipelines and can support strategic decisions on technology choices
RESTful APIs (usage, ability to write them is desirable, but not required)
SQL, both DDL, indexing when and why, DML
Experience in design and enhancing data models that have been used in production code
Visualisation experience to include dashboards, management or customer reports (e.g., Excel, ggplot, Matplotlib, Shiny, D3)
 #LI-LP1
Reed Exhibitions is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1-877-734-1938.","Norwalk, CT",Data Scientist,False
842,"Function: Engineering
Career Level: Senior professionals (> 5 years)
Legal Entity: Evonik Corporation
Business Line: Process Technology & Engineering


What we offer
You will work on exciting and challenging topics together with a team in an ultra-modern, innovative and creative environment. Intensive on-the-job training with expert colleagues guarantees you will quickly become familiar with your duties and perform them independently. Performance related pay and the opportunity for personal and professional development are of course part of the package. Since 2009 Evonik Industries AG has been certified as a family-friendly company by the German Hertie Foundation.
We are seeking an individual in our Mobile, AL location as an engineer in the area of Manufacturing Intelligence. Primary Responsibility: Develop and implement tools to enable productivity gains through Manufacturing Intelligence.


RESPONSIBILITIES
Specific Activities include but are not limited to:
Data aggregation from diverse sources including chemical process data
Contextualization providing structures and models to support customers utilizing data supported by standards (e.g., ISA-95)
Data analysis across data sources and plants
Visualization of KPIs through Dashboards
Propagation of data throughout diverse enterprise systems
Provide technical assistance to the chemical processing facilities of Evonik
Facilitate exchange of knowledge between Evonik plants in order to promote improvements company wide
Participate in technology and methodology exchange between international regions
Collaborate on a global basis with other departments within Process Technology & Engineering
Support development of new process engineering service offerings that provide value to Evonik Business Lines


REQUIREMENTS
Master’s degree or PhD in computer science, programming, data processing, or related field
Minimum 3-5 years of IT experience in full-stack system/software design and programming
Minimum 1-2 years of experience in Data Visualization and Data Analysis
Aspiration to develop new solutions in the area of Manufacturing Execution Systems and/or Manufacturing Intelligence and to drive implementation across Americas region
Ability to utilize process documentation (e.g., PFDs, P&IDs, Equipment Specifications)
Ability to utilize diverse data sources and associated applications (e.g., Plant Information Management Systems, Laboratory Information Management Systems, Enterprise Resource Planning Systems)
Ability to plan/organize tasks and consistently produce high-quality results
Ability to work efficiently and effectively in a multi-disciplined, cross-functional environment including in teams
Ability to work in international teams
Flexibility to respond quickly to changing job demands and prioritize multiple responsibilities
Experience managing projects involving complex scope
Excellent communication skills (both oral and written) across hierarchy levels ranging from plant operators to management level","Mobile, AL",Data Engineer,False
843,"Overview
We are expanding our efforts into complementary data technologies for decision support in areas of ingesting and processing large data sets including data commonly referred to as semi-structured or unstructured data. Our interests are in enabling data science and search based applications on large and low latent data sets in both a batch and streaming context for processing. To that end, this role will engage with team counterparts in exploring and deploying technologies for creating data sets using a combination of batch and streaming transformation processes. These data sets support both off-line and in-line machine learning training and model execution. Other data sets support search engine based analytics. Exploration and deployment of technologies activities include identifying opportunities that impact business strategy, collaborating on the selection of data solutions software, and contributing to the identification of hardware requirements based on business requirements. Responsibility also includes coding, testing, and documentation of new or modified scalable analytic data systems including automation for deployment and monitoring. This role participates along with team counterparts to develop solutions in an end-to-end framework on a group of core data technologies.
Responsibilities
JOB DUTIES
Contribute to the evaluation, research, experimentation efforts with batch and streaming data engineering technologies in a lab to keep pace with industry innovation
Work with data engineering related groups to inform on and showcase capabilities of emerging technologies and to enable the adoption of these new technologies and associated techniques
Contribute to the definition and refinement of processes and procedures for the data engineering practice
Work closely with data scientists, data architects, ETL developers, other IT counterparts, and business partners to identify, capture, collect, and format data from the external sources, internal systems, and the data warehouse to extract features of interest
Code, test, deploy, monitor, document, and troubleshoot data engineering processing and associated automation
REPORTING RELATIONSHIP

Data Integration Manager US
Qualifications
Knowledge
2+ years of hands-on experience with SQL, data modeling, and relational databases such as Oracle, DB2, and Postgres
1+ years of experience with software engineering to include Java, Scala, and Python
Experience with processing large data sets with Kafka, RabbitMQ, Flume, Hadoop, HBase, Cassandra and/or Spark or similar distributed system
Experience with NoSQL data stores such as MongoDB, Cassandra, HBase, Redis, Riak or other technologies that embed NoSQL with search such as MarkLogic or Lily Enterprise
Bachelors or higher degree in computer science or other quantitative discipline or equivalent work experience
Experience or familiarity with ETL and Business Intelligence technologies such as Informatica, DataStage, Ab Initio, Cognos, BusinessObjects, or Oracle Business Intelligence
Skills
Ability to quickly prototype and perform critical analysis and use creative approaches for solving complex problems
Excellent written and verbal communication skills
Work Condition
Work primarily in a controlled climate environment. Mostly stationary with occasional need to travel between nearby DFW office locations to visit business partner customers and attend meetings. Occasional travel to attend conferences or training for development pursuits.","Arlington, TX 76006 (North area)",Data Engineer (Hbase & Spark),False
844,"As a Data Engineer with Tredence, you will demonstrate ability to transform business requirements to code, specific analytical reports and tools. You will also provide business insights, while leveraging internal tools and systems, databases and industry data.


THE IDEAL CANDIDATE WILL

Very Strong engineering skills. Should have an analytical approach and have good programming skills.
Provide business insights, while leveraging internal tools and systems, databases and industry data
Minimum of 5+ years’ experience. Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter
Ability to document requirements, data lineage, subject matter in both business and technical terminology.
Guide and learn from other team members.
Demonstrated ability to transform business requirements to code, specific analytical reports and tools
This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
Must Have

Strong analytical background
Self-starter
Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights


ELIGIBILITY CRITERIA

Technical Requirements

Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata)
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes
Real time data ingestion (Kafka)
Nice to Have

Development experience with Java, Scala, Flume, Python
Cassandra
Automic scheduler
R/R studio, SAS experience a plus
Presto
Hbase
Tableau or similar reporting/dash boarding tool
Modeling and Data Science background
Retail industry background
Education

BS degree in specific technical fields like computer science, math, statistics preferred","Sunnyvale, CA",Data Engineer,False
845,"Job Summary:

The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The Data Engineer will create processes and data models to enable performance management reporting as well as data science, advanced analytics, and personalization efforts.

 The Data Engineer will work closely with Technology, Finance, and Commercial Analytics to drive value through best in class data architecture and data model design.

Key Responsibilities

Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics
Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer (data visualization, application, or predictive model)
Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs
Design, develop, test and deploy data models, data collection, and transformation components. Determine best point for transformations, calculations, and joins (e.g. data lake, data warehouse, or Tableau data source)
Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate
Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture
Required Qualifications:
2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server
Expert SQL scripting skills; R, Python, or SAS preferred but not required
Ability to effectively build relationships across the business at all levels
Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment
Strong technical understanding of current and emerging business intelligence and analytics technologies
 Education:

Bachelor's Degree in Technology, Computer Science, or Data Science preferred","Madison, WI 53717",Data Engineer,False
846,"The Role:

S&P Global Ratings is looking for an experienced Big Data Engineer to join Data Engineering team within Chief Data Office, a team of data and technology professionals who define and execute the strategic data roadmap for S&P Global Ratings. The successful candidate will participate in the design and build of S&P Ratings cloud based analytics platform to help develop and deploy advanced analytics/machine learning solutions.

The Team
You will be an expert contributor and part of the Rating Organization’s Data Services Team. This team, who has a broad and expert knowledge on Ratings organization’s critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.

Our Hiring Manager Says

If you are an individual that brings demonstrated experience of delivering big data projects as a data engineer, this is an excellent opportunity. I am looking for someone with sound technical knowledge, can be hands-on, worked on transformational initiatives, and can drive results.

Responsibilities:
Design and develop efficient and scalable data pipelines between enterprise systems and analytics platform
Work closely with Data Science team and participate in development of feature engineering pipelines
Provide technical expertise in the areas of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Build and maintain a data environment for speed, accuracy, consistency and ‘up’ time
Support analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organization
Work with the machine learning engineering team to build a data eco system that supports AI products at scale
Ensure data governance principles adopted, data quality checks and data lineage implemented in each hop of the data
Partner with the chief data office, enterprise architecture organization to ensure best use of standards for the key data domains and use cases
Be in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologies
Ensure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards
Experience & Qualifications:
BS or MS degree in Computer Science or Information Technology
5+ years of experience as data engineer at an innovative organization
3+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Expert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environment
Experience With Machine Learning Libraries and Frameworks (TensorFlow , MLlib) is an added advantage
Exposure to R , SparklyR , and Other R packages is a Plus
Experience with development frameworks as well as data and integration technologies such as Informatica, Python, Scala
Expert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.
Monitors industry trends and directions; develops and presents substantive technical recommendations to senior management
Excellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners
Ability to prioritize and manage work to critical project timelines in a fast-paced environment
Financial services industry experience","New York, NY 10041 (Financial District area)",Big Data Engineer (S&P Global Ratings),False
847,"American Specialty Health, Inc. is seeking a Data Engineer I to join our Information Technology team. This position will be responsible for delivering best-in-class analytical data solutions. Primary role will be participating in data curation projects and reporting system support and maintenance, with opportunities to develop innovative solutions and explore new technologies.
You are invited to learn more about American Specialty Health’s events on our events page.
Responsibilities
Data curation projects and reporting system support and maintenance.
Participate in the entire lifecycle for Business Intelligence Solution Delivery.
Design, build, document and manage data warehouse objects.
Design and develop ETL processing solutions.
Assist with BI Solutions (DBMS, SSIS, SSAS, SSRS, Tableau) development and performance tuning.
Develop, test and deliver error free data.
Stay abreast of BI industry best practices and relevant new technologies.
Design and develop reports/visualizations using SQL Serve Reporting Services and Tableau.
Perform requirements gathering and analysis.
Ability to work independently (as well as within a team environment) and unsupervised.
Ability to work a flexible day schedule required including occasional evenings.
Design and develop high performance reporting assets.
Work with SMEs (both technical and nontechnical) for knowledge transfer and documentation.
Work with end-users to support adoption of team deliverables.
Qualifications
Bachelor’s Degree or higher in Computer Science, Computer Engineering, MIS or related field or equivalent work experience is preferred. If equivalent experience, high school diploma required.
1 year or more of experience developing Business Intelligence solutions.
3 or more years of experience developing solutions using the MS SQL Server BI Stack.
Microsoft data-related certification.
Basic knowledge of Microsoft Excel.
Recent working knowledge of a major BI Tool (Tableau preferred).
Strong technical skills; able to work with multiple platforms, integrate data with various sources.
Strong problem solving and analytical skills; able to think outside of the box on a consistent basis.
Willingness to learn new technology.
Basic knowledge of TSQL
Basic knowledge of data modeling.
Basic knowledge of data visualization practices and methods.
Core Competencies
Demonstrated ability to interact in a positive, respectful manner and establish and maintain cooperative working relationships.
Ability to display excellent customer service to meet the needs and expectations of both internal and external customers.
Excellent listening and interpersonal communication skills to identify critical core competencies based on success factors and organizational environment.
Ability to effectively organize, prioritize, multi-task and manage time.
Demonstrated accuracy and productivity in a changing environment with constant interruptions.
Demonstrated ability to analyze information, problems, issues, situations and procedures to develop effective solutions.
Ability to exercise strict confidentiality in all matters.
Mobility
Primarily sedentary, able to sit for long periods of time with ability to travel within and outside the facility.
Physical Requirements
Ability to speak and hear other personnel and/or objects. Ability to communicate both in verbal and written form. Ability to travel within the facility. Capable of using a telephone and computer keyboard. Ability to lift up to 10 lbs.
Environmental Conditions
Within facility, normal office conditions, including lighting and ventilation. Minor noise from conversations and general office equipment (telephone, printer, etc.). When required to travel outside the facility, usual weather, traffic, and related conditions are applicable.
American Specialty Health is an Equal Opportunity/Affirmative Action Employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran Status, or any other characteristic protected by applicable federal, state, or local law.
Please view Equal Employment Opportunity Posters provided by OFCCP here.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access our career center as a result of your disability. To request an accommodation, contact our Human Resources Department at (800) 848-3555 x6702.
ASH will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company’s legal duty to furnish information.","San Diego, CA 92121",Data Engineer,False
848,"$77.30 an hourContractImmediate need for a talented Market Data Engineer that brings experience in Market Data platfroms. This is a 12-18 month + contract opportunity with long-term potential and is located in downtown NYC. If interested, please respond with the most current version of your resume referencing 18-10513 in the subject line. For information on similar positions, please refer to the website below or simply request this info when sending replying with your resume.

Hourly rate $77.30/hr W2

Key Responsibilities:
The new hire will be a member of the Market data engineering team and will play a key role in facilitating the integration of market data technologies within the firm. He /She will also be involved in design and deployment of market data systems and services that will be used by the different business units in the firm.

The candidate must have 7-10 years of experience in the design and implementation of large scale market data systems. The position will involve working closely with senior technologists from the engineering, operations and development communities, so excellent communication and technical skills are a must.

Expert knowledge of industry standard Market Data platforms (NYSE Technologies, Bloomberg, Reuters TREP-RT)

Demonstrates key experience with Low Latency and Enterprise Market data systems

Expert knowledge of consolidated and direct exchange feeds

Solid understanding of network and OS fundamentals

Strong knowledge of system tuning, performance and monitoring tools across OS platforms

Good working knowledge of Linux and Windows operating environments

Mastery in configuration management frameworks like Ansible,

Experience with CI/CD tooling like Jenkins

Understanding of the software deployment processes with a focus of standardization of global configuration and performance testing

Develop performance and diagnostic tools to assist in debugging issues with market data systems.

Monitor infrastructure utilization and develop a capacity planning program.

Work closely with business-aligned teams to optimize market data usage and improve system design to satisfy the requirements of ultra low latency and high frequency trading applications

Ability to plan, prioritize, communicate and perform tasks in a team environment.

Bachelors Degree

Experience with messaging middle ware (TIBCO Rendezvous, 29 West)

Understanding of Market Data API's (MAMA, RFA, BBG API V3)

Participate and contribute in design review sessions

Work with market data vendors on new requirements and ongoing issues

Regards,

Mary Martini

DIVERSANT LLC

Recruiting Manager

61 Broadway Suite 1702

New York, NY 10006

mmartini@diversant.com

View all of our open job requirements on our home page http://www.diversant.com under *Job Search*. If you know any IT Professionals who are looking for new job opportunities, please pass along my contact information!

DIVERSANT (diversant.com) is one of the largest African-American owned IT staffing firms in the U.S. We offer rewarding career opportunities with many of the nation’s leading corporations. Our experienced recruiters understand what hiring managers look for in a candidate and provide our applicants with the proper support and guidance along the entire application and interviewing process. We offer opportunities on a contingent, contract-to-hire, and direct hire basis. At DIVERSANT, we are committed to providing the highest level of service and satisfaction to our customers, consultants, and employees.

DIVERSANT provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, DIVERSANT complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.","New York, NY 10038 (Financial District area)",Market Data Engineer,False
849,"Purpose & Overall Relevance for the Organization:
This role will be responsible for designing, engineering, and scaling a “big data” data set (Analytical Data Asset) in a designated area such as “customer”, “quality”, “plant” etc. The role will be tightly interlinked to ZF’s divisions / functions and IT, closely partnering with respective technical teams and data subject matter experts in the business to deliver a comprehensive data set synthesized from multiple data sources. The Principal Data Engineer will constantly seek to improve the predictive power of Analytical Data Assets, as well as support the Digital Monetization team in monetization of Data and Analytics.

Principal Subject Matter Expert for Data Engineering:
Serve as the expert internal consultant for Data Engineering and its integration with Enterprise IT systems, IoT, I4.0, Data, and Analytics across all ZF Divisions and Function
Strong background, knowledge and understanding of Data Engineering both in data center and cloud environments with traditional or modern stacks. Masters or similar level of accomplishment evidenced by patents or papers.
Proven track record of working across the enterprise and with IT.
Data Engineering Management:
Serve as the Single Point of Contact (SPOC) for Data Engineering and Analytical Data Assets in a designated functional domain.
Work with Senior technology stakeholders to articulate opportunities for Analytical Data Assets across the ZF portfolio.
Identify and facilitate x-division/function synergies and collaborative Data Engineering opportunities – deliver “whole is greater than the sum of the parts” in the designated area.
Manage the technical content, education, best practices, and technical competency in data engineering and analytical data assets in designated domain.
Manage and Oversee Lighthouse Data Engineering Initiatives:
Overall responsibility for achieving the quality outcomes of the lighthouse Data Engineering projects to create large scale Analytical Data Assets in designated domain.
Develop modules and implementation patterns on the ZF platforms to accelerate delivery of Lighthouse Analytical Data Assets and other capabilities.
Manage the Transformational Change:
Foster company-wide commitment for Analytical Data Assets as part of the digital strategy and prioritize initiatives per the company needs, and align it with the overall strategy of the company.
Act as the ambassador within the company for Digital transformation
Act as a spokesperson in the external community helping ZF positioning as a Digital leader, attracting highly talented workforce

Key Responsibilities:


Relationship management with Divisions/Functions to ensure buy in to internal Analytical Data Assets.
Coordination work and relevant communication with other cross functional teams within the broader ZF organization (IT, R&D, …) to assure seamless and “Team Beats Silos” delivery.
Act as the change agent in building credibility for the Digital organization by consistently delivering to business expectations
Work with the internal teams (Digital Labs, Digital Factory, …) to maximize synergy.
Support the build-up of an efficient organization that allows for realization of ZF’s Digital goals
Lead and manage necessary project teams to successfully deliver the stated objectives for lighthouse Analytical Data Assets projects.
Use appropriate and effective communication methods to engage and manage stakeholder expectations.
Hands on development of the skeleton and key functions of the data pipelines and data wrangling required to deliver the Analytical Data Assets in designated domain.

Authorities:


Technical leadership in Data Engineering
Business Relationship Management
Life cycle ownership for Analytical Data Assets
Responsibility for realizing stated objectives for Data Engineering


Key Relationships:

Divisional / Functional technical leaders and data subject matter experts
Project teams within the ZF Digital Organization
Project vendors for development
Cross Functional Coordination with IT, R&D

Equal Employment Opportunity/Affirmative Action Employer M/F/Disability/Veteran
Job Requirements
Education Requirements
A technical degree from an accredited college or university is required; an advanced degree or equivalent technical standing

Experience and Skill Requirements
Demonstrated experience working both at either an OEM or the supply base, or equivalentPassion for technology with the requisite operations expertise and an entrepreneurial spirtA builder, not just a refiner, who is versatile and agile in an evolving environmentMinimum of 3-7 years of experience in Data Engineering with at least 3 years of technical leadership experience with function-wide responsibility preferably in a complex organizationStrong managing, organizing and evaluating skills with a track record of developing and implementing a sophisticated, end-to-end data pipeline producing a “flat table” for advanced analytics and machine learningDemonstrated capability to communicate findings, orally and visually, to senior leadership members and outside partners and customers in the technology ecosystemDeep technical understanding of Data Engineering as well as a demonstrated track record of envisioning the application of these methods to achieve documented results. In particular a demonstrated mastery of concepts such as “features” and their construction and “predictive power” and its quantification.Ability to impact outcomes via influence versus authorityEffective negotiation and influencing skills, including strong written and verbal communication and strong presentation abilitiesDevelops loyalty and commitment in others by articulating an appealing vision for the future, communicating high expectations and leading by exampleA player/coach who is focused on mentorship and developmentCan “work the matrix” and gain credibility quickly with internal and external constituentsA technical degree from an accredited college or university is required; an advanced degree or equivalent technical standing a plusCreative and organized problem solver who is passionate about driving strategy through technologyStrategic mind-set to maintain enterprise-level vision, while driving daily operational goals and controlsInspires heartfelt engagement, a collaborator with the ability to influence and build consensusConsistently achieves results, even under tough circumstancesClear and efficient communicatorLeader and collegial team player with strong interpersonal skills balancing low ego with management capabilities","Livonia, MI",Sr Data Engineer,False
850,"Hi there. We’re Zipcar, the world’s leading car-sharing network, driven to make cities better places to live.

Since 2000, we’ve worked hard to turn a brilliant and disruptive idea into a movement that serves more than a million members worldwide. We’re keeping the pedal to the metal and growing every day. That’s why we need talented, passionate people with great ideas to join the Zipcar family.

Want to work for a company that is shaping the future of urban mobility? Ready to join a dynamic, playful, diverse, and respectful company that’s seriously changing the world? Then apply! To learn more, visit zipcar.com or zipcar.com/careers.

Summary:

Zipcar is seeking a Data Engineer that will be a leader in our efforts to leverage data about our members and vehicles, so that we may continually improve the Zipcar experience and support internal operations.

Responsibilities:

Responsibilities include but are not limited to the following:

Design and build scalable data infrastructure to support real-time analytics
Manage data capture, transformation, and pipelines across our production applications and Hadoop cluster
Collaborate closely with data scientists and software engineers to deploy and scale machine-learning models into production
Support the development of backend and frontend tools serving a variety of internal business processes and consumer-facing products
Contribute to data architecture and technology decisions with expert understanding of data engineering best practices
Use modern software tooling for rapid development, integrated testing, and high performance


Qualifications:

The successful candidate must have the following experience, skills, and education:

BS in Computer Science, Mathematics, Statistics, Engineering, or equivalent experience
Experience and skill in data technologies, including:
Hadoop distributions (e.g. Cloudera, AWS)
Data pipelines (e.g. Kafka / Flume, Kinesis / Lambda)
Data storage (e.g. HDFS, S3)
Data processing frameworks (e.g. Spark, MapReduce)
Data querying (e.g. Impala, Redshift, SparkSQL)
Programming languages (e.g. Java, Python, Scala)
Experience supporting data-driven products, applications, and features
Excellent verbal and written communication skills


The Ideal Candidate Has:

Experience with data storage and processing at scale
Experience with geospatial data
Desire to learn new tools, frameworks, languages
Exposure to various software lifecycle tools such as git, JIRA, etc.
A commitment to Zipcar’s ideals of sustainable resource sharing and urban mobility


As a member of the Zipcar Team you get a great benefits package including health and dental insurance, 401k, vacation time, paid holidays, personal and sick leave, along with a full complement of other insurance and support programs.

Because our work has a global impact, we enthusiastically support each employee’s commitment to creating a better world by offering:

Free Zipcar membership & discounted driving rates for you and your significant other
Paid volunteer time off
Bicycle commuter reimbursement
Paid parental leave for mothers & fathers
Training and development programs to accelerate career growth
Global footprint of job locations and opportunities
Flexible and open work arrangements
Fun, respectful, passionate, and collaborative environment
Educational Assistance Program
Company sponsored parties, outings, and events
Discounts on a variety of products and services through Zipcar partners
Zipcar is an EEO Employer
 To all recruitment agencies: Zipcar does not accept agency resumes. Please do not forward resumes to our jobs alias, Zipcar employees or any other company location. Zipcar is not responsible for any fees related to unsolicited resumes","San Mateo, CA",Data Engineer,False
851,"S&P Global Ratings is looking for an experienced Data Science Engineer to join Data Engineering team within Chief Data Office, a team of data and technology professionals who define and execute the strategic data roadmap for S&P Global Ratings. The successful candidate will participate in the design and build of S&P Ratings cloud based analytics platform to help develop and deploy advanced analytics/machine learning solutions.

The Team
You will be an expert contributor and part of the Rating Organization’s Data Services Team. This team, who has a broad and expert knowledge on Ratings organization’s critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.

Our Hiring Manager Says
If you are an individual that brings demonstrated experience of delivering big data projects as a data science engineer,, this is an excellent opportunity. I am looking for someone with sound technical knowledge, can be hands-on, worked on transformational initiatives, and can drive results.

Responsibilities:
Design and develop efficient and scalable data pipelines between enterprise systems and analytics platformWork closely with Data Science team and participate in development and deployment of machine learning models and feature engineering pipelinesProvide technical expertise in the areas of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and SparkBuild and maintain a data environment for speed, accuracy, consistency and ‘up’ timeSupport analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organizationWork with the machine learning engineering team to build a data eco system that supports AI products at scaleEnsure data governance principles adopted, data quality checks and data lineage implemented in each hop of the dataPartner with the chief data office, enterprise architecture organization to ensure best use of standards for the key data domains and use casesBe in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologiesEnsure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards
Experience & Qualifications:
BS or MS degree in Computer Science or Information Technology8+ years of experience as data engineer at an innovative organization4+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and SparkExpert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environmentExperience With Machine Learning Frameworks, such as TensorFlow , PyTorch, H2O, scikit-learn, Theano, Caffe or Spark MLib is an added advantageExposure to R, SparkR, SparklyR or Other R packages is a plusExperience in constructing fast data staging layers to feed machine learning algorithmsExperience in building data APIs to consume analytic model outputFamiliarity with machine learning model training and deployment process is a plusExperience with development frameworks as well as data and integration technologies such as Python, Scala or InformaticaExpert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.Monitors industry trends and directions; develops and presents substantive technical recommendations to senior managementExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partnersAbility to prioritize and manage work to critical project timelines in a fast-paced environmentFinancial services industry experience","New York, NY 10041 (Financial District area)",Data Science Engineer,False
852,"JOB SUMMARY
Reports to the Manager, Business Intelligence. The Data Engineer will work under the supervision of the Data Architect to design, implement and maintain data warehousing and data management solutions to support business reporting and analytical requirements. This includes building source system data ETL processes, verifying data accuracy, and designing complex interactions between data fields within the data management solution. The Data Engineer will develop data set processes for data modeling, mining, and production, and create custom software components and analytics applications. Additionally, the Data Engineer designs, develops, and automates reports that communicate performance on value-based reimbursement contracts, strategic planning/market analysis, financial analysis, and a variety of other business needs. The Data Engineer is expected to anticipate business needs by designing and building custom queries and front-end reports that provide actionable insight to end-users.

MISSION & VISION

Mission
To enhance the physical, mental and emotional well-being of the communities we serve as the community’s provider of outstanding quality, superior value and comprehensive health care services.

Vision
Our vision is to achieve:
Innovative health care and well-being services of the highest quality at the greatest value
Easy access and convenience
Outstanding patient experiences
Ongoing education involving physicians, patients and the community

JOB SPECIFICATIONS

Education and Experience
The knowledge, skills and abilities as indicated below are normally acquired through the successful completion of a Bachelor's Degree in Information Systems, Data Analytics, Informatics, Database Management, Business, Economics, or a related field. Experience in data analytics, programming, or database management, preferably in a healthcare setting, is preferred.

Knowledge & Skills
Requires strong analytical skills, with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Requires a strong background in financial analysis and using data to support decision-making
Requires in-depth technical expertise regarding data models, data analysis and design, master data management, metadata management, data warehousing, business intelligence, and data quality improvement.
Requires proficiency in the use of Structured Query Languages for building, maintaining and extracting data from relational databases.
Understands relational database structure and is able to design and maintain robust custom database schemas.
Requires ability to research and implement best-practice database management techniques
Requires basic knowledge of the healthcare business needs, including a basic knowledge and understanding of the healthcare revenue cycle and healthcare delivery systems.
Requires strong skills in organization and time management.
Demonstrates well-developed communication skills necessary to effectively communicate both verbally and in writing, and work within the Business Intelligence Team, and with outside technical contacts.
Working Conditions
Works in an office environment.
Physical Demands
Requires the physical ability and stamina to perform the essential functions of the position.

ESSENTIAL JOB DUTIES

The below statements are intended to describe the essential job functions and level of work performed by individuals assigned to this classification. They are not to be construed as an exhaustive list of all job duties performed by the personnel occupying this position.

Assists the Data Architect in designing, maintaining, and enhancing a system-wide Enterprise Data Warehouse (EDW) system by:
Designing, building and maintaining ETL feeds for new and existing data sources
Ensuring ongoing accuracy of data ETL feeds, monitoring for changes in the source systems which may alter inbound data
Building appropriate linkages and relationships between data fields within EDW
Creating and maintaining source-of-truth relationship tables for key fields to be referenced throughout the EDW.
Implementing a standard nomenclature for use within the EDW, in accordance with established Business Intelligence policies.
Documenting all metadata regarding data source, field type, definition, etc. for each field and table created.
Serving as a subject matter expert on data sources, structure, or definitions.
Working with members of Business Intelligence, Finance, and Information Systems teams to optimize data and information usage.
Working with Information Systems teams to optimize the data warehouse through hardware or software upgrades or enhancements.
Structuring data in a way that is easy to understand, query, and display.



Provides information necessary to the financial and clinical success of Beacon organizations by:
Collecting, organizing, analyzing, and disseminating significant amounts of information with attention to detail and accuracy
Creating analyses of strategic planning and market research data to provide insight to business development and growth opportunities for Beacon Health System.
Creating analyses of population health and value-based reimbursement contract performance to determine future contracting opportunities and opportunities for improvement in cost or quality performance.
Developing, designing, and automating regular reports accurately and on a timely basis
Designing and building ad-hoc reports that provide actionable and meaningful information
Responding to analytics inquiries from various departments of Beacon Health System by identifying data that will provide appropriate and actionable answers.
Identifying, analyzing, and interpreting trends or patterns in complex data sets and connecting those trends to actionable insights and business needs.
Interpreting reports or contractual language to analyze the impact on Beacon’s overall operations and financials.
Communication of strategic priorities to Beacon departments based on insights gleaned from data analytics.

Assists the Business Intelligence Team in planning for the evolution of Beacon’s data management initiative by:
Providing input into strategic direction and decisions for the Business Intelligence Department
Researching new data management techniques or best practices.
Actively promoting and supporting a culture of data-driven decision making within various departments of Beacon Health System. This may include innovative data visualization and data science methodologies.
Completing all other duties as assigned

STANDARDS OF BEHAVIOR
Patient/Customer Centered
Anticipates and takes proactive steps to ensure customer’s needs are met
Places courtesy and service above routine and goes beyond customer expectations
Keeps patient/work environment neat and clean
Understands and applies job-related aspects of patient safety and identifies, reports and corrects safety concerns as quickly as possible
Respect
Keeps others well informed
Practices active listening
Develops and maintains positive working relationships
Uses problem solving techniques to resolve issues and makes decisions within personal sphere of influence
Seeks to understand patient's experience
Integrity
Demonstrates integrity and strong business ethics
Utilizes time and resources in a prudent manner
Strives to continually improve department processes and services
Projects professional image through enthusiasm towards work, behavior and appearance
Compassion
Demonstrates Beacon values verbally and through actions
Displays and exhibits caring behaviors with each interaction
Demonstrates self-awareness and sensitivity to the perceptions of others
Listens carefully to input and concerns and takes appropriate action
Interacts with dissatisfied customers in a calm, respectful manner and seeks resolutions
Trust
Maintains confidentiality at all times
Fosters a sense of trust and collaboration among associates
Verbal and written communications are clear and effective
Responds to change in a positive manner

ORGANIZATIONAL RESPONSIBILITIES

Associate complies with the following organizational requirements:
Attends and participates in department meetings and is accountable for all information shared.
Completes mandatory education, annual competencies and department specific education within established timeframes.
Completes annual employee health requirements within established timeframes.
Maintains license/certification, registration in good standing throughout fiscal year.
Direct patient care providers are required to maintain current BCLS (CPR) and other certifications as required by position/department.
Consistently utilizes appropriate universal precautions, protective equipment, and ergonomic techniques to protect patient and self.
Adheres to regulatory agency requirements, survey process and compliance.
Complies with established organization and department policies.
Available to work overtime in addition to working additional or other shifts and schedules when required.

Commitment to Beacon's six-point Operating System, referred to as The Beacon Way:
Leverage innovation everywhere.
Cultivate human talent.
Embrace performance improvement.
Build greatness through accountability.
Use information to improve and advance.
Communicate clearly and continuously.","South Bend, IN 46601",Data Engineer,False
853,"ABOUT BARK:
BARK is a company building products, experiences, and entertainment for dogs and the people who love them. The lasting brand that Disney has built for kids and families, BARK is building for the fast-growing market of dog people.

Our ambition-level is high, the opportunity is huge, and our love for dogs is through the roof! We launched in 2011 with BarkBox, a monthly themed subscription of all-natural treats and clever toys. Since then, we've shipped more than 50 million toys and treats to the dogs across the world and use all of that direct customer feedback to inform new initiatives and ways to make dogs happier.

But what kind of company is BARK? It's a company that has gone to the dogs. We are committed to a culture that is open, inclusive, generous, and enthusiastic — just like our pups. United by this dog-obsession, we embrace diversity of all stripes and spots. If your camera roll is 90% pictures of your dog, you belong here.

At BARK, we know that dogs aren't pets; they're family. Our people – crazy dog people – believe that their dogs deserve the best. The best treats, the best toys, the best seat on the couch. Together, we're driven to be the people our dogs think we are.

WHO WE'RE SNIFFIN' FOR:
Can you give a pup belly scratches until you get carpal tunnel? Listen to squeaky toys until your ears bleed? BARK is a fast-growing business for people who love, (*ruv) dogs.

As Data Engineer / Data Systems Architect at BARK, you will design and build our systems for ingesting and processing data to address data integrity, data security, and data scaling - both in size and complexity. Your end users will be Data Scientists and Analysts, who will rely on these systems to deliver accurate data for reporting and analysis. As we ingest data from multiple external sources and internal sources with frequent upstream schema changes, a strong engineering background with an eye for generalizability and abstraction will be key for this role.

DOODIES:

Productionizing existing ETLs, dashboards, pipelines, and models to produce reliable outputs that maintain consistency in data definitions
Implementing and maintaining a data warehousing solution with fine-grained access control and security levels to adequately protect sensitive data
Implementing software and platform services that empower analysts and data scientists to more efficiently automate and share analytical work product
Providing technology guidance to data scientists and analysts as needed for larger

EXPERIENCE

We are looking for a passionate and experienced technologist with the following competencies:


Expert in at least one programming language, comfortable with learning new ones
Adept at picking up new technologies and evaluating them against business requirements
Strong experience with traditional relational databases, including writing SQL, schema modeling, performance tuning, and query optimization
Understanding of Data technologies, algorithms, and design patterns
Experience working with Data Frames, with a passion to grow in this area
Comfortable with data structures, distributed systems, and fault tolerant systems design
Excellent grasp of AWS and cloud computing
Self-starter, positive attitude with an emphasis on supporting the team and achieving long term goals

BONUS POINTS

The Data Engineer / Data Systems Architect need not possess these skills on Day 1, but would be encouraged to grow into many of these areas over time, and recognized as an especially strong applicant if any of these points were met:


Heroku, Postgres, Redshift, Airflow, Nifi, SQS, Lambda, Spark, Docker
Experience implementing Microservices
Strong competency in Data and Systems Security
Experience integrating with SaaS tools such as NetSuite, Zendesk

This position is a full-time, salaried position. It is located on-site at our office in New York, NY. We offer health insurance for both you and your pup, 401k, wonderful team lunches, unlimited PTO and a dog to pet anytime you wish.","New York, NY",Data Engineer / Data Systems Architect,False
854,"Key Role:
Design, implement, and manage databases and data delivery systems and transform it into beautiful insights, analysis, and reporting. Comprehension of database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, memory management and networking enables you to create structure and value out of complex and ambiguous technical challenges with little guidance.

Basic Qualifications:

Experience with structured and unstructured data, streaming and batch data processing, ETL, data wrangling, data ingest, and data access
Experience in a professional work environment
Knowledge of SQL, Python and Data Engineering fundamentals
Ability to work with databases to extract and transform data
Ability to obtain a security clearance
BA or BS degree

Additional Qualifications:

Experience with Hadoop is a plus
Knowledge of Cloud Services including AWS and Azure
Ability to work in a fast-paced environment

Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.

We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, or veteran status—to fearlessly drive change.","Herndon, VA",Data Engineer,False
855,"About the Value-Based Care Solutions Group
Value-Based Care Solutions Group is a leading software provider that leverages technology and analytics to help healthcare providers across the continuum of care effectively manage their financial, clinical, and human capital workflows. Offering a comprehensive suite of innovative technology-enabled solutions, the Company aims to improve quality, increase efficiency, and reduce waste in the healthcare industry. Veritas Capital, a leading private equity investment firm with significant experience in the healthcare technology space, acquired Value-Based Care Solutions Group from GE Healthcare in July 2018

Position Overview
The Data Engineer will work on data structure implementation both relational and non-relational. This is a hands-on position which is responsible for working with software engineering teams, support, as well as clients. Below are the responsibilities for this role.
Fine tune and optimize database performance by tuning the queries and troubleshoot database/programming objects
Advocate for database best practices and engineering excellence
Provide internal and external database support
Create and maintain automated tests and other utilities for testing database systems
Coordinates conversion and migration of existing (or legacy) databases to next generation DBMS's
Educate others on database techniques and technology by providing guidance and learning sessions
Analyze problems, develop solutions, and provide support to application analysis, development, and client support teams
Provide basic database modeling services for engineering team
Support and maintain data and database systems to meet business delivery specifications and needs
Lend support to various business and technology teams as necessary during design, development and delivery to ensure solid, scalable, robust solutions
Data Warehouse database development and administration, including schema design, ETL process, monitoring, performance, query/database tuning and backup strategy
Help identify best practices for standardization of data names, definitions, usage, and structures, and maintenance.
Configure, monitor and tune database software and systems
Configuration management and environment setups, including file organization, indexing methods, and security procedures for system databases
Create and maintain backup and recovery plans
Install and upgrade database software
Maintain database servers by installing patches and checking for hardware issues

Qualifications and Education Requirements
5+ Years of experience in a data engineer or database administrator role.
Bachelor degree in Computer Science, Statistics, Informatics, Information Systems, or related field.
Effective presentation and training skills
Experience with Microsoft and Windows applications including Work, Excel, and PowerPoint
Ability to prioritize and multi-task between multiple projects and tasks
Experience with Windows and Linux operating systems
Ability to effectively manage day-to-day tasks / activities in coordination with the engineering team to effectively meet the deliverables and schedule of a data solution component within a larger application project
Ability to work independently when needed
Must be able to communicate well with teams. Team oriented
Experience with Agile practices, service-oriented environments, and better development practices
Knowledge of Innovation methodologies and tools
Continually strives to self-educate and learn new skills and tools
Can perform the role of a database administrator
Intermediate knowledge of T-SQL. Ability to author queries and procedures for databases
Intermediate knowledge of ETL packages. Ability to author ETL packages and write data transformation scripts
Intermediate knowledge of analysis services cubes. Ability to create and maintain analysis services cubes
Intermediate knowledge of Microsoft SQL Server data services, integration services, and analysis services
Intermediate knowledge on SQL clustering, SQL replication and SQL Encryption
Intermediate knowledge of other database systems including MySQL and Oracle
Understands other database systems and can speak intelligently about them and leverage other techniques to provide value to a team/enterprise
At VVC Holding Corp we believe that diversity fuels innovation. VVC Holding Corp is committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, non-job related physical or mental disability, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.","Hartford, WI",Data Engineer,False
856,"Who we are

Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 244 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.

Note:
Provide full legal first Name/Family Name
DO: Capitalize first letter of First and Last Name. Example: John Smith
DON'T: Capitalize entire First and/or Last Name. Example: JOHN SMITH
NOTE: Use correct grammar for Names with multiple cases. Example: McDonald or O'Connell

Provide full address details
Resume is required
Multiple attachments can be uploaded including Resume and Cover Letter for each application


Job Description Summary:
Does it excite you to work on systems that process billions of dollars in payments per year in 195+ countries? How about making an impact on 200 million+ PayPal users around the world?


PayPal Core Payment is looking for a talented, creative, and passionate engineer to help automate and operationalize payments data for business processes and insight. As a self-motivated and enthusiastic member of our team, you will work with extremely talented peers in a fun environment building performance efficient, highly scalable, configurable and available systems. You will work in an agile environment with a focus on problem solving and engineering excellence.

Job Description:
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Big Data technologies.
Execute and Automate Extract, Transform & Load operations on large datasets.
Work with stakeholders including the management, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated, isolated and secured.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train new hires, if needed
Prepare and review operational reports or project progress reports.
Assist in the daily operations of the Architecture Team (a functional unit within the Business Intelligence Group), analyzing workflow, establishing priorities, developing standards, and setting deadlines.
Continued growth as a technical expert in data integration technologies and solutions.
Work with cross functional operations teams such as systems, storage and network to design technology stacks
Exhibits strong business knowledge and builds strong customer relationships



Skills Required:

5-10 Years of experience of the Eco system such as schedulers, databases,
 Linux/Unix systems, ETL, PL/SQL, Python, other ETL tools
Experience in exception remediation flows in informatica workflows and scripts.
Data warehousing experience, data models, Database design etc.
Practical experience with Git
Comfortable working with open source tools in a Unix/Linux environment
Comfortable handling large amounts of data
Works independently without the need for supervision.
Experience with Hadoop environment, Hive/Presto is a plus.

Subsidiary:
PayPal

Travel Percent:
0

Primary Location:
San Jose, California, United States of America



Additional Locations:






We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.


Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.","San Jose, CA",LE Big Data Engineer,False
857,"$125,000 - $155,000 a yearSan Francisco based Series C Data Analytics company is now expanding to their 4th location and have chosen Pittsburgh, and one of the first few hires they are looking to make for that office is 2 Master Data Engineers. Ideally folks who are adept with: Postgres and/or RDBMS, Python programming, strong with Sql programming and quires, coming from an opensource environment working with linux based systems, and ideally a background with AWS and elastic search. As mentioned, this person will be one of the first employees hired in the Pittsburgh office and will have an aggressive growth path, and eventually be building out teams around them, thus leadership qualities are important as well.Key Qualifications:3+ years of experience and strong understanding of RDBMS concepts, query optimization etc.2+ years of experience in programming with PythonExpert level Sql programmingExperience utilizing open source technologies in a Linux environment preferable with Centos in terms of flavorBachelors or above in the fieldLeadership experience and qualitiesFamiliarity with Linux/Unix work environmentBackground using modern ETL toolsPreferred Qualifications:Background working in a fully hosted AWS environment and/or experience with AWSexperience working for a startup or mid-sized companyExpert level Elastic Search experience*The Offer: Up to 155k/ year salary(DOE)Health, Dental, VisionLife Insurance401k with 4% matchPaid Holiday and VacationEquityJob Type: Full-timeSalary: $120,000.00 to $155,000.00 /yearJob Type: Full-timeSalary: $125,000.00 to $155,000.00 /yearEducation:Bachelor's (Preferred)","Pittsburgh, PA",Master Data Engineer (Partial Remote),False
858,"As the Data Developer, Software Solutions you will use your talent for data focused development to support the architectures and procedures of Horizon Media’s institutional data warehouse. This may include ETL procedures for data sources, development of the core data lakes, data marts, or the data warehouse itself. Knowledge of data pre-aggregated data structures (OLAP) and unstructured data (NoSQL) is required. Previous experience with AdTech or MarTech data sources is desirable.

Responsibilities:
Strategy/Business:

Knowledge of Complex Event Processing (CEP) models for streaming and/or Near Real Time (NRT) data updates
Experience with topic creation using stream processing tools such as Kafka and/or Map R Streams
Development expertise with Lambda architecture models including Hadoop, Apache Flink, Apache Spark, or similar
Ability to provide batch processing direction when coupled with Lambda NRT processing
Deep knowledge of coding SQL RDBMS structures including Functions (Tabular & Scalar), Stored Procedures, in-memory processing objects, etc.
Knowledge of MDX OLAP query language and familiarity with processing with tuples
Expert in query path and execution analysis with focus on performance optimization
Ability to logically troubleshoot existing routines quickly and resolve identified issues
Adhere to defined structure policies including naming conventions, population of
routine headers, code comments, etc.

Collaborate with Software Solution team members and other staff to validate
desired outcomes for code prior to, during, and post development

Provide unit testing on any/all deliverables
Propose innovations to existing procedures to provide better user experiences
with data routine
Communications:
Communicate with Integrations Analysts, Business Consultants, and the AD, Media Technology & Data, Software Solutions to update status on current projects, convey issues encountered, and educate them on determined issue resolutions
Communicate material enhancements to data structures or processes and convey the value of these changes to the business users

Qualifications:
5+ years experience working with large volume “Big Data” storage and processing
challenges

Well versed in Lambda architecture design and implementation
Proficient in the object oriented programing concept and design patterns
Familiar with SOAP and REST (full) protocol
Experience with multi thread/process programming
Works well in a continuous build and unit testing model
BS/BA degree in Computer Science, Statistics, Mathematics, or related field; or
equivalent experience. Masters degree preferred

(Desired not required) Knowledge of the media transactional process, order of
operations, financial procedures, and dependencies across all media channels

Desired Technology:
Office
MS Excel
MS Powerpoint
MS Project
MS Visio
Database
MS SQL Server
mySQL
Kafka
AWS RedShift
Hadoop/Flink
OLAP
SSAS/SSIS
BIDS
MDX

Visualization
(one or more)
Tableau Desktop
Qlikview
JasperSoft

Media
Sizmek
DoubleClick
STRATA
CoreMedia
Omniture

Certificates, licenses and registrations:
None

Physical Activity/Work Environment:
None

Additional Information:
Infrequent travel may be required (<10%)

The statements herein are intended to describe the general nature and level of work being performed by employees, and are not to be construed as an exhaustive list of responsibilities, duties and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of the employer.","New York, NY","Data Engineer, Software Solutions",False
859,"The Hartford’s Data Asset Management organization is seeking entry-level talent to build careers in Data in Hartford, CT. In this role, associates will grow in the Business Data Analysis track, providing analysis, reporting and research support for The Hartford critical data assets. Associates will participate in the entire software development lifecycle process in support of information data projects and grow with emerging technologies and processes.

Overall Knowledge

You must be a team player with a positive ‘can do’ attitude and possess the following qualities:
Interest in data analytics, technology, and problem solving
Excellent communication, analytical, interpersonal, and organization skills required
Organizational and time management skills with the ability to adjust to changing priorities in a fast-paced environment
Entrepreneurial mindset
Responsibilities
Leads planning sessions with clients and team members to solicit business requirements
Responsible for writing business requirements. Obtain signoff from business partners, and develop test cases
Will serve as point of contact with business partners and communicates project plans, resource estimates and performs status reporting
Coordinates testing of reporting solutions, reinforcing use of testing principles and processes
Analytical thinking and problem solving abilities and ability to manage multiple projects and customer expectations
Ability to work independently on multiple projects with varying levels of priority
Work and collaborate closely with our IT counterparts
Involved in the creation of use cases with the business, verification of data, business training, and educate business partners on impacts of development decisions and data issues
Ability to adapt to change and willing to learn and develop new skill sets as applicable
Ability to work and communicate effectively with both business and technical communities
Ability to network and influence across the organization at all levels
Effective self-starter with a can-do attitude who takes ownership and accountability for project deliverables with a demonstrated ability to multitask.
Qualifications
Qualifications
Bachelors degree – seeking December 2017 and May 2018 grads
A desired cumulative GPA of 3.0 or higher (out of 4.0 scale or equivalent) at the time of graduation
Desired majors include, but are not limited to: Computer Science, Engineering, IT, Management Information Systems, Data Analytics, Applied Mathematics, and Business.
Desire candidates with prior Data Analysis and/or Data Engineer competencies
Understanding of current and emerging IT products, services, processes and methodologies
Prefer working knowledge of ETL process and experience with SQL


Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression

HIGCLG
Job Function
: Business Data Analysis
Primary Location
: United States-Connecticut-Hartford
Schedule
: Full-time
Job Level
: Entry Level
Education Level
: Bachelor's Degree (±16 years)
Job Type
: Standard
Shift
: Day Job
Employee Status
: Regular
Overtime Status
: Exempt
Travel
: No
Job Posting
: Oct 4, 2018, 8:27:10 PM
Remote Worker Option : No","Hartford, CT",Associate Business Data Analyst,False
860,"The Data Engineer will create tools and data pipelines that use the latest advances in data engineering to address research and business questions across research and development enabling scientists to build predictive models/algorithms.
Additionally, the selected candidate will:
· Automate existing manual data and new data flow processes into Extract-Translate-Load (ETL) or Extract-Load-Translate (ELT) operations using fit-for-purpose tools.
· Write clean, maintainable data pipelines that feed data scientists/software developers through integration/presentation layer.
· Correct, transform and enrich multiple sources of data and provide quality reports.
· Quickly and efficiently load bulk and streaming data.
· Work closely with the data science team and internal business partners to identify the path to successful products.
Perform other duties as required.
Qualifications
The successful candidate will possess the following minimum requirements:

· Education and work experience:
o Bachelor’s degree in Information Systems Engineering/Management, Computer Science, Computer Information Systems, Applied Mathematics, or closely related field of study and six (5) years+ of documented work experience in in PostgreSQL, MongoDB, MySQL, ORACLE database queries and programming, including three (3) years+ of documented work experience using Extract-Translate-Load (ETL) toolsets;
o OR Master’s degree in Information Systems Engineering/Management, Computer Science, Computer Information Systems, Applied Mathematics, or closely related field of study and five (4) years+ documented work experience in in PostgreSQL, MongoDB, MySQL, ORACLE database queries and programming, including three (3) years+ of documented work experience using Extract-Translate-Load (ETL) toolsets.
· Experience programming in Pentaho or Informatica.
· Familiarity with data quality, cleaning and masking techniques, and handling unstructured data.
· Experience working across multiple computer environments to create workflows and pipelines (e.g. HPC, cloud, Linux systems).
· Ability to interact with a variety of largescale data structures, e.g., SQL, noSQL, HDFS (preferable).
· Demonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively.
· Demonstrated experience with algorithms and performance optimization.
· Demonstrated experience with data vault methodology and implementation experience.
· Strong adherence to data privacy standards and ethics.
· Strong interpersonal and communication skills and a demonstrated ability to work and collaborate in a team environment.
· Excellent written and oral communications.
· Prior work experience in healthcare, life sciences, and/or pharmaceutical industries preferred.
· Prior work experience with AWS cloud technologies and stack preferred; knowledge of distributed data processing and management systems, big data analytics platforms and/or worktools preferred.
· Ability to work on-site Monday through Friday during normal business hours (8:00 am – 5:00 pm) at client facilities located in Research Triangle Park, NC.
Ability to pass a background investigation and a drug test.
Unusual or Special Physical Requirements of Position
Examples:
Specify
10% of Time

· Lifting
· Climbing
· Crawling
· Special clothing/equipment (wearing)
· Unusual physical requirements
· High noise level
· Other
Requires sitting for extended periods of time at a desk (90%). Requires sitting at a computer terminal for long periods of time (90%). There is a possibility that due to parking availability and location of work area walking moderate to long distances can sometimes be required.


Description of Work Environment
Examples:
Specify
100% of Time

· Inside? Outside? Where
· Combination of above
· Extreme (hot/cold) temperatures involved
· Hazardous conditions - What are they
· Travel/transportation requirements
· Other
Inside office/cubicle environment. Requires ability to interact professionally with co-workers and all levels of management (100%).


Equipment and Machines Involved in Work Tasks
Examples:
Specify
100% of Time

· Office equipment/machines electrically/battery operated - which?
· Vehicles, forklifts, etc. operated which?
· Tools used in trade - which ones?
· Other
Requires ability to operate a personal computer, a telephone, copier, and other general office equipment (100%). Ability to conduct evaluation of third and fourth generation or current state of the art computer hardware and software and its ability to support specific requirements, interfacing with other equipment and systems.


Criticality of Attendance
Examples:
Specify
100% of Time

· Overtime, regular days, off days
· Shift, 2nd, 3rd
· Necessity for regular attendance
· Urgency for punctuality
Attendance is critical. Work hours are normally 8 hours per day and 5 days per week, Monday through Friday. Being prompt is important to provide continuous and on-going service to customers. Attendance is important to maintain continuity of service. Work outside of normal duty hours may be required with as little as one hour advance notice. Overtime is infrequent, but important when required (1%).

Other Essential Functions
List other essential functions here.

Must be able to communicate effectively, both verbally and in writing. Must be able to interface with individuals at all levels of the organization. Must be able to obtain unescorted access to work areas. Grooming and dress must be appropriate for the position and must not impose a safety risk/hazard to the employee or others.


Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. Jacobs is a background screening, drug-free workplace.","Research Triangle Park, NC",Data Engineer,False
861,"Lumen is seeking a highly motivated, innovative Data Engineer & Architect to build and maintainsystem architectures for data processing. They will construct, administer, and improve componenttechnologies, including analytical databases and data pipelines to feed analytics solutions and reports.Data solutions established and maintained by the Data Engineer will collect data from a variety of sources (e.g.laboratory instruments, sensors, probes, LIMS). They will parse, transform, integrate, store, and make data available in formats useful for querying and analysis. These solutions will ultimately enable the generation of reports and dashboards that provide critical insights to scientists and managers.As an early information technology hire for the company, the Data Engineer will have the opportunity to work collaboratively on a variety of implementation projects and initiatives. The data environment created by the Data Engineer will help to kick-start Lumen’s analytics capabilities, supporting Lumen’s development of food, vaccine, and therapeutic solutions produced in the blue-green algae, Spirulina.Essential Job FunctionsThe major tasks for this position are as follows:Develop, build, test and maintain database and data processing system architecturesArchitect appropriate data solutions to meet research and business needs while ensuring fit within the broader platform ecosystemCollaborate with consultants and contractors to provide sound data architecture built for performance, reliability, and securityCreate and maintain best practices, policies, procedures, and standards for data architecture and testing/improvement processesDocument data designs and data flow diagramsDevelop analytical databases by designing proposed systems and creating their structure and functional capabilitiesSetup and populate databases, participating in the creation of table schemasAdminister databases, managing access, permissions, and backups to ensure data security and integrityTest and debug database architectureEmploy a variety of languages and tools to combine data sources and create reliable pipelinesDesign and implement ETL scripts to integrate and transform data into useful formats for analysis and interpretation by researchersPerform legacy data migrationsFacilitate and help lead IT implementation projects and initiativesAssist in the development and implementation of standard data tables, reports, dashboards, and visualizations supporting research and business goalsGather requirements from end users (e.g. scientific researchers) and facilitate the translation of these requirements into analytics solutionsCommunicate with end users to ensure meeting of requirementsProvide end-user support, documentation, and training where appropriateResearch opportunities for additional data acquisition and automation; work closely with scientists and managers to determine which data are neededProvide recommendations on how to expand data collection and improve data reliability, efficiency, and qualityEducation and Experience RequirementsB.S. degree in Computer Science, or related field, or equivalent work experienceIn-depth knowledge and experience with cloud (e.g. AWS, Google Cloud, Azure) architecture best practices, security, design, implementation, and operational supportExperience designing data systems/solutions to manage complex data in complex, distributed systemsIn-depth knowledge of databases and best engineering practices with 5+ years of experience in database programming, design, and analysisDemonstrated experience building analytical, relational databases and developing advanced SQL queriesProficiency in shell scripting and one or more scripting languages (including Python)Demonstrated experience with ETL integration developmentExperience processing large, multi-dimensional datasets from multiple sourcesExperience in development of professional database reporting with business intelligence tools (e.g. Tableau)Desired QualificationsFamiliarity with DevOps conceptsBroad range of technical skills covering systems and applicationsExposure to life science and research environmentsFamiliarity with 21 CFR Part 11 requirementsWork experience in regulated data environmentExperience estimating technical solution buildsThe successful candidate will have the following attributes:Excellent collaboration and communication skillsSelf-starter with ability to work as part of a diverse team and/or individuallyHigh integrity and ethicsCommitment to quality and timely delivery of resultsFlexible outlook for finding the best solution based on constraintsPhysical RequirementsStand or sit for extended periods working with computer screens (2 or more hours at a time)How to ApplyPlease submit a resume and cover letter via email address provided below with the job title in the subject line.This position is available immediately. Applications will be reviewed upon receipt. Only successful applicants will be contacted.About LumenBased in Seattle, Washington, Lumen Bioscience has developed proprietary technologies for synthesizing high-value products in a novel platform organism. Lumen completed its Series A financing in mid-2017 and is using these technologies to produce novel products that solve long-standing problems in human and animal health and nutrition. The Lumen team works out of a beautiful lab and office facility in Fremont, a picturesque neighborhood on the north shore of Lake Union in the heart of Seattle. At Lumen, you'll be challenged, you'll be inspired, and you'll be proud to be part of an innovative organization making a real impact on improving the quality of life globally.Lumen is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.NOTE TO EMPLOYMENT AGENCIES: Lumen values its relationships with recruitment partners and will only accept resumes from those partners that have been contracted by a member of the human resources team. Lumen is not responsible for any fees related to resumes that are unsolicited or are received by any employee of Lumen who is not a member of the human resources team.Job Type: Full-timeEducation:Bachelor's (Preferred)","Seattle, WA 98103 (Green Lake - Wallingford area)",Data Engineer & Architect,False
862,"Job Description

Information Technology at Abercrombie & Fitch is fundamental to designing, sourcing, developing, and delivering fashion-forward merchandise to our customers. We are committed to implementing new strategic and systematic approaches to generate dynamic technology solutions for our growing business. As an expert on the Data Science team, a Data Engineer is responsible for setting up datasets for data mining, building predictive models, and building insight apps. Our Data Science focus surrounds core retail areas including planning, merchandising, inventory management and real estate. This position will work closely with other Data Science team members and our business partners.
What will you be doing?
Use data engineering skills to build mineable datasets.
Mine prepared datasets for initial descriptive or anomalous insights.
Build accurate and reproducible supervised & unsupervised models.
Work with business owners to understand context within datasets & processes.
Deliver insight to leadership, helping drive strategic business decisions.
Hypothesize, experiment & drive insight by any means necessary.
Drive innovation by evaluating business processes and identifying data science solutions.
What will you need to bring?
A self-starter with a can-do attitude and resilient work ethic.
3+ years’ experience with coding in Python (pandas, numpy, scipy, etc).
1+ years’ experience with data warehousing/BI technologies including one or more of the following or similar: Hadoop (i.e. HBASE, Hive, MapReduce, Sqoop, Spark, etc), Netezza and/or DataStage (ETL).
Some experience developing complex SQL and database views in a large data warehouse environment.
Knowledge of web app development with high-level framework (like Flask or Shiny).
Experience munging/wrangling data to create workable datasets from messy or noisy data sets.
Experience with data processing techniques such as dimensionality reduction, normalization, imputation, and feature extraction.
Experience developing reproducible prediction/forecasting model(s) such as deep learning, neural nets, decision trees, GLM, ARIMA, regression, etc.
Well-practiced in version control with git.
Experience with virtual environments or dockers for containerization.
4-year degree in math, statistics, engineering, or information technology
2+ years’ full-time work experience
Nice to Haves
Experience with coding in R (dplyr, Shiny, ggplot, sparklr, etc)
Experience with unguided problem formulation or hypothesis development.
Experience with optimization techniques such as genetic algorithms, simulated annealing, etc.
Experience with data visualization (with tools like matplotlib, Tableau, ggplot, plotly, etc).
Knowledge of retail problems such as product classification, demand forecasting, supply chain optimization, etc.
Knowledge of NLP and text analytics.
Knowledge of web-scraping.

Qualifications

null

Additional Information

ABERCROMBIE & FITCH CO. IS AN EQUAL OPPORTUNITY / AFFIRMATIVE ACTION EMPLOYER © ABERCROMBIE & FITCH CO. 2012","Columbus, OH",Data Science Engineer,False
863,"Temporary, InternshipDescription

We are looking for a tech savvy engineer to join our innovation team and shape the future of the automotive industry.

This position will give you an opportunity to work alongside Enterprise Data Engineering Teams to migrate from our on-prem Hadoop environment to a next-generation Big data analytics platform in AWS. This platform will serve as a foundation for new products and services and enable other engineers and data scientists in solving real world data problems.

Key Responsibilities:
Participate in Agile ceremonies to estimate, refine, implement, and showcase features.
Collaborate with other developers to migrate key data ingestion workflows to AWS.
Collaborate with Data Engineers to design, implement, test, and automate data transformations to derive new strategic datasets.
Troubleshoot and resolve workflow issues on-prem and in AWS.
All other duties as assigned.

Qualifications

Understanding/experience with cloud technologies.
Experience developing in languages such as Java or Python.
Demonstrated proficiency with Structured Query Language (SQL).
Working knowledge of Linux operating system.
Overall ability to use best practice tools and write/modify scripts.
Working towards degree in computer science or related degree (graduate or undergraduate)



About Cox Automotive
Cox Automotive Inc. makes buying, selling and owning cars easier for everyone, while also enabling mobility services. The global company’s 34,000-plus team members and family of brands, including Autotrader®, Clutch Technologies, Dealer.com®, Dealertrack®, Kelley Blue Book®, Manheim®, NextGear Capital®, VinSolutions®, vAuto® and Xtime®, are passionate about helping millions of car shoppers, tens of thousands of auto dealer clients across five continents and many others throughout the automotive industry thrive for generations to come. Cox Automotive is a subsidiary of Cox Enterprises Inc., a privately-owned, Atlanta-based company with revenues exceeding $20 billion. www.coxautoinc.com
Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual’s age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.

Organization: Cox Automotive

Primary Location: US-GA-Atlanta-6205 Peachtree Dunwoody Rd NE

Employee Status: Temporary

Job Level: Intern/Co-op

Shift: Day Job

Travel: No

Schedule: Full-time

Unposting Date: Ongoing","Atlanta, GA",Data Engineer Intern (Summer 2019),False
864,"Description:
Seeking a Data Engineer that will play a key role in helping realize the organization’s dedication to creating data-driven solutions to improving the quality and lowering the costs of healthcare delivery in Camden, NJ. This person will be responsible for helping to develop and optimize a nascent data warehouse of clinical, social service delivery, and other individual-level data captured from a multitude of internal and external sources. The Data Engineer will help build data integrations, develop data best practices and governance, perform clinical and administrative reporting, and help optimize data flow and collection for cross functional teams. This person should be a problem solver, equipped with a variety of approaches and tools for handling complex problems that arise in large, messy data sets and someone who enjoys optimizing data systems and building solutions from the ground up. The Data Engineer will support our Information Architect, Data Analysts, and Data Scientists, as well as non-technical colleagues who generate and use the data on a daily basis.

Essential Functions


Data Modeling – evaluate structured and unstructured data, determine the most appropriate schemas for new fact tables, data marts, etc.
Data Integration – incorporate new and legacy data into the Coalition’s nascent data warehouse, while maintaining enterprise best practices and adhering to data governance standards.
ETL – help implement ETL procedures to apply business rules to our data as we migrate from source to target and validate data to ensure quality.
Reporting – collaborate with data and non-technical colleagues to scope requests. Extract data from various data sources, create relevant data visualizations, share with requesters, and validate results.
Rapid Feedback Analytics – develop, maintain, and automate, as appropriate, operational dashboards that foster a culture of end-user data literacy and critical inquiry
Governance / Best Practices – help develop, document, and adhere to data governance standards. Also educates and supports colleagues in best practices to ensure that data is used appropriately.
Assemble large, complex data sets that meet business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data using SQL and various programming technologies.
Develop analytics tools that utilize data resources to provide actionable insights, operational efficiency and other key business performance metrics.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.

KNOWLEDGE, SKILLS, and ABILITIES


Strong Proficiency in SQL (PostgreSQL preferred)
Competence in a high-level general purpose programming language (Python preferred)
Exposure working with APIs
Exposure to statistical data analysis tools: R, SPSS, MATLAB etc (R preferred).
Exposure to visual analytics tools (Tableau preferred)
Familiarity with electronic health record and medical financial systems
Experience performing root cause analysis on code and data processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large, messy and disconnected datasets.
Strong communication, project management and organizational skills.
Familiarity with cloud-based hosting environments (Azure preferred).
Strong analytical and problem solving skills.

Requirements:
3+ years of Business Intelligence / Data Warehousing experience, preferably in a Data Engineering role within a healthcare environment.

Bachelor’s or Advanced Degree in Computer Science, Informatics, Information Systems or other quantitative field preferred","Camden, NJ 08102",Data Engineer,False
865,"The Information Analytics Specialist provides data analysis support to the customer by assisting with development of reports and/or dashboards to monitor program and operational performance. Promote self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analysis and performs ad-hoc data analysis as directed. As a subject matter expert, this position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics.

The ideal candidate will have solid experience with SQL, use of business intelligence tools and detailed data analysis.

Education Level
Bachelor's Level Degree - Experience in lieu of education: Yes

Experience
Required: Data Analysis - 1 year, Financial Analysis - 1 year, Information Technology - 1 year, Statistics - 1 year

Preferred: None, unless noted in the ""Other"" section below

License
None, unless noted in the ""Other"" section below

Skills
Required: Communication, Complex Problem Solving, Critical Thinking, Service Orientation, Writing

Preferred: None, unless noted in the ""Other"" section below

Other
4 years related experience required in lieu of education.

Data Visualization & Analytics Consultant/Data Engineer – Skill Set: Strong analytical, statistical, and problem solving skills utilizing current visualization tools, i.e., Tableau, Qlik, Power BI, etc. Advanced knowledge and experienced SQL coder with working knowledge of other analytical and statistical packages such as Python and R. Knowledgeable of current informatics infrastructure, data structure, information systems and workflows. Excellent written/verbal communications skills; Excellent interpersonal skills and customer management.","Virginia Beach, VA 23450 (North Central area)",Information /Data Analyst,False
866,"Circle is a global internet finance company, built on blockchain technology, powered by crypto assets, and dedicated to helping people everywhere create and share value.

We've already made sending money around the world free and easy using blockchain technology with Circle Pay. With Circle Invest, we're expanding our offerings with a cryptocurrency investment product, enabling anyone to buy and sell crypto assets. Through Circle Trade, we're market makers for the top crypto coins and offer OTC trading services. In March 2018, Circle acquired Poloniex, one of the world's leading token marketplaces.

As a Data Engineer at Circle, you'll work closely with the Data Science team productizing machine learning solutions and developing the scalable data pipelines that support these solutions.

Working in either Boston or New York, you'll have the opportunity to significantly impact internal data consumption and decision-making (risk and compliance, growth, and marketing) as well as our customer experience.

Projects we are working on:

Productize machine learning models and build a world-class automated evaluation system to manage risk exposure while delivering the best possible experience to our customers
Design, build, and implement scalable streaming data pipelines and ETL frameworks to increase data access and decrease analysis and decision times across the organization
Build an open transaction protocol where users can send money and exchange value that utilizes both new and traditional technologies to deliver value the fastest and cheapest way possible
Leverage blockchain technology in a new transaction framework so that Financial Institutions an exchange value and associated metadata in a compliant manner.

You will also work on:

Own software throughout the entire development life-cycle
Share ideas to improve our product and processes, make decisions that have significant impact, and provide feedback so we can continue to get better
Develop your skills, collaborate across teams and continue to learn.

What you'll bring to Circle:

Experience writing high quality code in Python plus another OOP language (Java, Scala, C++, Go, etc)
Experience working with RDBM systems, particularly familiarity with SQL
3+ years experience building distributed solutions in Spark, MapReduce or other MPP system with associated data models and datastores (e.g., Redshift, Cassandra, HBase, Parquet)
Production development of event-based applications using frameworks such as Kinesis, Kafka, Spark Streaming, or similar
Familiarity with machine learning techniques, continuous deployment pipelines and tools, and AWS technology stack a plus
Desire to work across internal teams to identify requirements and iterate on solutions
Debug complex production issues across various levels of the tech stack
Interest in impacting the way money moves between people
Experience/familiarity with Slack, Mac OSX and GSuite

Circle was founded in 2013 by internet entrepreneurs Jeremy Allaire and Sean Neville. We're backed by $250 million from investors including Jim Breyer (Facebook), Goldman Sachs, IDG Capital (Baidu, Tencent), General Catalyst (AirBnB, Snapchat), Accel Partners, and Bitmain, with offices in Boston, New York, San Francisco, Dublin, London and Hong Kong.

Check us out at circle.com ( http://circle.com/ ) and download Circle Pay & Circle Invest for iOS and Android today.

We are an equal opportunity employer and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

About your personal information

We respect your privacy and are committed to protecting your personal information. Please refer to our candidate privacy notice here. ( https://www.circle.com/en-gb/candidate-privacy-notice ) for more information on how we will be using your personal information. By submitting your application, you agree that you have read and understood the candidate privacy notice ( https://www.circle.com/en-gb/candidate-privacy-notice ).","New York, NY","Software Engineer, Data",False
867,"Why We Work at Dun & Bradstreet

Life here at Dun & Bradstreet is changing – for the better. With almost two centuries of experience and a new modern vibe, work at D&B has never been more exhilarating. Our purpose is to grow the most valuable relationships in business by uncovering truth and meaning in data. We’re wildly passionate about our purpose, and it has us evolving everything we do – from how we engage with our customers to how we energize one another. So if you thrive in a fluid, agile culture but want the solidity of a storied and commanding brand, come join us!

Content is a global team delivering thought leadership and inspiration by building strategic relationships through modern experiences so our customers can grow. Our key focus areas are to 1) uncover truth and meaning from data, 2) drive content/value through data; 3) leverage modern technology, analytics and platforms, and 4) build relationships with influencers. There’s never been a more exciting time to join the team.

A Data Engineer’s role involves developing applications designed to accumulate, derive meaning from, and apply stewardship to, large datasets. A Data Engineer will have to both be able to work with the Global People Data team’s internal datasets and tools, as well as be able to coordinate with outside groups to continuously meet their needs. A Data Engineer is expected to be a key developer of Global People Data tools and products, both in maintaining and upgrading existing tools and in developing new products using state of the art techniques and programming concepts.


Responsibilities:
Develop new applications in a variety of programming languages
Take ownership of existing applications for further development/improvements
Work closely with related groups to ensure business continuity
Perform analysis on large datasets to make and implement recommendations for maximizing customer experience
Work as a member of one or more agile teams, using lean principles and SCRUM methodology

Requirements:
Bachelor’s degree (preferable in computer science or a related field)
2 - 5 yrs. experience with SQL
2 – 5 yrs. experience with Python
Ability to work closely with others to problem solve
Experience with hosted environments (AWS, and Azure recommended)
Understanding of software testing methodologies
Understanding of Unit testing and Automated Test tools


Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.

We are committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Dun & Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to TalentAcquisitionTeam@dnb.com. Determination on requests for reasonable accommodation are made on a case-by-case basis.

Please note that all Dun & Bradstreet job postings can be found at https://dnb.wd1.myworkdayjobs.com/Careers and all communication from Dun & Bradstreet will come from an email address ending in @dnb.com.","Waltham, MA",Data Engineer,False
868,"Are you looking for a full-time opportunity as a Senior Data Engineer in Scottsdale? As the Senior Data Engineer, you will research, evaluate and recommend appropriate technology and strategy for building products and delivery services. You will consult with customers on custom project requests, gather specifications, document statements of work/quotes for clients, develop and validate reports, coordinate with development team to implement specific requests, test, deliver and close projects.Benefits & Features: Comprehensive benefits with 401k matchCover 100% of employee insurance premiumsOpportunity to have a direct impact on the organization as a technology expertChance to represent the organization to clients and develop solutionsQualifications: Bachelor's Degree in Information Systems, Business Administration or related field7 to 10 years of SQL and C# experience, including infrastructure, development, SDLC and databaseExpert SQL skills-able to do SQL queries and leverage that knowledge.Able to do Data Optimization strategies, know how to manipulate unstructured data-more the thought process and predicting what could occur and what needs to be done to get the right results.Solid understanding of Development, specifically C#Strong understanding of business intelligence and reporting toolsExperience product implementation and customization to meet client needsOrganized, Project Management experience or Implementation coordinationAbility to learn new technologies quickly and ability to overcome obstaclesJob Type: Full-time","Scottsdale, AZ",Data Engineer,False
869,"Job Description:

We are seeking a Test Data Engineer with a minimum of 5+ years of experience based in our Pleasanton location.
Job Responsibilities:
Collaborates with application SMEs, project teams, and Data Management to establish test data requirements
Ability to understand business process information and associated data flows
Leads the analysis of test data requests, defines test data characteristics, and the design, development, and implementation of test data automation
Basic Qualifications:

Design, develop, and implement automated extraction, transformation, and load processes for test data
Collaborates with application SMEs, project teams, and Data Management to establish test data requirements
Ability to understand business process information and associated data flows – protocols (HL7, EDI etc.,) for each Health care applications such as EPIC, Tapestry, Membership system and other health care related systems etc.,
Ability to work with various databases (e.g. Oracle, SQL Server, Informix, DB2 etc.) and file formats (e.g. XML, MISMO, CLDF, EDF etc.)
Must have any of the following scripting experience python, shell, JAVA, Jscript.
Should have experience in Jenkins or any CI tools
Experience of industry ETL tools (e.g. Informatica, TDM, Grid Tools, or Oracle Test Data Management, etc.)
Leads the analysis of test data requests, defines test data characteristics, and the design, development, and implementation of test data automation.
Creates and validates test data all phases of testing
Ability to liaise with Business, QA and Dev stakeholders to understand processes, Data and system specs and test data requirements
Knowledge of data models and entity relationship diagrams
Knowledge and experience in data masking methodologies
Prior experience with Healthcare domain preferred
BA of Computer Science or related discipline.
Preferred Qualifications:
Good to have understanding on various databases (e.g. Oracle, SQL Server, Informix, DB2 etc.) and file formats (e.g. XML, MISMO, CLDF, EDF etc.)","Pleasanton, CA",Test Data-Engineer,False
870,"Requisition ID: 29059

Our reliability is one of the best in the nation, and we’re working to make it even better. We live here too. That’s why we’re committed to making Florida a better place. Join our team today Learn more

Position Specific Description

Smile you’ve found us! At NextEra Energy, our people are our greatest asset and our Enterprise Data Services Team is looking for a principal Big Data Engineer to join the organization! In this role you will be responsible for establishing and maintaining an enterprise data platform which will utilize the latest in big data technologies in order to enable a wide variety of analytical solutions across the enterprise.

Candidates with the following experience will be highly considered:
Designing, Architecting, and Developing solutions leveraging big data technologies (Open Source and/or AWS) to ingest, process and analyze large, disparate data sets
Expertise in data modeling, structured, unstructured or semi-structured data processing for cloud and on premise implementations
Experience in Big data stores such as Hadoop/Cassandra/MongoDB, AWS S3, RedShift and traditional data stores such as Oracle, SQL Server, Greenplum, and SAP BW/HANA.
Hands-on experience with AWS Big Data technologies including but not limited to: AWS, EMR, Hortonworks, Cassandra, Mongo, Redshift and Kafka
Designing and deploying dynamically scalable, highly available, fault-tolerant, and reliable applications on AWS
Experience in building micro-services, Web Services, Map Reduce, Spark based Analytics processing


Job Overview

Employees in this role are responsible for all activities associated with the administration of either mainframe or distributed computerized databases. This role leads database administration teams and/or projects.



Job Duties & Responsibilities

Designs, implements, maintains, and monitors databases
Develops, documents, and implements guidelines, standards and procedures, maintenance of database dictionaries, and integration of systems through database design
Performs other job-related duties as assigned



Required Qualifications

High School Grad / GED
Bachelor’s or Equivalent Experience
Experience: 7+ years



Preferred Qualifications

None



Employee Group: Exempt
Employee Type: Full Time
Job Category: Information Technology
Organization: Florida Power & Light Company
Location: Juno Beach, Florida
Other Work Locations: Florida
Relocation Provided: Yes, if applicable

NextEra Energy is an Equal Opportunity Employer. Qualified applicants are considered for employment without regard to race, color, age, national origin, religion, marital status, sex, sexual orientation, gender identity, gender expression, genetics, disability, protected veteran status or any other basis prohibited by law. We are committed to a diverse and inclusive workplace.

If you require special support or accommodation while seeking employment with NextEra Energy, please send an e-mail to AskHR@NEE.com, providing your name, telephone number and the best time for us to reach you. Alternatively, you may call 1-844-694-4748 (Option 1, Press 6) between 8 a.m. and 5 p.m. EST Monday-Friday. Please do not use this line to inquire about your application status.

NextEra Energy will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.

NextEra Energy does not accept any unsolicited resumes or referrals from any third-party recruiting firms or agencies. Please see our policy for more information.
#LI-AM1","Juno Beach, FL 33408",Big Data Engineer,False
871,"$80 - $90 an hourContractJob requirements: Qualifications and Skills5-10+ years of experience in data, web or mobile services.Data architecture skills.Experience in SQL or similar languages and development experience in at least one scripting language (Python, Perl, etc.).Experience with Redshift and other AWS technologies8+ years of experience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, Spark etc.)8+ years utilizing Object Oriented design and programming with Scala/Python skills to design, develop, and maintain large-scale web applications.Experience with data sets, Hive, and data visualization tools is a plus.BA/BS in Computer Science, Math, Physics, or another technical field.Additional bonus:Deep knowledge of machine learning, information retrieval, data mining, statistics, NLP or related field.3+ years of experience building machine learning networks to scale problems at scale with Knowledge using sci-kit learn, spark MLlibetc.Job Type: ContractSalary: $80.00 to $90.00 /hourExperience:spark: 5 years (Preferred)Java/Scala/Python: 5 years (Preferred)Big data: 5 years (Preferred)","Mountain View, CA 94043",Spark developer (Data Engineer),False
872,"$50 a dayTechnical Requirements
 Strong Experience working with large data sets, experience
working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
 Knowledge/experience on Teradata Physical Design and
Implementation, Teradata SQL Performance Optimization
 Experience with Teradata Tools and Utilities (FastLoad,
MultiLoad, BTEQ, FastExport)
* Advanced SQL (preferably Teradata) *
* Strong Hadoop scripting skills to process petabytes of data *
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
 Experience in ETL/ processes
 Automic scheduler

Engineering Skills
 Very Strong engineering skills. Should have an analytical
approach and have good programming skills.
 Provide business insights, while leveraging internal tools and
systems, databases and industry data
 Excellent written and verbal communication skills for varied
audiences on engineering subject matter
 Ability to document requirements, data lineage, subject matter
in both business and technical terminology.
 Guide and learn from other team members.
 Demonstrated ability to transform business requirements to code,
specific analytical reports and tools
 This role will involve coding, analytical modeling, root cause
analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
 Strong analytical background
 Self-starter
 Must be able to reach out to others and thrive in a fast-paced
environment.
 Strong background in transforming big data into business insights
 Experience in retail business will be a plus.

Nice to Have
 Development experience with Java, Scala, Flume, Python
 Cassandra
 Hbase
 Tableau or similar reporting/dash boarding tool
 Modeling and Data Science background
 Retail industry background

Education BS degree in specific technical fields like computer science, math, statistics preferred","Bentonville, AR",Senior Data Engineer.,False
873,"$120,000 - $145,000 a yearDriven by CuriosityAt Vistaprint, we believe that everything we do has a lasting impact on our customers and on each other. It begins and ends with a passion for helping our customers succeed. To give them our best, we empower our team with the autonomy they need to make smarter decisions and pursue higher value. We thrive on exploration, collaboration, and helping every customer grow their business beyond imagination. Fueled by technology and innovation, we are so much more than business cards.About Our TeamThe Data Engineering Team's mission is to enable our internal business partners with the information to make better decisions for our customers. All of our team members collaboratively work together to design, build, and sustain our Data Ecosystem, which includes technologies like our Data Warehouse, Data Lake, and proprietary predictive analytics tools. We build the right thing, the right way through positive relationships with Marketing, Analytics, and Finance.When you join our team, you will have a fantastic opportunity to advance your career by working in cutting-edge technology and growing your business expertise.Putting Your Expertise to WorkAs a Senior Data Engineer, you will help us scale our Data infrastructure to expand our proprietary predictive analytics and Big Data capabilities. You will work with our vast Data Ecoysystem, including our Data Warehouse and Data Lake, to build out our platform and provide tooling to empower our internal customers. Your technical experience and business insight will directly drive real business results. Your ability to seek complex challenges and collaborate in a team environment will mean you have an immediate impact.Essential FunctionsDesign, develop, and support our Data infrastructure utilizing various technologies to process terabytes of data, including SQL, Python, Microsoft Azure, and AWS.Create solutions to enable diagnostic and predictive analytics capabilities.Partner with the Analytics, Marketing, and Finance organizations to get feedback and iterate upon the Data Ecosystem developmentDevelop components and distributed ETL systems for our suite of large data platformsBe curious about trends and emerging technologies in the Data space, participate in user communities, and share what you learn with your teammatesExperience We're Looking ForFamiliarity with developing data processing solutions and data applications utilizing technologies like Python, C#, Java, SQL, Spark, or No SQL DBExperience working with all kinds of data-- clean, dirty, unstructured, semi-structured and relationalProblem solving and multitasking ability in a fast-paced, globally distributed environmentStrong communication skills, including the ability to work with business partners to understand and refine requirementsExperience with developing end-to-end data pipelines in large cloud-compute infrastructure solutions such Azure, AWS and Google is a plusMinimum of 1-2 years working directly on Big Data technologies preferred#LifeinVistaprintJob Type: Full-timeSalary: $120,000.00 to $145,000.00 /yearExperience:Cloud Computing: 1 year (Preferred).Net: 1 year (Preferred)Work authorization:United States (Required)","Boston, MA",Senior Data Engineer,False
874,"About Us!

At Maven Wave, we are relentless in hiring the industry’s top talent. Each employee is hand-picked not only for their skills, but for their personality and broad expertise. We are looking for this rare combination of talent that sets us apart in the industry.
Founded in 2008, Maven Wave has experienced rapid growth. We combine the experience and knowledge of a management consulting firm with the innovation and technology expertise of a cloud services firm, providing a truly unique work environment. Employees have the opportunity to gain invaluable experience and make a significant impact on the business outcomes of our clients and our company.

Over the past years, Maven Wave has received the following awards and accolades:
Google Cloud North America Services Partner of the Year, 2018
#21 Best Workplaces in Chicago, FORTUNE, 2018
Great Place To Work Certification, Great Place to Work, 2017 & 2018
Fast Fifty, Crain's Chicago Business, 2014, 2015, 2016, 2017, 2018
101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR), 2014, 2015, 2016, 2017, 2018
Top Google Cloud Partner, Clutch, 2017
Fastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine, 2016, 2017
Top IT Services Companies, Clutch, 2015
Google Global Rising Star Partner of the Year 2015

Maven Wave, Google, and YOU: help us build and deliver data driven cloud solutions.
We are looking for a skilled Big Data / Cloud Data Engineer to join our team. The ideal candidate has extensive experience building and implementing complex data solutions in the Cloud (AWS, Microsoft, and/or Google). The primary job role will be designing, building, and testing data ingestion and ETL programs with a strong focus on performance and data quality management.
Responsibilities:
Designing, building, and testing data ingestion and ETL programs from a variety of source systems (both unstructured and structured).
Uncovering and recommending remediations for data quality anomalies.
Investigating, recommending and implementing data ingestion and ETL performance improvements
Document data ingestion and ETL program designs, present findings, conduct peer code reviews
Develop and execute test plans to validate code
Work in a collaborative, agile environment
Requirements:
Bachelor's degree in a technical or quantitative field with preferred focus on Computer Science or Information Systems
4+ years experience building complex ETL programs with one of the following Informatica, DataStage, Spark, Dataflow, etc
Expert in data extraction experience
3+ years experience developing complex SQL
Experience using Cloud Storage and computing technologies such as BigQuery, RedShift, Snowflake
Experience configuring and developing big data solutions in a Cloud environment (AWS, Microsoft, or Google)
3+ years experience programming in Python, and/or Java
3+ years with UNIX shell scripting
2+ years experience with Git
2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years experience developing complex technical and ETL programs within a Hadoop ecosystem (Hadoop, YARN, Hive, Pig, Sqoop, Spark)
2+ years implementing and programming data ingestion and ETL programs with large datasets (10+ Terabyte analytical environment)
Experience with integration of data from multiple data sources
Experience developing and implementing streaming data ingestion solutions
3+ years working with relational database technologies
3+ years investigating, recommending and implementing solutions that resolve data quality issues
Demonstrated independent problem solving skills and ability to develop solutions to complex analytical/data-driven problems
Demonstrated experience developing data analytics solutions within AWS and/or Google Cloud Platform
Must be able to communicate complex issues in a crisp and concise fashion to multiple levels of management
Excellent interpersonal skills necessary to work effectively with colleagues at various levels of the organization
Check out our Data Team!","New York, NY",Cloud Data Engineer,False
875,"The Data Engineer is responsible for designing, developing, and supporting data management solutions. The position will develop data models, perform data analysis, construct technical designs, develop data integration solutions, collaborate with team members and business stakeholders, and support existing data solutions. This position will also lead and coordinate the work activities of offshore development and support resources.

People or Process Management Responsibility: Works on multiple, complex projects as technical lead and coordinates the work of other technical BI resources.

Position Responsibilities may include, but not limited to:

Responsible for the solution architecture, design, development, and support of data management applications

Drive data sourcing and integration solution design and development on hybrid (cloud & on-prem) data solutions

Perform data analysis and architect data models for analytics

Providing guidance and direction to offshore ETL support/development resources

Collaborate with cross functional teams such as Infrastructure, Support, DBA and Business team

Assist with task identification and effort estimates for ETL development

Assist with risk and issue identification and resolution

Provide off-hour/weekend ETL support (on a rotating basis)

Other duties as assigned

Position Requirements
Position Requirements:

Position Requirements: Minimum Requirements:

4 year degree in Computer Science, Information Systems

5+ years of relevant Business Intelligence/Data Warehousing/Data Integration work experience

4+ years of Hands-on experience developing data management solutions using MSBI (SSIS)

2+ years Data modelling (logical/physical, relational and document/object) and 2+ years of Data Integration solution design experience

1-3 years of developing solutions using Windows Server OS

Strong understanding of Dimensional Modeling techniques

Experience leading other developers

Strong written and verbal communication skills

Strong problem solving and analytical skills

Strong understanding of SDLC best practices with an emphasis on DW/BI practices

This position must pass a post-offer background and drug test



Preferred:

Experience with one of the ETL tools – MS SSIS, Informatics on SQL Server DB

1+ year of Public cloud experience (Azure, AWS)

Proficiency in scripting languages such as Power Shell

1+ year of experience leveraging Cloud Services (IaaS, PaaS)

1+ year hands-on development experience using open source data integration tool such as Talend, Azure Data Factory

Experience leading offshore resources

ITIL certification

Experience in Agile (SCRUM) and Waterfall methodologies

Distribution and Logistics Industry experience

Physical Demands and Work Environment:

The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Due to the nature of our business in regard to such things as delivery schedules, order inputs, selection, and Department of Transportation Hours of Service; overtime, attendance and punctuality are essential job functions. Should an individual in this classification not be able to adhere to this requirement due to a disability, they should contact their Human Resource department to see what, if any, reasonable accommodation may be made.","Rosemont, IL",Data Engineer,False
876,"Overview of the Role:
The Staff Data Engineer works on a team of Data Engineers and Administrators supporting a data platform centered around what is today a SQL Server Enterprise data warehouse. This platform holds clinical, financial, and population health data for our 6M+ patients, and serves business critical needs for our product and analytics teams. Our warehouse is central Privia’s results oriented culture, having leveraged our platform to achieve industry-best revenue cycle awards and earned ongoing benefits from 1- and 2-sided risk performance.
We are looking for a candidate with hard-won expertise to take our data platform and team to the next level. This role is for the candidate who excels at understanding business problems and providing hands-on technical leadership to build scalable, high-quality solutions.
Primary Job Duties:
Deliver value in the form of timely, high quality, performant software components and services
Collaborate with product owners and stakeholders to plan and define requirements
Know the business value of your work and ensure team success in meeting sprint commitments; be flexible, show initiative and develop additional skill sets to improve team capabilities and throughput.
Apply a thorough understanding of the data, structure, and business logic to effectively solve problems
Investigate data discrepancies and errors and determine the best resolution
Provide guidance to the data engineering team in best practices for coding conventions, architecture, quality, scale, productivity, and best practices
Lead and mentor other team members, both onshore and offshore, in DW best practices and development processes, including automation, validation, quality assurance, and monitoring/alerting.
Provide expertise in building for scale, including potentially migrating data warehouse operations to a cloud service such as Google Compute Engine, Snowflake, and/or other platforms
Advise and train members of the team to maximize overall productivity and effectiveness of the team
Identify skills gaps and silos on the team and advocate for resolution
Participate in and contribute to scrum meetings i.e. daily stand-up, sprint planning, and retrospectives
Minimum Qualifications:
5+ years of experience with designing, architecting, and implementing commercial data warehouses and data marts, including integration with commercial BI tools (e.g. Microstrategy, SAS)
3+ years’ experience with relevant scripting languages or programming frameworks desired (Python, Ruby, bash)
Deep knowledge of healthcare data, especially claims data
Strong data warehouse modeling expertise (i.e. integrated model, and star/snowflake schema data model).
Experience migrating a SQL Server DW to a cloud provider such as AWS, Google Cloud Services and Snowflake
Demonstrated knowledge of Unix/Linux, object-oriented programing, relational database technologies, database performance and tuning, distributed computing tech (Hadoop, spark), RESTful API
Proven experience mentoring engineers in DW best practices and development
Hands-on proficiency with SQL, SSIS, and ETL jobs, including stored procedures
Hands-on proficiency in working with cloud services such as Amazon Web Services, Google Compute Engine, and Snowflake
Experience with quality assurance and testing in a relational database environment.
Experience designing data management processes and standards for metadata management, data architecture, data quality, and data security.
Experience with Agile SDLC
Ability to work collaboratively on a multi-location, cross-functional team with a wide range of experience levels
Ability to communicate and work with both technical and non-technical audiences
Interpersonal Skills & Attributes:
Excellent communication skills (verbal and written) necessary to effectively interact with data engineering staff, product owners, and stakeholders
Ability to establish strong, harmonious working relationships with supervisors, peers, and stakeholders to accomplish objectives.
Able to manage competing priorities
Experience working with a distributed team.
Excellent analytical and problem solving skills.
Physical Demands:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. While performing the duties of this job, the employee is occasionally required to stand; walk; sit; use hands to finger, handle, or feel objects, tools or controls; reach with hands and arms; climb stairs; balance; stoop, kneel, crouch or crawl; talk or hear; and taste or smell. The employee must occasionally lift or move up to 25 pounds. Specific vision abilities required by the job include close vision, distance vision, color vision, peripheral vision, depth perception and the ability to adjust focus.
Privia Health is committed to providing equal employment opportunity to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Privia will provide reasonable accommodations for qualified individuals with disabilities. Privia's goal is for our people to reflect the communities that we serve and to increase representation of women, people of color, veterans and individuals with disabilities in our organization.",Remote,Staff Data Engineer,False
877,"ContractJob SummaryRole : Senior Big Data Engineer.Location : Bentonville,ARResponsibilities and DutiesTechnical RequirementsStrong Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).Knowledge/experience on TeradataPhysical Design and Implementation, Teradata SQL Performance OptimizationExperience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)Advanced SQL (preferably Teradata) *Strong Hadoop scripting skills to process petabytes of data * Experience in Unix/Linux shell scripting or similar programming/scripting knowledgeExperience in ETL/ processesAutomic schedulerBenefitsRegardssushmacont : 847 258 9595 Extn. 468Job Type: ContractExperience:big data: 1 year (Preferred)","Bentonville, AR",Sr. Big data engineer,False
879,"If you're motivated by the challenge of building sophisticated solutions that solve complex business challenges, we want to meet you!

Join our team a Data Engineer and help us develop innovative, enterprise-ready web, software, and mobile applications.

Here's what we're looking for:
Responsibilities
Assist with data collection and optimization of storage approaches
Provide support for scalable batch or real-time data processing for discovery and model creation
Implement scalable APIs for utilizing analytics results (e.g., utilizing models produced)
Collaborate with data scientists and help them evaluate the computation/data requirements for discovery and the deployed solution
Design, build, operationalize, and scale some of the largest data pipelines in the world
Advise on and manage big data infrastructure
Architect and develop data ingestion pipelines
Develop proofs of concept with emerging technologies
Assist with data preparation
Required Education and Skills
Bachelor's degree in Computer Science or a related technical field
3 years of experience as a Software Engineer or closely related position
3 years of of experience with the following:
Designing, integrating, and optimizing distributed data-processing pipelines
Utilizing database technologies, including: SQL and No-SQL (e.g., Hadoop, Splunk, Spark, Samza, MySQL, Postgres, MongoDB, Sqlite, Neo4j, Apache Giraph), within a cloud environment
Writing data processing code in Go, Java, Python, Scala, or other high-performance languages
Using distributed and fault-tolerant computing and map/reduce processing techniques
Utilizing Linux/UNIX systems
Systems-level debugging
Building REST APIs for analytics services
Working with or in support of multiple open source communities
Optimizing critical components in applications for efficiency using C or C++
Utilizing cloud deployment and virtualization and containerization technologies (e.g, Docker, Ansible, Terraform and Vagrant)
1 year of experience with the following:
Machine learning libraries, such as Google CloudML, DataFlow, DataLab, TensorFlow, SciKit Learn, Mahout, and MLib
Optimizing advanced SQL queries
Working in an agile environment with SCRUM and PODS
This position is located in our brand-new, state-of-the-art development center in St. Louis, Missouri.
Relocation assistance may be available.","St. Louis, MO",Data Engineer,False
880,"FIGS is looking for an extremely smart and curious Data Engineer who will own the data delivery internally that will help stakeholders make educated decisions. As a part of the Technology team, you will be deft at working with a wide variety of languages, such as Python and SQL, a variety of raw data formats, such as JSON and CSV, in a fast-paced and friendly environment. The ideal candidate is obsessed with how to provide actionable analysis into our business information. The role will report directly to FIGS' VP of Technology.

What You'll Do:

Build dashboards and reports using Mode Analytics to give stakeholders visibility into important data
Manage data warehouse plans for finance, operations and marketing teams
Work closely with department managers to understand the data needs across the company
Act as internal expert in each of the data sources so that you can own overall data quality
Design, build and deploy new data models and ETL pipelines into production
Experience contributing to full life cycle deployments with a focus on testing and quality
Make smart engineering and product decisions based on data analysis and collaboration
Act as in-house data expert and make recommendations regarding standards for code quality and timeliness

Qualifications:

Degree in Computer Science or a related field or a minimum of 3 year's working as a Data Engineer
2+ years professional experience in the data warehouse space
Expert Proficiency with Python and AWS Services (e.g. Redshift, S3)
In-depth knowledge of how to write and optimize SQL statements.
Deep familiarity with distributed processing (Map Reduce, MPP, etc.)
Innately curious and organized with the drive to analyze data to identify deliverables, anomalies, and gaps and propose solutions to address these findings
Ability to manage and communicate data warehouse plans
Thrives in fast-paced startup environment

Other must haves:

Positive attitude
Proven work ethic and integrity
Entrepreneurial mindset
Desire to excel and grow with FIGS
100% awesome. Like our scrubs

Please apply with your cover letter and resume to be considered.

A little bit about us…

The $50 billion medical apparel industry is antiquated, highly fragmented and, until FIGS, was driven solely by low-cost providers offering a limited selection of poor quality products sold through third party distributors. FIGS is revolutionizing the medical apparel industry by creating the highest quality medical apparel in the world and by selling directly to medical professionals through our branded ecommerce site.

FIGS' foundation is built on product quality, and we have a relentless focus on three key areas: fabric, fit and function. We developed our proprietary, performance-oriented fabric technology to meet the demands of the medical profession. FIGS' Core Collection fabric is antimicrobial, wrinkle resistant, stain and liquid repellant, moisture-wicking, odor proof, lightweight, breathable and offers four-way stretch. Our designs are tailored, sophisticated and innovative, incorporating features such as yoga waistbands, smart storage (pockets, zippers, hidden pockets) and inspirational sayings inside each garment that appeal to modern healthcare professionals.

By offering a branded and customer-centric online shopping experience, we are changing how medical professionals buy their workwear. Through our website, social media, and participation in medical conferences and events, we have built a strong following within the medical community and a meaningful connection with our customers, which allows us to understand their needs and to ensure that FIGS is continuously improving and innovating.

FIGS' Threads for Threads initiative is central to our mission. FIGS has donated hundreds of thousands of scrubs to healthcare providers in need in over 35 countries.","Los Angeles, CA",Lead Data Engineer,False
881,"We are searching for a Big Data Engineer to join our engineering team to play a key role in building our next generation cloud platforms. You will be an integral part of an engineering team that continues to work on a comprehensive internet ecosystem. If you are an engineer who understands closures and prototypes, writes clean and modular code and is constantly refactoring then we have a spot for you.

What you’ll do:Build core data capabilities and services for cloud platformDesign, develop, and implement data solutionsDesigning a flexible recommended systemBe an active hands-on team member, collaborating with other developers and architects in developing client solutions
Basic Qualifications:
Bachelors or Master of Science, in Computer Science or other IT-related major
Understanding of Big Data mechanics, analytical processing, modeling, and implementation
Solid understanding of SQL and NoSQL databases.
Experience with big data tools: Hadoop, Spark, Flink, Kafka, etc.
Experience with data pipeline and work flow management tools
Experience with stream-processing systems: Spark-streaming, Flink, etc.
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Perks + Benefits:
Stock options for every employee
Healthcare + dental + vision benefits (Free for you/discounted for family)
401(k) options
Engineering orientation + onboarding
Daily catered lunches onsite (nominal cost)
Healthy snacks + beverages 24/7
Relocation assistance + reimbursement
Free parking + carpool reimbursement
Casual dress code + relaxed work environment
Culturally diverse, progressive atmosphere
“Soul of Faraday” community outreach team","San Jose, CA",Big Data Engineer,False
882,"Job Description
Come help us shape the future of the best voice controlled computer in the cloud. Alexa and Echo are shaping the future of voice recognition and cloud-based content/services. Alexa is the name of the Amazon cloud-based voice service and the brain that powers Echo, the award-winning and groundbreaking Amazon device designed around your voice. Echo connects to Alexa, to provide information, answer questions, play music, read the news, check sports scores or the weather, and more—instantly. It's hands-free, and always ready. All you have to do is ask.

To achieve this, we blend of a variety of disciplines (such as NLP, data mining, machine learning, big data, semantic web, graph stores, cloud computing) in an effort to understand our customers and the things they're excited about. To complement our complex algorithms and extensive data analyses, we create elevated and inspirational mobile and web features across the entire communication experience. We use artificial intelligence, data mining and usability studies to develop new features, and we test them through hundreds of R & D experiments a year. We are also incredibly intent on solving some of the most complex computing problems to be found in industry and academia, and we get to test our solutions in the real world every day. And most importantly, we relentlessly ask: ""What haven't we thought of yet?""

The business intelligence engineer will work closely with data scientists, software engineers, and product managers to build out reporting to inform key stakeholders and decision-makers. In this role, you’ll design, execute and iterate on high visibility business intelligence reporting for the teams of people that are actively building out Alexa's capabilities. If you love working with huge data sets and delivering the insights you discover through business intelligence reporting and automated systems, then this is the job for you.

Key Responsibilities
Collect, analyze and share data to help product teams drive improvement in key business metrics and customer experiencePropose and prioritize changes to reporting and create additional metrics and processes based on program changes and customer requirementsWork closely with Alexa program teams to create ad-hoc reports to support timely business decisions and project workIdentify and implement new capabilities and best practices to develop and improve automated data analysis processesLearn and understand a growing range of Amazon data resources and discover how, and when to use which datasets
Basic Qualifications
Bachelor’s degree in math, statistics, computer science, or finance or equivalent experience.
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst
SQL writing experience and experience with ETL
Redshift experience
Preferred Qualifications
Expert understanding of best practices to handle extremely large volume of data
Ability to create extensible and scalable data schema that lay the foundation for downstream analysis
A clear passion for learning new BI skills and techniques independently and continuously
Ability to prioritize multiple concurrent projects while still delivering timely and accurate results
Experience working in a lean, successful start-up or on a new product team where continuous innovation is desired and ambiguity is the norm
Experience mentoring others in SQL, modeling, forecasting and the use of large datasets
Proficiency with scripting languages and Unix systems (Python, perl, bash, etc.)
Experience with the following is a plus: Looker, Tableau, Microstrategy
Experience in an internet-based company with large, complex data sources.
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.","Seattle, WA",BI Engineer - Alexa Engagement,False
883,"MySQL Data Engineer
At Xpanxion we focus on our people because we know they represent our most valuable asset. With training courses featuring the latest and greatest technologies that are tailored to your interests; you can build your future while building awesome software. Work towards a career you can be proud of, while having fun along the way.
We are currently looking for an experienced Senior Data Engineer.
Responsibilities:
Perform common duties of a MySQL DBA
Work on Linux systems
Migrate Data to a Cloud Environment
Work within a distributed team
Develop measurement and analytics products
Perform ad-hoc analytics
Create ETL
Skills We Look For:
MySQL DBA
Installation, Upgrade, Backup/Recovery, Tuning and Monitor

MySQL 5.6 & 5.7 (Oracle Enterprise)

Replication

InnoDB and MyISAM (moving to InnoDB)

MySQL Enterprise Backup

MySQL Enterprise Monito

Linux CentOS/Red Hat
Scripting (bash shell)
Firewalls
AWS Aurora
Moving MySQL to the cloud

System admin
SQL Server DBA
 Installation, Upgrade, Backup/Recovery, Tuning and Monitor
SS BI (T-SQL, SSIS, SSRS) and Visual Studio 2012
SQL Server 2012-2017
Business Intelligence
Dimensional Modeling
Understanding of OLTP systems
ETL/ELT
Data Warehouse
Architecture
Engineering
US Citizenship is required.
Participate in all of our employee benefit programs:
Health Insurance
Dental Insurance
Vision Insurance
Life Insurance
401k
Bonus Programs
18 PTO days and 11 holidays","Fort Collins, CO",MySQL Data Engineer,False
884,"ContractJob SummaryClairvoyant is looking for Big Data Engineer, With Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications. Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases, and troubleshoots any existent issues. Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Elastic Search, Storm, Kafka, etc. in both an on premise and cloud deployment model to solve large scale processing problems.Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scaleResponsibilities and DutiesResponsibilitiesParticipate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications. Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases, and troubleshoots any existent issues.Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Elastic Search, Storm, Kafka, etc. in both an on premise and cloud deployment model to solve large scale processing problems.Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business.Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.Keep up with industry trends and best practices on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance.Qualifications and SkillsRequired Qualifications: Bachelor’s Degree with a minimum of 6+ year’s relevant experience or equivalent.6+ years of experience with Big Data & Analytics solutions Hadoop, MapReduce, Pig, Hive, Spark, Storm, AWS(EMR, Redshift, S3, etc.)/Azure (HDInsight, Data Lake Design) and other technologies3+ years of experience in Large Scale, Fault Tolerance Systems with components of scalability, and high data throughput with tools like Kafka, Spark and NoSQL platforms such as HBase, Mongo DB.4+ years of experience in building and managing hosted big data architecture, toolkit familiarity in: Hadoop with Oozy, Sqoop, Pig, Hive, HBase, Avro, Parquet, Spark, NiFi2+ years of experience in deploying Big Data solutions (Hadoop ecosystem) on cloud technologies such as Amazon AWS, Azure etc.Experience in working with Team Foundation Server/JIRA/GitHub and other code management toolsetsStrong hands-on knowledge of/using solutioning languages like: Java, Scala, PythonOperations support with a solid understanding of release, incident, and change managementNeed someone who is a self-starter and team player, capable of working with a team of strategists, Architects, co-developers, and business/data analystsJob Types: Full-time, ContractExperience:Large Scale: 3 years (Preferred)Big Data & Analytics solutions Hadoop, MapReduce, Pig, Hive,: 5 years (Required)Strong hands-on knowledge of/using solutioning languages l: 9 years (Required)building: 4 years (Preferred)Education:Bachelor's (Preferred)Location:Pleasanton, CA (Required)Work authorization:United States (Required)","Pleasanton, CA",Big Data Engineer,False
885,"The Enterprise Analytics team is responsible for using data & analytics to drive value across our organization. In support of these activities the data engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for the Business Intelligence and Data Warehousing team and other teams that are represented in the Analytics Center of Excellence.
The Data Engineer will support our developers, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. This position will help to optimize and expand our existing data architecture to support our next generation of data initiatives.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing big data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable `big data data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases.
Experience with data pipeline and workflow management tools.
Experience with AWS cloud services.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages.
Organizational Information


At Children’s Hospital of Wisconsin, we believe kids deserve the best.
Children’s Hospital is a nationally recognized health system dedicated solely to the health and well-being of children. We provide primary care, specialty care, urgent care, emergency care, community health services, foster and adoption services, child and family counseling, child advocacy services and family resource centers. Our reputation draws patients and families from around the country.
We offer a wide variety of rewarding career opportunities and are seeking individuals dedicated to helping us achieve our vision of the healthiest kids in the country. If you want to work for an organization that makes a difference for children and families, and encourages you to be at your best every day, please apply today.
Please follow this link for a closer look at what it’s like to work at Children’s Hospital of Wisconsin: https://www.instagram.com/lifeatchw/","Milwaukee, WI",Data Engineer,False
886,"ContractSenior Big Data Engineer Required skills include: Spark, Kafka, Mongo, Hbase and Springboot. Real-time streaming, SQL and experience working with large data sets. Expert knowledge of Big Data concepts and common components. Familiarity in Dev Ops(Puppet, Chef, Python).Location: South Metro Denver, COStatus: ContractJob Type: ContractExperience:Big Data: 6 years (Required)","Englewood, CO 80111",Big Data Engineer,False
887,"The Company

We're a tech company that's changing how people bank and think about their finances. We value empathy, curiosity, craft and efficacy. Our mission is to help people feel confident with their money. We do that by bringing humanity, elegance and ease to the consumer banking experience. And we make banking beautiful.

The Team

The Data Engineering team builds and operates the pipeline that feeds Simple's data needs, currently using Postgres, Kafka, and Redshift. The team is composed of data management specialists working together to reduce operational defects and increase the capabilities of Simple's internal data customers in Product and Platform Engineering, Risk Management, Analytics, and Customer Support.

About You:
You love both the human and technical challenges of building data solutions that create awesome business impact. You think deeply about technology & data, and what it can unlock for customers. You thrive in a collaborative environment that values discussion and empirical reasoning, and believe these components lead to a successful outcome for the team and the business.

What You'll Do All Day

As a Senior Data Engineer you will design, implement and coordinate projects within the data team. You will mentor junior engineers, and participate in development and operational work that meets the standards of practice for both. You'll interact across the platform and engineering teams in order to have an impact on the business at large.


Ensure our data pipelines are healthy and operational
Respond to alerts and customer requests for data and analytics support
Lead design and implementation of pipelines, features and enhancements
Mentor Junior Engineers
Plan and coordinate small projects

We'd Like To See:

A minimum of 5 years of relevant experience
Experience designing and implementing data management systems for business impact
Proficiency with SQL and Python or a JVM language
Experience with data modeling, warehousing and analysis
Experience with AWS or other cloud services

Details

We recognize the dire lack of diversity in our industry, and we're not okay with it. We actively seek to address it with our hiring and retention processes, as well as our office culture. If you're on the fence about whether you're a fit, we say go for it, and apply!

Why Simple's a Great Place to Work


Based in Portland, Oregon-- a beautiful place to live and work (or just see in the background if you work remotely).
Competitive salary and benefits package.
A supportive and nurturing place to work. We actively consider how we can improve employees' quality of life--both inside and outside the office.
Committed to hiring quality human beings. Simple is a place where others will watch out for you and help you learn. We actually like and respect each other.
We give a damn about what we do, both as individual contributors and as a company on a mission to change banking. We're passionate and nerdy about our work; in fact we're kind of that way about things outside of work, too.

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. Email our team at careers@simple.com ( careers@simple.com ) if you need an accommodation in the application process.

A background check will be required for this opportunity.

Simple provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability,​ or genetics. In addition to federal law requirements, ​Simple ​complies with all ​applicable state and local laws governing nondiscrimination in employment in every location in which the company has ​employees. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

By submitting this application, you certify that the facts contained in your application are true and complete to the best of your knowledge. If you are employed, false statements on your application will be grounds for termination.","Portland, OR",Senior Data Engineer,False
888,"ContractJob TitleMarket Intelligence Technology Data Scientist.SummaryOur Technology team is looking to hire a senior Data Scientist for a central data initiative to build out a data science platform that will provide analytic and machine learning workspaces. These are 12+ month rolling contracts paying market leading rates (depending on experience) What you’ll doAs a data scientist you will be working on building out new technology platform that will be used by Firm’s data scientists and analysts. You are expected to work on fast paced and highly visible project that has significant business impact.Writing efficient, modular, and dependable code, packages, libraries, and scriptsWork closely with team’s data engineer/systems developer and platform customers to design research (e.g., back-testing software) and production (e.g., model serving API) processesDocumenting all work extensively and train teammates on use of work products (e.g., custom Python libraries or R packages)Collaborating regularly with firm’s Big Data group and other firm resourcesFacilitate building research hypotheses and assumptions of portfolio manager and investment analysts by designing methodology and writing necessary code to perform back-testing, cross-validation, event studies, data analysis, etc.,Function as an extension of data engineering team to build out data science use cases of the platformWhat’s requiredSeasoned data scientist with experience on buy side or sell side Firms doing equity research.An expert in Python or R with 5 years of programming experienceStrong programming skills in Relational SQL/NoSQL databasesStrong written and verbal communication skillsHold and advanced degree in Computer Science or relevant Engineering disciplineConversant with advanced statistical analysisJob Type: ContractSalary: $50.00 to $200.00 /hourExperience:Python: 2 years (Required)R: 2 years (Required)Location:New York, NY (Required)","New York, NY",Data Scientist - Hedge Fund - Manhattan & Stanford CT,False
889,"Company Information

PACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of high-quality light-, medium- and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates. PACCAR designs and manufactures advanced diesel engines and also provides customized financial services, information technology and truck parts related to its principal business.

Whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services — you can develop the career you desire with PACCAR. Get started!

Division Information
PACCAR Financial
PACCAR Financial facilitates the sale of premium-quality PACCAR vehicles in 20 countries on three continents worldwide by offering a full spectrum of creative, flexible financial products and value-added services specifically tailored to the transportation industry.

Requisition Summary
Does empowering teams to make data driven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with more data? PACCAR Financial is looking for a seasoned data engineer to join the team. Data Engineering focuses on making possible fast, accurate, and reliable access to data. We build data pipelines, manage a data warehouse, and support the production use of our data. We advocate for good data practices and make sure that our business users are able to make good data driven decisions.



Job Functions / Responsibilities

Provide data engineering on modern, cloud-based and legacy data processing technology stacks.
Build data pipelines, data validation frameworks, job schedules with emphasis on automation and scale.
Contribute to overall architecture, framework, and design patterns to store and process high data volumes.
Ensure product and technical features are delivered to spec and on-time.
Design and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology.
Proactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt.



Qualifications and Skills

5+ years of experience in large-scale software development with emphasis on data analytics and high-volume data processing.
3+ years of experience in data engineering development.
2+ years of experience implementing scalable data architectures.
2+ years of experience with AWS and related services (e.g., EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake).
Experience in data-centric programming languages (e.g., Python, GO, Ruby, Javascript, Scala).
Proficiency with ETL tools and techniques.
Knowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, IMS, MySQL, Postgres, SAP HANA, and Teradata.
Experience with participating in projects in a highly collaborative, multi-discipline team environment.
Work settings:
Requires frequent sitting and walking.
Availability to work “on-call” 24 hours/day for emergencies.
Position could be required to minimal traveling up to 20%.



Education

Masters' or Bachelors' degree in Computer Science or a related field.



Additional Job Board Information

PACCAR is an Equal Opportunity Employer/Protected Veteran/Disability.","Bellevue, WA",Data Engineer,False
890,"$140,000 - $170,000 a yearContractAre you excited about being in a start-up and being in the cyber security industry? This company in San Jose is changing internet security as we know it with their Security as a Service Platform that protects against cyber attacks and data breaches. They are looking to grow their team with a Big Data engineer that can enhance their current Big Data ecosystem, so if this is the fit for you, apply today!
Required Skills & Experience
B.S. in Computer Science or related field
4+ years experience
Java or Python
REST/RESTful
Experience with the Big Data ecosystem
Desired Skills & Experience
AWS
RabbitMQ or ActiveMQ
What You Will Be Doing
Tech Breakdown
100% Backend
Daily Responsibilities
100% Hands on
The Offer
Competitive Salary: Up to $170K/year, DOE
You will receive the following benefits:
Medical, Vision, Health Insurance
401(K)
Flexible PTO
Paid Sick Leave
Catered lunches
Unlimited snack bar


Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.
Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.","San Jose, CA 95113 (Downtown area)",Big Data Engineer,False
891,"Description:
Overview

The Data Engineer will be responsible for assisting the CX Lab Survey Research Team to create functionality for transforming and importing Qualtrics data to workbook sheets. These data will be sourced from multiple unique CX survey files. The survey data capture the experience and opinions of customers working with advertising programs and products. A second phase of development for the Data Engineer to accomplish will be to develop original programming to execute appropriate statistical testing, and also compute metrics such as error bars for export into client-focused top-line data decks.


Main responsibilities
Develop code to transform and import data from Qualtrics to sheets using Javascript to create functionality across App Script and/or SQL platforms.
Script programming to operationalize appropriate statistical testing routines for insightful comparisons.


Requirements
Experience programming with Javascript for App development
Proficiency with SQL programming and query script
3+ years of data engineering experience, especially working with data files and data pipelines
Masters degree in statistics, data science, data management, or other quantitative field
Experience in the application of analytical and data analysis procedures
Experience working with at least one statistical package (e.g., R, SAS, SPSS)
Good interpersonal/communication skills (written and oral)
Excellent organizational skills/ability to work under tight deadlines with little supervision
Working knowledge of Google Docs, Sheets, Slides


Nice to have
Experience developing data visualization programming and formatting using Google Sheets and Slides","Mountain View, CA 94043",Data Engineer,False
892,"$120,000 - $140,000 a yearWe are the fastest-growing fitness startup here in New York City right now, and we are looking for a Data Engineer to come work hand-in-hand with our Director of Data Engineering to work with our large amount of data.

Whats the Job?

The Data Engineer on our team will:

Work on building the data pipelines for all of our data across all of our divisions
Oversee setting up our data integrations, including with SalesForce
Be tasked with scaling up our analytics capabilities across the company
Work in a fun, high-energy startup environment
Have the ability to have a very flexible schedule with work-from-home hours
Use the latest and newest technologies in the market

What Skills Do We Need?


Advanced Python skills
Experience with core data engineering including data pipeline development and integrations
It's a ""Plus"" if you've worked with PySpark and/or Spark

Who Are We?

We have already generated over $60 million in profit this year and receive about 650,000 visits per second on our app at peak time. Headquartered here in New York, we are in the health and wellness space and believe that a healthy lifestyle including eating right and exercise will benefit you in the long run.

Compensation


$120,000 - $140,000
Great Equity (in a fast-growing company)
Very employee-friendly environment (healthy lifestyle, open PTO days, work-life balance)

How Should You Apply?

If you are interested, please apply online or email me your resume to: Matt.Stabile@AverityTeam.com.","New York, NY 10016 (Gramercy area)",Data Engineer (Startup),False
893,"Donnelley Financial Solutions (NYSE: DFIN) provides software and services that enable clients to communicate with confidence in a complex regulatory environment. With 3,500 employees in 61 locations across 18 countries, we provide thousands of clients globally with innovative tools for content creation, management and distribution, as well as data analytics and multi-lingual translations services. Leveraging advanced technology, deep-domain expertise and 24/7 support, we deliver cost-effective solutions to meet the evolving needs of our clients.

Position Summary

Donnelley Financial Solutions is seeking a Data Engineer to join our Data and Analytics Team. The ideal candidate is a self-starter who is effective working with large development teams in a highly collaborative environment.



Job Description

Develop, maintain, test, and troubleshoot data solutions, including Database Development, ETL / Data Migration Development, and Big Data Development.
Collaborate with technical and non-technical resources to solve issues and develop new functionality.
Participate in Requirements Gathering and Analysis meetings with Team Members, Internal Customers, and Stakeholders.



Required Skills

Database development skills (SQL, T-SQL)
SQL Server 2012 or later
SQL Server 2016 Experience is a plus
ETL / Data Integration
Troubleshooting of complex data solutions
Big Data Technologies including (Azure Data Factory, Spark, NoSQL, Hadoop, HDInsight, Big Data Architecture) a plus
Data Analysis
Requirements Analysis
Bachelor’s Degree in Computer Science, Information Technology or relevant experience.
2 + Years development experience with SQL and TSQL using SQL Server 2012 or later.
2 + Years development experience using ETL / Data Integration Tools (SSIS preferred).
1 + Years of experience performing Data Analysis / Data Profiling.
1 + Years of requirements analysis experience.
1 + Years of experience working in environments with formal SDLC (Software Development Life Cycle) experience, experience working on teams utilizing Agile Methodologies a plus.
1 + Years authoring technical and functional documentation.


It is the policy of Donnelley Financial Solutions to select, place and manage all its employees without discrimination based on race, color, national origin, gender, age, religion, actual or perceived disability, veteran's status, actual or perceived sexual orientation, genetic information or any other protected status.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access jobs.dfsco.com as a result of your disability. You can request a reasonable accommodation by sending an email to AccommodationRequests@dfsco.com.","Chicago, IL 60604 (Loop area)",Data Engineer,False
894,"$130,000 - $165,000 a year (Indeed Est.) Contract, OtherWe have an immediate project requirement for a Data Engineer with a Google Cloud Professional Data Engineer certification (preferred). This role will lead the team on client projects.The Data Engineer should have excellent time and task management skills, have proficientteam communications, and be able to either telecommute from home or work in one ofour regional offices.This role can be on contract, contract to hire or as a full-time/permanentemployee for the right candidate.Responsibilities Lead/oversee the design of industry-leading cloud-based data/software solutionsto run on Google Cloud Platform (GCP) Lead the design, delivery of, and maintenance of data structures anddatabases, data processing systems, analyze data and enable machinelearning, model business processes for analysis and optimization, visualizedata and advocate policy, design for security and compliance Assemble solution teams for data engineering projects Make internal recommendations to help improve and streamline the technicaland architectural processes Work with the teams to develop web and mobile applications, APIs, SDKs andother tools as required Document software designs, functional and design specifications, presentations,and other documents as needed Lead our team on pre-sales scoping as a technical expertExperience  10+ years hands-on experience in engineering with solution/data designleadership experience Previous experience leading/designing multiple Cloud data projects, GCP andother cloud platforms (AWS, Azure) Experience leading large scale data migrations Previous knowledge and experience with Machine Learning/AI Expert capabilities with structured, unstructured and real-time data Google Cloud Professional Data Engineer certification preferred Experience leading/designing with multiple Cloud platforms such as GoogleCloud, AWS or Azure; big data pipelines and management Experience with data security for data at rest and in transit (i.e., firewalls,hashing, encryption and SSL) Experience automating and managing key business data pipelines to deliver theoutput of models to targeted applications Excellent SQL coding and experience with a broad array of development toolsand platforms including a strong Linux background and big data environmenttools/languages, such as SQL, R, SAS, Python, etc. Experience with Google data tools such as MySQL, Postgres, Bigtable, Dataflow,Spanner and BigQuery Experience with machine learning an asset Hands-on experience with two or more of: Go, python, php, Java, .Net or node.js Expert knowledge of and experience with GCP budgeting and billing strategies MS in Computer Science or equivalent program from an accreditedUniversity/College Able to collaborate and thrive in a fast-paced diverse high-performanceenvironment Demonstrated excellence in written and verbal communicationJob Type: ContractExperience:Google Cloud: 2 years (Required)","New London, CT",Google Cloud Data Engineer - Remote,False
895,"Oath Ad Platforms is our unified ad tech solution for both advertisers and publishers. Our innovative ad tech gives one stop access to Oath's trusted data, high quality inventory and demand, creative ad experiences and industry-leading machine learning, at global scale.


A Lot About You
You get people. You have a unique blend of skills in developing deep consumer insights and competitive intelligence through data that drives product innovation to create the right experiences for people’s daily lives that achieve our goals to acquire, engage, and retain them.
You get data. You have a thirst for knowledge and insight. You thrive and strive to present data in ways that product, design, engineering, marketing, and executive teams understand and act upon. Your data is 100% accurate and credible. Your reports are always clear and actionable.
You get growth. You are a consumer-focused, data-driven, and growth-enabling analyst who has supported Growth strategies, roadmaps, scrums, and final product rollouts, across the analytics/insights, acquisition/referrals, activation/onboarding, and adoption/retention loop.
You get mobile/digital. You have significant industry experience – and a strong understanding of the mobile/digital ecosystem – from apps to advertising and analytics. You have successfully applied the latest mobile/digital tools to help drive reach, retention, and revenue growth.
You get it done. You have successfully worked with product, design, engineering, marketing, and executive teams to understand requirements, translate business needs into data requests, develop methodologies/plans, analyze data, and present findings that are embraced/enacted.
Your Day
Understand the marketplace trends and help answer revenue trends
Analyze supply as well as demand patterns and find revenue opportunities, explain model behaviors, suggest improvements etc.
Gain insights on what drives performance in terms of reach and revenue growth
Create dashboards and reports that provide analysis and commentary, explaining product, sales, and business trends for Executive reporting
Work closely with product and inform and update stakeholders on product performance, plans, and progress towards metrics
Define data testing plans and create methodologies that help teams to iterate fast and release new features for testing and, if successful, rollout to all users globally
Generate and go deep on consumer insights and competitive intelligence to help teams drive product innovation and iteration
Build strong partnerships with product, sales, engineering, and marketing teams and enable them to launch new Growth initiatives for testing/iteration
Provide feedback to product, sales and engineering teams on impact of product launches: target launch metrics, A|B testing, post-launch metrics
Investigate data and monitor data quality – partner closely with and provide requirements to the Data Engineering teams that can be clearly acted upon
Frame business problems into questions that can be answered through data analysis, and translate business needs into requirements

You Must Have
BS/MS in highly-quantitative field (Analytics, Computer Science, Mathematics) is preferred
Data analysis, generating insights for consumer-focused products
B2B or advertising experience is a must
Experience with big data technologies such as Hive, Hadoop, MapReduce, Spark, PIG etc.
Experience with scripting programming languages such as Perl/Python is good to have
Familiarity with Unix/Linux environment highly recommended
Significant experience, proficiency in, and passion for Mobile and/or Web products
Track record of proactively establishing and following through on commitments
Demonstrated use of analytics, metrics, and benchmarking to drive decisions
Excellent interpersonal, organizational, creative, and communications skills
Team player in driving growth results combined with a positive attitude
Strong work ethic and strong core values (honesty, integrity, creativity)
Problem solver who never stops thinking about ways to improve



Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","Sunnyvale, CA",Data Engineer/Analyst,False
896,"The requirements:

Bachelor or Master degree in Computer Science or a related study;
Knowledge of and experience with:
data modelling;
data warehouse architecture;
data integration and data clearing;
Python and/or Java, C# ;
Cloud computing: AWS and or Azure;
Hadoop, Hive, Hbase, RedShift;
RDBMS: SQLServer or Oracle or MySQL;
Spark, Scala is a plus;
English - intermediate+ ;
Flexible, proactive team player","McKinney, TX",Data engineer,False
897,"Temporary, InternshipAs a Data Engineer, the intern would have a variety of potential jobs available based on a candidate’s background experience and expertise. Duties would entail working in a team setting in order to design, implement and test web-based solutions, utilizing RLI’s technology fabric and Microsoft .Net technologies. Candidate would gain exposure to best practice Enterprise patterns for application architecture and systems integration.

The project that the intern will be assigned to will be based on 2019 planning and what the current priority of project work is during the intern period. Some examples include:

Data: Create, enhance and support data services, ETL processes and reports for Enterprise Data.
Document services, models, processes and policies. Modernize data warehouse architecture and visualization capability.
Retire legacy artifacts and process.


IT would place more than one student in this position if multiple qualified candidates are found.
Required Skills
Solid understanding of foundational knowledge appropriate for a developer: object-oriented programming, database management systems, etc. Additional knowledge specific to RLI’s technology is a plus, for example, specific .Net experience and web-based development.
Demonstrated success in a classroom setting, natural curiosity and desire to learn.
Excellent communication skills, ability to work in a team.


Required Experience
Junior or Senior student majoring in Computer Science; Management Information Systems; Computer Information Systems; or related discipline.
3.0/4.0 cumulative GPA required.","Peoria, IL 61615",IT Data Engineer Internship - Summer 2019,False
898,"Position Overview
At the heart of our Data Solutions team are our super talented, highly-technical Data Engineers. Data Engineers are data experts who dive right into new client projects and make it their job to understand how a client’s data fits together and what that data means.

Utilizing this knowledge and the industry’s newest technologies (Aunsight, Hadoop, Docker, etc.), they create data lakes (fed by real-time data streams) that become the very foundation of the work we do. Critical at all stages of the data science process, Data Engineers work cross-functionally with both external and internal teams – from business analysts to data scientists; mobile app developers to platform engineers; IT teams to high-level executives. Data Engineers also provide valuable feedback to our software team that helps to shape the development of Aunsight, our proprietary end-to-end cloud analytics platform; and the development of our proprietary mobile app, Sightglass.

The best Data Engineers are patient, persistent, focused, creative, and incredibly curious. They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems.

Essential Duties & Responsibilities:
Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources.
Ensure data integrity by developing and executing necessary processes and controls around the flow of data.
Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights.
Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies.
Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals.
Utilizes technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement.
Verifies accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate.
Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations.
Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule.
Stay up-to-date on data engineering and data science trends and developments.
Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices.
Additional duties as assigned to ensure client and company success.
Required Skills:
Bachelor’s degree in Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience..
Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development..
Proficiency in one or more of the following programming languages: PHP, Java, or Python and a familiarity with Node.js.
Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle.
Resourceful in getting things done, self-starter, and productive working independently or collaboratively—ours is a fast-paced entrepreneurial environment with performance expectations and deadlines.
Ability to learn quickly and contribute ideas that make the team, processes, and solutions better.
Ability to communicate your ideas (verbal and written) so that team members and clients can understand them.
Ability to defend your professional decisions and organize proof that your ideas and processes are correct.
Share our values: growth, relationships, integrity, and passion.
Preferred Skills:
Experience working in one of the following industries: healthcare, financial services, media, or manufacturing.
Experience working with commercial relational database systems such as electronic medical records or other clinical systems, customer relationship management software, or accounting systems.
Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/visualization tools.
Prior experience supporting business intelligence operations, managing technical, business, and process metadata related to data warehousing.
Experience working with NoSQL, Hive, MapReduce and other Big Data technologies is preferred but not required.
Willing to train the right candidate.
Experience working with distributed and/or parallel systems experience or knowledge of concepts.","South Bend, IN",Data Engineer,False
899,"Data Engineer
Agentis is a Chicago-based data analytics and SaaS platform provider that empowers energy providers and their business customers to optimize energy consumption. Our SaaS customer engagement platform has been deployed to over 1.5 million businesses nationwide and growing. We combine our strengths of data visualization, data science, and software development to create compelling and powerful customer experience. At the intersection of Clean Tech and Information Technology Agentis has an exciting and compelling mission with a very passionate and talented team leading the charge.
Interested in making a positive impact and creating great technology?
We're looking for a Data Engineer to be part of our growing core team.
As Data Engineer you will play an important role in helping to develop a highly scalable SaaS application. You must have solid communication skills, be detail oriented, and show leadership and an exemplary work ethic.
As a Data Engineer You Will:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and big data technologies using AWS servers and technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems
Required Skills
We are looking for a candidate with 3+ years of experience in a Data Engineer role, who has attained a Bachelors and/or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using some of the following software/tools: big data tools, such as Hadoop, Spark, Kafka, etc; relational SQL and NoSQL databases, such as MySQL, Postgres, Cassandra, HDFS; data pipeline and workflow management tools such as Azkaban, Luigi, Airflow, etc; AWS cloud services; object-oriented scripting and/or programming languages such as PHP, Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with and optimizing relational databases as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queues, stream processing, and big data databases.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Compensation
We offer a competitive salary, benefits and bonus structure based on experience and performance.
We will setup you up with a workstation of your choice and whatever software you need for development.
We value teamwork, innovation and productivity and foster an exciting and committed work environment that will help you become successful and grow in your position. To apply email resumes to: jobs@agentisenergy.com
Agentis Energy is an equal opportunity employer.
EEO Employer. No recruiters or phone calls, please.","Chicago, IL",Data Engineer,False
900,"Qualifications
Strong experience with at least two of the following technologies: Python, Scala, Java
Strong experience in traditional data warehousing / ETL tools (SSIS, Informatica, Talend, Pentaho, DataStage)
The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Good experience in multiple database technologies such as; Distributed Processing (Spark, Hadoop, EMR, Splunk);Traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL); MPP (AWS Redshift, Teradata); NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan, ElasticSearch, Lucene)
A proven ability in clearly communicating complex solutions
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data.
Experience and interest in Cloud platforms such as: AWS, Azure, Goole Platform or Databricks
Who You'll Work With
You will be joining the OrgSolutions team in Boston (we are also open to other North American locations).

OrgSolutions combines innovative design technology and advanced analytics with deep expertise to accelerate effective organizational decisions. OrgSolutions can help Human Resources and business leaders to make smarter organizational decisions by leveraging advanced data analytics, user-centered design technology and proven methodologies. These approaches help clients to answer critical questions related to cost reduction, efficiency and effectiveness streamlining, merger plans, divestments and carve outs, and talent deployment.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for –and expands our investment in –our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and we’re hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.
What You'll Do
You will be responsible for architecting, developing and deploying a wide range of real time applications, both on premise and in the cloud for the People Analytics team.

As a Data Engineer, you will write ETL pipelines to process large amounts of semi-structured data. You will be responsible for indexing and retrieving semi-structured documents from databases and search engines. You will build large scale fuzzy matching capabilities and architect & build data application in the cloud (AWS)

Working with People Analytics, you will model HR data, obtain data extracts and go on to define secure data exchange approaches. You will acquire, ingest, and process data from multiple sources and systems into large scale data platforms. In doing this, you will design, maintain & tune large data warehouse sets.","Boston, MA",Data Engineer - People Analytics (Contract),False
901,"As an IT consulting firm founded in 1986, Zirous has a solid history of providing the technology solutions our clients want and need to make their organizations successful. We have partnerships with Oracle, Hortonworks, SailPoint, Microsoft, and Splunk, in addition to the many other technologies we use to fit our clients’ needs.
We are seeking a Data Engineer to join our team!
Only candidates local to the Des Moines, IA area will be considered.
What you'd be doing:
Work on a variety of projects as a full-time consultant - length of projects and types of client industries will vary.
Work closely with Project Managers to ensure successful project implementation.
Provide expert and specialized hands-on technical support, consultation and coaching to clients.
Collect, aggregate, and analyze data. Solve complex problems through data and analytics, and provide insights that are logical, data driven,and aligned to client business goals.
Examine methods for data validation, building statistical models, and data visualization using a cutting edge technologies.
Integrate model inputs and outputs with existing systems for real time online usage.
Other duties as assigned.
Have the ability to create your career!
Requirements
Bachelor’s degree in Computer Science, Computer Engineering, Software Engineering, Data Science, Statistics, Mathematics, Information Systems, or in a related field or equivalent work experience.
Sound fundamentals of data structures, algorithms and object-oriented programming.
Previous experience with either machine learning, advanced analytics, or big data mining is highly desirable.
Previous experience with code-based visualization tools/libraries.
Ability to quickly learn and communicate the latest findings in data reporting & analytics tools, processes, methodologies, and best practices.
Clear and concise written and verbal communication skills. You can articulate complex technological concepts to fellow engineers and translate for non-engineers. You are adept at navigating technical conversations with both team members and clients alike.
1 - 5 years of experience with the following technologies:
SAS
Hive
SQL
Spark
Storm
Nifi
Kafka
Atlas
Ranger
Java
Benefits
Contributing to the success of a high caliber team
Competitive salary and benefits package including 401K
An environment that fosters personal and professional growth
Opportunities to work on exciting and varied projects
Flexible time off (FTO)
We value our employees' personal time, career desires and life goals
Zirous is an equal opportunity employer.","West Des Moines, IA",Data Engineer,False
902,"ContractJob Summary4+ Years of experience Expertize and hands-on experience on Java- Must Have exp with Hadoop, Hive, Mapreduce, Spark Must Have Good knowledge of Shell script- Must Have Good Knowledge of one of the Workflow engine like Oozie, Autosys- Good to Have Good knowledge of Agile Development- Good to Have Passionate about exploring new technologies -Good Communication SkillsResponsibilities and Duties4+ Years of experience Expertize and hands-on experience on Java- Must Have exp with Hadoop, Hive, Mapreduce, Spark Must Have Good knowledge of Shell script- Must Have Good Knowledge of one of the Workflow engine like Oozie, Autosys- Good to Have Good knowledge of Agile Development- Good to Have Passionate about exploring new technologies -Good Communication SkillsRequired Experience, Skills and Qualifications4+ Years of experience Expertize and hands-on experience on Java- Must Have exp with Hadoop, Hive, Mapreduce, Spark Must Have Good knowledge of Shell script- Must Have Good Knowledge of one of the Workflow engine like Oozie, Autosys- Good to Have Good knowledge of Agile Development- Good to Have Passionate about exploring new technologies -Good Communication SkillsJob Type: ContractExperience:Expertize: 4 years (Required)","Phoenix, AZ",Big Data Engineer,False
903,"Summary

Boxy Charm is seeking a highly motivated individual responsible for building the analytics data platform for the company.

The Data Engineer will work closely with data scientists and analysts across different business units to build data products that will enable fast decision-making and action across the company. The position will be part of the Data & Algorithms team and, as such, the ideal candidate will not only possess great communication skills, but will also be very technical; equally at home writing code, solving complex problems and working in a cutting-edge cloud-based platform and technology stack.

Essential Duties and Responsibilities


Build cloud-based data products using SQL, Python, Snowflake, Spark and other technologies
Build data life-cycle and health tools to enable monitoring of key business KPIs
Work with a wide variety of data ranging between social media and email to customer transactions and logistics
Partner with various business stakeholders and implement solutions that improve their business process
Break down complex projects and problems into actionable tasks that be delivered quickly and iteratively and provide value to the business stakeholders
Be a data advocate throughout the company

Education and/or Experience


Bachelor's degree or higher in Computer Science, Information Technology, Data Analytics or a related field
3+ years of experience programming in multiple languages such as Python, Java, Scala etc.
3+ years of experience working with SQL and relational databases and strong SQL skills are a must
Knowledge of Big Data and NoSQL systems such as Snowflake, Hadoop, Spark, MongoDB, etc.
Experience working in an Agile (SCRUM, XP etc.) development environment
Experience with AWS or other cloud environments is strongly desirable
Experience with streaming data systems is desirable

","Pembroke Pines, FL",Data Engineer,False
904,"$120,000 - $170,000 a yearContractWe are a leading provider of open, scalable, next-generation e-commerce and cloud technology solutions for a number of larger firms, and we work with some of the country's leading and best-recognized companies! We serve as e-commerce consultants and subject matter experts for them- helping to provide today's demanding and technologically savvy consumers with an exceptional shopping experience across multiple channels. We are also a pioneer in private and enterprise cloud technology, and work on the cutting edge of continuous delivery and release automation. Currently, we're looking for an experienced Big Data Engineer to work with one of our best clients in Cupertino.Responsibilities and DutiesDesign, support and continuously enhance the project code base, continuous integration pipelineHelps in design and development of Big Data analytical applicationsCarry out large-scale near real-time streaming data processing pipelinesCreate complex ETL processes and frameworks for analytics and data managementQualifications and SkillsExcellent knowledge of Hadoop and Spark, experience with data mining and Kafka, Spark, AkkaWorking Scala knowledgeExperience with version control like GitExperience with JVM build systems a plusLinux/Unix experience a strong plusBenefitsVery strong compensation & benefitsBenefits include medical, dental, vision and lifeGenerous 401k packageA client list of who's who in the retail spaceA fantastic and high-energy working environment with some very sharp colleagues!Job Types: Full-time, ContractSalary: $120,000.00 to $170,000.00 /yearLocation:Cupertino, CA (Required)","Cupertino, CA",Scala/Spark Big Data Developer,False
905,"ContractThis is a Contract to hire position for our direct client.The job requirements & responsibilities include· Knowledge of data management fundamentals and data storage principles· Knowledge of distributed systems as it pertains to data storage and computing· Proficiency in, at least, one modern programming language such as Java, Scala, or Python· Experience working with AWS Big Data Technologies (EMR, Redshift, S3)· Experience working with Open Source Big Data tools (Parquet, Spark, Hadoop, Presto)· Proven track record of delivering a big data solution· Experience developing tools for data engineers and machine learning· Experience working with both Batch and Real Time data processing systemsJob Type: Contract","Santa Ana, CA",Data Engineer - Big Data and AWS,False
906,"ContractJob SummaryJob Requirements/ Responsibilities: Demonstrate deep knowledge of data and the ability to lead others in the data engineering team to build and support non-interactive (batch, distributed) & real-time, highly available data, data pipeline and technology capabilities.Demonstrate focus in working towards defined business objectives and understanding the business value of work performedDemonstrate deep understanding of the ETL process (and variants there-of), including orchestration and development of data productsTranslate strategic requirements into business requirements to ensure solutions meet business needsWork with infrastructure provisioning & configuration tools to develop scripts to automate deployment of physical and virtual environments; to develop tools to monitor usage of virtual resources.Assist in the definition of architecture that ensure that solutions are built within a consistent framework.Lead resolution activities for complex data issuesDefine & implement data retention policies and proceduresDefine & implement data governance policies and proceduresIdentify improvements in team coding standards and help in implementation of the improvements.Leverage subject matter expertise to coordinate issue resolution efforts across peer support groups, technical support teams, and vendorsDevelop and maintain documentation relating to all assigned systems and projectsPerform systems and applications performance characterization and trade-off studies through analysis and simulationPerform root cause analysis to identify permanent resolutions to software or business process issuesLead by example by demonstrating the mission and values.Additional Requirements: Ability to apply knowledge of multidisciplinary business principles and practices to achieve successful outcomes in cross-functional projects and activitiesStrong working knowledge of Python, Java, Scala or C#Strong working knowledge of SQL and No-SQL PlatformsProficiency in debugging, troubleshooting, performance tuning and relevant toolingStrong working knowledge of Hadoop, YARN, MapReduce, Pig or Hive, SparkDemonstrated ability to “productionalize” at least 2 big data implementationsExperience using one of the public cloud (AWS or Azure preferred) for data applicationsProficiency in shell scriptingSolid understanding of data design patterns and best practicesProficiency in CI/CD toolsProficiency in logging and monitoring tools, patterns & implementationsUnderstanding of enterprise security, REST / SOAP services, best practices around enterprise deployments.Proven ability and desire to mentor others in a team environmentThanks & Regards,KANTHIBusiness Development ManagerMXPRACTICE , Alpharetta, GA 30004Contact Number : +1770-406-0486Job Type: Contract","Seattle, WA",Sr. Data Engineer,False
907,"Years of Experience: 5+ years
Responsibilities:
The data warehouse developer is responsible for the successful delivery of business intelligence information to the entire organization and is experienced in Data Warehouse and BI development implementations. He/she will be part of the team to review, analyze, modify, and create ETLs, testing, debugging, integration and implementation processes. This person will also help create a road-map for our client's technology platform for the next 5 years, as they are moving to Azure.Tasks will also include documenting technical needs for ETL processes and databases, and ensuring optimal technical infrastructure is utilized. This position involves the delivery of big data integration projects and requires leveraging best practices expertise and experience at implementation; as well as effective technology know-how to support the ongoing activity of those working within business intelligence and analytics teams.

ESSENTIAL FUNCTIONS
Develop and maintain logical & physical data model designs, data management standards and conventions, data naming standards and metadata (normalized, de-normalized and STAR-based structures)Perform reverse engineering of physical data models from databases and SQL scriptsEvaluate data models and physical databases for variances and discrepanciesValidate business data objects for accuracy and completenessParticipate in higher level planning efforts. Play a role in definition of process/guidelines/standardsDirect support of business intelligence and analytics teams












Required Skills:

Associates or Bachelor’s Degree – Computer Science (or related field)
-4+ years of data experience with Microsoft stack SSIS (ETL/DW) with modern enterprise data architectures and data toolsets (ex: data warehouse, data marts, data lake, 3NF and dimensional models, modeling tools, profiling tools) -Proficient with SQL: ability to work on wide variety of development needs from simple ad hoc queries to complex stored procedures and multidimensional analysis
Data integration: ETL and ELT development paradigms and platforms
Performance Tuning: Strong performance tuning of ETL/SQL code
Cloud Platforms: Azure (Data Lake, Data Bricks, HDSS, SQL DW), AWS (Redshift), SQL Azure DW
Data modeling: ERWin, Embarcadero, PowerDesigner; ability to model data using appropriate notation","Indianapolis, IN",Data Engineer,False
908,"Job Description

We are looking for a candidate with 3+ years of experience in a Data Analyst role, who has attained an undergraduate degree in Computer Science, Statistics, Informatics, Economics or another quantitative field. Successful candidate should be able to apply statistical and machine learning methods to analyze large, complex data sets and communicate the results and methods clearly.

Minimum Requirements


Programming experience of object-oriented/object function scripting languages: Python, Java, and SPARK. It's a plus if candidate knows SCALA.
Be part of a multi-year program to build and deliver a centralized customer model implemented as a ""Platform As a Service"" from the ground-up.
Ability to synthesize quantitative results to determine implications and make actionable recommendations.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Discover the information hidden in vast amounts of data and help us make smarter decisions to deliver even better products.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.
Work closely with product owners and other team members to meet delivery goals.
Work with data science and analytics experts to strive for greater functionality in our data systems.
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases..
Experience designing and implementing Machine Learning models.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Project management and organizational skills.

Company Summary

Pizza Hut, a subsidiary of Yum! Brands, Inc. (NYSE: YUM), serves and delivers more pizzas than any other pizza company in the world. With easy order options including the Pizza Hut app, mobile site, Facebook and Twitter messenger and Amazon devices, Pizza Hut is committed to providing an easy pizza experience – from order to delivery.

Founded in 1958, Pizza Hut has become the most-recognized pizza restaurant in the world, operating more than 16,400 restaurants in more than 100 countries.

Pizza Hut is also the proprietor of The Literacy Project, an initiative designed to enable access, empower teachers and inspire a lifelong love of reading. The program is rooted in the foundation set by the Pizza Hut BOOK IT! Program, which is the longest-running corporate supported literacy program, impacting more than 14 million students each year. For more information, visit www.pizzahut.com . Follow Pizza Hut on Facebook (www.facebook.com/PizzaHut ), Twitter (www.twitter.com/pizzahut ) and Instagram (www.instagram.com/pizzahut ).","Plano, TX",Data Engineer,False
909,"About Knotch:
-------------

Knotch is the independent standard for content marketing ROI. We help CMOs and their teams measure and impact the outcome of their content efforts via real-time, actionable intelligence across all of their content investment. Our end-to-end content intelligence platforms helps marketers plan, measure, optimize and benchmark their content efforts across all owned and paid strategies. We work exclusively with brands and we do not monetize from any distribution channels to make sure that our business model isn't invested in the success of what we are measuring.

We're based in SoHo, NYC and work with brands including GE, Unilever, JP Morgan Chase & Co., Sprint, TD Ameritrade, Ford, Colgate, and Citi. In 2018 alone, Knotch has been named to Inc. Best Place to work ( https://www.inc.com/best-workplaces/list )list and Built In NYC Top 50 Start-ups to work at ( https://www.builtinnyc.com/2018/01/16/50-nyc-startups-watch-2018 ).

Engineering at Knotch:
----------------------

Engineering is the cornerstone of our organization and we work hard everyday to build the most impactful products as possible. We love to experiment, find a deep joy in product iteration, achieve stability with thoughtful architecture and testing all while monitoring our performance and progress at every step.

Knotch's founding mission has always been to improve the advertising and marketing industries in a lasting and meaningful way. Transparency through data is our ethos and something every member of our company takes seriously. We are looking for highly motivated engineers who passionate about data and who are eager to transform an industry to join us on our journey.

Data Platform Engineering:
--------------------------

At Knotch, our data platform is vital to providing key intelligence and insights to our clients. As a result, our data platform is the most important element when it comes to successfully scaling our products and technologies. As our platform evolves, agility with stability are critical to avoid bottlenecks in our products and to ultimately increase our value. We're looking for an experienced data engineer who deeply values clever platform architecture and who emphasizes speed and stability to join our growing team.

What You'll Do at Knotch
------------------------


Design and implement resilient backend architectures that process gigabytes and beyond
Write software for backend services using Ruby, Python, and other languages
Work directly with our Data Science team facilitate new understandings and insights from our data
Work directly with our full-stack engineering team to expose value from our data

What We Want From You
---------------------


5+ years of data platform engineering experience
Working experience with AWS services including DynamoDB, RedShift, Athena, and other data warehouse and data lake services
Experience with various AWS offerings such as EC2, ECS, RDS, ElastiCache, Lambda and others
Experience with Ruby, Rails, Sidekiq, Python, Postgres, Redis
Experience working with Data Science teams and productizing Data Science models and applications

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","New York, NY",Senior Data Platform Engineer,False
910,"At Modernizing Medicine, we look for passionate, innovative, creative Rock Stars!


People's Choice UX Award for modmed® Kiosk - 2017
Customer Success Team of the Year by SIIA's Company CODiE - 2017
South Florida Business Journal Fastest Growing Technology Company - 2016, 2017
Inc. 5000 Fastest-growing Private Companies in America - 2015, 2016, 2017
Deloitte's Technology Fast 500™ - 2015, 2016, 2017

Modernizing Medicine ( https://www.modmed.com/ ) is delivering truly disruptive and transformative products and services that will impact the healthcare industry. The work we do makes a difference.

Our web and mobile applications are transforming healthcare information technology to increase practice efficiency and improve patient outcomes. We offer end-to-end specialty-specific solutions from practice management, through EMR to Revenue Cycle Management (RCM) that maximize office interactions, patient visits, collections and reimbursements.

Modernizing Medicine is looking for an exceptional Data Engineer with a passion for technology to help us revolutionize the world of Healthcare IT. Data engineers are at the core of a data-driven business: they build and maintain infrastructure that empowers analysts and data scientists to drive insights. We've built a team of passionate, creative and innovative engineers and data scientists that are changing the world and having fun doing it!

You may be a great fit for Modernizing Medicine's Data Engineering opportunity IF…


You are a gifted Data Engineer; passionate about technology in healthcare IT
You are a collaborator. You thrive in environments that freely exchange ideas and view points
You are an innovator that believes in making a difference and having fun doing it

The Role:

Create, maintain, and support ETL pipelines
Maintain Data Warehouse infrastructure
Work with application development teams to understand data representation
Maintain documentation for the Data Warehouse and other data products
Support data usage and infrastructure needs of downstream analytics teams

Skills & Requirements:

BS in Computer Science or equivalent work experience
Experience with Talend ETL or similar
SQL and relational databases
Object-oriented programming
Strong problem solving skills, adaptable, proactive and willing to take ownership
Strong commitment to quality, architecture and documentation
Experience using AWS or other cloud services, a plus
Experience with big data tools (Hadoop, Spark, Kafka, Airflow), a plus
Experience with Python, Scala and R, a plus

Modernizing Medicine Benefit Highlights:

Your compensation is a combination of base salary and bonus potential
Health Insurance, 401(k), Vacation, Employee Assistance Program, Flexible Spending Accounts
Weekly catered breakfast and lunch, treadmill workstations, quarterly onsite massages, onsite dry cleaning, onsite car wash and many more!

","Boca Raton, FL",Data Engineer,False
911,"Data engineers at Pandora are responsible for the services and infrastructure capable of processing and making available the extensive volume of data produced on its platform each day. Data Engineers build the infrastructure needed to enable analysts and scientists to query and author data products that operate against our largest collections (billions of events per day). At Pandora the data team supports a variety of business functions including our science, marketing, product, finance and sales teams. You should have a solid understanding of Java software development, and take personal responsibility for testing the code you write. You should have strong academic credentials and a degree in Computer Science or a related field. You should be enthusiastic about learning new technologies and skills. You must be capable of managing your time well and working collaboratively. Excellent communication skills, both written and verbal, are required.
Responsibilities:
5+ years development experience of which 2-3 years are focused on data or analytics engineering working with big data technologies (Hadoop: i.e. MapReduce, HDFS, Tez, Hive, Spark)
Experience with one of the following distributed databases: MySQL, Postgres, Redis, NoSQL or NewSQL
Experience developing in one of the following: Java, Scala, C/C++, or Python
Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server side software for deployment
Experience developing SQL applications of significant complexity
Experience developing service oriented architectures/orchestration including the support of data science
Experience with API design/development (i.e. RPC, REST, JSON)
Significant experience unit testing with frameworks i.e. JUnit
Requirements:
Experience collaborating with data scientists, exposure to machine learning algorithms and/or statistical modeling methods.
Experience with anomaly detection, recommender, or search systems.
Experience with Apache Spark and Kafka.
Experience working across the full technology stack
BA/BS or above in Computer Science or a related field
“We're considering candidates for multiple positions and levels. Successful candidates will be placed at appropriate level depending on their qualifications or experience.”","Oakland, CA",Senior Data Engineer,False
912,"THE OPPORTUNITY

Scholastic is seeking a Principal Data Engineer to lead ongoing development of the Scholastic Data Cloud (SDC). The candidate will demonstrate a successful track record of thought leadership and management skills coupled with deep experience engineering and managing cloud-based data platforms. The ideal candidate will be distinguished not only by their experience working in a cloud-based, multi-tenant environment, but also by high levels of creativity, passion and thought leadership.

YOUR RESPONSIBILITIES

Lead the Scholastic Data Cloud (SDC) development effort focusing on scalability, quality and performanceManage a team of data engineers located both on and off shoreServe as the organization’s expert on SDC platform technologies and architecturePerform employee performance reviews and regular one-on-one sessionsPerform code reviewsEnsure adherence to Scholastic’s enterprise best practicesDesign efficient, scalable processes to acquire, manipulate and project dataParticipate actively in all facets of the Agile processContribute to the SDC codebase directly by taking on development tasks
HOW YOU CAN FIT

Bachelor’s Degree in Computer Science, Math, Statistics or other quantitative discipline8+ years experience implementing enterprise data solutions3+ years experience in a management roleExtensive experience with AWS data services: Redshift, RDS, DynamoDB, Data Pipeline, EMRExtensive experience writing and tuning SQL queries5+ years using ETL / data movement tools (talend, Pentaho, Glue, SSIS, Sqoop, Matillion, Informatica)3+ years programming in one or more languages (Python, Java, Scala, C/C++)2+ years experience with NoSQL data platformsExperience implementing a data lake architectureExperience with Agile process methodology, CI/CD automation, Test Driven DevelopmentExperience with AWS Kinesis, Kafka, Storm, Spark, SonarQube highly desired
WHO WE ARE

Scholastic Corporation (NASDAQ: SCHL) is the world's largest publisher and distributor of children's books, a leading provider of core literacy curriculum and professional services, and a producer of educational and entertaining children's media. The Company creates quality books and ebooks, print and technology-based learning programs for pre-K to grade 12, classroom magazines and other products and services that support children's learning both in school and at home. With operations in 14 international offices and exports to 165 countries, Scholastic makes quality, affordable books available to all children around the world through school-based book clubs and book fairs, classroom collections, school and public libraries, retail and online. True to its mission of 97 years to encourage the personal and intellectual growth of all children beginning with literacy, the Company has earned a reputation as a trusted partner to educators and families. Learn more at www.scholastic.com.

Some benefits that we offer:

100% vested of 401(k) Retirement Plan after 5 years employmentUp to 1M worth of supplemental Life InsuranceTuition ReimbursementPurchase Scholastic stock at a 15% discount
Thank you for your consideration in choosing Scholastic.","New York, NY",Principal Data Engineer,False
913,"We're LendingHome. We're on a mission to revolutionize the world of mortgages and put the power, and the keys, where they belong—in your hands.

The Team

LendingHome is reimagining the mortgage process from the ground up, and data is at the core of everything we do. As a Data Engineer, you will be immersed in some of the most fascinating data sets in the world. Your work is high-leverage as it impacts business strategies, loan operations, credit decisions, product offerings, and risk management.

Responsibilities


Build data pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Manage and extend a reliable, effective, and scalable data infrastructure
Work closely with analysts, data scientists, and product engineers to understand business needs and design/maintain scalable data models
Implement systems for monitoring data quality and consistency
Productionize machine learning models that power our data products and integrate with our loan origination platform

Qualifications


3+ years of applied big data experience
Advanced skills in Java/Scala/Python
Expertise in distributed database/columnar data store (Redshift/BigQuery)
Expertise in a modern data processing and workflow management framework (Airflow/Luigi/Azkaban)
Understanding of dimensional modeling and familiarity with BI tools (Looker/Tableau)
Excellent teamwork and communication ability
CS, Engineering, Math, or related quantitative discipline
Mortgage background not required, but eager to learn the business

LendingHome is an Equal Opportunity Employer
San Francisco Fair Chance Ordinance Police Code, Article 49 ( http://sfgov.org/olse/sites/default/files/FileCenter/Documents/11600-Art%20%2049%20Official%20Notice%20Final%20091114.pdf )

( https://www.themuse.com/companies/lendinghome )

( https://www.linkedin.com/company-beta/3637074/ )

( https://twitter.com/lendinghome?lang=en )
( https://www.facebook.com/lendinghome/ )
( https://tech.lendinghome.com/ )","San Francisco, CA",Sr. Data Engineer,False
914,"Hinge Health’s mission is to improve the lives of people suffering from chronic conditions by digitizing the delivery of care - starting with musculoskeletal health. We’re already achieving remarkable outcomes - helping people overcome chronic pain, avoid surgeries, return to work, and get back to doing the things they love. We’re now rapidly signing large enterprise customers.

You’ll work at the intersection of product and engineering and be responsible for implementing data infrastructure that will enable the company to scale its data science and analytics operations. You’ll also be working on challenging data science problems, such as creating models to predict clinical outcomes and patient engagement.

We’re firm believers in growth and career development. You’ll own your role, be part of the decision-making process and manage your own time.
RESPONSIBILITIES
Guide the development of Hinge Health’s data infrastructure
Work closely with our product and business development teams to derive insights and think critically about user behavior and clinical data.
CONSIDERATIONS
Experience implementing and maintaining data infrastructure
Opinionated on how to organise and maintain functions / modules / scripts across a data science team
Opinionated on how data should be accessed by non-technical analysts and product people
SQL, python and/or R expertise expected
Ability to work autonomously when needed, and work in a fast-paced environment
Previous DBA experience is a huge PLUS
BONUS POINTS
Competitive salary
Meaningful stock options (We want everyone to feel like a real owner)
Medical, dental, vision, 401K
Flexible vacation policy
Lunch & snacks
Company outings (hikes, running, climbing events, and more)
Opportunity to join a fantastically talented, diverse, and passionate team that’s working on a meaningful problem
We’re firm believers in “Learn-it-all” versus “Know-it-all.” To that end, we are looking for a candidate with an open mind and willingness to hustle to drive success at Hinge Health. If you're interested - we'd love to hear from you. No recruiters, please.","San Francisco, CA",Senior Data Engineer,False
915,"As a Data Engineer on the Analytics Systems team, you’ll work with a talented team of engineers to improve Credit Karma’s data pipeline that powers our recommendation systems, enterprise tools, and data warehousing. You’ll help to build a general, secure, scalable, fast, and high throughput data pipeline to process many terabytes of data a day.
We are very passionate about performance, correctness, and data quality. The team spends their time day to day developing new pipelines or features that make it seamless for data to be moved throughout our infrastructure.This includes working with cutting edge tools such as Scala, Kafka, Spark, Akka and Google Cloud. We are looking for data engineers at all levels. If you enjoy working in a collaborative environment where everyone can have their say while still being able to set and hit deadlines, then this is the role for you.
Responsibilities
Work with analysts and data scientists to define processes and standards to inform system design, transform their needs into streaming or batch processing
Design infrastructure and systems to scale easily as data ingest grows
Implement new data pipeline features with verified high quality from unit test coverage and production monitoring
Focus on data quality! Detect data/analytics quality issues and implement bug fixes and data validation for prevention
Help understand our day to day operations for continuous improvement of production systems
Our Ideal Candidate
2+ years experience with Big Data technologies (we’re hiring all experience levels)
Experience with Scala/Java, Spark, Kafka, or demonstrated ability to pick up new technology quickly
Fundamental knowledge about databases and strong SQL skills
Familiar with Google Cloud ecosystem such as BigQuery, GCS, DataProc, etc
Enjoys working collaboratively; CK’s values include empathy and helpfulness
Able to estimate and meet deadlines
Excellent verbal and written communication skills
Nice to have
Experience working with cloud technologies
Experience scaling data throughput or building low latency streaming pipelines
Experience solving for data quality
Learn more about Credit Karma at creditkarma.com/careers
#LI-AG1","San Francisco, CA 94104 (Financial District area)",Senior Data Engineer,False
916,"Company Profile
Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management and wealth management services. The Firms 55,000 employees, located in 1,200 offices across 43 countries, serve clients including corporations, governments and individuals. As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence, a strong team ethic and giving back to our communities. Morgan Stanley provides a superior foundation for building a professional career - a place for people to learn, achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.

Division & Department Profile
The mission of the global Enterprise Technology & Risk (ETR) division is to provide a highly reliable and commercial technology platform, which supports the Firms strategy, delivered by an innovative, world-class team of professionals. Technology & Information Risk (TIR) is part of the ETR organization and manages operational and technology related risks on behalf of the Firm. TIR's mandate is to enable the Firm to manage its technology and data related risks through implementing proactive, comprehensive and consistent risk management practices across the Firm to protect the franchise while capturing business opportunities. The TIR team partners with the business by ensuring that Technology and Data understands how to manage escalate and monitor risk. The mission of the Cybersecurity organization within TIR is to identify and protect Firm assets through proactively assessing threats and vulnerabilities and detecting events, and ensuring resiliency through agile response and recovery.
With Cybersecurity, Morgan Stanley’s state-of-the-art Fusion Center (Fusion) is charged with understanding, detecting, and responding to cyber events, vulnerabilities and incidents that threaten the Firm’s clients, assets, and reputation. Partnering with key stakeholders across Enterprise Technology & Risk and the Business Units, Fusion manages cyber events from detection through response to resolution, and serves as the Firms focal point for cyber communications and reporting. Fusing together information received externally from our partners and internally from our detection and analytics teams to enable rapid decision-making, Fusion is the cornerstone of the Firms agile and adaptive cyber defence strategy, enabling rapid realignment of our defensive capabilities to adapt to changing adversary threats.

Team Profile
The Cyber Analytics team plays a critical role in the Fusion Centers ability of to detect and respond to threats against the Firm. The team is responsible for developing and delivering a suite of advanced monitoring capabilities to enable real-time threat detection delivered directly to incident response teams, as well as the incident response workflows and tools used by incident responders. The team is also responsible for the content and technology of the Fusion Ops Wall, an array of large high-resolution displays that provides situational awareness and real-time visualization of the Firms technology assets, applications, and security controls designed to allow cyber teams to quickly detect any evidence of anomalous activity.

Role Description/ Primary Responsibilities
The Cyber Analytics team is seeking a data engineer to collaborate with other developers and business users in an agile environment to develop state-of-the art detection and response capabilities to counter cybersecurity threats, including:

Engage with business users to define requirements
Discover and correlate data from disparate sources to report on and analyze computer and network activity
Collaborate with other developers on technical design of data discovery components
Automate data collection and delivery
Prepare data for use in analytics
Implement streaming and batch data transformations
Work in on premise and cloud environments

Skills Required

3+ years of data engineering experience with a demonstrable portfolio of achievement
Expertise with big data platforms such as Kafka, Splunk, Hadoop and Elastic
Knowledge of Linux and computer networking
Experience with relational databases is nice to have
Ability to provide technical leadership to junior team members and direct contingent resources
Excellent written and verbal communication skills
Proven collaborative abilities to work with other developers and business users to craft end-to-end solutions

Skills Desired

Positive attitude and enthusiastic desire to learn new technologies and expand professional skills
Strong interest in cybersecurity concepts and incident response process
Experience working on a team distributed across continents and time zones","Baltimore, MD",Cyber Analytics Data Engineer,False
917,"ContractIn immediate need for an experienced Big Data Engineer for a long-term engagement in Malvern, PA. Candidate will be required to work onsite.AWS Sagemaker enablement.o For enabling AWS SageMaker (build, train and deploy and infer), work within Customer’s Chief Technology Office (CTO) cloud analytics services team to assist Customer to meet requirements related to authentication (with Active Directory or RadiantLogic), authorization, auditing (ensure AWS CloudWatch logs have appropriate audit trail) and integration with Customer Anaconda Repo. Work with Customer’s CTO team to meet requirements related to working within Customer Virtual Private Cloud (“VPC”) using Amazon Simple Storage Services (Amazon S3) buckets.Assist Customer to support enablement of AWS SageMaker via AWS Service Catalog using standard Customer tools such as BitBucket, Bamboo, Ansible, AWS CloudFormation templates. Notifying, tracking and enabling any gaps in AWS SageMaker.Artificial Intelligence Integration. Provide engineering and AI related services to integrate (Amazon Transcribe, Amazon Comprehend, Amazon Rekognition and Amazon Lex), to the Customer CTO cloud analytics services team.· Automation of Domino Data Labs – ongoing improvement of automated install· Enabling of alternative data access patterns for Center for Analytics and Insights (CAI) IT to leverage data lakes· Enabling higher performance user-case based data stores such as Amazon Redshift in line with Customer requirements· Investigation of financial services data patterns and how to leverage those within IT· Improving Amazon EMR reliability in areas of network and Domain Name System (DNS) issues and provide recommendations to increasing reliability for Amazon EMR cluster operations· Assist in implementing recommendations and reference architecture that can will serve as the foundation for CAI’s Continuous Integration / Continuous Delivery pipeline for model deployment· Implementing Amazon EMR and Domino Data Labs integration with Customer business system· Implementing best practices for data science model stores in line with Customers data pattern use casesJob Types: Full-time, ContractExperience:Amazon EMR cluster operations: 2 years (Required)AWS Sagemaker: 5 years (Required)Artificial Intelligence (AI): 2 years (Required)Location:Malvern, PA (Required)","Malvern, PA",Big Data Engineer - AWS (Sagemaker enablement),False
918,"Sr. Data Engineer/Architect

About Capgemini
With almost 200,000 people in over 40 countries, Capgemini is one of the world's foremost providers of consulting, technology and outsourcing services. The Group reported 2017 global revenues of USD 15.78 billion. Together with its clients, Capgemini creates and delivers business and technology solutions that fit their needs and drive the results they want. A deeply multicultural organization, Capgemini has developed its own way of working, the Collaborative Business ExperienceTM, and draws on Rightshore®, its worldwide delivery model. Learn more about us at http://www.capgemini.com. Rightshore ® is a trademark belonging to Capgemini Rightshore® is a trademark belonging to Capgemini.
 Capgemini America Inc is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Job Description
Job Title: Sr. Data Engineer/Architect
Job Type: Full Time
Job Location: Multiple locations in NY, NJ, NC, IL
Role and Responsibilities:
The Insights & Data team is looking for a Senior Data Prep Engineer, with strong background in Big Data and Business Intelligence. This is a hands-on Data Engineer role where the individual will be responsible in the overall design, including Integrated Data Warehouse IDW, ODS and or Hadoop Data Lake. A person entering this role requires experience in data modeling, data architecture for near-real time data ingestion, API design, etc.
Responsibilities:
Perform detailed design reviews with product owners, developers and business stakeholders
Stay abreast of information management trends and standards, master data management, data services, self-service business intelligence, metadata management, data quality, and data governance.
Build & maintain Reference Model Architecture and Standards for communicating direction of Information Architecture.
Work with the platform team in optimizing Data Architecture and optimize use of data assets for Data Science.
Develop proof-of-concept and prototypes to help illustrate approaches to technology and business problems
Qualifications
Excellent English communication skills, both written and verbal
Experience with Linux, Grep, Shell scripts and command line tools
Development experience using some of the following technologies, Python, PHP, SQL or Java, Angular JS, SQL or SAS ETL
Ability to prioritize, and manage time efficiently
Capacity for independent and autonomous work within a team environment",North Carolina,Sr. Data Engineer/Architect,False
919,"Our client, an excellent company in New Haven, CT is seeking a Junior Data Engineer to join their growing team!
Please apply to this job posting with your resume and contact information and our team will reach out to you right away.
Responsibilities of the position will include the following:
Create and maintain reporting processes/workflows for assigned campaigns
Campaign setup in systems prior to launch date
Reporting file uploads into analytics database
Report build out for analytical purposes
API Queries & VBA macro implementation
Provide support in key development/process improvement projects
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, etc.
Work with stakeholders including the Analyst & Media teams to assist with data-related technical issues and support their data infrastructure needs
Excellent applicants for this position should have the following qualifications:
Bachelor’s degree is preferred
Interest in coding
Eagerness to start ASAP!


APPLY NOW

NOT READY TO APPLY?","New Haven, CT",Junior Data Engineer,False
922,"At Harry's we are building a cutting edge technology stack to support our multi-brand vision. We are already a successful men's grooming brand. We launched in 2013, focused on quality craftsmanship, simple design and modern convenience and now we have over 3 million customers worldwide.

With our most recent round of financing ($112 million), we are developing brands beyond men's grooming. The technology required to enable our goals is more sophisticated than first meets the eye. If you are a driven, talented engineer, come join us and help build the platform that will enable our new vision.

About the Role

As a Staff Data Engineer, you will be responsible for building scalable data pipelines to support a variety of business needs. You will drive the tooling and construction of Harry's data lake and warehouse solutions. You are an expert in constructing and maintaining performant and scalable pipelines leveraging Kappa & Lambda Architecture best practices. You will report to the Senior Manager of Data Engineering.

What you will accomplish:

Take ownership of the design and implementation of scalable ETL processes and data pipelines
Drive the construction of the Harry's data lake and data discovery platform
Drive the re-architecture and fine tuning of the Harry's data warehouse for use by BI tools, marketing platforms, and machine learning models
Partner with data scientists, data analysts and business users in enabling data driven decisioning including data wrangling, ad-hoc queries and supporting ML modeling
Work with vendors and other developers to ingest new sources of data
Contribute to improving key data engineering performance, availability and customer satisfaction metrics in support of additional brands, systems and geographies

This should describe you:

You have experience with the AWS stack including S3, Athena, Redshift, EMR, ECS, Lambda
You have 3-5 years of hands on experience with Python or Scala
You have 3-5 years of experience in the field of business intelligence, application development, database development and ETL and/or data analysis domains with strong Hadoop, Hive and/or Spark knowledge
You have hands on experience with event based streaming pipelines
You have a strong command of SQL
You have experience with NoSql databases
You know how to make large software projects maintainable and extensible in the long run
You are curious about software and how things work, you work to understand technical problems deeply, and you stay current.
You are able to productively collaborate with business users, data analysts, and data scientists. You invest in the success of your teammates, cross-functional partners and projects.

Harry's is committed to bringing together individuals from different backgrounds and perspectives. We strive to create an inclusive environment where everyone can thrive, feel a sense of belonging, and do great work together. As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability or genetic information. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our company.","New York, NY","Staff Engineer, Data",False
923,"Are you seeking an opportunity to be a key partner in the enterprise level data management operations and development for a large organization? Are ready for an opportunity that not only allows you the opportunity to grow but the opportunity to grow others? Do you enjoy working in collaborative environments? Are you a team player? Are you able to work with clinical/business/functional partners, 3rd party vendors regarding the the impact on data architecture?This position requires an advanced Data Engineer to work closely with other Data Engineers, DBA’s, Data Analysts, and Analytics experts. In addition, assist in mentoring level I Data Engineers and Data Analysts. The Data Engineer will be responsible for integration, migration, and optimization of data flow and collections across a variety of environments working with other departments across the entire enterprise. The Data Engineer will support other technology specialists not limited to Software Engineers, Data Architects, Data Scientist, Software Engineers, Web Developers, and Quality Analyst. Assist in the future strategy and direction of data innovations and implementing the next generation data advancements; able to adapt to constant change, growth, and new solutions. This Engineer must be comfortable working independently yet able to collaborate when necessary supporting multiple systems and groups within IT and the organization.Responsibilities Include, but not limited to:Identifyin innovative uses of data and information to drive improved patient care, productivity, and operational efficiency, increased departmental agility, and higher performance.Assist in establishing standards and guidelines for the design & development, tuning, deployment and maintenance of information, advanced data analytics, and text mining models and physical data persistence technologies.Create/maintain the Development Business Intelligence data architecture to increase the robustness, performance, and scalability of systems.Provide oversight and direction for the design and development of the data infrastructure.Maintain quality of data in the Development Data Mart.Ensuring, integrity of data in the Data Mart, correcting any data problems, data consistency, establish and design procedures to purge/archive old data.Provide guidance to HITS regarding data consistency between development's OLTP, Application, Ancillary Systems, and Enterprise Data Warehouse.Create/maintain the ETL architecture and design using Health Catalyst. Participation in the Data Advisory Committee or other Governance groups.Mentor BI, Data, Application, and Infrastructure Strategy team members on ETL tool and architecture.Create streamlined, scalable integration solutions to support BI and analytic environment.RequiredHigh School DiplomaBachelor’s Degree in Computer Science/Information Systems5 to 7 years' experience with ETL technologies (SSIS, Data Stage, Informatica).5 to 7 years’ experience with OOP (C#, C++, Java, etc)3 to 5 years’ T-SQL experience in a relational database modeling preferably in a large, complex healthcare environment; or an equivalent combination of education and experience3-5 years’ experience with consuming data from API’s (RPC, REST,WSDL, etc)5-7 years’ experience with one or more BI tools such as SAP Business Objects,Tableau, PowerPivot, QlikView, SAS.Job Type: Full-timeExperience:BI Tools: 5 years (Required)Data Warehouse: 5 years (Required)T-SQL: 3 years (Required)ETL Technologies: 5 years (Required)Open Object Programming: 5 years (Required)API: 3 years (Required)Education:Bachelor's (Required)","Mission, KS",Data Engineer,False
924,"Drillinginfo- Who We Are
At Drillinginfo, our mission is to provide better, faster decision-making support to the oil and gas industry through our data intelligence and analytics products. We are building an agile development culture that delivers great products to a wide variety of customers. We truly believe that diversity of experience, perspectives, and background will lead to a better workplace for our employees and better products for our clients. We need an experienced engineer with a passion for software development, someone to be hands-on in designing, implementing, and delivering features for our flagship product.

Job Overview
As a Data Engineer, a person will have the opportunity to shape the future of Drilling Info, Inc by building and maintaining business-critical customer-facing applications and helping deliver the next generation of Oil and Gas information retrieval and analytics.
A Data Engineer will be responsible for creating and maintaining the pipelines to assemble different datasets, all this following an architecture defined by the company.
A Data Engineer should write maintainable code, and work in a professional software engineering and teamwork environment (source control, shortened release cycles, continuous integration/deployment, documentation, agile development).
A Data Engineer should have excellent analytical and critical thinking skills, should show initiative and a positive attitude.
Competitive Candidate Profile
Bachelor’s Degree in Computer Science or similar
5+ years of experience building, maintaining and delivering data-based products
Advanced Level with Programing Languages: Python
Strong familiarity with database languages, SQL Server, SSIS
Strong familiarity with version control systems: Git, SVN
Strong familiarity with agile development
Experience with developing object-oriented, enterprise-sized systems
Desirable experience with Programing Languages: C#, .Net
Desirable experience with: Airflow, AWS, Kafka, Docker
Desirable experience with every layer of an Enterprise system (from Database to UX).","Austin, TX",Data Engineer,False
925,"Job Description

The Auto Club Group (ACG) provides membership, travel, insurance and financial services offerings to approximately 9 million members and customers across 11 states and 2 U.S. territories through the AAA, Meemic and Fremont brands. ACG belongs to the national AAA federation and is the second largest AAA club in North America.

Primary Duties and Responsibilities (details of the basic job functions):
The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. Responsibilities include:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google Cloud Platform ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and cloud vendor regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Supervisory Responsibilities (briefly describe, if applicable, or indicate None):
none

Preferred Qualifications:
Knowledge of Meta and Master Data Management
Familiar with Google Cloud Platform Service cloud services like: Cloud Sql, BigQuery, DataFlow, DataPrep, AppEngine
Knowledge of stream-processing systems: i.e.: Storm, Kafka, etc.

Work Environment:
Works in a temperature controlled office environment.

Qualifications

Required Qualifications (these are the minimum requirements to qualify):
Education (include minimum education and any licensing/certifications):
Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field

Experience:
3+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Proficiency building and optimizing ‘big data’ data pipelines, architectures and data sets.
Background performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.

Knowledge and Skills:
Strong analytic skills related to working with structured and unstructured datasets.
Project management and organizational skills.
Experience with relational SQL and NoSQL databases
Knowledge of data pipeline and workflow management tools: i.e.: Knime, DataFlow, DataPrep, Airflow, etc.
Familiar with object-oriented/object function scripting languages: i.e.: Python, Java, C++, Scala, etc.
Familiar with big data tools. Examples include: Hadoop, BigQuery, Kafka, etc.

The Auto Club Group offers a competitive compensation and benefits package including a base salary with performance based incentives; medical/dental/vision insurance, 401(k), generous time off, a complimentary AAA Membership and much more!

Important Note: The above statements describe the principal and essential functions, but not all functions that may be inherent in the job. This job requires the ability to perform duties contained in the job description for this position, including, but not limited to, the above requirements. Reasonable accommodations will be made for otherwise qualified applicants, as needed, to enable them to fulfill these requirements.

The Auto Club Group, and all of its affiliated companies, is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability or protected veteran status.","Dearborn, MI 48121",Data Engineer,False
926,"About The Opportunity
Got a taste for something new?

We’re Grubhub, the nation’s leading online and mobile food ordering company. Since 2004 we’ve been connecting hungry diners to the local restaurants they love. We’re moving eating forward with no signs of slowing down.

With more than 85,000 restaurants and over 15.6 million diners across 1,600 U.S. cities and London, we’re delivering like never before. Incredible tech is our bread and butter, but amazing people are our secret ingredient. Rigorously analytical and customer-obsessed, our employees develop the fresh ideas and brilliant programs that keep our brands going and growing.

Long story short, keeping our people happy, challenged and well-fed is priority one. Interested? Let’s talk. We’re eager to show you what we bring to the table.
Some Challenges You’ll Tackle
Working with high volumes of data to efficiently process and expose for analysis
Collaborating with other engineering teams on strategies for data
Work with cutting edge data processing technologies
Understand our stakeholder (Finance, Marketing, Product) requirements and write complex and efficient code to transform raw data into an easy to approach data marts.
Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company
Analyze data to measure impacts of data schemas and use it to iterate on improvements
Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way
You Should Have
Excellent knowledge on SQL, data modelling and patterns.
3-5 years experience with Python or another general purpose programming language
Background in writing ETL jobs within a Business Intelligence context
A bachelor's degree, preferably in a computer-related discipline.
Enthusiasm for the job. Are you excited about data? Do you love your users? Good, the same goes for us
Got These? Even Better:
Experience big data processing with Spark and other big data tools a plus
Excellent communication skills, including the ability to crystallize and broadly socialize insights
Problem analysis and problem-solving skills
Rigorous attention to detail and accuracy
Exposure to Amazon AWS or another cloud provider
Adaptability and collaborative skills
And Of Course, Perks!
Unlimited paid vacation days. Choose how your time is spent.
Never go hungry! We provide weekly GrubHub/Seamless credit.
Regular in-office social events, including happy hours, wine tastings, karaoke, bingo with prizes and more.
Company-Wide Initiatives encouraging innovation, continuous learning and cross-department connections.

We deliver favorites every day. Join us as we move eating forward.
Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. The EEO is the Law poster is available here:DOL Poster. Grubhub is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please send an e-mail to TalentAcquisition@grubhub.com and let us know the nature of your request and your contact information.","Chicago, IL 60602 (Loop area)",Data Engineer II,False
927,"ContractJob SummaryDoes empowering teams to make data driven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with more data? Data Engineering focuses on making possible fast, accurate, and reliable access to data. We build data pipelines, manage a data warehouse, and support the production use of our data. We advocate for good data practices and make sure that our business users are able to make good data driven decisions.Provide engineering on modern, cloud-based data processing technology stackBuild data pipelines, data validation frameworks, job schedules with emphasis on automation and scaleContribute to overall architecture, framework, and design patterns to store and process high data volumesEnsure product and technical features are delivered to spec and on-timeDesign and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners with an Agile / Scrum methodologyProactively support product health by building solutions that are automated, scalable, and sustainable - be relentlessly focused on minimizing defers and technical debtQualificationsMasters’ or Bachelors’ degree in Computer Science or a related field5+ years of experience in large-scale software development with emphasis on data analytics and high-volume data processing3+ years of experience in data engineering development2+ years of experience implementing scalable data architectures2+ years of experience with AWS and related services (e.g., EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake)Experience in data-centric programming languages (e.g., Python, GO, Ruby, Javascript, Scale)Proficiency with ETL tools and techniquesKnowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, IMS, IDMS, MySQL, Postgres, SAP HANA, and TeradataExperience with participating in projects in a highly collaborative, multi-discipline team environmentJob Type: ContractExperience:Data Engineer: 5 years (Preferred)","Renton, WA",Sr. Data Engineer,False
928,"$160 - $180 a dayOne of the world’s leading video-sharing platforms is hiring for a Big Data Engineer. This company attracts over 300 million unique visitors and 3 billion videos views worldwide per month by offering the best content from users, independent content creators and premium partners.This company is building its programmatic and monetization product by building its own video ad stack to deliver new monetization solution for its own ecosystem around online, mobile and TV, and provide innovative marketing solutions for advertisers.
In this role you will design and build highly-scalable data pipelines and data stores using cutting edge big data technologies.Design, build, and maintain petabyte-scale datastores, and high-performance, horizontally-scalable processing pipelines and create processes for large-scale ingest and export of data, as well as enabling fast data query capabilities
Required Skills & Experience
Hands-on experience using the Hadoop MapReduce and/or Spark distributed computing frameworks
Experience programming and/or architecting a back end language (Java, J2EE, Core)
Experience with non-relational & relational databases – (SQL, MySQL, NoSQL, Cassandra, Redis, Aerospike, RedShift, Vertica)
Strong skills with the SQL and HQL (Hive) data query languages.Strong coding skills in Java. (Python or Scala a plus)
Exposure to streaming technologies (e.g., Spark Streaming, Apache Storm, Flink) and/or containerization technologies (e.g., Mesos, Docker, Kubernetes) is a plus
Ad tech experience is a plus
The Offer
Strong compensation, competitive annual bonus, 401k, health, dental, vision, limitless PTO policy","New York, NY",Big Data Engineer,False
929,"Responsibilities
Business requirement analysis and assist in the implementation of new and existing systems.
Actuarial domain knowledge of business analysis, development, testing and implementation of software for insurance services for Hedging Product.
Continually increase business acumen and awareness of technology best practices to help the team deliver business solutions.
Supports process framework, governance standards, audit controls, architecture and financial management.
Work with various quantitative and actuarial staff members to determine and implement possible solutions for valuation and projection of annuities.
Communicates effectively with project sponsors and stakeholders of project status, progress, risks, issues and expected outcomes.
Devise, document and implement conceptual and quantitative models to solve business problems.
Gather pertinent information and data sources across disciplines to formulate solutions.
Analysis, design and Implement software using an agile approach.
Coordinate with the IT team the adoption of systems in the production environment under SDLC guidelines.
Is responsible for producing and presenting departmental level analysis to mid-level management.
Develop and maintain subject matter expertise required to advise businesses management.
This role may include work in ALM, modeling, research, trading/hedging strategies.
Required Qualifications
Requires a graduate degree in mathematics, actuarial science, finance, business, or related field with 4 years relevant work experience OR Bachelor’s degree and FSA/CFA/Equivalent Designation plus 5 years relevant work experience OR Bachelor’s degree plus 8 years relevant work experience may be substituted for graduate degree.
Must possess excellent understanding of investment and finance concepts, and be able to creatively apply them in solving analytical problems in the business setting.
Must possess excellent communication skills.
Preferred Qualifications
Degree in mathematics or engineering
Works well under pressure and within time constraints to effectively accomplish individual and team objectives.
Able to work within a fast-paced environment with quickly changing priorities.
Advanced computer skills, including SQL and UNIX.
Knowledge of ETL tool, preferably Informatica.
At least 1 year of experience in insurance product.
Experience in big data technologies like Hive, Impala.
Communication skills to convey complex information to business both verbally and in writing at an appropriate level of detail for each audience.
Developed and delivered data-driven solutions to support business needs and analytics.
Expertise in relational database and SQL, preferably SQL Server and Oracle.
Strong understanding of data warehousing, analytics, reporting and best practices.
Experience in financial services industry.
Knowledge of equity, fixed income, credit, and derivative instruments.
Behavioral and Leadership Competencies:
Make tactical, data driven decisions leveraging experience and with consideration to competing priorities. Consult with end-users and respond with solutions.
Able to exercise judgment as it relates to business decisions and their effects on stakeholders.
Highly organized and detail oriented with the ability to maintain a high level of accuracy.
Demonstrated business acumen and the ability to apply technology solutions to solve business problems.
Our Culture:
At Transamerica we promote a Future Fit mindset. What is a Future Fit mindset?
Acting as One fosters an environment of positive collaboration
Accountability allows us to own the problem as well as the solution
Agility inspires new ideas, innovation and challenges the status quo
Customer Centricity encourages an above and beyond approach to our customer","Cedar Rapids, IA",Data Engineer,False
930,"About Us

Teza is a quantitative asset management firm that strives to develop innovative, high-Sharpe investment products for its clients. Originally founded in 2009 as a science and technology-driven global quantitative trading business, Teza derives its unique edge in asset management from its high-frequency trading past and science-based investment approaches. Under the leadership of CEO Misha Malyshev, Teza's innovative approaches to quantitative research and platform engineering distinguish us from other quant trading firms. We have successfully attracted and assembled a group of top talent, including widely recognized experts in quantitative trading. Teza has over 50 professionals worldwide with offices in Austin, Berkeley, Chicago, London, New York, and Shanghai.

About the Role

Teza Technologies is looking for a lead data engineer to join our core services technology team. Data drives systematic trading and is critical to all aspects of the firm's business. This is a hands-on senior position on a team of 3 data engineers with significant growth potential, as this team will grow rapidly over the next couple of years. firm is looking for outstanding technical skills and significant experience architecting and building data platforms.

Responsibilities


Design and build a data platform that will leverage state-of-the-art data technologies to minimize

development time and maximize performance and flexibility

The platform will facilitate data pipelines from vendors and other sources.
Clean and store the data
Automated anomaly detection
Mange access/entitlements
Build mechanisms that allow our researchers and analysts to interact with the data
Manipulate the data and store it back for reuse
Provide a platform for back-testing
Be an SME for company-wide questions about the nature, completeness and correctness of the data
On-board new data sources and maintain external relationships

Requirements


It is critical that the candidate will bring a CS view to the process of data management.
Significant experience in systems design is desired
Experience with SQL and No SQL databases
Postgres, MongoDB is preferred
Experience with Big Data technologies (Hadoop, Spark)
Preference for Python, Java is a plus

","Austin, TX",Data Engineer,False
931,"$75 - $85 an hourContractOur client is seeking experienced Data Architects that take advantage of data, analytics architectures & various tools to deliver solutions. This is an excellent opportunity for an individual to grow their capabilities in a thriving organization.ResponsibilitiesDesign & develop solutions for present data from a variety of sources and formats for analysis and use across use cases.Extensive hands-on experience using Hadoop in large enterprise environments.Perform data profiling and discoveryWork with source system and business SME’s to develop an understanding of the data requirements for the organizationPerform hands on data development to accomplish the data extraction, movement and integration, leveraging state of the art tools and practices, including both streaming and batched data ingestion techniques.Assist in creation of data requirements and data model design as necessary and appropriate.QualificationsExperience working with the Apache Hadoop Ecosystem of tools and technologiesKey sets of toolsSparkKafkaHiveSqoopPythonExperience working with enterprise wide ETL and streaming dataExperience working with Data Governance frameworksSome experience performing conceptual and logical data model designStrong NoSQL, SparkSQL, and ANSI SQL query language skillsStrong verbal and written presentation skillsRemote work/with some travelContact Johnny Allen 678.730.6966Job Types: Full-time, ContractSalary: $75.00 to $85.00 /hourWork authorization:United States (Required)","Philadelphia, PA",Hadoop Data Engineer,False
932,"Vets First Choice (VFC) is the market leader in providing veterinarians and their clients with cost effective home delivery service of medications and food. Our advanced web tools, client marketing programs, and competitive eCommerce offering far surpass the competition. VFC is licensed in all 50 states with the highest quality accreditation possible, ensuring the safest products for our pet customers and their loving owners. In 2013, Vets First Choice was named #24 on the Inc 500 Fastest Growing Companies list, and we are still growing!
At Vets First Choice, you’ll work on solving interesting problems with current technologies and best practices in a collaborative team that builds an exciting, well-designed product used by veterinarians nationwide.
SUMMARY
As a Data Engineer use your professional expertise to enhance and support the current Data Services machine learning/artificial intelligence model training and services execution environments. Working in an agile environment, this position collaboratively partners across administrative and functional areas of the company. This position works side-by-side with other data developers, data scientists, data architects and analysts utilizing expert hands-on data and development skills.
RESPONSIBILITIES
Thrives in a fast moving environment, working on a variety of projects and technologies in an iterative, team based culture
Strong desire to learn new skills and adapt to new technologies while maintaining attention to detail
Converting product requirements into a well thought through, cleanly designed data integration solutions
EDUCATION
Bachelor's degree in computer science, computer programming, computer engineering or related field preferred, or comparable job-related experience and training
3-5+ years of relevant work experience
COMPETENCIES
Programming languages (Python, Java)
Web services development
System integrations (e.g., Web Services, REST, JSON)
Relational databases (MySQL, Postgres/RedShift and MS SQL Server preferred)
Advanced SQL
Experience in data centric processing environments
Experience developing data migration processes
PLUSES
AWS Database, Machine Learning, Analytics, Security, Identity & Compliance Services
Apache Big Data processing technologies (e.g., Hadoop, Hive, Sqoop, Spark, Kafka)
NoSQL (e.g., DynamoDB, Mongo) and Graphing Databases (e.g., OrientDB, neo4j)
PHYSICAL DEMANDS/ WORK ENVIRONMENT: The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Normal office environment with extensive use of computer.","Portland, ME",Data Engineer,False
933,"Who You’ll Work With
Are you excited to be part of a new team? Does the thought of building a new, highly secure cloud infrastructure excite you? Are you ready to tackle the challenge of ensuring system security from development to deployment? Do you want to become an expert in the system and undertake new challenges?
This is an opportunity for you to join the security team within Cisco AMP for Endpoints, one of the world’s leading security products. The AMP team ships security software for Windows, Mac, Linux, iOS, and Android operating systems.
As a member of the security team, you will have two essential responsibilities. You, along with the team, will be responsible for building a brand-new Government Cloud environment and maintaining the associated FedRAMP certification. You will engage across development teams to ensure Secure Development of the system. You will contribute positively by building efficient tooling and processes which enable the organization to deliver secure code at an unprecedented pace. You will work alongside product owners and engineers to ensure new features meet strict security requirements. You will be tasked with maintaining a deep technical knowledge of the information system and need to be able to write code and features.
In this role, you will work closely with the Data team. The team is responsible for running malware identification on incoming event data streams, storing and indexing that data. Data is indexed both for future detailed investigations of malware incidents, and to retrospectively detect previously unidentified malware in stored data.
We inspire employees to hone their talents and skills every single day with innovative and challenging projects. We recognize and reward quality results and dedication to our company's purposes and principles.
What You’ll do
We are looking for a Software Engineer, Computer Scientist, or Data Engineer to join a team whose goals are to focus on the FedRAMP system and Secure Development. Your primary responsibilities will be:
Build and maintain a new cloud environment for FedRAMP
Ensuring new code meets security requirements and writing supporting documentation
Provide guidance to teams on building secure software
Craft tooling to improve processes and coordination among groups
Supporting continuous monitoring and audit requirements
Implementing new features to drive customer success
Bug fixes and refactoring
Assisting with production issues
Who You Are
You are self-motivated, results-driven, and engaged. You are passionate about back-end development and enjoy collaborating in a team-based environment. You do not back down when faced with complex problems.
Required Skills
Bachelor’s degree in Computer Science, Math, or Physics
Enjoy writing server-side code and unit tests
Knowledge of algorithmic complexity
Ability to debug, diagnose, and resolve occasional production problems
Have four or more years of experience developing in object-oriented Ruby, Java, JRuby and Scala
Experience with Linux command-line and system administration basics: ssh, permissions, packages, log files, &c.
Desired Skills
Experience with distributed systems architecture
Production experience with streaming platforms like Storm and Flink
Production experience with distributed databases like Mongo and Cassandra
Open-source contributions
Peer-reviewed publications
Experience with agile software development methods
Experience in Secure Development methodologies
Experience performing secure code reviews
Experience in security
Experience working with security standards (FedRAMP, ISO, PCI, etc.)
Why Cisco
The Internet of Everything is a phenomenon driving new opportunities for Cisco and it's transforming our customers' businesses worldwide. We are pioneers and have been since the early days of connectivity. Today, we are building teams that are expanding our technology solutions in the mobile, cloud, security, IT, and big data spaces, including software and consulting services. As Cisco delivers the network that powers the Internet, we are connecting the unconnected. Imagine creating unprecedented disruption. Your revolutionary ideas will influence everything from retail, healthcare, and entertainment, to public and private sectors, and far beyond. Collaborate with like-minded innovators in a fun and flexible culture that has earned Cisco global recognition as a Great Place To Work. With roughly 10 billion connected things in the world now and over 50 billion estimated in the future, your career has exponential possibilities at Cisco.","Research Triangle Park, NC",Data Engineer,False
934,"POSITION:

Data Engineer - Bangor

DEPARTMENT:
Business Intelligence



FUNCTION: Design & deploy data warehouses, marts and lakes where appropriate as data stores for business intelligence solutions. Identify gaps and improvements to the data management systems with an emphasis on automation, quality and data delivery solutions. Implement full end-to-end Data Warehousing solutions including data architecture, data provisioning, data integration, data publishing, and execute effectively as part of a team.

A Senior-level Data Engineer is additionally expected to have a practical working knowledge of data modelling concepts and will be able to successfully lead teams or projects related to data design and deployment. An individual in the senior role will require a demonstrated ability to work harmoniously with teams and business lines throughout the bank in a productive and thoughtful manner. This position is expected to act as a role model for other BI team members in all aspects of daily work, including the education of less experienced team members and by leading by example.

A Principal-level Data Engineer is additionally expected to provide leadership and mentoring to other BI team members – both analysts and technical staff. This role additionally takes a proactive approach to continue their education and maintain an expert level working knowledge. Persons in this position are considered the most senior of BI staff, and as such will be expected to take primary responsibility for developing solutions and fulfilling job duties. This position may also act as a backup for management in several capacities including but not limited to; training, coaching and mentoring fellow employees. They will lead within Business Intelligence and work on strategic initiatives that are key to the success of business lines.


ACCOUNTABILITIES:
Understand the data architecture needs and data structures in the source systems and business processes.
Design data marts for business units and collaborate with development teams during the implementation.
Collaborate with internal & external data consumers to understand their data needs and drive towards unifying collections of data requirements for key data elements across the organization.
Document and maintain documentation related data mapping and other data design artifacts that encompass data specifications, business & transformation rules.
Have a high proficiency in a MS SQL environment.
Collaborate with vendors and internal developers in requirements gathering sessions with stakeholders to determine user needs and capture data requirements.
Translate business requirements and data needs into solutions easily used for reporting, scorecards and dashboards.
Apply Bank standards and industry best practices to the ongoing management of the database infrastructure and related technologies.
Demonstrate ownership of database and related technologies and all issues that arise with them.
Ensure the highest levels of availability and performance within BI systems and infrastructure.
Perform BI Administrative functions as requested.
Be responsible for ensuring that all necessary documentation is completed and maintained.


Compliance and Control:
Assists in ensuring that the Bank is in compliance with local, state and federal regulations


General:
Interact harmoniously and effectively with others, focusing upon the attainment of bank goals and objectives through a commitment to teamwork.
Conform to acceptable punctuality/attendance standards as expressed in the Employee Handbook.
Perform additional duties as requested.


Competencies:
Analytical – Observe processes and trends. At the Senior role, make recommendations for process changes that help achieve departmental and individual goals. At the Principal role, proactively make strategic recommendations and work with business line leaders to implement them.
Adaptability/Flexibility – Adapt to change, be open to new ideas, be willing to take on new challenges, handle pressure, adjust plans to meet changing needs, and be able to work in a fast-paced and dynamic environment. At the Senior role, drive change and promote new ideas. At the Principal role, work with the team to adopt change, help prioritize tasks and take on complex challenges.
Initiative – Take independent action, operate as a pro-active self-starter, act on opportunities, and practice self-development. Be willing and able to convey highly technical concepts to non-technical audiences. At the Senior role, proactively works to cross train others on the team. At the Principal role, actively mentors fellow associates and assists with their development.
Integrity/Ethics – Deal with others in a straightforward, honest manner, be accountable for actions, maintain confidentiality, support company values, and convey news good or bad.
Interpersonal Skills – Exhibit good listening skills, be able to participate successfully in team endeavors, and positively influence decision-making processes. At the Senior role, have presentation skills and be able to convey ideas to a large and varied audience. At the Principal level, will be able to help leadership convey new ideas and processes to bank leadership.
Vision/Values – Support company mission/values through daily actions and decisions, communicate the Bank’s vision, mission and values to others, incorporate vision when planning.


Knowledge/Skills/Experience Requirements:
A BS or MS degree in Computer Science or a related technical field or relevant work experience in the field.
Experience and implementation of Data Architecture, Data Lake, Data Marts, Operational Data Store, Analytical systems & Metadata management initiatives.
Experience with schema design and dimensional data modeling.
Experience in one or more programming languages like Python, JavaScript, C#, Java, etc.
Experience working with APIs like REST APIs, SDKs and CLI tools as part of ETL provisioning.
Experience working with multi-format files likes JSON, XML, CSV, Flat, etc.
Relevant technical certification(s) strongly preferred.
Exceptional troubleshooting abilities.
Strong verbal and written communication skills.
Strong documentation skills, to include proficiency with MS-Word, MS-Excel and MS-Visio.
Expert-level knowledge of modern databases and their related toolsets, reporting packages, and underlying technologies.
Strong knowledge of SQL development, performance tuning, index management.
Hands-on experience with data modeling techniques, including with star schemas and contemporary ETL strategies.
Strong knowledge of relational and multi-dimensional databases.
Analytical approach to problem solving and process improvement.
Willingness and ability to maintain knowledge regarding relevant current and emerging technologies and industry trends and best practices.


Physical Demands/Conditions Requirements:
General office environment.
Moderate lifting (to 35 lbs.) required. Moderate reaching, walking, sitting and standing required.


Equipment Used:
General office equipment.


External and internal applications, as well as position incumbents who become disabled, must be able to perform the essential functions (as listed) either unaided or with the assistance of a reasonable accommodation to be determined by management on an individual basis.","Bangor, ME",Data Engineer,False
935,"Did your things make you go ""wow"" today? We're dedicated to improving lives by making the everyday objects around us more useful and with our open platform the opportunities are endless. We believe that the Internet of Things should be accessible to everyone, and we strive to create easy-to-use, secure, and, above all, intelligent devices that take your home to the next level. Our fun, creative, supportive team needs your help to make things that are a little more connected - and a lot smarter.

Looking for a motivated Sr Data Engineer to Staff Data Engineer to contribute towards the success of our Data and Analytics Technology initiatives. This person will be responsible for defining, building and managing architecture strategies, data standards, digital data management, data integration, tools, and technology. The right candidate will play a deep dive hands-on critical development role in the using data to power data driven decision for SmartThings IOT platform and in shaping how we acquire, ingest, transform and deliver data.
Responsibilities:
Responsibilities: Accountable for the big data platform, from strategic design all the way to framework development and daily operations
Provide technical direction within the organization
Hands-on experience developing “Big Data” framework at scale (using Hadoop and related technologies to manage high volume high velocity structured and unstructured data)
Establish monitoring and management practices to proactively provide necessary capacity and performance
Define and champion engineering best practice
Work with other infrastructure teams to ensure platform stability
Technical liaison with technology vendors
Develop procedural and technical documentation, define best practice
Customer focused and process driven“Can do “ attitude, and Ability to think out of the box and rapidly prototype and deliver innovative solutions.
Ability to collaborate with Data Scientists and Business Analysts to define solution requirements and develop processes for provisioning data for wide ranging analytics.
Translate complex functional and technical requirements into architecture/platform design.
Design for now and future successBig data/Hadoop overall architecture including high availability, disaster recovery, multi-tenancy management, replication etc.
Design and implement big data development framework with standardized module to increase ETL development efficiency and quality
Design and implement best practice for security and data privacy that adhere to Samsung business requirement and policy
Fine tune overall system performance and continuously identify performance bottleneck/improvement opportunity
Provide guidance and support to big data ETL engineer on performance and facilitate ETL performance tuning
Following a data driven approach in defining and measuring platform/operation success
Production support and operation planning/hardening
Requirements:
8+ years’ experience in designing and implementing high available and high scalable big data systems with Hadoop technology
4+ years of hands on experience with Hadoop, Hive, Pig, Impala, and Spark, noSql (hbase/Cassandra) and other big data technology experience with demonstrated technical proficiency
Full lifecycle project development involving Hadoop and related technologies
4+ years Experience overseeing Hadoop environments and operating big data system to enable business successIndustry expertise of database structures, theories, principles, and practices.
Extensive experience working with AWS, Google components.
Analytical and problem solving skills, applied to Big Data domain Demonstrated hands-on success with “can do” attitude
B.S. or M.S. in Computer Science or Engineering","Mountain View, CA",Staff Data Engineer,False
936,"Software Data Engineer I(Job Number: R106769)
Description

Why our company is a great place to work …

Join a Fortune 300® company in the growing healthcare industry and work for their largest technology division. Henry Schein Practice Solutions, a subsidiary of New York-based Henry Schein, Inc., develops practice management software and electronic services that help dental practices run their businesses. Our solutions lead the market in technology advances and market share, and include product leaders such as Dentrix, Dentrix Enterprise, and Dentrix Ascend (cloud-based). Our customers include many of the dental industries’ highest profile constituents, including the U.S. Department of Defense.

Based in American Fork, Utah, Henry Schein Practice Solutions is committed to providing our team members with the tools, training, and technology they need to excel in their roles. Our dedication to giving back to our community is illustrated in the state-of-the-art, volunteer-staffed dental center located on the first floor of our building, which provides free dental care to those in need.

Our parent company, Henry Schein, Inc. is the world's largest provider of health care products and services to office-based dental, medical, and animal health practitioners. A Fortune 300® Company and a member of the S&P 500® and NASDAQ 100® Indices, Henry Schein employs over 22,000 Team Schein Members throughout the world and serves more than one million customers. The Company's sales reached a record $12.5 billion in 2017. Henry Schein has also been recognized by Ethisphere for six consecutive years as the “World’s Most Ethical Company” in the Healthcare Products category, underscoring the company's longstanding commitment to leading ethical business standards and practices.

JOB OVERVIEW:

This position is responsible for performing basic programming tasks for the maintenance and enhancement of a new or existing product. Leverage a basic understanding of the business domain and existing frameworks for the success of development projects.

KEY RESPONSIBILITIES:
Design and code moderately complex solutions that meet business requirements on schedule and within budget
Establish a high level of code quality by writing unit tests, participating in code reviews, reducing cyclomatic complexity, removing code duplication, and debugging software modules
Assist user documentation and technical support by assembling and providing concise and accurate information in regards to software functionality
Implement code that follows established standards and demonstrates a basic understanding of user interface design patterns, object oriented design, database management systems, database design, database access, memory management, design patterns, test automation, continuous integration/deployment, and versioning
Modify existing user interfaces by leveraging a basic understanding of user experience design
Attend all meetings necessary for the seamless delivery of the product as part of the Software Development Life Cycle


Qualifications

WORK EXPERIENCE:

Typically 2 to 4 years of related professional experience.

PREFERRED EDUCATION:

Typically a Bachelor's Degree or global equivalent in related discipline.

GENERAL SKILLS & COMPETENCIES:
Basic understanding of industry practices
General proficiency with tools, systems, and procedures
Basic planning/organizational skills and techniques
Good decision making, analysis and problem solving skills
Good verbal and written communication skills
Basic presentation and public speaking skills
Basic interpersonal skills
Developing professional credibility
SPECIFIC KNOWLEDGE & SKILLS:
Basic knowledge of application design patterns
Basic ability to remain current on new technology within the software industry
Good ability to implement code derived from technical specifications
Basic ability to problem solve/diagnose in a technical space
Good knowledge of an applicable programming language
Basic knowledge of data storage formats, tools, and languages
Ability to keep skills current with changing industry demands as identified
Full-Time Benefits Available:
Earn generous PTO
Earn 7 Paid holidays
Get evenings and weekends off!
Competitive Medical, Dental, and Vision benefits
401K with competitive company match
Flexible Spending Account (FSA)
Life Insurance, Short and Long Term Disability, AD&D
Lunch delivered daily from local restaurants for purchase
Onsite gym with personal trainer options

Henry Schein, Inc. is an Equal Employment Opportunity Employer and does not discriminate against applicants or employees on the basis of race, color, religion, creed, national origin, ancestry, disability that can be reasonably accommodated without undue hardship, sex, sexual orientation, gender identity, age, citizenship, marital or veteran status, or any other legally protected status.

For more information about career opportunities at Henry Schein, please visit our website at: www.henryschein.com/careers.
Primary Location: USA-UT-American Fork","American Fork, UT",Software Data Engineer I,False
937,"We are hiring a Fullstack Engineer to continue to build out our data processing platform and React-based in-CRM extension. You will help build and maintain parsing and data ingestion from 100s of sources, including ingestion, classification and scoring systems.
What you will be helping us architect and build…
Real-time search infrastructure for 100+ Million records.Crawling TBs of content per day and processing live content into ingestible formats.Monitor and control the GCP / Docker-based DevOps CD workflow and production infrastructure.Machine Learning and NLP based systems to associate articles, job postings, and news releases with companies and events.Development, training and deployment of Machine learning and NLP based systems for personalization and classification of unstructured data.
What we are looking for…
Self-organizing, we don't have project managers.Care about code quality and testing, and strive for consistent standards as we grow our products and team.You should have a deep understanding of Web Data Mining and Data Aggregation/Analytics or be willing and passionate about learning it quickly.You know how to test, package, and deploy a Node.js app and can demonstrate it easily.You love changing things around on daily basis and thrive in dynamic, start-up environments, require minimal supervision, can pick up ideas from the whiteboard, and manage the shipping of deliverable code.Experience in hiring (e.g. hiring manager, interviewers, etc.), and team-building is a major bonus, but not required.
-
We are a funded startup headquartered in the heart of Tempe, AZ. We have 5 full-time team members (technology and sales) and work with several contractors. We work with small startup customers up to Fortune 500s. The problems we solve require creative solutions, you must enjoy brainstorming and prototype ideas that at first make you think, that would never work, and thinking outside the box.","Tempe, AZ",Senior NodeJS and Data Engineer,False
938,"Hagerty, the leading provider of classic car insurance, valuation tools and roadside service for people who love cars has an opening for a Data Engineer . This individual will be responsible for building and maintaining our data pipeline and scalable analytics platform. In addition, this person will partner with Data Scientists to develop real time predictive models. This role can be based in our Traverse City or Ann Arbor locations.


RESPONSIBILITIES:

Build robust and scalable data integration (ETL) pipelines using Python, SQL, Spark, and other AWS/Salesforce cloud solutions.
Interface with other vendors and internal teams to extract, transform, and load data from a wide variety of data sources with frequency varying from batch to streaming.
Develop solutions to catalog and manage vast amounts of data in varying formats.
Implement data structures using best practices in data modeling ETL/ELT processes.
Develop automated test cases to validate ETL processes and data integrity.
Partner with Data Scientist to design, code, train, test, deploy and iterate on large scale machine learning algorithms and systems.
Implement algorithms with production quality code.
Lead the implementation and testing of these solutions as an active (hands-on) part of a small multi-disciplinary team.

REQUIREMENTS:
Experience using open source data processing technologies like Kafka, Hadoop, Hive, Presto, Spark, GraphX.
Experience following development rigor to ensure high code quality, automated testing and other engineering best practices.
Experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation, and maintenance.
Experience cataloging and processing non-relational data.
Expert ability in one or more of the following languages: Python, Scala, R, Java, SQL.
Strong working knowledge of relational databases and query authoring (SQL).
Experience or willingness to learn data science frameworks such as numpy, ML Spark, pandas, scikit-learn, tensorflow, MOA, mlpack, etc.
Knowledge of or willing to learn one or more of the following: natural language processing, deep learning, Bayesian reasoning, recommendation systems, learning for search, speech processing, learning from semi structured data, reinforcement or active learning, ML software systems, machine learning on mobile devices.
Experience or willingness to learn machine learning techniques, including supervised and unsupervised algorithms, clustering, graph analytics, and time series analysis, K-means clustering, Gaussian distribution, decision tree, etc.
Experience or willingness to learn feature engineering, feature selection and other practical machine learning issues, such as overfitting.

Hagerty offers a progressive work environment along with a competitive wage and an impressive benefits package.
To apply for this position, please visit our career site at www.hagerty.com/hagerty-corporate/careers EEO/AA
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!","Traverse City, MI",Data Engineer,False
939,"About Bill.com
Bill.com is the leading business payments network, with 3 million members paying and getting paid over $52 billion per year. Bill.com saves companies more than 50% of the time typically spent on financial back-office operations and helps businesses get paid 3 - 4 times faster by automating end-to-end payment processes. The company is the choice of 4 of the top 10 U.S. banks; leading accounting software providers QuickBooks Online and Xero; and over 50 percent of the top 100 U.S. accounting firms. It is the only business payments solution endorsed by the American Institute of CPAs (AICPA). The recipient of more than 70 awards, Bill.com proudly received multiple PC Magazine's Editor's Choice Awards and CEO René Lacerte was recently recognized as an E&Y Entrepreneur of the Year.

Mission:
Bill.com moves over $50B per year and we have 10 years worth of customer data. We are leveraging this data to make data driven decisions, and apply data science and machine learning to solve a variety of tough problems. We are in the middle of a large-scale transformation to the public cloud and are developing data pipelines, data warehouse, and machine learning infrastructure in AWS.
AWS Data engineers at Bill.com will be responsible for building data pipelines and theinfrastructure to enable data science, data analytics, and machine learning at scale in AWS. Some of the problems we’re currently working on include: detecting payment fraud, extracting semantic data from customer documents, and increasing customer acquisition through advanced analytics. Data engineers will own and build the data platform that makes all of this possible. We have multiple positions available at different levels of seniority.
Professional Experience/Background to be successful in this role:
5+ years of experience owning and building data pipelines.
Extensive knowledge of data engineering tools, technologies and approaches
Ability to absorb business problems and understand how to service required data needs
Design and operation of robust distributed systems
Proven experience building data platforms from scratch for data consumption across a wide variety of use cases (e.g data science, ML, scalability etc)
Demonstrated ability to build complex, scalable systems with high quality
Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases. Experience with specific AWS technologies (such as S3, Redshift, EMR, and Kinesis) a plus
Experience in one or more of Java, Scala, python and bash.
Expected Outcomes:
Design and implement data infrastructure and processing workflows required to support data science, machine learning, BI and reporting in AWS
Build robust, efficient and reliable data pipelines consisting of diverse data sources
Design and develop real time streaming and batch processing pipeline solutions
Own the data expertise and data quality for the pipelines
Drive the collection of new data and refinement of existing data sources
Identify shared data needs across Bill.com, understand their specific requirements, and build efficient and scalable pipelines to meet various needs
Build data stores for feature variables required for machine learning
Bill.com Culture:

Humble – No ego
Fun – Celebrate the moments
Authentic – We are who we are
Passionate – Love what you do
Dedicated – To each other and the customer","Palo Alto, CA 94303 (Duveneck-Saint Francis area)",AWS Data Engineer,False
942,"Innate Intelligence delivers predictive analytics solutions that help businesses improve business performance and make strategic data-based decisions. At Innate, we are dreamers, we challenge the status quo, and have a passion to make a difference in the world. We are a team of game changers that thrive on challenge with a vision to deliver solutions that our clients love.In this role you will work with data science and data analytics teams in the delivery of predictive analytics solutions to our clients. As a member of our team, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and data collection for analytics teams. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. In addition, you will support our software developers, database architects, data analysts and data scientists on data initiatives and ensure optimal a data delivery architecture that is consistent throughout ongoing projects. The candidate must have a can-do attitude, ability to deliver results, and a willingness to work as part of an entrepreneurial environment.Position Requirements: Bachelors Degree in Computer Science, Information Technology, Information Systems or related field.2-3 years of experience building and maintaining data pipeline architectures and infrastructure.Experience building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, MS Azure, and/or AWS cloud technologies.Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Strong analytic skills related to working with unstructured datasets.Experience with data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Experience supporting and working with cross-functional teams in a dynamic environment.Experience with big data tools: Hadoop, Spark, Kafka, Hive, etc.Experience with relational SQL and NoSQL databases, including Cassandra and Mongo DB.Experience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, R.Willing to travelExperience in the Oil & Gas Industry a plusJob Type: Full-timeExperience:building: 2 years (Preferred)Education:Bachelor's (Preferred)Work authorization:United States (Preferred)Required travel:75% (Preferred)","Irving, TX",Data Engineer,False
945,"Los Gatos, California
Data Engineering and Infrastructure
Netflix is the world's leading internet streaming service with over 125 million members in 190 countries. Netflix has released over 600 Originals and plans to spend over $7B on content in 2018. At the heart of all our initiatives lies data. The foundation of almost all our analysis is built on Member data. Netflix derives a significant competitive advantage from the highly analytical approach we take to managing the company.

We are looking for an exceptional, highly driven Senior Data Engineer that will propel the Member data platform to a new level. In this role you will own the data engineering pipeline for one of the most heavily leveraged data sets across Netflix: Member life cycle data. You will have freedom to innovate as you work closely with our partners to see the big picture, and figure out new ways to track and store data to make decisions. The best person will have a strong engineering background with the ability to tie engineering initiatives and business impact.
What you will do?
Immerse yourself in all aspects of member data pipeline, understand the problems, and tie them back to data engineering solutions
Transform raw data from different sources (both batch and near-real-time) and using different tools (Spark, Hadoop, Redshift, Kafka, internal systems) into intuitive data models
Come up with architectural patterns to validate and consume source data, and think end to end
Build robust data pipelines and improve them to support the growing needs of our business
Constantly evolve our data model to balance scalability and performance
Build and experiment with different tools and tech, and share learnings with the larger team
Partner with analysts, engineers, data scientists, and business to push Netflix forward
Who are you?
Experience building production data pipelines (using Hadoop, Hive, Pig, Spark, etc.) on web-scale datasets. You should have an unmistakable passion for elegant and intuitive dataset design. Hands on and deep experience with schema design and data modeling.
Programming proficiency in at least one major language (e.g. Java, Python, Scala). You strive to write beautiful code and you're comfortable working in a variety of tech stacks.
Software engineering mindset and ability to write elegant, maintainable code
Analytical mindset to bring together engineering solutions and business impact
Knowledge and familiarity with other distributed data stores (ElasticSearch & Druid)
Strong SQL skills
Excellent communication to effectively collaborate with partners & stakeholders
A few more things to know:
Our culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.","Los Gatos, CA","Senior Data Engineer, FinTech and Membership Platform",False
946,"Shutterfly is seeking an experienced Senior Data Engineer with Software Engineering skills to join the Data Warehouse Development team. You will own, manage and drive end-to-end solutions and data infrastructure. You will work with analytics and business partners to deliver data solutions in support of insights and analysis of a multi-million customer ecommerce business with both internal and external data. If you like applying your expertise with data-warehousing technical concepts, CS fundamentals and data and system architecture to multi-terabyte, multi-source data, come join the Shutterfly Data Warehouse Development team!

Responsibilities
Build data expertise and own data quality for the pipelines you build
Architect, build and launch new data models and data marts that provide intuitive analytics to your customers
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) into and out of the Shutterfly Data Warehouse
Design and develop new systems and tools to enable folks to consume and understand data faster
Use your coding skills across a number of languages including Python and Java
Have a clear understanding of the reports/analyses/insights to be driven by data and build data solutions to optimally support the analytics needs
Integrate third party data to enrich our data environment and enable new analytic perspectives
Become fully immersed in the context of Business Development and Partner business initiatives
Work across multiple teams in high visibility roles and own solutions end-to-end
Work with program managers, business partners and other engineers to develop and prioritize project plans

Qualifications
5+ years of experience with implementing big data business solutions at productions scale
Expert knowledge of SQL
History of building, maintaining and automating reliable and efficient ETL, ELT jobs
Strong CS fundamentals and experience developing with object-oriented programming (Java, Python)
Expertise with dimensional warehouse data models (star, snowflake schemas)
Experience with Cloud Data Warehouses, like Google BigQuery and Amazon Redshift, is preferred
Experience with multi-Terabyte MPP relational databases, such as Teradata, and concepts
Understanding of Hadoop or Spark, including manipulating data with with Pig, Hive and potentially with Java or Python
Understanding of streaming technologies and concepts used with data warehouses, is preferred
Understanding of automation and orchestration platforms such as Automic (formerly UC4) and Airflow","New York, NY",Senior Data Engineer,False
948,"Contract, OtherWe have an immediate project requirement for a Data Engineer with a Google Cloud Professional Data Engineer certification (preferred). This role will lead the team on client projects.The Data Engineer should have excellent time and task management skills, have proficientteam communications, and be able to either telecommute from home or work in one ofour regional offices.This role can be on contract, contract to hire or as a full-time/permanentemployee for the right candidate.Responsibilities Lead/oversee the design of industry-leading cloud-based data/software solutionsto run on Google Cloud Platform (GCP) Lead the design, delivery of, and maintenance of data structures anddatabases, data processing systems, analyze data and enable machinelearning, model business processes for analysis and optimization, visualizedata and advocate policy, design for security and compliance Assemble solution teams for data engineering projects Make internal recommendations to help improve and streamline the technicaland architectural processes Work with the teams to develop web and mobile applications, APIs, SDKs andother tools as required Document software designs, functional and design specifications, presentations,and other documents as needed Lead our team on pre-sales scoping as a technical expertExperience  10+ years hands-on experience in engineering with solution/data designleadership experience Previous experience leading/designing multiple Cloud data projects, GCP andother cloud platforms (AWS, Azure) Experience leading large scale data migrations Previous knowledge and experience with Machine Learning/AI Expert capabilities with structured, unstructured and real-time data Google Cloud Professional Data Engineer certification preferred Experience leading/designing with multiple Cloud platforms such as GoogleCloud, AWS or Azure; big data pipelines and management Experience with data security for data at rest and in transit (i.e., firewalls,hashing, encryption and SSL) Experience automating and managing key business data pipelines to deliver theoutput of models to targeted applications Excellent SQL coding and experience with a broad array of development toolsand platforms including a strong Linux background and big data environmenttools/languages, such as SQL, R, SAS, Python, etc. Experience with Google data tools such as MySQL, Postgres, Bigtable, Dataflow,Spanner and BigQuery Experience with machine learning an asset Hands-on experience with two or more of: Go, python, php, Java, .Net or node.js Expert knowledge of and experience with GCP budgeting and billing strategies MS in Computer Science or equivalent program from an accreditedUniversity/College Able to collaborate and thrive in a fast-paced diverse high-performanceenvironment Demonstrated excellence in written and verbal communicationJob Type: ContractExperience:Google Cloud: 2 years (Required)","New London, CT",Google Cloud Data Engineer - Remote,False
949,"Title: DevOps- Big Data Engineer
Location: New York City
Reporting Relationship: Director, DevOps & Data

About the Company:
Company X is the recognized market innovator with the technology and tools that accurately authenticate the quality of digital media and drive ad performance for the world's largest brands. Company X provides media transparency and accountability to deliver the highest level of impression quality for maximum advertising performance. Since 2008, Company X has helped hundreds of Fortune 500 companies gain the most from their media spend by delivering best in class solutions across the digital ecosystem that help build a better industry. Learn more at doubleverify.com.

Position Overview:
As a Big Data Engineer, you will be designing and implementing systems that crunch and process billions of records a day and make them available in Company X analytics platform, helping our clients to make smarter decisions that continuously improve their ad-impression quality.

What you will do:

Install Configure and Maintain Big Data platform as Apache Kafka, spark and Hadoop
Install Configure and Maintain Kubernetes clusters both on GCP and on premises
Building scalable infrastructure for stream processing as well as batch jobs
Contribute to the design and implementation of new products and features, making sure they are all developed so they fit nicely in our Continuous Delivery framework and processes
Contribute to general system health, monitoring and automation
Using diverse technology knowledge and a sense of curiosity to explore new and better ways to solve problems
Administer project management tools to continuously improve the efficiency of our organization

What you have done/Who you are:

Experience with container technology – Docker and Kubernetes (a must)
3+ years of experience building production infrastructure and supporting SDLC (especially with Java, C# and Python software)
Experience with microservice architecture
DevOps Tools experience: Git/GitHub, Atlassian Suite, Teamcity, Maven and Nuget
Experience with software development methodologies such as continuous integration, continuous delivery and automated tests.
Expert in scripting (Bash, Python, C#, Java etc.) and good coding skills
Experience working with GCP or other public cloud
Experience working with GIT and GIT administration tools
Strong system skills in Windows and Linux environments
You have a genuine desire to automate processes and workflows
BSc in Computer Science/Engineering
MSc is an advantage

","New York, NY",DevOps- Big Data Engineer,False
950,"This position will be located within (Dev/IT) and work closely with computer scientists, IT and data scientists to deploy and optimize machine learning models in the Paycom production system environment.
Responsibilities
Work closely with IT/Computer Scientists on technical aspects of deploying machine learning models in production.
Work closely with Data Scientists to understand, implement, refine, design, and test deployment of machine learning models in production.
Optimize the environment for production machine learning models to access and handle data more efficiently and ensure scalability.
Designs new processes and builds large, complex data sets needed for machine learning processes.
Serve as SME on machine learning technology and recommend acquisition of appropriate technology for production purposes.
Advise and assist IT/infrastructure on install and configuration of machine learning systems.
Explore, design, and implement a robust production-grade data processing pipeline that can ingest, aggregate, and transform large datasets.
Independently conduct literature search to keep informed of best practices and new methods.
Serve as on-call for production issues related to machine learning processes.
Qualifications
Education
BS degree in Computer Science or related field with 5 years machine learning engineering experience or MS/PhD degree in Computer Science or related field with 3+ years of machine learning engineering experience.
Experience
3+ years hands-on experience deploying production-level machine learning algorithms and productionizing them at scale in a distributed computational environment.
1+ year experience with R. Working knowledge of R required.
Experience working with large, messy real-world data.
Experience with SQL, Ruby, Python, C#, Pig and other query and programming languages.
Experience with machine learning database tools and platforms such as HBase, Mongo, Hive, Cassandra, MySQL, SQL Server, PostgreSQL, Hadoop, Spark.
Experience with machine learning optimization tools and related technologies such as H2O, Theano, mlpack, TensorFlow. Experience with H2O required.
Experience with machine learning platforms for production models such as Apache, Pattern, Shogun.
Skills and Abilities
Strong expertise in computer science fundamentals: data structures, performance complexities, algorithms, and implications of computer architecture on software performance such as I/O and memory tuning.
Working knowledge software engineering fundamentals: version control systems such as Git and Github, workflows, ability to write production-ready code.
Strong knowledge of data architecture and system/pipeline and data processing engines such as Spark and Hadoop.
Working knowledge of R and Rstudio.
Working knowledge of SQL, Pig, Python, and other query languages.
Knowledge of C++, PHP, Java and other languages.
Knowledgeable with machine learning tools and frameworks like Python, Spark, H2O, Theano, mlpack, TensorFlow.
Knowledge of machine learning platforms such as Amazon, IBM Watson, Azure, Google Predict, BigML
Strong trouble-shooting skills.
Knowledge of technical infrastructure.
Knowledge of installation and configuration of machine learning systems/technology.
Strong technical aptitude.
Basic knowledge of statistics, calculus and probability, experimental design, and machine learning techniques to enable conceptual understanding of Data Scientist’s models.
Has strong critical thinking skills and the ability to relate them to the products of Paycom.
Possesses a combination of creative abilities and business knowledge.
Demonstrates excellent verbal and written communication skills as well as the ability to bridge the gap between data science and business management.
Displays exceptional organizational skills and is detailed oriented
Paycom provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, pregnancy, military and veteran status, age, physical and mental disability, genetic characteristics, or any other considerations made unlawful by applicable state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Paycom expressly prohibits any form of workplace harassment based on race, color, religion, sex, national origin, pregnancy, military and veteran status, age, physical and mental disability, or genetic characteristics.","Oklahoma City, OK 73102",Data Engineer,False
951,"Analyze and interpret the data from different sources including file systemsStrong Knowledge in SQL conceptsAbility to Identify and define new process to analyze the dataAnalytical skill with ability organize, analyze, and report information with attention to detail and accuracy (reporting is mostly via excel)Good in verbal and written Communication skillsKnowledge in Excel, E-commerce is added advantage


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Qualifications



(Includes Data Modeler, Data Miner.) Responsible for importing, cleaning, transforming, validating and modeling data with the purpose of understanding and drawing conclusions from data (may be presented in charts, graphs, and/or tables). Also, design and develop relational databases for collecting and storing data and build and design data input and data collection mechanisms.

Required Skills and Experience:

You are responsible for data related activities such as data extraction, profiling, cleansing, de-duplication, standardization, conversion, transformation and loading, data mining, warehousing, archiving and reporting. Responsible for all activities required to ensure optimum performance and data integrity of databases in production environments, in line with the requirements. Responsible for providing support of server based databases in development and test environments including database software installation, database creation, performance and capacity design, backup and recovery design, security design, providing Analytical feedback as appropriate.
Qualifications: 1-4 years experience, Bachelor’s Degree.
Should have progressing skills in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.
Should have baseline skills in Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant : http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
About Capgemini

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in over 40 countries. The Group reported 2017 global revenues of EUR 12.8 billion (about $14.4 billion USD at 2017 average rate).


Visit us at www.capgemini.com. People matter, results count.","Milwaukee, WI",Data Analyst - Data Engineer / Analyst,False
952,"We are currently seeking a creative and highly motivated Data Engineer for current and future needs within the organization. This role will be located in St. Louis, MO, reporting to the Vegetable R&D Analytics Lead.

This role will establish state-of-the-art data infrastructure to support advanced analytics, spanning discovery (trait genetics and new phenotypic data), breeding (graph-based ancestry and ML-guided predictive breeding), evaluation of varieties based on mobile analysis (building frameworks to combine deep expertise, image recognition, UAVs and sensors), manufacturing and supply chain (global operations research).

Key Responsibilities:
Design, build, and maintain Vegetable data infrastructure
Leverage Crop Sciences data and infrastructure to the advantage of Vegetables
Accelerate Vegetable IT tools development, and therefore the Vegetable business, by creating and advocating simplified and sustainable methods for research data storage, discovery, and use
Enable quantitative genetics and predictive breeding at scale
Facilitate the rapid growth and use of data from lab, greenhouse, and field, including images, UAV output, IoT telemetry
Stabilize, streamline, and integrate business critical data currently in ad hoc databases
Drive excellent IT and data citizenship by advocating good practices which support the larger strategy for the digital transformation of Ag
Establish data practices to enable data science and prudent security
Cultivate digital mastery for the Vegetable Division, tempering fashionistas and accelerating conservatives
Bring and spread expertise

Required Skills/Experience:

Minimum of a Bachelor’s Degree
Demonstrated passion and success in modeling and deploying data architectures which enable transformative analysis.
Experience with at least 2 of the following data ecosystem elements such as Hbase, MongoDB, Cassandra, and/or CouchDB; graph databases such as Neo4j; Hadoop, MapReduce, and/or Spark; AWS, Google Cloud, and/or Azure
Minimum of 3 years of fluency in a JVM language such as Java or Scala, or demonstrated mastery of another language
Excellent data management and software development practice
The ability and desire to coach and learn from other excellent practitioners
Exceptional verbal and written communication, interpersonal and problem-solving skills such as required to negotiate scope and resources, manage projects, and synchronize activities with team members, stakeholders, and management
Desired Skills/Experience:

Experience with Platform-as-a-Service software such as Cloud Foundry or Kubernetes; demonstrated experience building cloud native applications
Knowledge of data science practices, to better steer our efforts to support them through the infrastructure we create
Public contributions to conference presentations, community forums (e.g. Stack Overflow, GitHub, etc.), and/or open source projects and code samples

About Us:

Bayer successfully completed the acquisition of Monsanto in June 2018, bringing together Monsanto’s leadership in seeds and plant traits with Bayer’s leadership in chemical and biological crop protection. By joining forces, we will create even more extensive career opportunities for talent around the world. We’re a global team working to shape agriculture through breakthrough innovation that will benefit farmers, consumers, and our planet.

While we are now Bayer, we will continue to hire using separate career sites until we can integrate our career platforms. We invite you to explore the career opportunities available at the combined company by visiting advancingtogether.com/careers

#LI-POST","St. Louis, MO",Data Engineer,False
953,"Senior Data Engineer
--------------------

Quizlet's mission is to help students (and their teachers) practice and master whatever they are learning. Every month over 30 million active learners from 130 countries practice and master more than 200 million study sets on every conceivable topic and subject. We are developing new learning experiences by modeling how students learn and by drawing upon knowledge acquisition, retention, and tracing pedagogy in cognitive science. We are always seeking to help students master any subject by optimizing study efficiency and engagement.

We're looking for a data engineer to help us improve our product and make strategic decisions using one of the largest education datasets in the world. As one of the top 20 most visited websites in the US, we have incredibly rich and high-velocity data that allow us to continuously experiment and improve our user experience, affecting millions of students worldwide. In this role, you will help us improve our analytics-facing data and ETL infrastructure and work with analysts and data scientists in order to unlock the potential of our data. This role is critical to everything that we do at Quizlet; experimentation and data-driven decision making is at the core of our philosophy, and you will play a central role in shaping our strategic capabilities.

Quizlet is a leading company in education technology with proven traction and huge growth ahead. Our business model is strong and got us to profitability before raising venture capital money. Come help us scale one of the fastest growing and highest quality consumer learning brands, as we develop innovative simple-to-use study tools that help students everywhere. Quizlet is a fun and unique company in a truly exciting phase of its growth.

The Role
--------


Take ownership of our analytics-facing data pipelines; lead architectural decisions
Interface with analysts and stakeholders to understand their data needs
Understand our logging systems and work with application engineers to ensure our logging is reliable
Lead data quality efforts to support analytics as a strategic assets
Build out abstractions and tooling to ensure that our pipelines are robust and maintainable

Required Qualifications
-----------------------


Strong programming fundamentals; proficiency in a scripting language
Strong proficiency in SQL
An affinity for data modeling and rigorous, well-tested data transformation pipelines
Excellent communication skills, and the ability to empathize with others
A genuine interest in improving education

Preferred Qualifications
------------------------


Experience with PHP, JVM
Experience designing elegant APIs for data processing
Experience with Google Cloud Platform

Quizlet's Team Culture
----------------------------

We are here to make education better and more accessible. We strive to improve the lives of students and teachers at every stage and in every setting. We have a bias for action, take initiative, and hustle to deliver results. We make informed decisions whenever possible but are unafraid to take calculated risks on great ideas to promote learning. We embrace challenges and see effort as the path to mastery. We're constantly seeking opportunities to learn and we embrace curiosity. Quality matters at Quizlet, and we hold the bar high on everything we do. We sweat the details and take personal accountability and pride in anything that carries the Quizlet name. We speak up, jump in and work with each other to fix problems, and never say ""that's not my job."" We treat each other with honesty and respect, encourage vigorous debate, and seek critical feedback. We value diversity, humility, transparency, and collaboration as the best paths to our success — as individuals, as a team, and as a company.

Quizlet's success as an online learning community depends on a strong commitment to diversity, equity and inclusion. We are actively working to build a team that is representative of the diverse communities we serve, and an open, inclusive work environment where all employees can thrive. As an equal opportunity employer and a tech company committed to societal change, we welcome applicants from all backgrounds. Women, people of color, members of the LGBTQ+ community, individuals with disabilities, and veterans are strongly encouraged to apply. Come join us!","San Francisco, CA","Senior Data Engineer, Analytics",False
954,"Description:
Providence is calling a Data Engineer – PSJH to Providence Health & Services in either of the following locations: Renton, WA, Seattle, WA, Portland, OR, Beaverton, OR or Anaheim, CA.
We are seeking a Data Engineer – PSJH who designs and builds modern data-centric software applications to support clinical and operational processes across all parts of the health system. These applications leverage cloud computing, big data, mobile, data science, and modern software development methodologies and frameworks. Builds the data pipelines, enrichment processes, provisioning layers, APIs and user interfaces to meet the requirements of key initiatives. Enjoys a fast pace and has a focus on regular delivery. Seeks simple solutions to complex problems through the use of modern and emerging methods and tools. Emphasizes sharing and enables collaboration with meticulous source control and documentation. Works closely with the Product, Platform, and Architecture teams to deliver on joint efforts.
In this position you will have the following responsibilities:
Design, build and deliver quantitative applications that improve operations and generate value
Participate in DevOps, Agile, and continuous integration frameworks
Stay abreast of emerging technologies, open source projects, and best practices in the field
Data warehousing, big data, enterprise search, business intelligence, analytics, modern and mobile applications
Build processes that are fault-tolerant, self-healing, reliable, resilient and secure
Work effectively and in real-time with other developers, product managers, and customers to deliver on collective goals
Actively participate in code reviews, support the overall code base, and support the establishment of standard processes and frameworks
Take an open and transparent approach to the work by sharing code and expertise, by consulting peers for problem-solving, and by being a mentor to your peers
Qualifications:
Required qualifications for this position include:
Bachelor’s Degree in in computer science, engineering, mathematics, MIS or similar field.
3 years in technology roles.
Demonstrated analytical skills
Demonstrated problem solving skills
Possesses strong technical aptitude.
Cloud computing, Linux, Hadoop, MapReduce, Spark, Hbase, Kudu and NoSQL platforms in general; Apache Solr and Lucene
Java, Scala, C#, Python, shell scripting and/or similar languages
Relational database platforms, database design, and SQL
APIs, JSON, REST and other relevant W3C open standards
Modern application development frameworks
Familiarity with commercial or open source ETL tools
Preferred qualifications for this position include:
Master’s Degree
About the department you will serve.
Providence Strategic and Management Services provides a variety of functional and system support services for all eight regions of Providence Health & Services from Alaska to California. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.
We offer a full comprehensive range of benefits - see our website for details
http://www.providenceiscalling.jobs/rewards-benefits/
Our Mission
As expressions of God’s healing love, witnessed through the ministry of Jesus, we are steadfast in serving all, especially those who are poor and vulnerable.
About Us
Providence Health & Services is a not-for-profit Catholic network of hospitals, care centers, health plans, physicians, clinics, home health care and services guided by a Mission of caring the Sisters of Providence began over 160 years ago. Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.
Schedule: Full-time
Shift: Day
Job Category: Information Technology
Location: Washington-Renton
Other Location(s): Washington-Seattle, Oregon-Portland, Oregon-Beaverton, California-Anaheim
Req ID: 198066","Anaheim, CA 92805",Data Engineer – PSJH,False
955,"Instacart is building the best way for people anywhere in the world to shop for groceries. Since Instacart started in 2012, we've launched same-day delivery in 200 US markets. We are laser focused on delivering groceries from your favorite stores right to your door. We now cover over 60% of US households and aim to have 80% coverage by the end 2018—that's 90 million households! From a technology point of view, the platform is complex, rapidly scaling and processing millions of transactions in real-time all of the time. Our technology coupled with operational expertise enables Instacart to deliver fresh groceries in as little as an hour. This is a difficult problem to master and we are making it happen. Every day, we solve incredibly hard problems to create an experience for our customers that is absolutely magical.

The Data Engineering team at Instacart is rapidly growing and you will have the opportunity to shape its direction and create large impact. The team is looking for a motivated, self-starter with a drive to tackle a variety of data challenges at Instacart.

Responsibilities


Design and build high performance batch and near real-time data pipelines
Lead partnerships with product managers on building new tools and analytics
Contribute to the continual improvement of the Instacart data platform
Work in an agile collaborative environment

Requirements


At least 5 years of hands-on experience working in a data-driven company
Expert-level proficiency in one or more of the following domains:
Building Data Warehouses, dimensional modeling, SQL
Building and managing ETL pipelines, e.g. Airflow, Alooma, Informatica
Python programming (Pandas, SciKit, NumPy)
Big Data technologies, e.g. Hadoop, Spark
Excellent written and verbal communication skills; able to effectively collaborate with diverse teams.
BS/MS in Computer Science, Engineering, Math, other quantitative field, or equivalent experience

Benefits


Talented and collaborative coworkers who will both push and support you
Market competitive salary and equity
Medical, dental, vision benefits
Take what you need vacation (and we really mean it)
16 weeks maternity leave / 8 weeks paternity leave so you can truly bond with your child
Complimentary Instacart express membership

Resources


Tech Blog ( http://tech.instacart.com/ )
Life at Instacart ( https://twitter.com/lifeatinstacart )
Team Stories ( https://medium.com/life-at-instacart )

","San Francisco, CA",Senior Data Engineer,False
956,"MORE ABOUT THIS JOB
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.

Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.

Who We Look For
Goldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.
Consumer and Commercial Banking (CCBD)
Consumer and Commercial Banking brings innovative solutions to traditional banking activities. We are a global team of lenders, investors, risk managers, skilled marketers, web experts and banking specialists. We provide a suite of solutions to help our customers meet their personal financial goals. We make direct investments in, and risk manage, a portfolio of corporate loans and securities. And we help transform distressed communities through investments and loans of private capital.

Digital Finance
Digital Finance, a business unit within CCBD, is comprised of the firm’s digitally-led consumer businesses, which include the Marcus deposits and lending businesses, as well as the personal financial management app, Clarity Money. Digital Finance combines the strength and heritage of a 148-year-old financial institution with the agility and entrepreneurial spirit of a tech start-up. Through the use of machine learning and intuitive design, we provide customers with powerful tools that are grounded in value, transparency and simplicity to help them make smarter decisions about their money.
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processesDevelop data visualizations using BI tools and web-based technologiesWork as part of a global team using Agile software methodologiesPartner with Marcus risk, product, acquisition and servicing teamsUse Marcus data to drive change throughout the Marcus business

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
Minimum 3 years of relevant professional experienceBachelor’s degree or equivalent requiredExperience with SQL and relational databasesProficient at Python, Spark and the Hadoop ecosystemSelf-starter, motivated, and good communication skills Strong sense of ownership and driven to manage tasks to completion

Preferred Qualifications
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2018. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.","San Francisco, CA",CCBD Technology - Data Engineer,False
958,"$106,000 - $160,000 a year (Indeed Est.) At Peloton Technology, we are transforming the trucking industry, bringing groundbreaking safety, efficiency and data to the trucks that drive the economy. We are a hands-on team of action with diverse backgrounds, united by a common goal.
Peloton seeks a Back End Data Engineer to own data ingestion and usage of our diverse data collected from our platooning fleets. The Back End Data Engineer will contribute to a rich set of batch and streaming analytic products.
Responsibilities include design, production setup and management of database schemas and implementation of back-end applications operating on large data sets; integration with data storage such as Cassandra, Redis, and AWS S3; and the creation of analytic data access APIs for internal and external customers. This role will interface with the Network Operations team, the web Front End team, and the DevOps team.
Required Skills
CS/CE degree with strong analytic focus preferred, 5-7 years industry experience.
Currency in batch and streaming analytics systems.
Analytic algorithm implementation in a production context; experience designing and coding multi-tenant systems in the cloud or colo-based environments where designing for scale and resilience are critical.
Preferred languages: Go, Java, Python. Experience with C++ for integrating with in-vehicle embedded systems is a plus.
Experience building applications in AWS or similar is highly desired, especially those serving IoT ecosystems.
Experience with NoSQL-based storage, preferably Cassandra and Redis. Strong data modeling experience required in a mixed NoSQL and SQL environments.
Passionate about leading edge systems, tools and processes; uncompromising on test driven development, continuous integration, agile methodologies and tight collaboration.","Mountain View, CA",Backend Data Engineer,False
961,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.



Summary & Key Responsibilities

This role on the Enterprise Data team will support our Data Engineering initiatives, including the development of our “Data Lake” architecture. This role is primarily project based, but also may provide support and maintenance to applications in production.
General Summary
This role on the Enterprise Data team will support our Data Engineering initiatives, including the development of our “Data Lake” architecture. This role is primarily project based, but also may provide support and maintenance to applications in production.

Key Responsibilities
Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time
Create metrics and apply business logic using Spark, Scala, R, Python, and/or Java
Model, design, develop, code, test, debug, document and deploy application to production through standard processes
Harmonize, transform, and move data from a raw format to consumable, curated views
Analyze, design, develop, and test applications
Contribute to the maturation of Data Engineering practices, which may include providing training and mentoring to others


Qualifications
Minimum Experience/Education
Bachelor’s degree in Computer Science, Computer Engineering, Programming, Management Information Systems, or related field. Insurance industry experience is a plus.
Minimum of two years of prior data engineer experience.
Strong hands-on experience in Spark, Scala, R, Python, and/or Java.
Programming experience with the Hadoop ecosystem of applications and functional understanding of distributed data processing systems architecture (Data Lake / Big Data /Hadoop/ Spark / HIVE, etc).
Amazon Big Data ecosystem (EMR, Kinesis, Aurora) experience is a plus.

Communication and Collaboration Skills
Written: Must be able to convey key messages in technical terms and business terms. Must be able to create technical documentation, such as specifications, design documents, and testing documents.


Oral: Ability to collaborate and communication with a wide range of partners, including IT and business, across all levels of the organization. Must actively manage expectations with stakeholders.

Problem Solving

Must understand the business need and develop technical solutions to meet those needs. Innovation, creativity, and critical problem solving skills are required to be successful in this role. Solutions need to be comprehensive, flexible for future changes, and delivered with a high degree of quality.


Communication and Collaboration Skills
Written: Must be able to convey key messages in technical terms and business terms. Must be able to create technical documentation, such as specifications, design documents, and testing documents.


Oral: Ability to collaborate and communicate with a wide range of partners, including IT and business, across all levels of the organization. Must actively manage expectations with stakeholders.

Problem Solving
Must understand the business need and develop technical solutions to meet those needs. Innovation, creativity, and critical problem solving skills are required to be successful in this role. Solutions need to be comprehensive, flexible for future changes, and delivered with a high degree of quality.


State Auto offers a competitive salary, an annual bonus program, an excellent benefits program including medical, dental, vision and prescription insurance coverage, life insurance, matching 401(k) plan, flexible spending accounts, tuition assistance, and a stock purchase plan.

State Auto is committed to the principle of equal employment opportunity for all associates and applicants and to providing associates with a work environment that is free from discrimination and harassment. All employment decisions (hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, training and work assignments) are based on business needs, job requirements and individual qualifications without regard to race, color, religion, gender, sex, sexual orientation, gender identity, national origin, age, disability, genetic information, marital status, citizenship status, military status, or status as a covered veteran in accordance with applicable federal, state and local laws. State Auto will not tolerate discrimination or harassment based on any of these characteristics.

State Auto is a smoke-free work environment. We utilize drug screening and background checks as conditions of employment. For all exempt positions, we also obtain motor vehicle reports (MVR s).

State Auto will not accept candidates from third-party recruiters without a signed agreement with State Auto.



Full Time / Part Time

Full time


Worker Sub-Type

Regular


If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!","Columbus, OH 43215 (Downtown area)",Data Engineer,False
963,"ContractJob SummaryOur client is looking for GCP Data EngineerFind below the Job Description. Kindly reply with your updated resume, contact details and best time to reach you.I apologize if the job is not of your interest however I will highly appreciate if you can refer some body suitable for this positionJob Title: GCP Data EngineerLocation: Chicago (Downers Grove), ILDuration: ContractJob Description: As a Data Engineer, you will assist in the development of data warehouse data flow/data warehouse functions on Google Cloud. An ideal candidate will be a proficient warehouse developer versed in data integration services development, report development, data analysis, and GCP Big Data stack.Responsibilities: Build Data Flow jobs in GCP: Supports technology tools, systems, capabilities, processes, and financials to enable delivery and drive business results across the enterprise.Build/Supports Analytics & Data Lake, ensure communications both formal and informal are clear and aligned with supported functions and related stakeholders.Technology/Business Plan Development: Supports execution of technology improvements that drive capacity within the enterprise.Build Capability to Drive Growth and Eliminate Waste: Deploys tools, processes and resources to support the enterprise.Experience: Must Have:GCP BigQuery and/or Google DataFlow ExperienceExperience with operationalization including security of Google BigQuery Experience with Postgres SQL Experience with setting up secure access(ACLs, IAM roles) to Google Cloud Storage Experience with writing Cloud Functions Experience with Cloud Pub/Sub setup and configuration. 3-5+ years of experience in data ingestion and storage systems for big data environment using at least one of the COTS integration tools, like - SnapLogic, webMethods, TIBCO, Talend, Informatica, and/or custom scripting in Python/JavaJob Type: ContractExperience:Google Cloud Platform: 2 years (Preferred)","Chicago, IL",GCP Data Engineer,False
965,"Aavalar Consulting is a trusted technology staffing partner that helps technology leaders connect with and deploy in-demand, skilled IT and Engineering professionals at client sites across the Mid-Atlantic region. Since 1999, Aavalar Consulting has built an award-winning reputation with over one hundred of the most innovative Fortune 500 and mid-market companies to deliver substantial value through a broad set of technology consulting and IT staffing services that include: Staff Augmentation, Interim Technology Executives, and Search and Recruitment for IT and Engineering.

Position Title: Data Engineer

Work Location: Plymouth Meeting, PA Area

Work environment (private office, cube, high rise office building): Office Building

Who does this position report to CIO, Head of Software Development

Why is this position open Company is Growing

Responsibilities: Help development of company products by collecting and analyzing data using new cutting edge technology.

Required Skills:

Solid work history building data processing systems with Hadoop, MapReduce using Python and Java.
Working experience with query tools (Pig and Hive)
Working experience building stream data processing systems using Apache Spark and PySpark
Knowledge of troubleshooting, optimizing, and administering NoSQL databases (MongoDB or DynamoDB)
History of utilizing Agile and creating Agile culture

Nice to Have


AWS experience
Scripting language experience with Java, Linux, C++, PHP, Ruby and Python.

Education/Training:

B.S. Computer Science/Engineering Preferred

Selling point of the job: Company is growing extremely fast great opportunity to grow from within.

Work Hours and Schedule: Normal Business Hours

Is there travel involved If so how much No Travel

Dress code: Business Casual

Base Salary: Open

Bonuses: TBD

Who is involved in the interview process Sr Engineers, CIO, Director of Software Development.","Plymouth Meeting, PA",Data Engineer,False
966,"Senior Java / Big Data Engineer - Enterprise Content & Delivery
New York, NY
Posted Oct 17, 2018 - Requisition No. 71337

This Enterprise Content & Delivery development team is responsible for building highly-performant distributed systems that manage large volumes of client data. We work with multiple groups within Bloomberg to gather millions of data points per day and develop tools to transform and store the data in an efficient manner for trend analysis, billing and business intelligence.
We are looking to enhance our software suite to develop a new automated data ingestion and analytics pipeline. We want every application that we on-board to define new schemas and leverage them for data ingestion. Secondly, we want to leverage or develop a framework that can provide for analysis against this data store and do this all in a manner that provides for operational independence. The right candidate will be expected to have extensive hands-on experience in Big Data Technologies such as Hadoop and HBase as well as frameworks like Apache Spark and/or Apache Kafka.
We'll trust you to:
Understand the capabilities of our current system and enhance it to support the capabilities of this new pipeline
 Work with other groups within the organization to set up and configure big data clusters and assist with data volumetrics as well as hardware and software needs
 Research, design and assist in building tools that can be utilized to analyze the data by internal users and support staff
 Maintain systems to ensure they are highly available
You'll need to have:
3+ years of current Java development experience
 Proven experience with a range of big data architectures, including Hadoop, HBase or other big data frameworks
 Experience building large scale distributed data processing systems
 Solid understanding of data structures, algorithms & object-oriented design concepts
 A passion for big data technologies and a flexible, creative approach to problem solving
 Excellent communication skills
We'd love to see:
Experience with languages such as Python/Perl
 Experience developing software using agile methodologies
 Working knowledge of development tools such as debuggers, memory profilers, and performance analysis
If this sounds like you, apply!","New York, NY",Senior Java / Big Data Engineer - Enterprise Content & Deliv...,False
968,"Req ID: 13421
Shift: Days
Employment Status: AF - Active - Regular - Full Time

Job Summary

We are looking for a Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data warehouse and building data integrations, developing data best practices and governance, performing clinical and administrative reporting and data visualization, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate has knowledge of and is excited to learn about all aspects of data from multiple complex sources who enjoys optimizing data systems and building them from the ground up. The Data Engineer I will support our developers, database architects, data analysts, and data scientists and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also support non-technical colleagues in the collection and appropriate use of clinical and non-clinical data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.




Job Responsibilities

1. Data Modeling – evaluate structured and unstructured data, determine the most appropriate schema for new fact tables, data marts, etc.
2. Data Integration – incorporate new business and system data into the CHOP Data Warehouse while maintaining enterprise best practices and adhering to data governance standards.
3. ETL – apply business rules to our data as we migrate from source to target using Informatica or scripting language. Validate data to ensure quality.
4. Reporting – collaborate with colleagues across the enterprise to scope requests. Extract data from various data sources, validate results, create relevant data visualizations, and share with requester. Develop dashboards and automate refreshes as appropriate.
5. Governance / Best Practices – adhere and contribute to enterprise data governance standards. Also educates and supports colleagues in best practices to ensure that data is used appropriately.
6. Assemble large, complex data sets that meet functional / non-functional business requirements.
7. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
8. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including ground, hybrid cloud, and cloud) using SQL and various programming technologies.
9. Develop analytics tools that utilize data resources to provide actionable insights, operational efficiency and other key business performance metrics.
10. Work with stakeholders including the Executive, Clinical, and Analyst teams to assist with data-related technical issues and support their data infrastructure needs.
11. Develop optimized tools for analytics and data scientist team members that assist them in building and optimizing projects into an innovative industry leader.
12. Proficient at integrating predictive and prescriptive models into applications and processes.
13. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
14. Strong analytic skills related to working with structured and unstructured datasets.
15. Build processes supporting data transformation, data structures, metadata, dependency and workload management.
16. A successful history of manipulating, processing and extracting value from large disconnected datasets.
17. Working knowledge of message queuing, stream processing, and highly scalable data stores.
18. Strong communication, project management and organizational skills.
19. Experience supporting and working with cross-functional teams in a dynamic environment.
20. Participate in a shared production on-call support model.
21. Be a critical part of a scrum team in an agile environment, ensuring the team successfully meets its deliverables each sprint.




Required Education and Experience

Two (2) Certifications or proficiency in appropriate Business intelligence/Data Warehousing technology or subject domain.
Bachelor’s degree in computer related field required.
3-5 years of Business Intelligence/Data Warehousing experience, preferably in a healthcare environment.



Additional Technical Requirements

Proficient in SQLExposure to big data tools: Hadoop, Spark, Kafka, BigSQL, Hive, etc.Experience with relational SQL and NoSQL databases, including IBM PDA (Netezza), MS SQL Server and HBase.Exposure to data integration tools: Informatica, MS Integration Services, Sqoop, etc.Exposure to stream-processing systems: IBM Streams, Flume, Storm, Spark-Streaming, etc.Exposure consuming and building APIsExposure to object-oriented/object function programming languages: Python, Java, C++, Scala, etc.Exposure to statistical data analysis tools: R, SAS, SPSS, etc.Exposure to visual analytics tools: QlikView, Tableau, Power BI etc.Familiarity to Agile methodology for developmentFamiliarity with electronic health record and financial systems. i.e. Epic Systems, Cerner, WorkDay, Infor, Strata etc.


All CHOP employees who work in a patient building or who provide patient care are required to receive an annual influenza vaccine unless they are granted a medical or religious exemption.
Children's Hospital of Philadelphia is committed to providing a safe and healthy environment for its patients, family members, visitors and employees. In an effort to achieve this goal, employment at Children's Hospital of Philadelphia, other than for positions with regularly scheduled hours in New Jersey, is contingent upon an attestation that the job applicant does not use tobacco products or nicotine in any form and a negative nicotine screen (the latter occurs after a job offer).
Children's Hospital of Philadelphia is an equal opportunity employer. We do not discriminate on the basis of race, color, gender, gender identity, sexual orientation, age, religion, national or ethnic origin, disability or protected veteran status.
VEVRAA Federal Contractor/Seeking priority referrals for protected veterans. Please contact our hiring official with any referrals or questions.
CHOP Careers Contact
Talent Acquisition
2716 South Street, 6th Floor
Philadelphia, PA 19146
Phone: 866-820-9288
Email:TalentAcquisition@email.chop.edu

Nearest Major Market: Philadelphia

Job Segment: Database, Medical, Developer, Patient Care, Java, Technology, Healthcare","Philadelphia, PA 19104 (Belmont area)",Data Engineer,False
970,"ContractPosition: Big Data EngineerDuration: 1+ yearLocation: Bentonville, ARJob Duties and ResponsibilitiesMinimum 6 years of proven experience.Contribute to the job and data flow design for data management on Hadoop platformsExcellent knowledge of Hadoop, Hive, Teradata, Automic, SQL.Strong verbal and written communications skills are a mustEducation : Bachelor degreeMaster degree preferred.Best Regards,Naveen PratapPhone: (678)-666-0120www.linkedin.com/in/naveen-raghavJob Type: ContractExperience:Hive: 3 years (Required)Teradata: 3 years (Required)Hadoop: 4 years (Required)SQL: 3 years (Required)Education:Bachelor's (Required)","Bentonville, AR",Big Data Engineer,False
971,"Why VARIDESK?

We’re award winners:

Best Place to Work Top 10 – Dallas Business Journal
National Entrepreneurs of the Year – EY
Fastest Growing Company in DFW – SMU
We take health seriously:

On-site gym with Peloton Cycle and daily group classes
Comprehensive Health Plans
Healthy Foods and Snacks
Wellness Program and Insurance Premium Discounts
Nicotine Free Workplace
We value our employees:

All employees use VARIDESK products
Enhanced Paternity / Maternity Programs
Three weeks of Personal Time Off a year
Up to five days of time off for Volunteering
Offsite Events and Happy Hours
Short-Term and Long-Term Disability Premiums Covered at 100% by VARIDESK
401k Plan with employer match
SUMMARY

As a Senior Data Engineer, you will be responsible for integrating, modeling, and reporting on data to create actionable insights for the organization. You will analyze, design, build, and support the data warehouse (Microsoft SQL Server/Azure), analytical models in SSAS, and reporting in Power BI. You will also interact with a variety of business professionals (including C-Levels), and work on critical projects within all aspects of the organization.

DUTIES AND RESPONSIBILITES

 Create and maintain optimal data pipeline architecture using Microsoft SQL Server, SQL Data Warehouse, and Azure Data Lake/Blob Storage
 Develop ETL processes using SQL, SSIS, Azure Data Factory with consideration to fault-tolerance, error logging, auditing and data quality
 Build/automate reports/dashboards (in Power BI and Microsoft Analysis Services) to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
 Implement data cleanup procedures, transformations, scripts, stored procedures, and execution of test plans for landing data successfully into the appropriate destinations
 Create tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
SKILLS AND ATTRIBUTES

 Must be an experienced thought leader and expert in Business Insights, Warehouses, Data Enrichment, and Analytics
 Understanding of scrum and agile development methodologies
 Ability to be a quick problem solver
 Outstanding client management skills and the ability to work with customers to execute on an implementation plans
 Excellent communication on both technical and non-technical contexts
 Ability to work under deadlines in a fast-paced environment while prioritizing competing demands
 Experience performing root cause analysis on internal and external data and processes, to answer specific business questions and identify opportunities for improvement.
 Strong Data Modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning and high volume data loads
QUALIFICATIONS

 5+ years of progressive experience in relevant Business Data & Insights Administration & Support positions
 Advanced working knowledge of SSIS, T-SQL, SQL Server, and SQL Data Warehouse
 SSAS Tabular/Multidimensional (including DAX)
 Power BI or Reporting Services (SSRS)
 Experience building and optimizing big data pipelines, architectures and data sets.
 Excellent communication skills
 Experience working in an agile team environment","Coppell, TX 75019",Senior Data Engineer,False
972,"$130,000 - $140,000 a yearContractBICP, a dedicated BI, Analytics & Big Data consulting firm, is currently looking to hire 2x Data Engineers to support a major transformation effort at our longstanding and direct Fortune 500 retail in Portland, OR. We’re looking for candidates with 2+ years of experience with cloud based data engineering with an emphasis on delivering platforms to enable near real time data analytics and reporting. Ideal candidates must possess strong relational database engineering (Oracle, SQL Server, or Teradata) and have expert-level SQL abilities. Must have experience with large scale data processing, data structure, optimization and scalability. Experience with Azure is preferred however experience with AWS or Databricks is fine as well. Scripting experience with Python or PowerShell is preferred. We’re looking to hire candidates that can thrive in an Agile environment and who possesses great interpersonal and communication skills to work effectively with parallel technical teams and an closely collaborate with the business. Start date is ASAP and this will be a 5 day on-site work week. Remote is NOT an option.If you’re looking to join an organization where there is tremendous growth opportunity, that operates with transparency and with a highly collaborative approach then BICP could be a great career choice for you. We offer excellent compensation and we will reimburse the cost of relocation to Portland (telecommuting is not an option).Job Types: Full-time, ContractSalary: $130,000.00 to $140,000.00 /yearExperience:Azure: 1 year (Required)Education:Bachelor's (Required)Work authorization:United States (Required)","Portland, OR",Data Engineer - Azure | Fortune 500 Retailer,False
973,"Honey is helping millions save money every day and we're growing! Our data pipelines process hundreds of millions of events per day. We are looking for engineers who love distributed systems and are motivated by the challenge of scale. Data engineering at Honey is a small team, so you'll have opportunities to work on a variety of product features and technology platforms.

What You'll Do:

Implement machine learning models to production.
Design, build and operate Honey's data pipelines with a focus on performance and reliability.
Participate in new feature development for recommendations, product catalogs, and mobile applications.
Propose and evaluate storage technologies and methodologies with an eye toward scalability and performance.
Design and implement data pipelines that handle a thousand messages per second streaming.

About You:

5+ years programming in at least one modern programming environment. Python, Scala, or Node.js are helpful but not required.
5+ years architecting with both SQL and no-SQL data stores. We use Big Table/HBase, Spark, Dataflow, Spanner, BigQuery, and Elasticsearch, but if you have experience using Hive, Hadoop, or Pig that works too!
Experience designing schemas and maintaining representations for low latency, request- cycle queries.
Experience with streaming platforms (PubSub, Kafka, Kinesis) and near-real-time data pipelines
Working knowledge of statistics and experimental design
Comfortable building and maintaining data infrastructure in the cloud

Honey is an equal opportunity employer. We are committed to building a diverse and inclusive company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, disability status or genetic information, in compliance with applicable federal, state and local law.","Los Angeles, CA",Senior Data Engineer,False
974,"$65,000 - $70,000 a yearDATA ENGINEER


Job Summary

Looking for a Data Engineer to help build the foundation of a new Strategy & Insights department. Seeking someone skilled at data extraction, preparation, analysis, and visualization geared towards making the Sales & Service department more efficient and effective.


This individual will also be involved in the execution of predictive modeling projects such as lead scoring, retention modeling, and pricing. To be successful in the role, candidates must have a strong understanding of data engineering principles as well as statistical coding and data visualization skills.


Principal Duties and Responsibilities

Work with the Sales & Service department to improve reporting and analysis around their KPIs.
Assist with data mining, cleansing, manipulation, insights, and reporting.
Manage the data flow between our core technology suite and ticketing.
Knowledge of all major aspects of a CRM system (e.g. automated processes, lead assignment, reporting, etc.).
Coding proficiency in at least one modern programming language (e.g. Python, R).
Expert Microsoft Excel skills.


Education & Experience

BA/BS in Computer Science, Engineering, Math, Economics, or another related field with a year to three years of backend or data engineering experience.

Entry level with a MS or PhD","Los Angeles, CA",Data Engineer,False
975,"Oath, a subsidiary of Verizon, is a values-led company committed to building brands people love. We reach over one billion people around the world with a dynamic house of 50+ media and technology brands. A global leader in digital and mobile, Oath is shaping the future of media.


A little about Yahoo
Yahoo is an Oath brand. Oath, a subsidiary of Verizon, is a values-led company committed to building brands people love. We reach over one billion people around the world with a dynamic house of 50+ media, communication and technology brands. A global leader in digital and mobile, Oath brands are shaping the future of communication and media.

What We Do and What You'll Learn:
Build data pipelines to automate flow of ads data
Build machine learning models using deep learning or logistic regression to optimize performance
Data mining on Hadoop to power search autocompletion
Find topics in web documents for better ranking or filtering

Responsibilities:

As a software engineer / data engineer you will perform
Ads performance data collection, cleaning, and analysis to find revenue opportunities and suggest improvements
Use Hadoop related technologies for data analysis if necessary
Gain insights on what drives performance
Turn improvement ideas into production features or new models
Design, implement and test the production features or new models

Qualifications :
BS, MS or PhD in Computer Science or related field
Strong data analysis and problem solving skills
Deep understanding of algorithms and data structures
Strong programming proficiency with C++ or Java
Strong programming proficiency with scripting language such as Python
Proficiency with object-oriented programming and design, relational databases, SQL, and UNIX/Linux environments
Excellent interpersonal, organizational, creative, and communications skills
Team player in driving growth results combined with a positive attitude
Strong work ethic and core values (honesty, integrity, creativity)
Fluency in Japanese

Bonus points if you have :
Experience on machine learning modeling and technologies related to big data such as Hadoop, Spark, Hive, and Pig



Oath is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on, age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Oath is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. Please let us know if you need a reasonable accommodation to apply for a job or participate in the application process.


Currently work for Oath? Please apply on our internal career site.","Sunnyvale, CA",Software Dev Engineer / Data Engineer,False
976,"ContractTitle: QA_Big Data EngineerLocation: Phoenix, AZDuration: Full time with M3biRequired:* 5+ years of direct functional testing experience, working with Big Data testing (Must have Hadoop,Hive, Hbase experience).* Strong experience with data transformation and working on Hive.* Must have 3+ years of Test Automation experience using Selenium or any other automation tools and proficient in writing test automation code in JAVA.* Prefer experience with test automation, including the Cucumber automated acceptance test software.* Deep expertise in test methodology, process, planning, and execution.Responsibilities*Write and execute System and User Acceptance strategies, plans and scripts for the migration of a relational data warehouse that is being retired.* Create appropriate test scenarios and data sets.* Work with lines of business, technology, and operations to determine priorities, and schedule of releases.* Document test procedures, and issues.* Support testing AND automation of testing for a range of projects from defect fixes and enhancements to strategic initiatives.* Log bugs and enhancements into the defect tracking and planning tool.* Escalate key testing issues and provide feedback to the stakeholders.* Ensure that all requirements are covered in the testing, that it is completed within established time and that objectives conform to the user requirements and line of business.* Provide input and shape SIT & UAT processes in line with industry best practices.Job Types: Full-time, ContractExperience:Hadoop: 1 year (Preferred)Selenium: 1 year (Preferred)Test Automation: 1 year (Preferred)Hive: 1 year (Preferred)Java: 1 year (Preferred)","Phoenix, AZ",QA_Big data,False
978,"$115,000 - $135,000 a yearData Engineer

San Francisco Bay Area

115k-135k + Benefits

A late stage luxury e-commerce start-up, focused on ethical business practices in San Francisco, is seeking Data Engineer to join their team! Come join a company whose mission is creating absolute transparency towards their customers and a strong focus on ethical sourcing of their products.

The Role

As a Senior Big Data Engineer, you will have the opportunity to take ownership over end-to-end solutions. This is an opportunity to be product facing and help to personalize products, systems and infrastructure in an e-commerce setting. You will also be supporting the data science team by building data tools.

The Role

Building data pipelines out from scratch
Working with stakeholders in e-commerce, marketing and supply chain to create data tools and address problems
Build self service tools access to data across the business
Required Skills & Experience

Strong Python, Scala or R programming
Expert SQL skills
Experience working with relational databases
Previously worked with distributed frameworks: Spark, Hadoop
Strong communication skills with business stakeholders
Please register you interest by sending your CV to Angela Rego () via the apply link on this page.","San Francisco, CA",Data Engineer,False
980,"At SailPoint, we do things differently. We understand that a fun-loving work environment can be highly motivating and productive. When smart people work on intriguing problems, and they enjoy coming to work each day, they accomplish great things together. With that philosophy, we’ve assembled the best identity team in the world that is passionate about the power of identity.

As the fastest-growing, independent identity and access management (IAM) provider, SailPoint helps hundreds of global organizations securely and effectively deliver and manage user access from any device to data and applications residing in the data center, on mobile devices, and in the cloud. The company’s innovative product portfolio offers customers an integrated set of core services including identity governance, provisioning, and access management delivered on-premises or from the cloud (IAM-as-a-service).

SailPoint has been voted a best place to work in Austin, 8 years in a row.

SailPoint is looking for a Data Engineer to build, maintain, monitor, and improve a real time scalable, fault tolerant, data processing pipeline, and productize machine learning algorithms, for a new cloud-based, multi-tenant, SaaS analytics product.

You will be integral in building this product and will be part of an agile team that is in startup mode. This is a unique opportunity to build something from scratch but have the backing of an organization that has the muscle to take it to market quickly, with a very satisfied customer base.

Responsibilities
Implementing ETL processes
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies
Productizing and operationalizing machine learning algorithms
Be part of a team that is creating a brand-new product line
Collaborate with team members to help shape requirements
Actively engage in technology discovery that can be applied to the product

Requirements
1+ year of data engineering or related experience
Strong Java and/or Scala experience
Proficient understanding of distributed computing principles
Ability to solve any ongoing issues with operating the cluster
Experience with integration of data from multiple data sources
Strong knowledge of data cleaning and various ETL techniques and frameworks
Great communication skills
BS in Computer Science, or a related field

Preferred
Proficiency with Spark
Experience with Machine Learning using Mahout/Deeplearning4j/Spark ML
Experience with stream processing using Spark Streaming/Storm/Beam/Flink
Experience with Elasticsearch
Experience with messaging systems, such as Kafka or Kinesis
Experience with NoSQL databases, such as Redshift, Cassandra, DynamoDB

Compensation and benefits
Experience a Small-company Atmosphere with Big-company Benefits
Competitive pay, 401(k) and comprehensive medical, dental and vision plans
Recharge your batteries with a flexible vacation policy and paid holidays
Grow with us with both technical and career growth opportunities
Enjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.



All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.","Austin, TX","Data Engineer, IdentityAI",False
981,"$50 - $60 an hourContractJob SummaryExcellent knowledge of Spark, Spark MLib, Scala, HBase, HiveGood knowledge of AWS environment - EC2, S3, EBS. Knowledge of CloudFormation template is a plusGood knowledge of Cloudera CDHExperience of Unix Shell scripting is a plusExperience of TFS-GIT, Eclipse/Maven is a plusShould be able to lead the team at onsite and share insite to offshorecoordinate closely with customer to derive the project plan and execute them with close monitoring and on timeattend customer discussions as and when required and schedule weekly / monthly meetings to go over the project governance explore and participate in Proof of Concepts to build confidence to customer and present the best possible solutionsResponsibilities and DutiesExcellent knowledge of Spark, Spark MLib, Scala, HBase, HiveGood knowledge of AWS environment - EC2, S3, EBS. Knowledge of CloudFormation template is a plusGood knowledge of Cloudera CDHExperience of Unix Shell scripting is a plusExperience of TFS-GIT, Eclipse/Maven is a plusShould be able to lead the team at onsite and share insite to offshorecoordinate closely with customer to derive the project plan and execute them with close monitoring and on timeattend customer discussions as and when required and schedule weekly / monthly meetings to go over the project governance explore and participate in Proof of Concepts to build confidence to customer and present the best possible solutionsJob Type: ContractSalary: $50.00 to $60.00 /hourExperience:big data: 9 years (Preferred)","Louisville, KY",Sr Big Data engineer,False
983,"Forcepoint is transforming cybersecurity by focusing on what matters most: understanding people’s intent as they interact with critical data and intellectual property wherever it resides. Our uncompromising systems enable companies to empower employees with unobstructed access to confidential data while protecting intellectual property and simplifying compliance. Based in Austin, Texas, Forcepoint supports more than 20,000 organizations worldwide. For more about Forcepoint, visit www.Forcepoint.com and follow us on Twitter at @ForcepointSec.


Forcepoint is seeking a qualified full-time Professional Services Data Engineer working on client site to facilitate configuration, data integration and training of our commercial-off-the-shelf regulatory surveillance and cybersecurity User and Entity Behavioral Analytics (UEBA) products. This individual must be highly motivated, have great interpersonal skills, and be technically proficient.

In this role you will be joining a small but rapidly growing team in New York. Your role will include working directly with customers to understand their goals, help shape requirements, own the design and implementation of analytic strategies, and develop robust ETL pipelines to support these analytic strategies. Additionally, you will interface with the broader Forcepoint Professional Services team to drive analytic capabilities of the platform, and overall facilitate an efficient, effective, and robust deployment of the Forcepoint UEBA platform to characterize and detect insider threats and compliance violations.
The successful candidate will receive specialized training to support our technologies and is expected to become proficient in all aspects of complex software solution deployment. Initially, the focus will be working with one of our major multi-national clients to build out a new deployment of UEBA. This position requires 5-10% travel domestic and international as needed to meet customer and project requirements.

Work Location: New York City
Must have U.S.Citizenship or Permanent resident green card holder status

Responsibilities Include:
Provide exceptional implementation services to new customers, including supporting data ingestion, analytic configuration, customer training, and troubleshooting
Work directly with customers to design and implement robust analytic strategies in the Forcepoint UEBA platform to address use cases in cyber security and regulatory surveillance
Partner with the UEBA Delivery team to integrate with and automate ingest for a wide variety of data sources (databases, remote servers, flat files, APIs, etc.) to ensure data is quickly and reliably available in all contexts
Prepare technical documentation to include as-built design, requirements, and Standard Operating Procedures
Interface with the broader Forcepoint data science team about analytic opportunities and accomplishments in the field to drive the evolution of the Forcepoint UEBA platform
Provide technical briefings to customer leadership and Forcepoint corporate leadership as required
Coordinate tasks and activities with various groups within Forcepoint, the government or partners

Required Skills & Experience:
Experience writing modular and reusable code in Python
Facility in scripting and troubleshooting application errors in Linux/Unix environments
Experience with the ETL: cleaning, transforming, and ingesting large datasets
Experience with full Software Development Life Cycle (SLDC) from requirements through to testing and deployment
Possess strong analytical, verbal, and technical written communication skills
Must be able to coordinate collaboratively across traditional engineering disciplines and effectively engage with customers
Must be eligible to work in the US

Desired Skills:
Prior technical experience in finance and/or information security organisations
Experience with Apache NiFi and high volume ETL tasks
Integration experience with data stores such as Elasticsearch, PostgreSQL, Splunk, ArcSight, Cloudera, etc.

Required Education:
Degree in a technical field such as Computer Science or equivalent work experience","New York, NY",Data Engineer - Professional Services,False
984,"Join a team recognized for leadership, innovation and diversity
At Honeywell, we are blending physical products with software solutions to link people and businesses to the information they need to be more efficient, more productive, and more connected. We have a real passion for the place where physical meets digital.
In this emerging Internet of Things era, the world is moving from simple digital transactions to complex digital-to-physical interfaces. Half of our engineers globally are developing software to augment our extensive technology portfolio, which include:
Technologies for connected buildings
The world's most advanced cockpits, jet engines, auxiliary power units, and turbochargers
Mobile Computing
Refining and petrochemicals process technologies and controls
Voice-automated systems
As a Data Engineer at Honeywell Building Technologies, you’ll be a member of our global team and help bring our Connected Buildings Platform and Solutions to life. Our solutions power Connected Buildings, which allow occupants to be safer and more productive, and the building to be more energy efficient and sustainable.

You will expand and optimize our data pipeline architecture, and you’ll optimize the data flow and collection across teams. You are experienced – you’ve been there, done that before. You enjoy building data pipes and wrangling data – so much so that you like to clean things up when they don’t meet your needs and build new pipes when they’re needed. You’re not scared of redesigning the pipeline – you just want it to be efficient and maintainable.
You will identify and implement process improvements – and you don’t like to the same thing twice so you’ll automate it if you can. You are always keeping an eye on scalability, optimization, and process. Oh – and GDPR is not a dirty word to you. You’ve worked with big data before, IoT data, SQL, Azure, AWS, and a bunch of other acronyms.
You will work on a team of 10-12 people including: scrum masters, product owners, designers, software engineers, data scientists and DevOps. You and your team collaborate to build products from the idea phase through launch and beyond. The software you write makes it to production in 12 short weeks. We work on important new features and functionality - those things that will impact our bottom line immediately. Your team will be working on creating a new platform using your experience of APIs, microservices, and platform development.
Our teams live by its ‘teach and learn’ mantra. We value our more seasoned engineers because they bring additional value to our company by using their years of experience to guide the next generation. We'll assign you a mentor on day one so you can take advantage of this amazing learning opportunity. We want you to get to your next level, whatever that might be. We also continue to support your professional development by paying for conferences, advanced degrees, and involvement in local and national organizations.
Honeywell has a strategic goal to make our company stronger and work better by growing gender diversity. By changing several of our practices and working hard to recruit women into the company, we’ve made significant progress. We welcome all qualified candidates to apply, but we especially encourage women and other groups underrepresented in technology.
As a Data Engineer with Honeywell, this is your opportunity to:
Create and maintain data pipeline architecture
Assemble complex data sets to meet a bunch of needs
Build the infrastructure for ETL of several data sources
Help us make the data actionable
Actively represent our culture by leading and participating in efforts around continuous learning, personal and professional development, community service and team building
BENEFITS HIGHLIGHTS

We know benefits are important to you. They are to us, too. We offer health, vision, and dental, and we take our benefits to the next level. At Honeywell in Atlanta, you'll free lunches every day, and you will eat in our cafeteria where you can collaborate with your peers. You can also play games with your co-workers
(board games, video games, chess, ping-pong, and foosball). Be prepared with the games - we occasionally do game Olympics where you can show your style.
We have 401(k) contributions and a generous leave package as well. What's not to love? We are located in the heart of Midtown at 3rd and Peachtree St. - and you can choose between free parking onsite or free Marta transport. We also offer a discount on the bouldering gym in the building.
25 SDLC

50 Data Engineering

25 Process Improvement




YOU MUST HAVE
Bachelor’s degree in Engineering, Computer Science, Data Analytics, Statistics, Information Systems, or some other STEM degree
5+ years of experience
5+ Full lifecycle development experience – your products have made it to production and have scaled globally.
2+ years experience with SQL and NoSQL databases, data pipelines, and workflow management tools
WE VALUE
Understanding software development lifecycle
Analytical skills & software development skills
IoT domain expertise
Scaled Agile Experience
An ownership mindset with a servant leadership attitude
Be familiar with several programming languages. We use Python, Java, C#, JavaScript, HTML5, CSS, and Bash Shell Scripting
Some frameworks we use are Spark, Hadoop, Hive, Pandas, Tensorflow and Keras. Some of the tools we use include Docker, Kubernetes, Bamboo, October, and more
Knowledge and experience in multi-structured data modeling and NoSQL technologies such as HBase and Cassandra
Data wrangling and Data Mining skills – Integrate data sources coming from different products, restructure columns, clean bad data, standardize data fields and types, ultimately increase data quality and usability
Exempt How Honeywell is Connecting the World
INCLUDES
1st Shift
Continued Professional Development

ADDITIONAL INFORMATION
Job ID: HRD41752
Category: Engineering
Location: 715 Peachtree Street, N.E., Atlanta, GA 30308 USA
Honeywell is an equal opportunity employer. Qualified applicants will be considered without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.","Atlanta, GA 30308 (Old Fourth Ward area)",Data Engineer,False
985,"We are currently looking for a Data Engineer to join the Thales Avionics, a subsidiary of Thales USA team in Irvine, CA.

The Data Engineer will work closely with big data, reporting and analytics. They should be familiar with data extraction, preparation, loading of data from a variety of relational and non-relational sources into a cloud based Big Data environment. This individual will be familiar with contract and business required reporting compiled from large Data Extracts. This position is necessary to continue to be able to complete the necessary customer reports outlined in existing business contracts.

Key Responsibilities:
This individual will work with existing team members to produce all reports to existing release schedules.
This person shall have the skills to be able to work alone as well in a team environment.
Review existing tools utilized by the department and offer suggestions for tool improvement in the handling of all data sources.
Shall suggest and build data and analytic tools that will offer insight into the pipeline to better existing key performance indicators.

Required Education, Competencies and Experience:
Bachelor’s degree in Computer Science, Information Systems or equivalent quantitative field or equivalent blend of education and experience
3+ years of experience in a similar Data Engineer role
Experience working with large data sets and extracting from structured or unstructured datasets
Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience with displaying data Geospatially.
Strong interpersonal skills and ability to project manage and work with cross-functional teams.
Experience with the following tools and technologies a plus:
Cloudera Hadoop, Spark, Kafka, NiFi, ElasticSearch, Hive, Solr
Relational SQL and NoSQL databases
Data visualization tools such as D3js, leaflet.
AWS cloud services such as EC2, EMR, RDS and Redshift
Stream-processing systems such as Storm and Spark-Streaming
Programming languages and related web technologies such as Python, Java, Javascript, C++, JSON, R etc.
Thales champions inclusion and we believe diversity strengthens the fabric of our culture. We are an Equal Opportunity Employer/AA/Minorities/Females/Veterans/Disabled.","Irvine, CA",Data Engineer,False
986,"ContractJob SummarySenior Data Engineer(Sunnyvale/Santa Clara, CA)This position(s) is a critical project position with one of our marquee clients in the Bay Area (Sunnyvale/Santa Clara area) which is redefining the world of eCommerce.Local candidates (Bay Area, California) are highly preferred.Type: FTE with Clairvoyant/ C2C/ Contract-W2/ 1099Position SummaryVery Strong engineering skills. Should have an analytical approachand have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ years’ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaborationwith the business partners, product managers other engineering team.Must HaveStrong analytical backgroundSelf-starterMust be able to reach out to others and thrive in a fast-paced environment.Strong background in transforming big data into business insightsResponsibilities and DutiesTechnical RequirementsStrong Handson expertise in SQLExperienced in Hive/HiveQLAdvanced SQL (preferably Teradata)Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).Strong Hadoop scripting skills to process petabytes of dataKnowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance OptimizationExperience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)Experience in Unix/Linux shell scripting or similar programming/scripting knowledgeExperience in ETL/ processesReal time data ingestion (Kafka)Nice to HaveDevelopment experience with Java, Scala, Flume, PythonCassandraAutomic schedulerR/R studio, SAS experience a plusPrestoSpark, DruidHbaseTableau or similar reporting/dash boarding toolModeling and Data Science backgroundRetail industry backgroundEducationBS degree in specific technical fields like computer science, math, statistics preferredSiteUS - SunnyvaleJob Posting Service TypeContractorJob Types: Full-time, ContractExperience:Large data, experience with distributed computing: 8 years (Preferred)Education:Bachelor's (Preferred)Location:Sunnyvale, CA (Preferred)Work authorization:United States (Preferred)","Sunnyvale, CA",Senior Data Engineer,False
987,"$50,000 - $140,000 a yearContractJob SummaryBachelor's Degree in Engineering or equivalent work experience2+ years of hands on Health Care Domain Experience2+ years of Quality engineering testing experience in a web application environment Testing User Interfaces, backend database, Web services, API’s, Batch/ETL application components2+ years of experience writing Functional/Integration test plans and test cases for complex software application2+ years of hands-on testing experience in a Continuous Integration/Delivery/DevOps environment supporting weekly or daily production deployments2+ years of hands-on testing experience working with XML/JSON, MQ interface, EDI X12, SQL transactions1+ years of hands-on experience working with UNIX operating system and writing DB SQL scriptsExposure to incremental agile software development methodologiesPrior experience in working closely with Developers and Architects in solving software defectsExperience in creating and implementing testing methodologies, processes to solve testing problem in handResponsibilities and DutiesFlexible to take both manual and automation testing needsUnderstand complex HealthCare Domain workflows and work on creating test scenarios, execution plan and automation of those areas.Responsible for developing test strategy, test scenarios, QA project plan, coordinating test data setup, environment readiness, test execution and issue resolutionSystem/Integration testing REST API’s, User interface, Database, reporting, ETL/Batch application componentsExplore new software, tools and capabilities to automate System integration testing to supports continuous delivery/DevOps agendaRequired Experience, Skills and QualificationsBachelor's Degree in Engineering or equivalent work experience2+ years of hands on Health Care Domain Experience2+ years of Quality engineering testing experience in a web application environment Testing User Interfaces, backend database, Web services, API’s, Batch/ETL application components2+ years of experience writing Functional/Integration test plans and test cases for complex software application2+ years of hands-on testing experience in a Continuous Integration/Delivery/DevOps environment supporting weekly or daily production deployments2+ years of hands-on testing experience working with XML/JSON, MQ interface, EDI X12, SQL transactions1+ years of hands-on experience working with UNIX operating system and writing DB SQL scriptsExposure to incremental agile software development methodologiesPrior experience in working closely with Developers and Architects in solving software defectsExperience in creating and implementing testing methodologies, processes to solve testing problem in handPreferred Qualifications: Job Type: ContractSalary: $50,000.00 to $140,000.00 /yearEducation:Bachelor's (Preferred)","McLean, VA",Big Data Engineer,False
988,"At Morningstar, helping investors is what brings us together and drives our work. We are looking for a Data Engineer with relational, SQL, no-SQL, ETL/ELT, sysops and general programming experience. Having a proven track record of modeling data, solving analytical problems, aggregating data, bring novel engineering solutions and helping the business move forward is a big bonus. You will have the opportunity to help build, move, transform and model customer behavior, existing models, warehouses and influence our transformation into the cloud. Overall, this role is designed to help us move forward. Every day, you’ll work with team members across disciplines developing products for investors. You’ll interact daily with our product managers to understand our domain and create technical solutions that push us forward. We want to work with other engineers who bring knowledge and excitement about our opportunities. This position is based in our Chicago office.

Responsibilities
Understanding the business and working with product managers, architects and dev to source, transform and build datasets into AWS data technology solutions
Break down data silos and provide our business with a competitive advantage
Bring focus and attention to the future of our platform in the cloud and open new data technology possibilities there
Ensure that our data is secure and compliant to necessary compliance and governance standards
Align with Morningstar overall data strategy and assist our overall firm success
Work closely with the software engineering teams
Help the business define its Data Recovery Point Objectives (DRPOs)
Solve investors’ problems with technology and data
Be passionate about quality, process, engineering, and investing in general, seeing opportunities for improvement, seizing them, and then sharing your findings with others.
Familiarity or desire to become proficient with data technology and operating in an AWS cloud environment

Additional Items to Stand Out from the Crowd:
Excellent relational data skills
Experience working with streaming data and solving near real time problems for customers
Design indexes for existing applications, choosing when to add or remove indexes
Advise others on efficient database designs (tables, datatypes, stored procedures, functions, etc.…)

Requirements
A driven curiosity about our data and a proactive nature to get things done
Understanding of ETL best practices and data engineering
Strong need to communicate clearly, consistently, and proactively, in person and in writing
Focus on balancing internal and external customer desires with delivering results the right way, in the right timeframe
3+ years data engineering experience
Expertise with RDBMS concepts including stored procedures, tables, views etc.
Beginner to Midlevel experience with cloud (AWS) based software systems
Some experience with performance tuning and query optimization
Experience with Agile methodology and tools like JIRA.

Morningstar is an equal opportunity employer.


001_MstarInc Morningstar Inc. Legal Entity","Chicago, IL",Data Engineer,False
989,"About Evolytics: Evolytics is evolving analytics by inspiring people to use data in ways that make a difference in the world. With a focus on optimizing consumer experiences and strengthening business growth, Evolytics works with world-class brands to support data initiatives that span the data lifecycle. Capabilities include: Measurement Strategy & Planning, Data Audits, Analytics Implementation Services, Data Engineering, Reporting & Analysis, Data Visualization, Testing & Optimization, Predictive Analytics and Customized Training.Position Overview: The Data Engineer will work directly with client and internal team members to:Create report/dashboard inputsCultivate and fine-tune data automationConduct data analysis with outcomes that include delivery of actionable insights and business intelligence designed to optimize digital marketing performance of paid/owned/earned campaigns, social media marketing, web sites and mobile apps/sites.What You’ll be Doing: As a Data Engineer at Evolytics, you will be expected to:Develop processes and procedures for ingesting data from disparate sourcesDevelop and/or maintain database architecture, including fine tuning and optimizing query plans, indices/projections, etc.Create and maintain customized SQL queries, develop reporting structures using SQL, develop and build data modelsAssess client implementation efforts and conduct data QA to determine and document any gaps between the data being collected and what is needed for reporting outputsProject manage multiple client requests and detailed project activities at any one time to ensure accurate, timely and efficient reporting and analysis deliverablesSQL skills are critical for this Data Engineer role. Consideration will be given to qualified candidates with varying levels of experience, with compensation commensurate with experience.What experience would we like to see?Experience with at least one of the following scripting languages: Python, Bash, Java, Scala, R, Perl and/ or Node.jsExperience with Linux command lineExperience with SQLExperience with a leading analytics or relational database system, such as Redshift, Vertica, BigQuery, PostgreSQL, or MySQLExperience developing data solutions on AWS, Azure, or Google CloudExperience with Big Data solutions such as Hadoop, Hive, or SparkExperience with change release processes and tools such as gitExperience developing/implementing data transformation via ETL processes and data pipelinesMinimum of a Bachelor’s degree in Computer Science, Information Systems, Business, Marketing or a related disciplineOther experience that is helpful, but not required: Experience with any of the following database systems will be a plus: NoSQL, Mongo DB and/ or Couch DBExperience working with clickstream web analytics tools such Adobe Analytics (Omniture SiteCatalyst), Adobe Discover, Google Analytics, or working knowledge of the field of web analyticsKnowledge of commonly-used digital metrics, analytics concepts and online marketing channel best practicesExperience with TableauKnowledge of BI methodologies and toolsProficiency in Excel and PowerPointThis role includes the unique opportunity to work alongside some of the best talent you will find in the Digital Analytics industry worldwide.Culture: Relaxed Work Environment: Casual Dress Code, Pool Table, Break Room, Treadmill DesksOpen Office Floor Plan Enhances CollaborationLearning Opportunities and Company-Provided TrainingOffsite Team-Building Events and Company Happy HoursFree Weekly Lunch on Taco TuesdayFree Snacks, Fruits and BeveragesBenefits: Competitive Benefits Package including Health, Dental & Vision & Life InsuranceGreat Compensation Package with Paid Time Off, Performance Bonuses and IRA Matching ContributionsJoin Evolytics!Awesome people make powerful teams. Let’s do great things together!Please include a cover letter describing your experience with analytics as well as your resume.Job Type: Full-timeExperience:SQL: 1 year (Required)Location:Kansas City, MO (Preferred)Work authorization:United States (Required)","Kansas City, MO 64152",Data Engineer,False
990,"Can you own the data layer, guiding the underlying data architecture to support a fast growing application stack? Do you enjoy working closely with software engineers to develop and refine software systems? Can discriminate use cases between transactionally consistent, ACID-compliant systems of record vs. eventually consistent options vs non-traditional data stores?
This role will be part of our data engineering group that works closely with our feature product teams facilitating the underlying data model and architecture as our product evolves. Knowledge and experience within systems and software architecture is required.
Porch’s tech stack has evolved around Microservices Architecture and underlying micro-databases. We integrate the application of DDL and DML into our software CI systems via Flyway so that our services have true ownership of our data. We have our domain / canonical model represented behind their own APIs as part of a master data management solution. PostgreSQL is our relational database of choice.
Responsibilities:
The Architecture Stuff:
Data Architecture - You will need to ensure we have the correct level of normalization to support that application features. Maintain the integrity of the underlying data, ensure we have a transactionally consistent view of the data when necessary and support eventually consistent data representations when appropriate.
Domain Modeling - Is A a part of B or a part of C or is A actually D? Sometimes microservices get scope creep, it is generally most apparent and least reversible at the data layer. Be comfortable asking questions and giving frank answers.
OLTP vs. OLAP - Is the request a functional requirement of the application or a reporting need? Keep analytical requirements out of transactional systems and evangelize the usage of the data warehouse. Be the bridge between the Engineering, Product, and Development (EPD) teams and the Analysts, Data Scientists, and Data Warehouse.
The Engineering Stuff: You will work with several software engineering teams to ensure the application is supported by a high quality data layer. This means you need to own:
Data Migrations - Underlying data model changes? New features? Migration plans may be needed to support the evolution of the systems while maximizing availability.
Technologies - Not everything has to be in a relational database. Be open to leveraging key-value stores, document databases, and other technologies to provide the right tool for the job.
Process Improvement - Is there a better way? Periodically question what, how and why you are doing a task. Don’t be a drone.
Collaboration - Vertically: fulfill the needs of the application layer and front end. Horizontally: communicate and coordinate with data engineering team to establish conventions, utilize best practices and share lessons learned.
Scaling - Identify appropriate applications of partitioning and sharding.
Experience:
At least five years as a software, systems, or data engineer during which you:
Have been the go to for query optimization and performance troubleshooting.
Have had 3+ years owning DDL and data model implementations in production environments.
Are comfortable being part of the on-call rotation. Everyone is on the on-call rotation.
Have done all of these things in a cloud environment—AWS, GCP, or other public cloud.
About Porch:
Porch sprang to life to tackle an age old, recurring problem: moving in, maintaining, and managing one’s home is hard. Porch is becoming the partner for the home, helping homeowners to get any home project done, both by providing Porch Services to get all small jobs done ourselves and a large professional network to provide homeowners with multiple quotes for larger projects. Home services is a $400 billion market and Porch is positioned to build a large and rapidly growing company with an opportunity to change home ownership. Porch’s mission is to ensure every home project gets done, with every customer satisfied. We aim to build a lasting brand that delivers quality, data-driven, delightful results.
Porch was founded by successful entrepreneurs and is led by an experienced team of techies who build beautiful products, are operationally-minded, data-driven, have relentless customer focus, and put team before self. Based in Seattle, Porch is backed by Valor Equity Partners, Lowe’s Home Improvement, and many others.","Seattle, WA",Senior Application Data Engineer,False
991,"$55 - $95 an hourContractTitle:  Senior Software Engineer | Java, Python, Big Data | 6-12+ month ContractOverview:  Are you a Senior Software Engineer, Senior Java Engineer, Java Developer, Software Engineer, Software Contractor, or Software Consultant with solid Java and Big Data (Map Reduce, R, Python, Hive, Sqoop, or Hbase) experience? Are you looking for a lengthy 6-12+ month contract assignment (immediate hire)? In this role you will be customer facing- strong communication both written and verbal, is required. Experience beyond Java and Python would be a huge plus. This means any Scala or Spark experience is highly desired. You will have a solid grasp of data structures, algorithms, interpreting UML diagrams, and the ability to develop software solutions to complex business challenges. This position will be working in and around Northern NJ or New York, NY (Manhattan area). For best results apply today.What’s in it for you: Immediate hire opportunity6-12+ month contractSolid compensationInteresting projectsHere is a little more about what you will be doing: Working in a team environment with individual accountabilityHands-on Java, Python, and Spark projectsSoftware designInterpreting UML DiagramsWorking with internal and external stakeholdersHere is the background and experience we are looking for: BS in Computer Science, Math, Science, Technology, Engineering, Physics, or related field or equivalent experienceSolid communication and interpersonal skillsAbility to work in Northern New Jersey (Jersey City) and/or New York, NY (Manhattan)So, if you have the background and experience listed above please call me or send me your resume today! Must have the ability to work in the US for any employer and preferably be located in the Jersey City, NJ or New York, NY (Manhattan) area, but if you are looking to relocate here, please apply.Job Type: ContractSalary: $55.00 to $95.00 /hourExperience:Spark: 3 years (Preferred)Scala: 3 years (Preferred)Hive: 3 years (Preferred)UML Diagrams: 5 years (Preferred)Python: 5 years (Required)MapReduce: 3 years (Required)Java: 5 years (Required)Big Data: 5 years (Required)Education:Master's (Preferred)Location:New York, NY (Required)Work authorization:United States (Required)","New York, NY",Senior Big Data Engineer,False
992,"ABOUT US
Our Company:
Datafiniti provides instant access to web data for businesses. We're like a B2B Google for data. We can provide data sets on every product, business, or property listed online, and we continue to add more data. Our data is used by startups, Fortune 500 companies, and every sort of company in between.

Our Culture:
The Datafiniti team is smart, driven, and passionate about the hard work required to build an amazing company, create an ambitious product, and serve a wide variety of customers. We celebrate our wins and grind through our challenges. We're a small, close-knit team dedicated to each other and our vision.

THE ROLE
What you'll do at Datafiniti:
At Datafiniti, we face a variety of fascinating data challenges that all relate to producing high-quality data from the wild expanse that is the Internet. As a Senior Data Engineer, you'll take on different software projects related to those challenges, including:

Analyzing the HTML source of an online business, product, or property listing to automatically extract structured data around name, address, brand, pricing, and more.
Deriving additional information around business, product, or property records based on what information has already been collected.
Validating existing data through development of heuristics, data models, and other tools.
Any software, models, or other technologies developed to handle these tasks must operate at-scale across tens of millions of records.

What we must see from you:

Highly proficient software developer - includes excellent understanding of software design and coding best practices
Deep understanding of machine learning and other data science technologies, along with previous experience (at least 2 years) using these technologies in a production environment
Critical thinker, extremely detail-oriented, and process-driven
What we'd love to see (but it's ok if we don't):

Experience with NLP when used in data extraction
Experience with supervised and unsupervised classifiers
Experience with technologies we use in our stack: Java, Elasticsearch, AWS, Docker
BENEFITS

How you'll feel the love:

Competitive pay
Complete insurance package (medical, dental, vision, life)
Equity in a growing startup
Flexible vacation policy - take it when you need it
Foodie Fridays - lunch is catered every Friday
Monthly happy hours
Company adventures - mini golf, board game nights, and other shenanigans
Working with awesome people that get stuff done","Austin, TX",Senior Data Engineer,False
993,"Amyx is seeking to hire a Data Quality Analyst/Data Engineer to be located in Washington, DC.This position will be supporting the Security Exchange Commission - Center for Risk and Quantitative Analytics (CRQA) contract. This is a specialized quantitative group that resides under the Office of Market Intelligence (OMI). The Data Quality Analyst/Data Engineer will support two service lines within CRQA: The Fraud Analysts and the Data Cleaning Group. The FAs provide investigation-specific analysis solutions to large datasets in order to identify anomalies that could be indicative of wrongdoing. The DCG is a group that provides data manipulation and data management services across Enforcement. Their solutions include converting unstructured data into structured output (PDF to Excel) and merging disparate datasets into a centralized, usable product (bringing together trade blotters from multiple brokerages).
While industry knowledge of fraud detection and securities markets would be beneficial, we are putting more of an emphasis on technical proficiency. The contractor should have basic knowledge of a robust database platform, such as Oracle, SQL or SAS. Deliverables are often created in Excel and intermediate knowledge of Excel (chart creation, pivot tables, and standard formulas such as vlookup/sum/count/subtotal) would be a plus; knowledge of VBA as a back-end support to Excel is useful. We plan to expand into visual dashboard solutions, such as Tableau and R packages, so knowledge of those types of platforms, or an ability to learn new systems, is beneficial.
Responsibilities and Daily Tasks:
Data Profiling
Contractor shall conduct a data audit against all identified data sources identifying the data that needs to be corrected.
Contractor shall provide a data analysis report from the data audit (e.g. description of problem, data source, number of occurrences, missing data, impact on production data).
Contractor shall identify who (the Contractor, the vendor of the data or the business owner) will be responsible for correcting the data elements that are in error, along with an explanation as to how they came to this conclusion.
Will assist CRQA Data Quality Manager in working with the Business owners to identify the data cleansing needs and identify criteria for the data sampling for the data audit (e.g. trade dimensions like volume, amounts, date dimensions, identifiers such as TAX IDs, ACCOUNT Numbers).
Contractor shall ensure the data rules are in compliance with the agency rules, policies and statutes (PII, confidential, sensitive)
Contractor shall provide data analysis to support both the FAs and the DCG. The primary function will be to provide high-level analysis of data that has been processed by the DCG group. These would be relatively simple aggregates of large datasets to provide summary information of the data that is returned to the case team. The contractor may also serve to support the FAs in more complex analyses that involve the identification of patterns within large datasets, such as analysis of trading activity.
Data Engineering
Perform data engineering tasks – data mapping (source to target) , ETL design and development in order to extract and transform the data as required for delivery to customers.
Work to develop repeatable modules for data extraction/transformation
Develop automated data cleansing techniques
Develop unit testing framework and modules
Supported Technologies
Various file formats (CSV, .xls, XML, PDF, JSON)
Oracle Database
PDF conversion tools
IBM Data Stage (ETL)
Basic unix/linux knowledge
Python

Bachelor's Degree required
Solid Experience (3-10 years) in relational database concepts with a solid knowledge of Oracle, SQL and PL/SQL
Experience in Data Integration , Cleansing and Profiling using Oracle Data Base and Access and/or other technologies
Expert level in Excel and Access and ability to manipulate disparate data sources
Exposure to working with multiple data sets - CSV, Excel, Flat Files, PDF, JSON and other file formats
Experience with Unit Testing framework
Ability to solve problems and work through conflicts
Extreme attention to detail and common sense
Excellent written, interpersonal and communication skills
Highly organized, detail oriented, strong work ethic and team player
Ability to follow rules and direction and work independently
Positive attitude
Ability to obtain public trust clearance
Preferred Skills:
Python for data analytics experience is
Familiarity with IBM Infosphere DataStage Development (ETL Tool)
Experience with the financial sector","Washington, DC 20001 (Shaw area)",Data Quality Analyst/Data Engineer,False
994,"At Volusion, we make products that people love. Our teams are dedicated to providing SaaS ​e​commerce solutions and services for all business types, from startups to well-established companies. If you're a creative professional who loves working with teams, has a passion for driving positive change and wants to better the world with your ​ideas, we want to hear from you!

The rundown:
As a Data Engineer, you will assist in the development of data warehouse information architecture on both Google Cloud BigQuery and Microsoft SQL Server, optimize SQL performance, and provide operational support to our high availability Microsoft SQL Server Warehouse infrastructure. An ideal candidate will be a proficient warehouse developer versed in data integration services development, report development, data analysis, and database administration.

You will:

Assist in building and maintaining data warehouse data integrations leveraging both MS SQL Integration services and Python technologies
Assist in the development of BigQuery Data Model
Develop and Optimize SQL Queries leveraged by existing integration services ,reporting processes and data analysis reports
Develop dashboards and visualizations for ongoing measurement and KPIs
Plan for non-transactional data storage for reporting and analytics
Database Schema design and data flow architecture planning
Maintain current reports in existing reporting platforms
Profile source system data as needed to provide feedback for business requirements
Analyze and verify Data Warehouse data
Assist in the design, build and maintenance of SSAS cubes

We are looking for someone with:

MUST be a team player with excellent oral and written communication skills
Bachelor's degree in Computer Science or Engineering
SQL Skills for data analysis is a strong must have
2+ Years Microsoft SQL Server Integration Services development or similar Extract Transform Load development
2+ years of experience Microsoft SQL Server database administration
2+ years of experience with Python 2.7 or 3.x
Strong experience with performance optimization and tuning with database applications is a MUST
Kafka experience- required
Python Scripting and .NET knowledge preferred
Familiarity with Cloud Services. Google Cloud Platform (GCP) and the GCP Data Services experience is a major plus
Git experience is also a plus
Highly organized, self-starter with an eye for detail who can maintain multiple ongoing projects simultaneously

Who is also the embodiment of our culture code ( https://culture.volusion.com/ ) (we hope you are nodding your head in agreement as you browse through it!):


Humble: Have humility and be respectful; no egos allowed.
Effective: Get stuff done!
Adaptable: Willing to fill any role, anytime. Going above/beyond the call of duty.
Transparent: Open and honest to self and others.
A founder: Think big, go fast and solve for the customer.

Benefits & Perks:

Competitive compensation packages
Medical, Dental, Vision, and Voluntary Life Insurance
Flexible Paid Time Off
401(k) with Company Matching
Paid Parental Leave
On-site Fitness and Yoga Classes
Casual Dress and Beer Fridays
Endless Supply of Coffee and Snacks
Two Volunteer Days Off
Bring Your Dog to Work Days
Chair Massages
Team Sports and Team Outings

","Austin, TX 78758 (North Austin area)",Data Engineer,False
995,"A-Line Staffing is seeking a qualified candidate for the position of Senior Data Engineer located in Hygiene, CO.This is a direct hire opportunity!Please review the desired qualifications listed below and apply or contact Greg Wagner with questions.This is a blended role of Business Intelligence and Data Engineering. The core focus of this role will be: Designing, developing, and deploying of assigned data processes, including ETL/ELT data pipelines, file feeds, reports, and dashboards; while managing and actively participating in the data governance processOur ideal candidate has a passion for data and BI, is an analytical thinker, and can act as Data Subject Matter expertIn addition, someone who has strong communication skills, is a great collaborator and mentor, possesses a high level of experience with Hadoop and one who will complete tasks on-time with high quality results will be successful in this roleResponsibilities include, but are not limited to: Proactively Communicate IssuesBuild and tune data modelsDevelop data visualization, dashboards, reportsDevelop data pipelines and file feeds, ETL, ELT processesCreate a data driven story with data visualizations and recommendations, package it via tools like PPT, Tableau, etc. and deliver to internal and external audience at various levelsManage Data Governance (integrity, quality, lineage, metadata)Be a thought leader in the data and analytics space; be a consultant and a liaison to internal and external teamsMentor and encourage newer, less experienced members of the teamCommunicate clearly, concisely, and professionally with internal West Teams, Customers, and ExecutivesQualifications: Bachelor's degree from an accredited college or university with major course work in computer science, MIS, or a related field is requiredEquivalent work experience in a similar position may be substituted for education requirementsExperienceMinimum 5 years of experience with data analysis and migration to include experience in the analysis or design of applications or systems to store and extract dataMinimum 3 year of experience with telecommunications billing preferredMinimum 3 years of experience with one or more of the following areas: software development, database development (Oracle preferred), business analysis, systems integration or system administrationMinimum 1 year of experience writing detailed test plans for small to medium sized projects preferredTechnical Skills/ProficiencyMinimum 4 years of experience with SQL requiredAny of the following scripting languages: Python, Pig, R, SAS, SPSS, Perl, C or any other scripting languageAbility to produce production ready code, deploy code into productionAny of the following Relational Databases: PostgreSQL, MySQL, Informix, SQL Server, Oracle DB, Greenplum, Vertica, DB2 or any other RDBMS systemHadoop environments and the tools associated within the environment such as: YARN, Sqoop, Impala, Flume, Hive, Dril, Hawk or any other similar toolsIntermediate knowledge of Word, Excel, and PowerPoint requiredKnowledge of Systems and / or Database Security mechanisms (Encryption, Data Loss Prevention, etc.) is highly preferredGreg Wagner – A-Line StaffingJob Type: Full-time","Hygiene, CO",Senior Data Engineer,False
996,"A Sr. Data Engineer builds, manages, integrates, and optimizes our reservoirs for data in support of delivering relevant information for business consumption. They promote advances in data preparation for predictive analytics and machine learning. This individual develops, constructs, tests and maintains designs for databases and large-scale data processing systems in support of underlying business, solution, and enterprise architectures. They support and maintain pipelines delivering relevant data sets for business consumption and analysis. This individual works closely in the selection of appropriate data management systems (on-premises and in cloud) for the business to achieve optimal analysis and predictive analytics. They will wrestle with complex rules associated with data integration from many types of source systems to achieve the ultimate goal of providing clean, usable data to the enterprise.
Responsibilities include: - Promotion and facilitation of Data Governance through coordination of business stakeholder participation enforced with the onboarding of MDM & MDS tools - delivering large and complex solution data needs creating requirements and technical specifications resulting in data architectures and designs - participate in strategic data innovations - implement and advance data models and configurations in support of integrating data from multiple source systems and environments - research, design, and implement next generation analytics and machine learning platforms - build tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality.
Education:

B.S. in Computer Science, Mathematics, Software/Computer Engineering, Information Systems or science related field.
A Data Professional Certification (e.g., ICCP Certified Data Professional, BI Professional, Big Data Professional, Data Governance Professional, or vendor equivalent) is encouraged.
Years of Related Work Experience

7-10 years of data design & development experience in relevant technologies/systems required including technical experience implementing and delivering from system architecture, design, integration, implementation, security, and capability roadmap for a data environment.
Experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) - awareness/exposure to NoSQL technologies (Key/Value, Columnar, Document, Graph) -
Experience with MDM, MDS, Data Dictionary/Marketplace a plus - ability to facilitate technical and non-technical stakeholders in governance discussions with distinct outcomes - creative problem-solving leveraging experience with multiple, diverse technical configurations, technologies and processing environments - intellectual curiosity for advancements in data visualization.
Power BI, Microstrategy, Tableau), big data systems including MapReduce technologies, NoSQL technologies (Key/Value, Columnar, Document, Graph), and monitoring platforms - awareness/exposure to development and modeling programming languages (e.g., Java, C#, R, Python) - exposure to cloud data architectures.
Symetra is a dynamic and growing financial services company with 60 years of experience and customers nationwide. In our daily work delivering retirement, employee benefits, and life insurance products, we're guided by the principles of VALUE, TRANSPARENCY AND SUSTAINABILITY. That means we provide products and services people need at a competitive price, we communicate clearly and honestly so people understand what they're getting, and we build products that stand the test of time. We work hard and do what's right for our customers, communities and employees. Join our team and share in our success as we work toward becoming the next national player in our industry.

Learn more at www.symetra.com/careers.","Bellevue, WA 98004 (Downtown area)",Sr Data Engineer,False
997,"Marstone Inc. is seeking a Data Engineer that is interested in pushing boundaries and disrupting the financial industry using Machine Learning, Artificial Intelligence, and Predictive Analytics to help clients plan their financial future and to get a wider view of their investment roadmap. The candidate should be passionate about Analytics and Big Data architecture. The candidate brings experience designing Data Lakes and Data Warehouses related to Machine Learning (ML), Artificial Intelligence (AI) and advanced analytics. The candidate will work on an architecture team developing a greenfield predictive analytics application that integrates with various data sources to provide deep learning insight to historical investment patterns.We offer a competitive salary, health benefits, and 401K plan. Our development team is based in Providence, Rhode Island’s historic Jewelry District. Marstone offers a bright and attractive loft office space within walking distance of many nearby lunch spots, cafes, and other amenities.Responsibilities: Contribute to the design and development of a scalable and cost-effective cloud-based data lake design.Collaborate with an architecture team to develop an advanced analytic platformDevelop data transformation components in a cloud environment to ingest data and events from cloud and on-premises environments as well as third parties and partnersCreate ETL pipelines and data services to validate, catalog, aggregate and transform ingested dataDesign automated data pipelines and services to integrate data from the data lake to internal and external consuming applications and servicesThe candidate will bring some of the following qualifications to Marstone: Hands-on experience architecting/developing data lake solutions using Amazons AWSAWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormationWorking experience and detailed knowledge in Java, Scala, or PythonExperience with ETL, and pipeline tools (Glue, EMR, Spark)Experience with large or partitioned relational databases (Aurora, MySQL, PostgreSQL)Experience with NoSQL databases (DynamoDB, Cassandra)NumPy, Pandas, PySpark, RAdvanced SQL and ORM/RDBMS skills using any of the following: Oracle, SQL Server, MySQL, PostgreSQL, AuroraAdvanced SQL scripting skills including ETL design, Stored Procedures, Triggers, IndexesCaching and Object/Key Value stores such as Redis, MemcachedSearch and Logging tools such as ELK stack or CloudWatchCloud architecture & design in AWS or AzureExperience with Agile/Scrum SDLC process and tools such as Pivotal Tracker, Confluence, JiraProject coordination and estimationversion control repositories such as Bitbucket, GIT, TFS, SVNRelated bachelor's degree with 3-5 years of experience, or 5-8 years professional work experience with demonstrated ability to fulfill the roleAdditional familiarity with any of the following tools and concepts is a plus: Experience developing financial analytic applicationsAuthoring application design documentation and unit test cases.More about Us: Marstone is a rapidly expanding fintech company whose mission is to empower investors to take control of their financial lives with knowledge and confidence. We are an experienced team of designers, technologists, and wealth-management professionals who have developed a new breed of intuitive and relevant financial investment solutions used in support of global institutional clients.We are motivated technology professionals that thrive in a creative and collaborative software engineering environment. We offer a collaborative and supportive team atmosphere that encourages developers and engineers to expand their own skills and contribute to our team through skillful problem solving, knowledge sharing, and mentorship. We are passionate about implementing horizon technologies and solving the technical challenges of providing leading edge investment solutions to our clients.As a SEC-registered investment advisor, our digital advice platform, Powered by Marstone, assists clients in creating personalized investment portfolios that suit their needs, goals, and aspirations. We continually monitor and maintain each portfolio to assure those investments stay aligned with their original financial targets.Job Type: Full-time","Providence, RI",Data Engineer,False
998,"About Syapse

Maybe you've supported a friend or family member who's dealing with cancer. Maybe you've battled it yourself. At Syapse, our mission is to enable healthcare providers to deliver the best care to every cancer patient through precision medicine. Our personal connections to this mission unite us.

We integrate genomics and clinical data on a single platform and allow doctors to share important treatment and outcomes information across a national network. Our customers manage more than 1 million active cancer cases at hundreds of hospitals across the US and Asia. Now, we're racing to meet growing demand and bring precision cancer care to every patient, regardless of location or income.

About the Engineering Team

The Engineering team at Syapse builds our revolutionary oncology decision support platform that empowers physicians to deliver consistent, high-quality precision cancer care. We're scaling our data platform using distributed systems, stream processing, and microservices. To do this and tackle all our engineering challenges, our autonomous, customer value focused scrum teams work in short sprints and own product areas end to end. Our domain-based guilds (e.g. Data Semantics, Web Technology, Architecture) balance that by encouraging the building of communities and foster collaboration across teams.

As Syapse continues to grow, we'll use advanced software engineering methods to expand the capabilities of the platform—and help deliver the best cancer care to every patient.

About the role

You will join our newly-formed ecosystem team whose focus is to develop data-intensive applications that are the bridge between our health-system customers and companies in the healthcare ecosystem pioneering the use of real world evidence in the treatment of cancer. As an early member of the ecosystem team, you will have a unique opportunity to establish new infrastructure and processes that enable us to deliver insights that drive the practice of precision medicine forward. You will design and build out the applications and tooling that make data more accessible and enable analytics pipelines, data visualization, and other services.

Our team is based out of Syapse's East Coast office and takes pride in our unique culture and environment. Our mission as a company is a serious one, so you can expect to join a nimble team that strives for ever increasing standards of delivery with quality. Fortunately, when you love what you do and you are surrounded by great people, you can grow and accomplish more than you think possible.

What you bring to the table


BS, MS, or Ph.D. in computer science or related field
5+ years relevant work experience
Successful track record of manipulating, processing, and extracting value from large complex datasets
Expertise in writing SQL and working with large-scale relational databases built on PostgreSQL, MySQL, or similar platforms
Experience building solutions that leverage non-relational database technologies (DynamoDB, Cassandra, MongoDB, etc…)
Strong programming skills in languages like Python, Ruby, or Java
Skilled at working with a mix of structured and unstructured data from a variety of sources
An affinity for simple solutions to complex problems
A strong motivation to meet our mission

Bonus points if you


Have experience working with AWS cloud technologies, especially DynamoDB, RedShift, Athena, and Kinesis
Exposure to other big data tools: Hadoop, Spark, Kafka, etc…
Understand distributed systems and patterns for data replication
Switched from pre-med or biology to computer science in college

Benefits and perks

Amazing Coworkers

Competitive pay and ownership in the company

100% company paid medical, dental and vision for employees

401(K) matching

Flexible time off

Transit assistance & Pre-tax commuter benefits.

Weekly catered lunches and office snacks

Company sponsored gym membership & lots of other perks

Next steps

After submitting an application a team member will reach out to you shortly. While each interview is unique to the role, our interview process typically consists of an introductory phone conversation with a recruiter, a second phone or video interview with a hiring manager or senior team member, and wraps up with a visit our to our office, usually lasting +/- 4 hours.

Have a quick question about the role? Email careers@syapse.com ( careers@syapse.com ) or simply apply here.","Radnor, PA",Data Engineer,False
1002,"ContractJob Title: Sr.Big Data EngineerLocation: Atlanta, GA(Multiple locations)Duration: 12+ MonthsMinimum Required Skills:Big Data, Scala, Hadoop, Data Mining, Kafka, GIT, UNIX, Akka Streams, MavenIf you are a Big Data Engineer with Scala, Hadoop and Spark experience, we would like to hear from you! We are an engineering services company working with leading technology companies to develop a high performance data analytics platform that can handle petabytes of datasets. If you are a truly talented Big Data engineer with super skills in Scala, Hadoop and Spark, and you are looking for an opportunity to help design and implement an amazing Big Data solution that will be deployed on a massive scale, this is your chance!What You Need for this Position9+ Yeras of Ot experience4+ years of Big Data experienceBachelor's degree in Computer Science or related field (Master's degree a PLUS)strength with Scalastrong knowledge of Hadoop and Sparkexperience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)experience with version control systems, particularly Gitdesire and ability to quickly learn new tools and technologiesunderstanding of the best practices in data quality and quality engineeringknowledge of Unix-based operating systems (bash/ssh/ps/grep etc.) - a PLUSexperience with Github-based development processes - a PLUSexperience with JVM build systems (SBT, Maven, Gradle) - A PLUSPlease share updated resume in MS word format with below client submission information.Full Name:Contact Number:Email ID:Immigration Status:Current Location:Relocation Yes/No:Availability:Best time to reach:Rate :Skype ID:Total EXP :USA EXP :DOB :SSN Last 4Key Skills :Domain Worked :Linkedin Id Must :Job Type: ContractExperience:Spark: 4 years (Preferred)SSH: 4 years (Preferred)Data Mining: 4 years (Preferred)Shell Scripting: 4 years (Preferred)Akka: 4 years (Preferred)","Atlanta, GA",Sr Big Data Engineer,False
1003,"TDB Communications, Inc. is seeking qualified MarkLogic Data Engineers:

Description of specific Duties in a typical workday for this position:
Provides design recommendations based on long-term IT organization strategy.
Develops enterprise level application and custom integration solutions including major enhancements and interfaces, functions and features. Uses a variety of platforms to provide automated systems applications to customers.
Provides expertise regarding the integration of applications across the business.
Determines specifications, then plans, designs, and develops the most complex and business critical software solutions, utilizing appropriate software engineering processes – either individually or in concert with a project team. Will assist in the most difficult support problems.
Develops programming and development standards and procedures as well as programming architectures for code reuse. Has in-depth knowledge of state-of-the art programming languages and object-oriented approach in designing, coding, testing and debugging programs.
Understands and consistently applies the attributes and processes of current application development methodologies.
Researches and maintains knowledge in emerging technologies and possible application to the business.
Viewed both internally and externally as a technical expert and critical technical resource across multiple disciplines. Acts as an internal consultant, advocate, mentor and change agent.


Position Requirements:
At least 8 years of experience in developing cloud based multi user application with expertise in designing, building, testing and implementing IT application.
Must have a strong background in software engineering principles and techniques.
8 years of overall experience in information technology.
2 years implementation experience in MarkLogic.
Experience in translating the business requirement into a Technology solution roadmap.
Ability to consult and advice customers in the NoSQL implementations.
Extremely good in Communication skills.
Experience with Java development, XML and Web Technologies.
Experience in implementing XQuery and MarkLogic API development.
Experience in rolling out large NOSQL implementations.
Excellent design, Development, Implementation, Documentation and problem solving skills.
Experience with integration methodologies and tools.
Experience in Big Data technologies (Hadoop and NoSQL) Experience in DEVOPS functions.
Experience in defining best practice and patterns for ingestion and retrieval of data from MarkLogic.
Familiarity with other NoSQL and Big Data technologies
A Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline is required.




TDB Communications, Inc. is an EEO Affirmative Action Race/Sex/Sexual Orientation/Gender Identity/National Origin/Veteran/Disability Employer
Required Skills

Required Experience","Linthicum, MD",MarkLogic Data Engineer,False
1004,"Summary

Boxy Charm is seeking a highly motivated individual responsible for building the analytics data platform for the company.

The Data Engineer will work closely with data scientists and analysts across different business units to build data products that will enable fast decision-making and action across the company. The position will be part of the Data & Algorithms team and, as such, the ideal candidate will not only possess great communication skills, but will also be very technical; equally at home writing code, solving complex problems and working in a cutting-edge cloud-based platform and technology stack.

Essential Duties and Responsibilities

Build cloud-based data products using SQL, Python, Snowflake, Spark and other technologies
Build data life-cycle and health tools to enable monitoring of key business KPIs
Work with a wide variety of data ranging between social media and email to customer transactions and logistics
Partner with various business stakeholders and implement solutions that improve their business process
Break down complex projects and problems into actionable tasks that be delivered quickly and iteratively and provide value to the business stakeholders
Be a data advocate throughout the company
Education and/or Experience

Bachelor’s degree or higher in Computer Science, Information Technology, Data Analytics or a related field
3+ years of experience programming in multiple languages such as Python, Java, Scala etc.
3+ years of experience working with SQL and relational databases and strong SQL skills are a must
Knowledge of Big Data and NoSQL systems such as Snowflake, Hadoop, Spark, MongoDB, etc.
Experience working in an Agile (SCRUM, XP etc.) development environment
Experience with AWS or other cloud environments is strongly desirable
Experience with streaming data systems is desirable","Pembroke Pines, FL",Data Engineer,False
1005,"$109,000 - $138,000 a year (Indeed Est.) Job Title:
DW Big Data Engineer
FLSA:
Exempt

Department:
Engineering
Band:
Individual Contributor

Location:
Los Angeles
Job Level:
Reports To:
Senior Data Warehouse Architect
Position Type:
Full Time

Travel Required:
JD Creation Date:
Job Summary:

This position is to support increased DW engineering responsibilities especially around SportsBook, focusing on but not limited to big data integration, API data pipelines and scripting based automation.

Essential Functions:
Implements Big Data integration framework and batch/real-time analytical solutions leveraging transformational technologies.
Support design, development of all data pipelines required in the data warehouses to help business solve complex business challenges.
Works on multiple projects as a technical team member and support design and development of data applications, testing, and builds automation tools.
Codes, tests, and documents new or modified data systems to create robust and scalable applications for data pipeline
Implements security and recovery tools and techniques as required.
Ensures all automated processes preserve data by managing the alignment of data availability and integration processes.
Conducts logical and physical database design and designs key and indexing schemes
Monitors performance and fine tune any necessary infrastructure changes

Required Qualifications:

BS in Information Systems, Computer Science, Engineering or related field preferred
3+ years of solid data engineering or software engineer experience with Big Data components/frameworks (Hadoop, Streaming, Yarn, Spark, Hive, HBase, MapReduce, HDFS, Pig, Hive, Sqoop, Flume, Ozie, etc.) in large-scale data infrastructure.
Experience with how algorithms work and have experience building algorithms.
Experience within large relational Data Warehouse environment, including technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
Experience developing data engineering using Python, Java or any other programming or scripting languages.
Experience with RESTful web services, open API development, and SOA concepts
Expertise writing SQL queries

Preferred Qualifications:

Physical demands (ADA):

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this job, the employee is regularly required to walk and talk or hear. The employee is frequently required to use hands to finger, handle, or use calculator or numerical keys on computer keyboard. The employee is frequently required to sit for long periods of time as well as bend, reach, and stoop, or kneel. Moderate physical activity is required, including the ability to lift 15 pounds.

Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus.

Work Environment:

Work is normally performed in a typical interior/office work environment. Must be able to multi-task in a constantly changing environment. Requires the ability to meet pressure deadlines and time constraints.

The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.

TVG Network/Betfair US is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, pregnancy, national origin, ancestry, citizenship, age, legally protected physical or mental disability, protected veteran status, in the U.S. uniformed services, sexual orientation, gender identity or expression, marital status, genetic information or membership in any other legally protected category.



Employee Name________________________________

Employee Signature_____________________________ Date___________________________

I have received a copy of the job description.","Los Angeles, CA",DW Big Data Engineer,False
1006,"Job SummaryWe are looking for a savvy Data Engineer to join our team of Field Engineers.The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing big data systems and building them.The Data Engineer will support our Solution Architects, Sales Executives and Technical Account Managers on Big Data implementations and will ensure optimal big data solution delivery throughout ongoing projects& client engagements.The candidate must be self-directed and comfortable supporting the needs of multiple project teams and clients.The right candidate will be excited by the prospect of optimizing or re-designing our clients’big data architecture/tools/echo system using our industry-leading Automated Data warehouse Engine to support data needs and initiatives.Responsibilities and DutiesDesign, Create and Maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Up to 50% travel may be needed.Required Experience, Skills and QualificationsMust have worked for at least 3 years in a Big Data Environment.Must have at least intermediate/expert skills in various Dataware house, Reporting & ETL technologiesAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. * Experience building and optimizing big data pipelines, architectures, and data sets. * Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:1.Experience working with one or more of the Hadoop distributions (Cloudera, Hortonworks, MapR)2.Experience with big data tools: Hadoop, Spark, Hive, HBase, YARN, Map Reduce, Kafka, Sqoop3.Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.4.Experience with data pipeline and workflow management tools: Airflow5.Experience with Public Cloud Services: AWS, GCP, Azure, IBM Bluemix6.Experience with stream-processing systems: Storm, Spark-Streaming7.Experience with object-oriented/object function scripting languages: Python, Java, C++, ScalaBenefitsYes, there will be benefits.Job Type: Full-timeExperience:Big Data: 4 years (Preferred)Location:Atlanta, GA (Preferred)Work authorization:United States (Preferred)","Seattle, WA",Big Data Engineer(USC & GC),False
1007,"What We're Looking For

Engineering at 2U is fast paced, innovative and full of people passionate about delivering on the promise of higher education via technology. We’re growing rapidly and are seeking collaborative and results-driven individuals to join us. In return, we can offer you a fun learning environment where your skills, experience and creativity will make a material impact on the company’s success and its overall mission. Your tenure at 2U will be a highlight of your career.

We strive for our teams to be cross-functional, self-organizing and autonomous. You’ll be working directly with product managers and business analysts in a highly collaborative manner.

About The Role

We are looking for a collaborative and results-driven data engineer with experience in system management and monitoring within AWS infrastructure, automated testing and continuous deployment. Agile techniques like test driven development (TDD) is a plus.

Above all, you care about delivering quality software in a sustainable and timely manner and about software craftsmanship. You have experience in, and passion for, high-quality, maintainable code that confers low operating costs, high change velocity and is a point of professional pride for you and the team.

Responsibilities Include, But Are Not Limited To

You are a competent data engineer and a technical leader with the following competencies:

Strong database background:
Data Warehouse architecture and data modeling
Monitoring, administration and performance tuning of the database servers
Design and implementation of ETL data flows
Data governance and security
Writing maintainable high-performance code
Refactoring to keep code maintainable
Conducting exploratory and automated testing
Job scheduling and monitoring
Debugging complex problems under time constraints
Lead technical design of applications and participate in larger system design efforts
Providing technical guidance and mentoring more junior team-members
Stay current with the industry trends and best practices
You have experience in:

Collaborating with the stakeholders (product managers, business analysts, and data science teams) to define and refine requirements
Planning and estimating development tasks and short-term projects
Relating your project deliverables to products you are building and strategy behind them
Advocating for end user needs in software you are building
Over time, we expect engineers in this role to grow by learning and practicing the following skills:

Participating in application and system level technical design
Conducting technical interviews
Participating in planning staffing needs on your team
Metrics driven software development
This role reports to Director of Engineering for Data Systems team.

About The Team

The Data Systems team maintains and develops a reliable, well-supported, and frequently updated data lake comprising raw and processed data from 2U's platforms. We provide a curated set of standardized and consolidated data from all of 2U's programs, as well as financial data that can be used by the business to build mission critical reporting services. We also strive to provide generic facilities for product engineering teams to move data between systems. All our hardware infrastructure is in deployed on the Amazon Web Services cloud and we make extensive use of SAAS services like Salesforce, Segment, Streamsets and others.

We are motivated to build a team where members bring sound computer science fundamentals and a diverse set of skills and experiences to the table. We looking for experience and expertise in the following technologies:

T-SQL using Postgres and MS SQL
Python and Shell Scripting
AWS: Aurora, Redshift, Kinesis, CloudWatch
Big Data applications (Spark SQL, Apache Drill, AWS Athena) and Data Visualization tools (e.g. Tableau) are a plus
About 2U Inc. (NASDAQ: TWOU)

2U partners with great colleges and universities to build what we believe is the world’s best digital education. Our platform provides a comprehensive fusion of technology, services and data architecture to transform high-quality and rigorous campus-based universities into the best digital versions of themselves. 2U's No Back Row® approach allows qualified students and working professionals around the world to experience a first-rate university education and successful outcomes. To learn more, visit 2U.com.

2U Diversity and Inclusion Statement

At 2U, we are committed to creating and sustaining a culture that embodies diverse walks of life, ideas, genders, ages, races, cultures, sexual orientations, abilities and other unique qualities of our employees. We strive to offer a workplace where every employee feels empowered by the ways in which we are different, as well as the ways in which we are the same.

Why It’s Great to Work at 2U

2U offers a high-energy work environment that’s both challenging and fun. We work hard, but our offices are casual and social places. We wear jeans to work and fuel brainstorming sessions with snacks and seltzer.

Benefits

2U offers a comprehensive benefits package:

Medical, dental and vision coverage
Life insurance, disability and 401(k)
Unlimited snacks and drinks
Generous paid leave policies
Tuition reimbursement program
Spontaneous dance parties
No Asshole policy
Note: The above statements are intended to describe the general nature and level of work performed by individuals assigned to this position, and are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required. All employees may be required to perform duties outside of their normal responsibilities from time to time, as needed.

2U is an equal opportunity employer that does not discriminate against applicants or employees and ensures equal employment opportunity for all persons regardless of their race, creed, color, religion, sex, sexual orientation, pregnancy, national origin, age, marital status, disability, citizenship, military or veterans’ status, or any other classifications protected by applicable federal, state or local laws. 2U’s equal opportunity policy applies to all terms and conditions of employment, including but not limited to recruiting, hiring, training, promotion, job benefits, pay and dismissal.","New York, NY",Senior Data Warehouse Engineer,False
1008,"JW Player is looking for a Principal Data Engineer to partner with senior leaders, product managers and other stakeholders to enable actionable insights that accelerate the growth of the business through the implementation of a robust data management infrastructure.

This role offers the opportunity to influence data-driven culture through the analysis of large amounts of data and the building of metrics and business cases around key performance. The Principal Data Engineer understands and owns the health of the services and drives necessary changes as needed.

The ideal candidate is a self-starter with excellent analytical abilities as well as a passion for problem-solving and a penchant for tackling the ambiguous. The Principal Data Engineer will architect the data platform from multiple sources and spearhead best practices throughout the evolution of data -- from structured data warehouse methods to big data analytics -- while keeping ahead of the technology curve.

Most importantly, the Principal Data Engineer will help lead the charge in leveraging data to the fullest extent for both customers and the JW Player business as a whole.

Responsibilities:

Drive and implement data management strategy; design and deliver automated solutions whenever applicable
Work closely with business and software engineering leaders to lead catalog data improvements so as to maximize customer improvement impact
Lead business intelligence and data engineers to design and develop data infrastructure strategy for the quality and software development organization
Partner with data scientists and product analysts, enable effective decision making by streamlining data pipelines and make data from from multiple sources available, with highest quality
Perform deep-dives to find the root causes behind variances and identify opportunities for quality control automation.
Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment
Collaborate with software engineering teams to integrate experimental capabilities into large-scale, highly complex JW Player production systems
Ensure results in a manner which is both statistically rigorous and compellingly relevant
Assist in recruiting, mentoring, developing, and training other data engineers and business intelligence and product analysts within the organization

Requirements:

BA/BS in Computer Science, Engineering, Mathematics or related field or experience equivalent
10+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree
5+ years of operations and/or multi-source data engineering experience (e.g. S3 Data Lake)
Strength in writing and tuning SQL, data modeling, ETL development, and data warehousing
Proficiency with scripting languages (Python/R) or other modern program languages
Advanced ability to draw insights from data and clearly communicate them (verbal/written) to the stakeholders and senior management as required
Self-driven, with ability to deliver on ambiguous projects with incomplete or dirty data
An ability and interest in working in a fast-paced and rapidly-changing environment
Experience in working with very large data (petabytes) warehouse environment
Familiarity with AWS and latest Big Data technical stack (e.g. Spark, Storm, Kafka, Flink …)

About JW Player

JW Player pioneered video on the web over a decade ago and continues to innovate as the world's largest network-independent platform for video delivery and intelligence. Current the company serves over 1.5 billion unique users a month and ingests over 1.5 terabytes a day. Media companies including Fox, VICE, Business Insider, and Univision, in addition to hundreds of thousands of creators of all types and sizes, rely on JW Player to deliver and monetize their content across all devices. JW Player's massive global footprint of over 2 billion unique devices creates a powerful data graph of unique consumer insights and generates billions of incremental video views. The company is headquartered in New York, with offices in London and Eindhoven, and was named to Deloitte's Technology Fast 500™ in 2017. For more information, visit http://www.jwplayer.com ( http://www.jwplayer.com ).

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","New York, NY","Principal Engineer, Data Engineering",False
1009,"ContractRequired Experience, Skills and QualificationsETLAWS RedshiftBig data – (Hadoop, Hive, Pig, Hbase, Python, Spark / Scala)EC2S3OraclePL/SQLJob Type: Contract","Woodland Hills, CA",Data Engineer,False
1010,"Req Id: 121909
As a member of the Micron Data Science Team, the data engineer:
Develops and codes software programs, algorithms, and automated processes to consolidate, integrate, and evaluate large datasets from multiple sources.
Assists in the design of user interface and business application prototypes.
Interacts with product and service teams to identify questions and issues for data analysis and experiments.
Interprets actionable insights from large data and metadata sources and communicates the findings to product, service, and business managers for product enhancement.

Requirements:

Must have 3 to 5 years of relevant work experience.
Experience working with structured, semi-structured, and un-structured data sources.
The ability to problem solve and provide complex solutions with limited direction.
Strong experience with design, development, and implementation of complex architecture.
Extensive experience with web front end development: HTML/CSS, JavaScript (JQuery or Angular preferred).
Hands-on experience with various machine learning methods.
Excellent communication and leadership skills.
Demonstrated capability in Python, Java, and at least one other programming language.
Ability to learn quickly and become productive on new technologies, APIs, development languages, and frameworks.
Some working knowledge of Hadoop Environment, including: Hive, Map Reduce, No/SQL, HBase, and Spark.

Education:
Bachelor’s Degree or Master’s Degree in: Computer Science, Science, Engineering, Information Science; or a related field

Desired Skills:

Image analytics with OpenCV or equivalent.
Proficiency in R Language.
Signal analysis methods.
Deep Neural Networks (DNN).
Experience with GUI development.
Complex data visualization techniques.
Flow simulation and analytics methods.
Experience using a version control system.

We recruit, hire, train, promote, discipline and provide other conditions of employment without regard to a person's race, color, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, pregnancy, veteran’s status, or other classifications protected under law. This includes providing reasonable accommodation for team members' disabilities or religious beliefs and practices.

Each manager, supervisor and team member is responsible for carrying out this policy. The EEO Administrator in Human Resources is responsible for administration of this policy. The administrator will monitor compliance and is available to answer any questions on EEO matters.

To request assistance with the application process, please contact Micron’s Human Resources Department at 1-800-336-8918 (or 208-368-4748).

Keywords: Manassas || Virginia (US-VA) || United States (US) || Frontend Manufacturing || Entry || Regular || Engineering || #LI-AD1 ||","Manassas, VA",Data Engineer,False
1011,"Who We Are:
Twitter users generate many terabytes of data every day; Twitter engineers run hundreds of experiments; Twitter Data Engineers build data pipelines and data processes that calculate metrics and scale increasingly sophisticated models of users and content.

The Data Science team at Twitter is at the intersection of all this data and strives to make it actionable to all business units around Twitter. Data Engineers work alongside Data Scientists analyze this data via observational analyses, trend analyses, modeling, and new measurement strategies. We also implement metrics to track the impact of new product experiments and more generally find ways to make very large scale data approachable to guide our decisions.

What You’ll Do:
Twitter has very large and complex datasets. As a Twitter Data Engineer you will build datasets and make them accessible to our partner teams by writing great production code to simplify the complexity. Your work will enable Product Managers and other decision-makers across the company to bring together insights and inform our product and strategy. In every decision that you influence, you will see the product improve and be more valuable to Twitter users.

We are trying to improve Twitter. To improve something, we need to be able to measure it. As a Data Engineer you will enable better measurements and ensure measurement accuracy so that we know where we are doing well and where we want to improve.

As such, you will:
Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.
Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.
Collaborate with other engineers and Data Scientists to discover the best solutions.
Support your colleagues by reviewing code and designs.
Diagnose and solve issues in our existing data pipelines and envision and build their successors.

Who You Are:
You want to be part of a community of the most talented, forward-thinking Data Scientists and Engineers in the industry. You are a strong Scala or Java developer. You demonstrate clear and concise communication and data-driven decision-making.

You are passionate about learning or growing your expertise in some or all of the following:

Data Pipelines
Data Warehousing
Statistics
Metrics Development

Requirements:
B.S. and/or M.S. in Computer Science or a related technical field, or equivalent experience
2+ years of experience in either data infrastructure or backend systems
Strong understanding of SQL
Broad knowledge of the data infrastructure ecosystem
Experience with Hadoop or other MapReduce-based architectures
Experience working with large data volumes
Good understanding of one or more of the following: Scala, C++, or Java

Experience with any of the following is a plus:
Scalding
Full Stack Development
Presto or Hive
Spark

Applicants will be considered for this role at all levels from SWE I to Senior SWE depending on qualifications.

﻿We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran status, genetic information, marital status or any other legally protected status.

San Francisco applicants: Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.","San Francisco, CA 94103 (South Of Market area)","Data Engineer, Data Science",False
1012,"We’re here to ensure the advertising industry and the people in it are healthy and engaging positively and effectively with those around them. We’re here, ultimately, to improve the lives of people working in the media industry. And we take our responsibility seriously.

ABOUT THE PRODUCT AND ENGINEERING TEAM

At Centro, we’ve always been about making people’s lives better—from our employees to our clients. Today, we’re building a unified platform to execute every digital media advertising transaction, thus providing a new level of automation and intelligence in ad tech. This is an enormous task that will disrupt an industry and improve the lives of those who work within it.

To do this, we’re aggressively growing our team of engineers, product developers and designers. We are imaginative, passionate, determined and relentless in our efforts. Of course, we are fascinated by the complexity of engineering problems at this scale—it’s what brings bright minds together. But what keeps us coming back (and why we love coming to work everyday) is the chance to improve how work gets done: freeing up people’s time so that they can dream bigger and make life better.

Come build something amazing with us.

ABOUT THE ROLE

We are seeking forward-thinking Data Engineer to join Data team. You will be involved in the design and implementation of the Data Platform. We're looking for a Data Engineer who has passion for data processing and the challenges presented by different types of data at high velocity.

CORE RESPONSIBILITIES

Implement scalable, fault tolerant and accurate ETL pipelines.
Gather and process raw data at scale from diversified sources.
Build enterprise business analytics and reporting applications.
Develop platform services to operate the data applications at scale.

QUALIFICATIONS

Proficiency with relational databases and SQL queries (Postgres, MySQL, Oracle or similar).
Knowledge of Hadoop ecosystem components (Spark, Hive, Impala, Kafka, Oozie) is a plus.
Understanding of factors affecting performance of ETL processes and SQL queries, ability to work on performance tuning.
Experience in data modeling for OLTP and OLAP applications
Experience implementing data pipelines.
Experience coding in Python or Scala.
Experience with tools such as Git, Jenkins, Jira, IntelliJ.
Experience with Pentaho is a plus.
Experience with other big data technologies such as Cassandra, MongoDB, Elastic Search is a plus.
Ability to work independently as well as part of a team.
Strong aptitude toward problem solving and working with different data sets.
Having a passion and knowledge of AdTech industry is a plus.
Have a Bachelor’s degree in computer science or software engineering.

Centro is an Equal Opportunity Employer and does not discriminate against any employee or applicant on the basis of race, gender, age, disability or any other basis protected under the law.","Chicago, IL",DATA ENGINEER,False
1013,"Adaptive Management is a SaaS company building a unified ecosystem for leveraging data. Our cloud-based platform, DataMonster™, enables companies to interface with thousands of data providers to quickly find answers from data. Our team includes former financial investors, NSA cryptology and applied mathematics experts, and Silicon Valley software veterans. Adaptive Management is headquartered in Manhattan, New York City.

This is an exciting time to get in on the ground-floor of a well-funded and fast-growing startup that is solving complex problems and setting the standard for how data is found, visualized, and ultimately used. We have revenue, a healthy sales pipeline, and a clear pathway to the next funding round. If you are looking to join a highly talented and passionate team of software engineers, data scientists and business leaders then reach out to us at careers@adaptivemgmt.com or apply now.

THE ROLE

We have relationships with hundreds of vendors in the alternative data space and this number will only continue to grow. We are seeking a Senior Data Engineer to leverage industry best-practices to spearhead the design, creation and management of our data warehouse and all related extraction, transformation and loading of partner data.

The Senior Data Engineer will play a critical role in leading the review, interpretation, and on-boarding of new data sets. They will be adept at designing and implementing data management frameworks/ETL processes and will possess a deep technical knowledge and skillset.

We are a well-funded startup offering full benefits and competitive compensation. The position will be full-time. We are a startup but prioritize work/life balance.

RESPONSIBILITIES


Understand the big-picture view of the company's data landscape and longer-term goals in order to develop and implement innovative Big Data architectures on AWS
Govern ETL Best Practices & Standards
Learn key vendor data sets and become proficient with extracting data from them
Build ETL mechanisms to onboard vendor data and publish it via existing channel
Develop database and API representation strategies that reflect the needs of our end users
Design quality checks to ensure the highest level of data quality and accuracy; finding root causes of potential issues and outliers when needed
Develop target data models including data policies for sensitive data assets, security policies, back-up, and recovery specifications
Help implement data strategy, data governance processes, and maintenance of business definitions, policies and glossaries for data assets
Support quality processes that ensure that the data is timely and correct as well as serve as the steward of data sets across vendors

REQUIREMENTS


Bachelor's degree or higher in Computer Science or a related discipline
10+ years of Systems Engineering Experience
5+ years of design and implementation of custom data warehouse solutions
Experience working in cloud environments (AWS, GCP, Azure)
Hands-on development mentality with a willingness to solve complex problems
Strong communication skills; ability to simplify complex designs for developers to code
Experience working on an Agile scrum team a plus
Experience in a startup environment a plus

","New York, NY",Senior Data Engineer,False
1014,"Requisition ID: 28593

NextEra Energy Resources is one of the largest wholesale generators of electric power and renewable energy from the wind and sun in North America.

Position Specific Description

We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
1. Create and maintain optimal data pipeline architecture,
2. Assemble large, complex data sets that meet functional / non-functional business requirements.
3. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
4. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using MQTT, SQL, and AWS ‘big data’ technologies (KFKA,CASSANDRA,HIVEMQ).
5. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. (DataDog Dashboards, PowerBI)
6. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
7. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
8. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
9. Work with data and analytics experts to strive for greater functionality in our data systems.

Qualifications for Data Engineer
1. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
2. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. (HiveMQ, Kafka, Cassandra, MQTT, S3)
3. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. (Data quality dimension analysis based on statistical measurements)
4. Strong analytic skills related to working with unstructured datasets.
5. Build processes supporting data transformation, data structures, metadata, dependency and workload management. (Preferably in Java, C++, or Linux Tools)
6. A successful history of manipulating, processing and extracting value from large disconnected datasets.
7. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
8. Strong project management and organizational skills.
9. Experience supporting and working with cross-functional teams in a dynamic environment.
10. We are looking for a candidate with 5+ years of experience in a Data Engineer/Data Science role, who has attained a Graduate degree in Computer Science, Computer/Electrical Engineering, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.Experience with AWS cloud services: EC2, EMR, RDS, RedshiftExperience with stream-processing systems: Storm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Job Overview

This position develops and integrates new or existing applications into the technical infrastructure and existing business processes. Employees in this role provide technical or functional guidance to project or work teams, as needed within a specific discipline.



Job Duties & Responsibilities

Analyzes, designs, develops, tests, debugs, implements, maintains and/or enhances existing or new systems, that are reliable and efficientDevelops customized programming solutions, and maintains existing system functionalityDevises or modifies procedures to solve complex problems, and prepares detailed specifications from which programs will be writtenApplies appropriate development methodologies, system development lifecycles, tools and technologyCollaborates on an on-going basis with the Business Systems AnalystParticipates in the ticket management and resolution processes, including receiving, resolution monitoring and customer satisfactionLeads projects when neededProvides direction, training and guidance for less experienced staffBuilds strong working understanding of the solution being deliveredEnsures user satisfaction by providing preventative maintenance, troubleshooting, and timely resolution of more complex problemsFollows and participates in the defined Software Development Lifecycle (SDLC), Sarbanes Oxley (Sox) compliance, and General Computing ControlsDefines metrics and monitors Service Level Agreements (SLAs) for systems being developedMeets daily, weekly and monthly reporting requirementsPerforms other job-related duties as assigned


Required Qualifications

High School Grad / GEDBachelor's or Equivalent ExperienceExperience:7+ years


Preferred Qualifications

Bachelor's - SciencesWeb TechnologiesInformation Coding StandardsProgrammingStrategic Planning


Employee Group: Exempt
Employee Type: Full Time
Job Category: Information Technology
Organization: NextEra Energy Resources, LLC
Location: Juno Beach, Florida
Other Work Locations: Florida
Relocation Provided: No

NextEra Energy is an Equal Opportunity Employer. Qualified applicants are considered for employment without regard to race, color, age, national origin, religion, marital status, sex, sexual orientation, gender identity, gender expression, genetics, disability, protected veteran status or any other basis prohibited by law. We are committed to a diverse and inclusive workplace.

If you require special support or accommodation while seeking employment with NextEra Energy, please send an e-mail to AskHR@NEE.com, providing your name, telephone number and the best time for us to reach you. Alternatively, you may call 1-844-694-4748 (Option 1, Press 6) between 8 a.m. and 5 p.m. EST Monday-Friday. Please do not use this line to inquire about your application status.

NextEra Energy will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.

NextEra Energy does not accept any unsolicited resumes or referrals from any third-party recruiting firms or agencies. Please see our policy for more information.","Juno Beach, FL 33408",Data Engineer,False
1015,"About Us
We are a full stack data science company and a wholly owned subsidiary of The Kroger Company. We own 10 Petabytes of data, and collect 35+ Terabytes of new data each week sourced from 62 Million households. As a member of our engineering team you will use various cutting edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at Kroger. We use agile development methodology, starting with Big Room Planning bringing everyone into the planning process to build scalable enterprise applications.

Data Developer – What you'll do
As a data developer, we develop strategies and solutions to ingest, store and distribute our big data. Our developers use Scala, Hadoop, Spark, Hive, JSON, and SQL in 10 week long scrum teams to developer the products, tools and features.

Responsibilities
Take ownership of features and drive them to completion through all phases of the entire 84.51° SDLC. This includes external facing and internal applications as well as process improvement activities such as:

Lead design of Hadoop and SQL based solutions
Perform development of Hadoop and SQL based solutions
Perform unit and integration testing
Collaborate with senior resources to ensure consistent development practices
Provide mentoring to junior resources
Participate in retrospective reviews
Participate in the estimation process for new work and releases
Bring new perspectives to problems
Be driven to improve yourself and the way things are done

Education

Requirements

Bachelor's degree typically in Computer Science, Management Information Systems, Mathematics, Business Analytics or another technically strong program.
5+ years proven ability of professional data development experience
Strong understanding of Agile Principles (Scrum)
Proficient with relational data modeling
5+ years proven ability of developing with SQL (Oracle, SQLServer)
5+ years proven ability of developing with Hadoop/HDFS
Full understanding of ETL concepts
Full understanding of data warehousing concepts
Exposure to VCS (Git, SVN)
3+ year developing experience with either Java, Scala or Python
Experience with Spark
Preferred Skills – Experience in the following
Exposure to NoSQL (Mongo, Cassandra)
SOA
Junit
CI/CD

","Cincinnati, OH",Big Data Engineer,False
1016,"The GBG Loqate Engineering Team

The GBG Loqate Engineering Team develops and maintains Loqate’s product suite consisting of address verification, geocoding and power search solutions. Team members contribute to development and provide assistance to customers.

The Role (Vision)

Design and Development and testing of data manipulation systems and providing solutions to customer issues that relate to data. Organise day-to-day operations and mentor team members.

What you’ll do (Objectives)

Develop data manipulation systems (or ETL) using a mixture of technologies including SQL, shell scripting, JavaDebug, fix, and test modules and provide solutions to customer issuesTest and integrate code changes to our data manipulation systemsWrite user documentationImprove on existing data processing while designing future systemsMentor team members and assist or embed with other teams on data processing
How you’ll deliver it… (Strategies)
These will be determined by you, in collaboration with your manager, and you’ll update them regularly to keep your contribution relevant as we evolve. You’ll do this via your Personal VOS.

To help you be successful, we’re looking for

Bachelor’s degree in computer science or an equivalent technical subjectFluent in SQL, Java and Shell ScriptingWorking knowledge of MySQLDemonstrable software development and/or data engineering experienceExperience with ETL software and/or Data Manipulation systems (Nice to have)Big Data (Nice to have)Willingness to work hard, learn and possess a positive attitudeMeticulous attention to detailIndependent thinkerProblem solverMotivated to learn and empower others, and when meeting obstacles, not afraid to test new concepts or ask for guidance



Tweet","San Mateo, CA",Senior Data Engineer,False
1017,"ContractClient is seeking to hire a Big Data Engineer to help with our current demands in big data technologies. We are looking for strong, experienced candidates with 3-4 years of big data ecosystem experience who have done similar work elsewhere (Financial Services experience is a plus)Responsibilities: Lead definition and socialization of end to end big data enablement solution architectureDesign, develop, and implement a real time data, integration, and cognitive fabric using Big Data technologies such as Hadoop, Spark, HBaseBe accountable for thesolution design and development of Hadoop/Spark environments integration with analytic platforms,Enterprise Information Management (EIM), and Data Warehouse (DW) platformsFormulate approaches and gather data to solve business problems, develop conclusions and present solutions through formal deliverablesA successful history of manipulating, processing and extracting value from large disconnected datasetsRequired Technical and Professional Expertise:Strong experience in Hadoop platform and data architectureStrong experiencein use of open source tools such as: Hadoop, Hive, HBase, Spark, Storm,Kafkaand file storage formats (Parquet, Avro, ORC)Candidate must have prior experience working with one or more of the following: Cloudera Hadoop Distribution, Hortonworks Data Platform, MapR, Microsoft HDInsightsFluency in several programming languages such as Java,Scala or Pythonwith the ability to pick up new languages and technologies quicklyExperience working on Hadoop cluster setting, Map Reduce, Job Optimizations, Queue management, Job Scheduling/Orchestrationusing OozieWorking experience on development, build & deployment tools such as Eclipse, Maven, Git, Jenkin, GradleExperience working with Tableau and Big Data Lake integration is a plusJob Type: Contract","Phoenix, AZ",Big Data Developer,False
1018,"Overview
Since 2011, Branch Creative Network has been bringing the digital world to life with strategic, innovative, and entrepreneurial solutions. Clients from a wide variety of industries rely on our expertise in social media strategy, web design, analytics, research, and mobile implementation.

Branch Creative Network, a division of Jackson Dawson, is seeking a talented Database Administrator/Data Engineer to join our Analytics and Business Intelligence team in our fast paced, opportunity rich, business environment. The role of the Database Administrator is to perform a range of functions with respect to information collection, analysis, and presentation, including responding to requests for reports and dashboard design, and pro-actively providing relevant data and information in support of program initiatives within BCN.
Responsibilities
Through the ETL process, prepare data for Data Scientist and other team members
Design, build/maintain data structures
Confidently speak to front end output
As an SQL practitioner coach and mentor others on the team
Work collaboratively with Developers
Actively participate in data quality assurance
Research and recommend data solutions
Recommend effective marketing channels, (on-line and offline) and develop test plans
Collaborate with social media team to ensure proper campaign metrics are being captured
Maintain project time-line according to projection
Performance monitoring and tuning, capacity planning
Drive business by ensuring key programs goals are met
Travel as required to meet client/partners
Perform all other duties as assigned by supervisor
Qualifications
Bachelor’s degree in related field of business, marketing, statistics, computer science, mathematics
Must have at least 7 years of experience in a DBA role
Minimum 5 years’ experience building/managing data warehouses, data marts, and data lakes
Expert SQL practitioner with minimum 7 years of experience
Proficient in: SAS, SQL (Write SQL and use SQL Server), Adobe Analytics, Microsoft Outlook, Word, Power-Point and Access
Minimum 5 years of experience managing marketing data, such as demographic and psychographic data
Must possess data management experience
Visualization experience with MS Power BI and/or Tableau, considered a plus
Highly adept with ETL, (Extract, Transform, Load)
Database security experience
Knowledge in Python or R programming, Data Mart, Data Lake, Data Governance
Expert knowledge in Excel required
Understanding of market research and analysis techniques
Automotive marketing experience preferred
Clearly demonstrated business-related problem solving and produce a viable solution
Excellent written and oral communication skills
Ability to manage client relationships, including sensitive data and confidential materials
Ability to manage deadlines and multiple priorities, including competing demands
Must be a self-starter, work well with minimal supervision, and accept responsibility
Team participation and effectiveness
Must have ability to collaborate across functions to form strong working relationships","Dearborn, MI",Database Administrator/Data Engineer,False
1019,"Specific qualifications for the Big Data Engineer position include:Demonstrated growth over 3+ years’ experience working as a data engineer.High level understanding of big frameworks.Development experience with Big Data/NoSQL platforms, such as Hbase, MongoDB or Apache Cassandra.Expert knowledge of SQL and NoSQL tools.Knowledge of MapReduce and MapReduce generating tools like Pig or Hive.Experience with message buses or real-time event processing platforms is a plus.Java development experience.Scripting language experience (Perl, Python etc.).Understanding of NoSQL data modeling.Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance.Codes and performs unit and integration testing on following Hadoop ecosystem tools like HDFS, Hive , Yarn, Flume, Oozie, Kafka, Storm, Scala, Spark and Spark Streaming including Nosql database knowledge to ensure proper and efficient execution and adherence to business and technical requirementsJob Type: Full-timeExperience:Big Data: 3 years (Required)","St. Louis, MO",Big Data Engineer,False
1020,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
At Facebook, we have many opportunities to work with data each and every day. How would you like to work on data and build some of the tools that are critical to moving & transforming this data into valuable & insightful information? If so, this is the right job for you. Join a team of Data Engineers and Scientists in the Camera Team! The mission of the Camera Team is to empower people through Augmented Reality (AR), and in doing so build a world class Camera. The Data Engineers support everything from helping product teams build new pipelines to track new and existing AR experiences, measure the performance of the underlying AR tech (e.g. face and hand trackers, plane detectors, etc.).

The Camera team is also a platform team, meaning that the technology we build supports not just Facebook’s main app, but also Instagram, Messenger, Bonfire, etc. Data Engineers on this team will be uniquely positioned to analyze data, influence teams and create impact across the Facebook family of apps!
RESPONSIBILITIES

Manage data warehouse plans for a product or a group of products.

Interface with engineers, product managers and product analysts to understand data needs.

Build data expertise and own data quality for allocated areas of ownership.

Design, build and launch new data models in production.

Design, build and launch new data extraction, transformation and loading processes in production.

Support existing processes running in production.

Define and manage SLA for all data sets in allocated areas of ownership.

Work with data infrastructure to triage infrastructure issues and drive to resolution.

Analyze data to identify deliverables, gaps and inconsistencies.
MINIMUM QUALIFICATIONS

2+ years experience in the data warehouse space.

2+ years experience in custom ETL design, implementation and maintenance.

2+ years experience working with either a MapReduce or an MPP system.

2+ years experience with object-oriented programming languages.

2+ years experience with schema design and dimensional data modeling.

2+ years experience in writing SQL statements.
PREFERRED QUALIFICATIONS

BS/BA in Technical Field, Computer Science, Mathematics or equivalent experience.

1+ years experience in Python or Java.","Menlo Park, CA","Data Engineer, AR/VR Camera",False
1021,"$115,000 - $125,000 a yearAzure DATA ENGINEER
$115,000 - $125,000 + 10% annual bonus
Des Plaines, IL
Permanent, Full time. THIS IS NOT A CONTRACT OPPORTUNITY
US Citizen or Permanent Residency Status required.

Over the next 12-18 months, this team will focus on Modernization of their Data and BI tools (Re-platform/rewrite existing Legacy ETL Solutions from Decision Stream and custom Java to the latest SSIS in Azure environment and schedule them using Orchestrator). You will also develop BI//ETL/DM solutions in Azure IaaS environment using CONA and HANA sources. You will be part of a team responsible for designing, developing, and supporting data management solutions for this $10 Billion Enterprise.

REQUIREMENTS:
Experience in Azure Data Factory and/or Azure Data Lake.
You should already have 5 or more years of relevant Business Intelligence and/or Data Warehousing and/or Data Integration work experience, which should include 3-5 years in the Microsoft BI stack - SSIS, SSRS, etc.
Experience in Data modelling (logical/physical, relational and document/object) and Data Integration solution design experience (understanding of Dimensional Modeling techniques)
Experience / ability to quickly / easily parse data from sources other than MS SQL Server either on a real-time basis or nightly batch.

BIG PLUS IF YOU HAVE
Experience leveraging Cloud Services (IaaS, PaaS)
Experience using open source data integration tool such as Talend, Azure Data Factory
ITIL certification

Would you be interested in exploring this opportunity ?","Des Plaines, IL",Data Lake Engineer,False
1022,"At Abbott, we're committed to helping people live their best possible life through the power of health. For more than 125 years, we've brought new products and technologies to the world - in nutrition, diagnostics, medical devices and branded generic pharmaceuticals - that create more possibilities for more people at all stages of life. Today, 99,000 of us are working to help people live not just longer, but better, in the more than 150 countries we serve.
Main Purpose of Role
This will be a foundational member role in a small team of talented and highly motive data engineers, and big data architects to create big data based advanced analytical platforms and products

Main Responsibilities Create, deploy and optimize large scale data Use extensive data engineering expertise to design and build solutions/ products for analyzing large data sets and identify patterns and relationships Manage data sources, organize data and create data assets using identified open source or proprietary tools Work closely with SMEs, functional experts in Commercial, R&D, finance, etc. for building data pipeline from structure and unstructured data sources Work on newest tools and technologies powering these analytics wave - Scala, Scalding, Spark, Hadoop Deploy advanced machine learning techniques and algorithms and work in a truly Big Data way
Travel occasionally per needs of the assigned project.

Qualifications

Education Level
Bachelors Degree (± 16 years) Bachelor’s degree in any of the following – Math, Physics, Computer Science, Statistics, Economics, Quantitative Sciences

 Experience

Minimum 16 yearsStrong problem-solving skillsExperience in any from AWS, Python, R, Spark, hive, HBase, Hadoop, Kafka, YARN etc. will be a plusAttention to detail and organization/ documentation skillsAbility to prioritize and triage deadline-driven tasks in a high-pressure environmentBasic knowledge of distributed computing, parallel processing and large scale data managementExperience manipulating and analyzing complex, high-volume data from varying sourcesAbility to communicate complex quantitative analysis in a clear, precise, actionable manner","Chicago, IL",Data Engineer,False
1023,"ContractJob SummaryAs developer within the Big Data team you will contribute to high quality technology solutions that address business needs by developing data applications for the customer business lines. You will contribute to the development and ongoing maintenance of a number of strategic data initiatives and data and analytic applications. The ability to communicate effectively is required as you will work closely with other groups, including development and testing efforts of your assigned application components to ensure the successful delivery of the projectResponsibilities and DutiesHands on development role focused on creating big data and analytics solutions - Coding of mission critical components - Analyze business and functional requirements and contribute to overall solution - Participate in design reviews, provide input to the design recommendations - Participate in project planning sessions with project managers, business analysts and team members - Hands-on expertise with Graph databases.Required Experience, Skills and QualificationsExperience with middle-tier/backend systems development in Java/Linux –Minimum 3-5 years working experience with Hadoop in an enterprise setting –Experience with Java enterprise development, Python and Scalar –Hands-on expertise with Sql & NoSql data platforms –Hands-on expertise with Big data technologies (HBase, Hive, Sqoop) –Experience with Pub/Sub messaging (JMS, Kafka, etc.), stream processing (Storm, Spark Streaming, etc.) –Understanding and application of security best practices as they relate to big data technologies –Experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency –Experience profiling, debugging, and performance tuning complex distributed systems –Experience with UNIX shell scripts and commands –Experience with data modeling –Ability to clearly document solution designs –Agile/Scrum methodology experience –Experience with ETL/ELT tools –Experience with BI solutions (Tableau, Microstrategy, D3 etc) Complexity Works on complex issues where analysis of situations and data requires an in-depth evaluation of variable factors.Exercises judgment in selecting methods, techniques and evaluation criteria for obtaining results.Supervision Acts independently to determine methods and procedures on new assignments, and may provide work direction to others.Works under minimal supervision.Minimum Education/Training/Certification Bachelor's degree in an information technology area of study.Master's degree preferred specializing in Computer Science, Information Management, Data Science or equivalent combination of education and experience.Job Type: ContractLocation:San Francisco, CA (Required)","San Francisco, CA",Big Data Engineer,False
1024,"Machine Learning Data Scientist

Who We Are

Murmuration is an organization dedicated to sustainable policy change in the U.S., and we believe that the path to real change in America’s education system is through political activism and participation. Together with our partners, we develop shared infrastructure and coordinated support for organizations on the ground to massively improve outcomes for all children. We do this by working directly with local schools and organizations to amplify the voices of parents and families as key stakeholders in setting education policy.

As part of our work, Murmuration has developed a technology product called “m{insights” that seeks to aggregate data from a combination of sources including member data from partner organizations, publicly available data, consumer data, voter file data, and more. The compiled data are then used to build a variety of analytic models and tools that enable Murmuration’s partner organizations to activate, expand, and mobilize their supporter bases effectively for sustained political change.

About the Position

We are looking for a creative and endlessly inquisitive Data Scientist, with a machine learning emphasis, to join our team. You will be working hand-in-hand with our Senior Data Engineer and Senior Data Scientist to develop highly optimized predictive models from a large volume of regularly updated data. In addition, you will be a key contributor to our Python analytics code base to build and automate custom analytics to help analyze our partners’ work in the political and advocacy spheres. You will be working in a small but dedicated, detail-oriented team that is committed to our mission. The Murmuration work environment is friendly, driven, and non-corporate but professional.

The Machine Learning Data Scientist will:
Create predictive analytics using the latest machine learning algorithms on relatively “big” data (hundreds of features on > 100 million records)
Provide analytic insights for partners throughout the lifecycle of their campaign(s); build, review, and recommend custom models; provide real-time visualizations and analysis of campaign work; and produce post-campaign analytics to help our partners constantly learn from their efforts
Work with Senior Data Engineer to help build and optimize our predictive analytics pipeline to perform well on large datasets stored in Amazon Web Services (AWS)
Help develop and maintain Python analytics code base to build and automate custom analytic products
Analyze the performance of our production machine-learning models
Work closely with the data engineering team to analyze and augment our data
Candidate Profile

Murmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates. The Data and Analytics teams are highly collaborative, friendly and hard-working, and we are looking for a Machine Learning Data Scientist who embodies those values.
The ideal candidate will have:

PhD in political science, sociology, mathematics, statistics or other related field or 4-5 years of experience building production level machine learning models on large datasets
Excellent general data science skills (data cleaning, munging, data visualization)
Excellent Python skills (with additional experience in R desired)
Experience with Amazon Web Services, in particular applying analytics in that setting
Experience identifying appropriate statistical techniques and applying them to a variety of real-world datasets
Strong problem-solving skills as well as the ability to manage several tasks/projects concurrently and prioritize work effectively
Experience with databases and knowledge of SQL preferred
Interest in politics and/or educational policy
Location

This position will be based in New York, NY, and requires occasional travel.

Compensation

The Machine Learning Data Scientist position is a full-time, salaried position with a comprehensive benefits package. Compensation for this position is commensurate with experience.

An Equal-Opportunity Employer with a Commitment to Diversity

Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","New York, NY 10005 (Financial District area)",Machine Learning Data Scientist,False
1025,"Data Engineer – Wilmington, DE or Gaithersburg, MD

At AstraZeneca we turn ideas into life changing medicines. Working here means being entrepreneurial, thinking big and working together to make the impossible a reality. We’re focused on the potential of science to address the unmet needs of patients around the world. We commit to those areas where we think we can really change the course of medicine and bring big new ideas to life.

We are looking for a savvy Data Engineer to join our team of analytics experts in either Wilmington, DE or Gaithersburg, MD. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Essential requirements:
Degree or equivalent experience
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
8+ years of Application Services administration and system engineering experience
5+ years of 24*7 operational production support experience supporting multiple time zones/geographical regions
Strong communication skills
Developing on an AWS platform (S3, Aurora, Redshift, Kinesis, Elastic Beanstalk, Docker)
Developing with MySQL or PostgreSQL
SAP data model experience (esp. FICO, SD)
Data modelling (particularly in ERWIN)
Database design
Agile development (SAFe, SCRUM)
Experience with 3/5NF and Star Schemas

Desirable requirements:
Masters degree
Demonstrated leadership skills interacting with senior leaders
Should have demonstrable skills in handling projects of mid-large size
Should have worked in a team environment and possess good written and oral communication skills
Good organizational and interpersonal skills
Aptitude and motivation learn new technologies and make appropriate recommendations for consideration
Ability to prioritize tasks and work in a dynamic environment support with minimal supervision in very large user community that is geographically dispersed
Demonstrated skills in communicating directly with user community

Next Steps – Apply today!

To be considered for this exciting opportunity, please complete the full application on our website at your earliest convenience – it is the only way that our Recruiter and Hiring Manager can know that you feel well qualified for this opportunity. If you know someone who would be a great fit, please share this posting with them.

AstraZeneca is an equal opportunity employer. AstraZeneca will consider all qualified applicants for employment without discrimination on grounds of disability, sex or sexual orientation, pregnancy or maternity leave status, race or national or ethnic origin, age, religion or belief, gender identity or re-assignment, marital or civil partnership status, protected veteran status (if applicable) or any other characteristic protected by law.","Wilmington, DE 19850",Data Engineer,False
